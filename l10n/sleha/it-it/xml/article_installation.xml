<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="article_installation.xml" version="5.0" xml:lang="it" xml:id="article-installation" xmlns:its="http://www.w3.org/2005/11/its">
 <title><citetitle>Riferimento rapido per l'installazione e la configurazione</citetitle></title>
 <info>
  <productnumber>15 SP5</productnumber>
  <productname>SUSE Linux Enterprise High Availability Extension</productname>
  <date><?dbtimestamp format="d B Y"?>
</date>
  <xi:include href="common_copyright_gfdl.xml"/>
  <abstract>
   <para>
     Questo documento guida l&apos;utente attraverso la procedura di configurazione di un modello base di cluster a due nodi mediante gli script di bootstrap forniti dallo shell CRM. È prevista la configurazione di un indirizzo IP virtuale come risorsa cluster e l&apos;uso di SBD nello storage condiviso come meccanismo di isolamento dei nodi.
   </para>
  </abstract>
  <dm:docmanager>
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <meta name="title" its:translate="yes">Riferimento rapido per l'installazione e la configurazione</meta>
  <meta name="series" its:translate="no">Products &amp; Solutions</meta>
  <meta name="description" its:translate="yes">How to set up a basic two-node cluster, using the bootstrap scripts provided by the crm shell</meta>
  <meta name="social-descr" its:translate="yes">Set up a basic two-node cluster</meta>
  <meta name="task" its:translate="no">
    <phrase>Installation</phrase>
    <phrase>Administration</phrase>
    <phrase>Clustering</phrase>
  </meta>
  <revhistory xml:id="rh-article-installation">
    <revision>
     <date>2023-06-20</date>
      <revdescription>
        <para>
          Updated for the initial release of SUSE Linux Enterprise High Availability 15 SP5.
        </para>
      </revdescription>
    </revision>
   </revhistory>
 </info>

  <sect1 xml:id="sec-ha-inst-quick-usage-scenario">
   <title>Scenario di utilizzo</title>
   <para>
    Le procedure descritte in questo documento prevedono una configurazione minima di un cluster a due nodi con le seguenti proprietà:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Due nodi: <systemitem class="server">alice</systemitem> (IP: <systemitem class="ipaddress">192.168.1.1</systemitem>) e <systemitem class="server">bob</systemitem> (IP: <systemitem class="ipaddress">192.168.1.2</systemitem>), connessi l&apos;uno all&apos;altro tramite rete.
     </para>
    </listitem>
    <listitem>
     <para>
      Un indirizzo IP virtuale mobile (<systemitem class="ipaddress">192.168.1.10</systemitem>) che consente ai client di connettersi al servizio indipendentemente dal nodo sul quale è in esecuzione. Questo indirizzo IP viene utilizzato per connettersi allo strumento di gestione grafica Hawk2.
     </para>
    </listitem>
    <listitem>
     <para>Un dispositivo di storage condiviso, utilizzato come meccanismo di isolamento SBD. Per evitare questi scenari split-brain.
     </para>
    </listitem>
    <listitem>
     <para>
      Failover di risorse da un nodo all&apos;altro in caso di mancato funzionamento dell&apos;host attivo (configurazione <emphasis>attiva/passiva</emphasis>).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    È possibile utilizzare il cluster a due nodi a scopo di prova o come configurazione minima del cluster con la possibilità di estenderla in un secondo momento. Prima di utilizzare il cluster in un ambiente di produzione, vedere il <xref linkend="book-administration"/> (Guida all&apos;amministrazione) per modificarlo secondo i propri requisiti.
   </para>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-req">
   <title>Requisiti di sistema</title>
   <para>
    In questa sezione vengono descritti i requisiti di sistema fondamentali per lo scenario descritto nella <xref linkend="sec-ha-inst-quick-usage-scenario" xrefstyle="select:label"/>. Per regolare il cluster per l&apos;uso in un ambiente di produzione, fare riferimento all&apos;elenco completo nel <xref linkend="cha-ha-requirements"/>.
   </para>

  <sect2 xml:id="vl-ha-inst-quick-req-hw">
   <title>Requisiti hardware</title>
   <variablelist>
    <varlistentry>
     <term>Server</term>
     <listitem>
      <para>
       Due server con software secondo quanto specificato nella <xref linkend="il-ha-inst-quick-req-sw"/>.
      </para>
      <para>
      I server possono essere bare metal o macchine virtuali. Non sono necessari gli stessi requisiti hardware (memoria, spazio su disco e così via), ma devono avere la stessa architettura. I cluster multipiattaforma non sono supportati.
     </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Canali di comunicazione</term>
     <listitem>  <para>
       Almeno due moduli di comunicazione TCP/IP per nodo cluster. Le apparecchiature di rete devono supportare i mezzi di comunicazione che si desidera utilizzare per la comunicazione cluster: multicast o unicast. I moduli di comunicazione devono supportare una velocità dati di 100 Mbit/s o superiore. Per la configurazione di un cluster supportato, sono necessari almeno due percorsi di comunicazione ridondanti. La configurazione può essere eseguita tramite:</para>
       <itemizedlist>
        <listitem>
         <para>
          Associazione dei dispositivi di rete (preferita).
         </para>
        </listitem>
        <listitem>
         <para>
          Un secondo canale di comunicazione in Corosync.
         </para>
        </listitem>
       </itemizedlist> </listitem>
    </varlistentry>
    <varlistentry>
     <term>Isolamento dei nodi/STONITH</term>
     <listitem>
      <para>
       Un dispositivo di isolamento dei nodi (STONITH) per evitare scenari split-brain. Ciò può essere un dispositivo fisico (un interruttore di alimentazione) o un meccanismo come SBD (STONITH by disk) abbinato a un watchdog. È possibile utilizzare SBD con lo storage condiviso o in modalità senza disco. Questo documento descrive l&apos;uso di SBD con lo storage condiviso. Devono essere soddisfatti i seguenti requisiti:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Un dispositivo di storage condiviso. Per informazioni sulla configurazione dello storage condiviso, vedere <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html"><citetitle>Storage Administration Guide</citetitle> for SUSE Linux Enterprise Server</link> (Guida all&apos;amministrazione dello storage di SUSE Linux Enterprise Server). Se ai fini dei test è necessario solo lo storage condiviso di base, vedere <xref linkend="ha-iscsi-for-sbd"/>.
        </para>
       </listitem>
       <listitem>
        <para>Il percorso al dispositivo di storage condiviso deve essere persistente e coerente attraverso tutti i nodi nel cluster. Utilizzare nomi di dispositivi stabili come <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
        </para>
       </listitem>
       <listitem>
        <para> Il dispositivo SBD <emphasis>non deve</emphasis> utilizzare RAID basati su host, LVM2, né risiedere su un&apos;istanza DRBD*.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Per ulteriori informazioni su STONITH, vedere <xref linkend="cha-ha-fencing"/>. Per ulteriori informazioni su SBD, vedere <xref linkend="cha-ha-storage-protect"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
 </sect2>
  <sect2 xml:id="il-ha-inst-quick-req-sw">
   <title>Requisiti software</title>
   <para>
   Tutti i nodi che fanno parte del cluster richiedono almeno i seguenti moduli ed estensioni:
  </para>

<itemizedlist>
   <listitem>
    <para>Basesystem Module 15 SP5</para>
   </listitem>
   <listitem>
    <para>Server Applications Module 15 SP5</para>
   </listitem>
   <listitem>
    <para>SUSE Linux Enterprise High Availability Extension 15 SP5</para>
   </listitem>
  </itemizedlist>

  </sect2>
  <sect2 xml:id="vl-ha-inst-quick-req-other">
   <title>Altri requisiti e raccomandazioni</title>
   <variablelist>
    <varlistentry>
     <term>Sincronizzazione dell&apos;orario</term>  <listitem>
   <para>
     I nodi del cluster devono sincronizzarsi a un server NTP al di fuori del cluster. A partire da SUSE Linux Enterprise High Availability Extension 15, chrony è l&apos;implementazione di default di NTP. Per ulteriori informazioni, vedere <link xlink:href="https://documentation.suse.com/sles-15/html/SLES-all/cha-ntp.html"><citetitle>Administration Guide</citetitle> for SUSE Linux Enterprise Server 15 SP5</link>.
    </para>
    <para>
     Se i nodi non sono sincronizzati, il cluster potrebbe non funzionare correttamente. Inoltre, i file di registro e i report dei cluster sono particolarmente difficili da analizzare in assenza di sincronizzazione. Se si utilizzano gli script di bootstrap, l&apos;utente verrà avvisato nel caso in cui l&apos;NTP non fosse ancora stato configurato.
    </para>
  </listitem> </varlistentry>
    <varlistentry>
     <term>Nome host e indirizzo IP</term>
     <listitem>
      <itemizedlist>
       <listitem>
        <para> Utilizzare indirizzi IP statici. </para>
       </listitem>
       <listitem>
        <para>
         È supportato solo l&apos;indirizzo IP primario.
        </para>
       </listitem>
       <listitem>  <para>
     Elencare tutti i nodi del cluster nel file <filename>/etc/hosts</filename> con il relativo nome host completo e breve. È fondamentale che i membri del cluster possano cercarsi per nome. Se i nomi non sono disponibili, non sarà possibile garantire la comunicazione interna al cluster.
   </para> </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SSH</term>
     <listitem>  <para>
    Tutti nodi del cluster devono essere in grado di accedere agli altri tramite SSH. Strumenti quali <command>crm report</command> (per la risoluzione dei problemi) e <guimenu>History Explorer</guimenu> di Hawk2 richiedono l&apos;accesso SSH senza password tra i nodi, altrimenti possono solo raccogliere dati dal nodo corrente.
  </para> <para> Se per configurare il cluster si utilizzano gli script di bootstrap, le chiavi SSH verranno create e copiate automaticamente. </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-bootstrap">
  <title>Panoramica degli script di bootstrap</title>
  <para>
   I comandi seguenti eseguono script di bootstrap che richiedono tempi e interventi manuali ridotti al minimo.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Con <command>crm cluster init</command>, è possibile definire i parametri fondamentali necessari per la comunicazione del cluster. In questo modo all&apos;utente rimane un cluster a un nodo in esecuzione.
    </para>
   </listitem>
   <listitem>
    <para>
     Con <command>crm cluster join</command>, è possibile aggiungere più nodi al proprio cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Con <command>crm cluster remove</command>, è possibile rimuovere nodi dal proprio cluster.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Tutti gli script di bootstrap effettuano l&apos;accesso a <filename>/var/log/crmsh/crmsh.log</filename>. Consultare questo file per eventuali dettagli sul processo di bootstrap. Tutte le opzioni impostate durante il processo di bootstrap possono essere modificate in un secondo momento con il modulo cluster YaST. Per informazioni, vedere <xref linkend="cha-ha-ycluster"/>.
  </para>
  <para>
   Lo script di bootstrap <command>crm cluster init</command> controlla e configura i seguenti componenti:
  </para>

  <variablelist>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      Verificare se NTP è stato configurato per essere avviato al momento del riavvio. In caso contrario, viene visualizzato un messaggio.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>Crea chiavi SSH per il login senza password tra nodi del cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Csync2</term>
    <listitem>
     <para>
      Configura Csync2 per replicare file di configurazione attraverso tutti i nodi in un cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Corosync</term>
    <listitem>
     <para>Configura il sistema di comunicazione del cluster.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD/watchdog</term>
    <listitem>
     <para>Verifica che sia presente un watchdog e chiede all&apos;utente se configurare SBD come meccanismo di isolamento dei nodi.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>IP virtuale mobile</term>
    <listitem>
     <para>Chiede all&apos;utente se configurare un indirizzo IP virtuale per l&apos;amministrazione del cluster con Hawk2.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Firewall</term>
    <listitem>
     <para>Apre nel firewall le porte necessarie per la comunicazione del cluster.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Nome del cluster</term>
    <listitem>
     <para>Definisce un nome per il cluster, per impostazione predefinita <systemitem>hacluster</systemitem>. Facoltativo e utile per i cluster Geo. Di solito, il nome del cluster riflette l&apos;ubicazione e semplifica la distinzione di un sito all&apos;interno di un cluster Geo.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>QDevice/QNetd</term>
    <listitem>
     <para>
      Chiede se configurare QDevice/QNetd per partecipare alle decisioni sul quorum. Si consiglia di utilizzare QDevice e QNetd per i cluster con un numero pari di nodi, specialmente per i cluster a due nodi.
     </para>
     <para>
      Questa configurazione non viene trattata qui, ma è possibile impostarla successivamente, come descritto in <xref linkend="cha-ha-qdevice"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-installation">
    <title>Installazione dei pacchetti High Availability</title>
    <para>
      I pacchetti per configurare e gestire un cluster sono inclusi nel modello di installazione di <literal>High Availability</literal>. Questo modello è disponibile solo dopo l&apos;avvenuta installazione di SUSE Linux Enterprise High Availability Extension.
    </para>
    <para>
     È possibile effettuare la registrazione in SUSE Customer Center e installare High Availability Extension durante o dopo l&apos;installazione di SUSE Linux Enterprise Server. Per ulteriori informazioni, vedere <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-register-sle.html"><citetitle>Deployment Guide</citetitle></link> (Guida alla distribuzione) per SUSE Linux Enterprise Server.
    </para>
    <procedure xml:id="pro-ha-inst-quick-pattern">
      <title>Installazione del modello High Availability</title>
      <step>
       <para>
        Installare il modello di High Availability dalla riga di comando:</para>
<screen><prompt role="root"># </prompt><command>zypper install -t pattern ha_sles</command></screen>
      </step>
      <step>
       <para>
          Installare il pattern Alta disponibilità su <emphasis>tutte</emphasis> le macchine che faranno parte del cluster.
       </para>
       <note>
        <title>Installazione dei pacchetti software su tutti i nodi</title>
        <para>
         Per un&apos;installazione automatizzata di SUSE Linux Enterprise Server 15 SP5 e High Availability Extension, utilizzare AutoYaST per clonare i nodi esistenti. Per ulteriori informazioni, vedere il <xref linkend="sec-ha-installation-autoyast"/>.
        </para>
       </note>
      </step>
    </procedure>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-sbd">
  <title>Utilizzo di SBD per l&apos;isolamento dei nodi</title>
   <para>
    Prima di poter configurare SBD con lo script di bootstrap, è necessario abilitare un watchdog su ogni nodo. SUSE Linux Enterprise Server è dotato di diversi moduli kernel che forniscono driver watchdog specifici per l&apos;hardware. High Availability Extension utilizza il daemon SBD come componente software che <quote>alimenta</quote> il watchdog.
   </para>
   <para>
    La procedura descritta di seguito utilizza il watchdog <systemitem>softdog</systemitem>.
   </para>

   
   <important>
    <title>limitazioni Softdog</title>
    <para>
     Il driver softdog suppone che almeno una CPU sia ancora in esecuzione. Se tutte le CPU sono bloccate, il codice nel driver softdog che dovrebbe riavviare il sistema non viene mai eseguito. Al contrario, i watchdog hardware continuano a funzionare anche se tutte le CPU sono bloccate.
    </para>
    <para>Prima di utilizzare il cluster in un ambiente di produzione, si consiglia di sostituire il modulo <systemitem>softdog</systemitem> con il rispettivo modulo hardware più adatto all&apos;hardware in uso.
    </para>
    <para>Tuttavia, se nessun watchdog corrisponde all&apos;hardware in uso, è possibile utilizzare <systemitem class="resource">softdog</systemitem> come modulo watchdog del kernel.</para>
   </important>

   <procedure xml:id="pro-ha-inst-quick-sbd-setup">
    <title>Abilitazione della sorveglianza Softdog per SBD</title>
    <step>
     <para>
      Abilitare su ciascun nodo il watchdog softdog:
     </para>
     
     <screen><prompt role="root"># </prompt><command>echo softdog &gt; /etc/modules-load.d/watchdog.conf</command>
<prompt role="root"># </prompt><command>systemctl restart systemd-modules-load</command></screen>
    </step>
    <step>
     <para>Controllare che il modulo softdog sia caricato correttamente:
     </para>
     <screen><prompt role="root"># </prompt><command>lsmod | grep dog</command>
softdog           16384  1</screen>
    </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-1st-node">
  <title>Configurazione del primo nodo</title>
   <para>
   Configurare il primo nodo con lo script <command>crm cluster init</command>. Il tempo e l&apos;intervento manuale necessari sono ridotti al minimo.
  </para>

  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-init">
   <title>Impostazione del primo nodo (<systemitem class="server">alice</systemitem>) con <command>crm cluster init</command></title>
   <step>
    <para>
     Eseguire il login nel primo del nodo cluster come <systemitem class="username">root</systemitem> o come utente con privilegi <command>sudo</command>.
    </para>
    <important>
     <title>chiave di accesso SSH utente <command>sudo</command></title>
     <para>
      Il cluster utilizza l&apos;accesso SSH senza password per la comunicazione tra i nodi. Lo script <command>crm cluster init</command> verifica la presenza di chiavi SSH e le genera qualora non esistessero già.
     </para>
     <para>
      Se si intende configurare il primo nodo come utente con privilegi <command>sudo</command>, è necessario assicurarsi che le chiavi SSH esistano già (o verranno generate) localmente sul nodo e non in un sistema remoto.
     </para>
    </important>
   </step>
   <step>
    <para>
     Avviare lo script di bootstrap:
    </para>
    <screen><prompt role="root"># </prompt><command>crm cluster init --name <replaceable>CLUSTERNAME</replaceable></command></screen>
    <para>Sostituire il segnaposto <replaceable>CLUSTERNAME</replaceable> con un nome significativo, come la posizione geografica del cluster (ad esempio, <literal>amsterdam</literal>). Questo risulta particolarmente utile per creare in seguito un cluster Geo, in quanto semplifica l&apos;identificazione di un sito.
    </para>
    <para>
     Se per la comunicazione cluster occorre utilizzare un multicast invece di unicast (predefinito), utilizzare l&apos;opzione <option>--multicast</option> (o <option>-U</option>).
    </para>
    <para>
     Lo script verifica la configurazione NTP e un servizio di watchdog hardware. Se richiesto, genera le chiavi SSH pubbliche e private utilizzate per l&apos;accesso SSH e la sincronizzazione Csync2 e avvia i rispettivi servizi.
    </para>
   </step>
   <step>
    <para>
     Configurare il livello di comunicazione del cluster (Corosync):
    </para>
    <substeps>
     <step>
      <para>
       Immettere un indirizzo di rete al quale collegarlo. Per impostazione predefinita, lo script propone l&apos;indirizzo di rete di <systemitem>eth0</systemitem>. In alternativa, immettere un altro indirizzo di rete, ad esempio l&apos;indirizzo di <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Accettare la porta suggerita (<literal>5405</literal>) o immetterne un&apos;altra.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
    Configurare SBD come meccanismo di isolamento dei nodi:</para>
    <substeps>
     <step>
      <para>Confermare con <literal>y</literal> che si desidera utilizzare SBD.</para>
     </step>
     <step>
      <para>Immettere un percorso persistente nella partizione del proprio dispositivo di blocco che si desidera utilizzare per SBD. Il percorso deve essere coerente attraverso tutti i nodi nel cluster.</para>
       <para>Lo script create una piccola partizione sul dispositivo da utilizzare per SBD.</para>
     </step>
    </substeps>
   </step>
   <step xml:id="st-crm-cluster-init-ip">
    <para>Configurare un indirizzo IP virtuale per l&apos;amministrazione del cluster con Hawk2:</para>
    <substeps>
     <step>
      <para>Confermare con <literal>y</literal> che si desidera configurare un indirizzo IP virtuale.</para></step>
     <step>
      <para>Immettere un indirizzo IP inutilizzato da utilizzare come IP di amministrazione per Hawk2: <literal>192.168.1.10</literal>
      </para>
      <para>Anziché accedere a un singolo nodo del cluster con Hawk2, è possibile connettersi all&apos;indirizzo IP virtuale.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Scegliere se configurare QDevice e QNetd. Per la configurazione minima descritta in questo documento, per il momento declinare con <literal>n</literal>. È possibile configurare QDevice e QNetd in un momento successivo, come descritto nel <xref linkend="cha-ha-qdevice"/>.
    </para>
   </step>
  </procedure>
  <para>
   Infine, lo script avvierà i servizi del cluster per portare il cluster online e abilitare Hawk2. L&apos;URL da utilizzare per Hawk2 viene visualizzato sullo schermo.
  </para>

  <para>
   A questo punto è in esecuzione un cluster a un nodo. Per visualizzarne lo stato, attenersi alla seguente procedura:
  </para>
  <procedure xml:id="pro-ha-inst-quick-hawk2-login">
   <title>Accesso all&apos;interfaccia Web di Hawk2</title>
   <step>
    <para> Su un computer, avviare un browser Web e assicurarsi che JavaScript e i cookie siano abilitati. </para>
   </step>
   <step>
    <para>Per l&apos;URL, immettere l&apos;indirizzo IP virtuale configurato con lo script di bootstrap:</para>
    <screen>https://192.168.1.10:7630/</screen>
    <note>
     <title>Avviso certificato</title>
     <para> Se quando si tenta di accedere all&apos;URL per la prima volta, viene visualizzato un avviso certificato significa che è in uso un certificato firmato da se stessi. I certificati firmati da se stessi non sono considerati attendibili per default. </para>
     <para> Chiedere i dettagli del certificato all&apos;operatore del cluster per verificare il certificato. </para>
     <para> Per continuare ignorando l&apos;avviso, è possibile aggiungere un&apos;eccezione nel browser. </para>
    </note>
   </step>
   <step>
    <para> Nella schermata di login di Hawk2, immettere il <guimenu>nome utente</guimenu> e la <guimenu>password</guimenu> dell&apos;utente che è stato creato durante lo script di bootstrap (utente <systemitem class="username">hacluster</systemitem>, password <literal>linux</literal>).</para>
    <important>
     <title>Password di protezione</title>
     <para>Sostituire la password di default con una di protezione non appena possibile:
     </para>
     <screen><prompt role="root"># </prompt><command>passwd hacluster</command></screen>
    </important>
   </step>
   <step>
    <para>
     Fare clic su <guimenu>Login</guimenu>. Per impostazione predefinita, l&apos;interfaccia Web di Hawk2 mostra la schermata Stato:
    </para>
    <figure xml:id="fig-ha-inst-quick-one-node-status">
     <title>Stato del cluster a un nodo in Hawk2</title>
     <mediaobject>
      <imageobject>
       <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-2nd-node">
  <title>Aggiunta del secondo nodo</title>
  <para>
    Aggiungere un secondo nodo al cluster con lo script di bootstrap <command>crm cluster join</command>. Lo script deve accedere solo a un nodo del cluster esistente e completa automaticamente la configurazione di base nella macchina corrente.
  </para>
  <para>
   Per ulteriori informazioni, vedere la pagina man <command>crm cluster join</command>.
  </para>
  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-join">
   <title>Aggiunta del secondo nodo (<systemitem class="server">bob</systemitem>) con <command>crm cluster join</command></title>
   <step>
    <para>
     Eseguire il login nel secondo nodo come <systemitem class="username">root</systemitem> o come utente con privilegi <command>sudo</command>.
    </para>
   </step>
   <step>
    <para>
     Avviare lo script di bootstrap:
    </para>
    <para>
     Se si configura il primo nodo come <systemitem class="username">root</systemitem>, è possibile eseguire questo comando senza alcun parametro aggiuntivo:
    </para>
<screen><prompt role="root"># </prompt><command>crm cluster join</command></screen>
    <para>
     Se si configura il primo nodo come utente <command>sudo</command>, è necessario specificare tale utente con l&apos;opzione <option>-c</option>:
    </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster join -c <replaceable>USER</replaceable>@alice</command></screen>
    <para>
     Se NTP non è configurato per essere avviato al momento del riavvio, viene visualizzato un messaggio. Lo script verifica anche la presenza di un dispositivo watchdog hardware. Se non è presente alcun dispositivo l&apos;utente viene avvisato.
    </para>
   </step>
   <step>
    <para>
     Se non è già stato specificato <systemitem class="server">alice</systemitem> con <option>-c</option>, verrà richiesto l&apos;indirizzo IP del primo nodo.
    </para>
   </step>
   <step>
    <para>
     Se non è già stato configurato un accesso SSH senza password tra entrambe le macchine, verrà richiesta la password del primo nodo.
    </para>
    <para>
     Dopo il login a un nodo specificato, lo script copia la configurazione Corosync, configura SSH e Csync2, porta online la macchina corrente come nuovo nodo del cluster e avvia il servizio necessario per Hawk2. 
    </para>
   </step>
  </procedure>
  <para>
   Controllare lo stato del cluster in Hawk2. In <menuchoice>
    <guimenu>Stato</guimenu>
    <guimenu>Nodi</guimenu>
   </menuchoice> si dovrebbero vedere due nodi con uno stato di colore verde:
  </para>

  <figure xml:id="fig-ha-inst-quick-two-node-cluster">
   <title>Stato del cluster a due nodi</title>
   <mediaobject>
    <imageobject>
     <imagedata width="80%" fileref="installquick-two-nodecluster-status.png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-test">
   <title>Test del cluster</title>
   <para>
    I seguenti test possono essere utili per identificare eventuali problemi con la configurazione del cluster. Tuttavia, un test realistico prevede casi d&apos;uso e scenari specifici. Prima di utilizzare il cluster in un ambiente di produzione, testarlo in modo approfondito secondo i propri casi di utilizzo.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Il comando <command>sbd -d <replaceable>DEVICE_NAME</replaceable> list</command> elenca tutti i nodi visibili a SBD. Per la configurazione descritta in questo documento, l&apos;output deve mostrare sia <systemitem class="server">alice</systemitem> che <systemitem class="server">bob</systemitem>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec-ha-inst-quick-test-resource-failover"/> è un semplice test per verificare se il cluster sposta l&apos;indirizzo IP virtuale nell&apos;altro nodo se il nodo sul quale è attualmente in esecuzione la risorsa è impostato su <literal>standby</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec-ha-inst-quick-test-with-cluster-script"/> simula gli errori del cluster e segnala i risultati.
     </para>
    </listitem>
   </itemizedlist>
   <sect2 xml:id="sec-ha-inst-quick-test-resource-failover">
    <title>Test del failover delle risorse</title>
    <para>
     La procedura seguente è un test rapido per la verifica del failover delle risorse:
    </para>
   <procedure xml:id="pro-ha-inst-quick-test">
    <title>Test del failover delle risorse</title>
    <step>
     <para>
      Aprire un terminale ed effettuare il ping del proprio indirizzo IP virtuale, <systemitem>192.168.1.10</systemitem>:
     </para>
     <screen><prompt role="root"># </prompt><command>ping 192.168.1.10</command></screen>
    </step>
    <step>
     <para>
      Accedere a Hawk2.
     </para>
    </step>
    <step>
     <para>
      In <menuchoice><guimenu>Stato</guimenu><guimenu>Risorse</guimenu></menuchoice>, verificare su quale nodo è in esecuzione l&apos;indirizzo IP virtuale (risorsa <systemitem>admin_addr</systemitem>). Questa procedura presuppone che la risorsa sia in esecuzione su <systemitem class="server">alice</systemitem>.
      </para>
    </step>
    <step>
     <para>
      Impostare <systemitem class="server">alice</systemitem> in modalità <guimenu>Standby</guimenu>:
     </para>
     <figure xml:id="fig-ha-inst-quick-standby">
      <title>Nodo <systemitem class="server">alice</systemitem> in modalità standby</title>
      <mediaobject>
       <imageobject>
        <imagedata width="60%" fileref="installquick-standby-node.png"/>
       </imageobject>
      </mediaobject>
     </figure>
     
    </step>
    <step>
     <para>
      Fare clic su <menuchoice>
       <guimenu>Stato</guimenu>
       <guimenu>Risorse</guimenu>
      </menuchoice>. La risorsa <systemitem>admin_addr</systemitem> è stata migrata in <systemitem class="server">bob</systemitem>.
     </para>
    </step>
   </procedure>
   <para>
    Durante la migrazione, si dovrebbe vedere un flusso ininterrotto di ping verso l&apos;indirizzo IP virtuale indicante che la configurazione del cluster e l&apos;indirizzo IP mobile funzionano correttamente. Annullare il comando <command>ping</command> con <keycombo>
       <keycap function="control"/><keycap> C</keycap>
      </keycombo>.
   </para>
   </sect2>

   <sect2 xml:id="sec-ha-inst-quick-test-with-cluster-script">
    <title>Esecuzione del test con il comando <command>crm cluster crash_test</command></title>
    <para>
     Il comando <command>crm cluster crash_test</command> attiva gli errori del cluster per trovare i problemi. Prima di utilizzare il cluster nell&apos;ambiente di produzione, si consiglia di utilizzare questo comando per assicurarsi che tutto funzioni come previsto.
    </para>
    <para>
     Il comando supporta i controlli seguenti:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>--split-brain-iptables</option></term>
      <listitem>
       <para>
        Simula uno scenario split-brain tramite il blocco della porta Corosync. Consente di controllare se è possibile applicare una recinzione a un nodo come previsto.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--kill-sbd</option>/<option>--kill-corosync</option>/ <option>--kill-pacemakerd</option></term>
      <listitem>
       <para>
        Termina i daemon per SBD, Corosync e Pacemaker. Dopo l&apos;esecuzione di uno di questi test, è possibile trovare un report nella directory <filename>/var/lib/crmsh/crash_test/</filename>. Il report include una descrizione del caso del test e una spiegazione dei possibili risultati.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--fence-node <replaceable>NODE</replaceable></option></term>
      <listitem>
       <para>
        Consente di applicare una recinzione a un nodo specifico trasmesso dalla riga di comando.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Per ulteriori informazioni, vedere la <command>crm cluster crash_test --help</command>.
    </para>
    <example xml:id="ex-test-with-cluster-script">
     <title>Test del cluster: isolamento dei nodi</title>
<screen><prompt role="root"># </prompt><command>crm_mon -1</command>
Stack: corosync
Current DC: alice (version ...) - partition with quorum
Last updated: Fri Mar 03 14:40:21 2020
Last change: Fri Mar 03 14:35:07 2020 by root via cibadmin on alice

2 nodes configured
1 resource configured

Online: [ alice bob ]
Active resources:

 stonith-sbd    (stonith:external/sbd): Started alice

<prompt role="root"># </prompt><command>crm cluster crash_test</command><command> --fence-node bob</command>

==============================================
Testcase:          Fence node bob
Fence action:      reboot
Fence timeout:     60

!!! WARNING WARNING WARNING !!!
THIS CASE MAY LEAD TO NODE BE FENCED.
TYPE Yes TO CONTINUE, OTHER INPUTS WILL CANCEL THIS CASE [Yes/No](No): <command>Yes</command>
INFO: Trying to fence node "bob"
INFO: Waiting 60s for node "bob" reboot...
INFO: Node "bob" will be fenced by "alice"!
INFO: Node "bob" was successfully fenced by "alice"</screen>
    </example>
    <para>
      Per osservare lo stato di modifica di <systemitem class="server">bob</systemitem> durante il test, accedere a Hawk2 e selezionare <menuchoice><guimenu>Stato</guimenu>
      <guimenu>Nodi</guimenu></menuchoice>.
     </para>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-next">
   <title>Fasi successive</title>
   <para>
    Gli script di bootstrap forniscono un modo rapido per configurare un cluster High Availability di base utilizzabile per i test. Tuttavia, per espandere questo cluster in uno High Availability funzionante utilizzabile negli ambienti di produzione, sono richiesti altri passaggi.
   </para>
   <variablelist xml:id="vl-ha-inst-quick-next-rec">
    <title>Passaggi consigliati per completare la configurazione del cluster High Availability</title>
    <varlistentry>
     <term>Aggiunta di altri nodi</term>
     <listitem>
      <para>
       Aggiungere altri nodi al cluster utilizzando uno dei seguenti metodi:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Per i nodi individuali, utilizzare lo script <command>crm cluster join</command> come descritto in <xref linkend="sec-ha-inst-quick-setup-2nd-node"/>.
        </para>
       </listitem>
       <listitem>
        <para>
         Per l&apos;installazione di massa di più nodi, utilizzare AutoYaST come descritto nel <xref linkend="sec-ha-installation-autoyast"/>.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Di norma, un cluster può contenere fino a 32 nodi. Con il servizio <systemitem class="daemon">pacemaker_remote</systemitem>, è possibile estendere i cluster High Availability in modo da includere altri nodi oltre questo limite. Per ulteriori dettagli, vedere la <xref linkend="article-pacemaker-remote"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Configurazione di QDevice</term>
     <listitem>
      <para>
       Se il cluster ha un numero pari di nodi, configurare QDevice e QNetd per partecipare alle decisioni sul quorum. QDevice fornisce un numero configurabile di voti, consentendo a un cluster di sostenere più errori dei nodi rispetto a quelli consentiti dalle regole del quorum standard. Per informazioni, vedere <xref linkend="cha-ha-qdevice"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Abilitazione di un watchdog hardware</term>
     <listitem>
      <para>
       Prima di utilizzare il cluster in un ambiente di produzione, sostituire il modulo <literal>softdog</literal> con il rispettivo modulo hardware più adatto all&apos;hardware in uso. Per informazioni, vedere <xref linkend="sec-ha-storage-protect-watchdog"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-moreinfo">
   <title>Per ulteriori informazioni</title>
    <para>
     Maggiore documentazione per questo prodotto è disponibile su <link xlink:href="https://documentation.suse.com/sle-ha/"/>. Per altre attività di configurazione e amministrazione, vedere la <link xlink:href="https://documentation.suse.com/sle-ha/html/SLE-HA-all/book-sleha-guide.html"><citetitle>Administration Guide</citetitle></link> (Guida al Geo Clustering) completa.
    </para>
   </sect1>
 <xi:include href="ha_iscsi_for_sbd.xml"/>
 <xi:include href="common_legal.xml"/>
</article>
