<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="article_installation.xml" version="5.0" xml:lang="fr" xml:id="article-installation">
 <title>Démarrage rapide de l&apos;installation et de la configuration</title>
 <info>
  <productnumber><phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase></productnumber>
  <productname><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase></productname>
  <date><?dbtimestamp ?>
</date>
  <xi:include href="ha_authors.xml"/>
  <abstract>
   <para>
     Le présent document explique comment configurer une grappe très basique de deux noeuds à l&apos;aide des scripts d&apos;amorçage fournis par le shell crm. Ce processus inclut la configuration d&apos;une adresse IP virtuelle en tant que ressource de grappe et l&apos;utilisation d&apos;un détecteur de vues de grappe divergentes (SBD pour Split Brain Detector) sur un stockage partagé en tant que mécanisme d&apos;isolation de noeud.
   </para>
  </abstract>
  <dm:docmanager>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

  <sect1 xml:id="sec-ha-inst-quick-usage-scenario">
   <title>Scénario d&apos;utilisation</title>
   <para>
    Les procédures décrites dans le présent document permettent de configurer une grappe de base à deux noeuds présentant les caractéristiques suivantes :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Deux noeuds : <systemitem class="server">alice</systemitem> (adresse IP : <systemitem class="ipaddress">192.168.1.1</systemitem>) et <systemitem class="server">bob</systemitem> (adresse IP : <systemitem class="ipaddress">192.168.1.2</systemitem>), connectés entre eux via le réseau.
     </para>
    </listitem>
    <listitem>
     <para>
      Une adresse IP virtuelle flottante (<systemitem class="ipaddress">192.168.2.1</systemitem>), qui permet aux clients de se connecter au service quel que soit le noeud physique sur lequel il s&apos;exécute.
     </para>
    </listitem>
    <listitem>
     <para>Un périphérique de stockage partagé, utilisé comme mécanisme d&apos;isolation SBD afin d&apos;éviter les scénarios de vues de grappe divergentes.
     </para>
    </listitem>
    <listitem>
     <para>
      Un système de basculement des ressources d&apos;un noeud vers l&apos;autre en cas de défaillance de l&apos;hôte actif (configuration <emphasis>active/passive</emphasis>).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Après l&apos;installation de la grappe avec les scripts d&apos;amorçage, nous surveillons la grappe avec le Hawk2 graphique. Il s&apos;agit de l&apos;un des outils de gestion de grappe inclus dans <phrase role="roductnamereg"><phrase os="sles">SUSE® Linux Enterprise High Availability Extension</phrase></phrase>. Pour tester le bon fonctionnement du basculement des ressources, nous mettrons simplement l&apos;un des noeuds en mode veille et vérifierons si l&apos;adresse IP virtuelle est migrée vers l&apos;autre noeud.
   </para>
   <para>
    La grappe à deux noeuds peut être utilisée à des fins de test ou comme une configuration de grappe minimale que vous pouvez étendre par la suite. Avant d&apos;utiliser la grappe dans un environnement de production, modifiez-la en fonction de vos besoins.
   </para>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-req">
   <title>Exigences système</title>
   <para>
    Cette section décrit la configuration système minimale requise pour le scénario présenté dans la <xref linkend="sec-ha-inst-quick-usage-scenario" xrefstyle="select:label"/>. Pour que la grappe puisse être utilisée dans un environnement de production, reportez-vous à la liste complète dans le <xref linkend="cha-ha-requirements"/>.
   </para>

  <sect2 xml:id="vl-ha-inst-quick-req-hw">
   <title>Configuration matérielle requise</title>
   <variablelist>
    <varlistentry>
     <term>Serveurs</term>
     <listitem>
      <para>
       Deux serveurs exécutant les logiciels spécifiés à la <xref linkend="il-ha-inst-quick-req-sw"/>.
      </para> <para>
      Ces serveurs peuvent être des machines virtuelles ou sans système d&apos;exploitation. Ils ne nécessitent pas une configuration matérielle identique (mémoire, espace disque, etc.), mais doivent avoir la même architecture. Les grappes multi plate-forme ne sont pas prises en charge.
     </para>
      
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Canaux de communication</term>
     <listitem>  <para>
       Au moins deux médias de communication TCP/IP par noeud de grappe. L&apos;équipement réseau doit prendre en charge le moyen de communication que vous souhaitez utiliser pour les communications au sein de la grappe, à savoir la multidiffusion ou la monodiffusion. Les médias de communication doivent prendre en charge un débit de données de 100 Mbit/s ou plus. La prise en charge d&apos;une grappe requiert au moins deux chemins de communication redondants. Cette configuration peut être obtenue via :</para>
       <itemizedlist>
        <listitem>
         <para>
          Une liaison de périphérique réseau (de préférence).
         </para>
        </listitem>
        <listitem>
         <para>
          Un deuxième canal de communication dans Corosync.
         </para>
        </listitem>
       </itemizedlist> </listitem>
    </varlistentry>
    <varlistentry>
     <term>Isolation de noeud/STONITH</term>
     <listitem>  <para>
      Pour éviter un scénario de <quote>vues de grappe divergentes</quote>, les grappes ont besoin d&apos;un mécanisme d&apos;isolation de noeud. Dans un scénario de vues de grappe divergentes, les noeuds de grappe sont divisés en deux groupes ou plus qui ne se connaissent pas (en raison d&apos;une défaillance matérielle ou logicielle, ou d&apos;une interruption de la connexion réseau). Un mécanisme d&apos;isolation isole le noeud en question (généralement en le réinitialisant ou en le mettant hors tension). Ce processus est également appelé STONITH (<quote>Shoot the other node in the head</quote>). Un mécanisme d&apos;isolation de noeud peut être un dispositif physique (un bouton marche/arrêt) ou un mécanisme de type SBD (STONITH par disque) associé à un système de surveillance (watchdog). L&apos;utilisation d&apos;un SBD nécessite un stockage partagé. 
     </para> </listitem>
    </varlistentry>
   </variablelist>
 </sect2>
  <sect2 xml:id="il-ha-inst-quick-req-sw">
   <title>Configuration logicielle requise</title>
   <para>
   Tous les noeuds destinés à faire partie de la grappe doivent disposer au minimum des modules et extensions suivants :
  </para>

<itemizedlist>
   <listitem>
    <para>Basesystem Module <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase></para>
   </listitem>
   <listitem>
    <para>Server Applications Module <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase></para>
   </listitem>
   <listitem>
    <para><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase></para>
   </listitem>
  </itemizedlist>

  </sect2>
  <sect2 xml:id="vl-ha-inst-quick-req-other">
   <title>Autres conditions requises et recommandations</title>
   <variablelist>
    <varlistentry>
     <term>Synchronisation horaire</term>  <listitem>
   <para>
     Les noeuds de la grappe doivent se synchroniser avec un serveur NTP situé en dehors de la grappe. Depuis <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> 15, chrony est l&apos;implémentation par défaut du protocole NTP. Pour plus d&apos;informations, reportez-vous au manuel <link xlink:href="https://documentation.suse.com/sles-15/html/SLES-all/cha-ntp.html"><citetitle>Administration Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase></link> (Guide d&apos;administration de SUSE Linux Enterprise Server 15 SP4).
    </para>
    <para>
     Si les noeuds ne sont pas synchronisés, il se peut que la grappe ne fonctionne pas correctement. En outre, les fichiers journaux et les rapports de grappe sont très difficiles à analyser en l&apos;absence de synchronisation. Si vous utilisez les scripts d&apos;amorçage, le système vous avertira si un service NTP n&apos;est pas encore configuré.
    </para>
  </listitem> </varlistentry>
    <varlistentry>
     <term>Nom d&apos;hôte et adresse IP</term>
     <listitem>
      <itemizedlist>
       <listitem>
        <para> Utilisez des adresses IP statiques. </para>
       </listitem>
       <listitem>  <para>
     Répertoriez tous les noeuds de la grappe, avec leur nom d&apos;hôte complet et leur nom d&apos;hôte abrégé, dans le fichier <filename>/etc/hosts</filename>. Il est essentiel que les membres de la grappe puissent se trouver sur la base de leurs noms. Si les noms ne sont pas disponibles, la communication interne au sein de la grappe ne fonctionnera pas.
   </para> </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SSH</term>
     <listitem>  <para>
    Tous les noeuds de la grappe doivent pouvoir accéder les uns aux autres via le protocole SSH. Des outils tels que les <command>rapports CRM</command> (pour le dépannage) et l&apos;<guimenu>explorateur d&apos;historique</guimenu> de Hawk2 requièrent un accès SSH sans mot de passe entre les noeuds, faute de quoi ils peuvent uniquement collecter des données du noeud actif.
  </para> <para> Si vous utilisez les scripts d&apos;amorçage pour configurer la grappe, les clés SSH sont automatiquement créées et copiées. </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-bootstrap">
  <title>Présentation des scripts d&apos;amorçage</title>
  <para>
   Les commandes suivantes exécutent des scripts d&apos;amorçage qui ne nécessitent qu&apos;un minimum de temps et d&apos;effort manuel.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Le script <command>crm cluster init</command> permet de définir les paramètres de base nécessaires pour la communication au sein de la grappe. À ce moment-là, vous disposez d&apos;une grappe opérationnelle comportant un seul noeud.
    </para>
   </listitem>
   <listitem>
    <para>
     Le script <command>crm cluster join</command> permet d&apos;ajouter d&apos;autres noeuds à votre grappe.
    </para>
   </listitem>
   <listitem>
    <para>
     Le script <command>crm cluster remove</command> permet de supprimer des noeuds de votre grappe.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Tous les scripts d&apos;amorçage consignent des données dans le fichier <filename>/var/log/crmsh/crmsh.log</filename>. Pour toute information à propos du processus d&apos;amorçage, consultez ce fichier. Toutes les options définies au cours du processus d&apos;amorçage peuvent être modifiées ultérieurement à l&apos;aide du module de grappe YaST. Pour plus d&apos;informations, reportez-vous au <xref linkend="cha-ha-ycluster"/>.
  </para>
  <para>
   Le script d&apos;amorçage <command>crm cluster init</command> vérifie et configure les composants suivants :
  </para>

  <variablelist>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      Si un service NTP n&apos;a pas été configuré pour se lancer au moment du démarrage, un message s&apos;affiche. Depuis <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> 15, chrony est l&apos;implémentation par défaut du protocole NTP.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>Le script crée les clés SSH nécessaires pour que les noeuds de la grappe puissent accéder les uns aux autres sans mot de passe.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Csync2</term>
    <listitem>
     <para>
      Le script configure Csync2 de manière à répliquer les fichiers de configuration sur tous les noeuds d&apos;une grappe.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Corosync</term>
    <listitem>
     <para>Le script configure le système de communication de la grappe.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD/surveillance</term>
    <listitem>
     <para>Le script vérifie si un système de surveillance existe et vous demande si vous souhaitez configurer un SBD comme mécanisme d&apos;isolation de noeud.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Adresse IP virtuelle flottante</term>
    <listitem>
     <para>Le script vous demande si vous souhaitez configurer une adresse IP virtuelle pour l&apos;administration de la grappe à l&apos;aide de Hawk2.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Pare-feu</term>
    <listitem>
     <para>Le script ouvre les ports du pare-feu qui sont nécessaires pour les communications de grappe.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Nom de la grappe</term>
    <listitem>
     <para>Le script définit un nom pour la grappe. Par défaut, il s&apos;agit de <systemitem>hacluster</systemitem>. Ce nom est facultatif et principalement utile pour les grappes géographiques. En règle générale, le nom de la grappe reflète l&apos;emplacement et facilite la distinction d&apos;un site à l&apos;intérieur d&apos;une grappe géographique.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>QDevice/QNetd</term>
    <listitem>
     <para>
      Cette configuration n&apos;est pas traitée ici. Pour utiliser un serveur QNetd, configurez-le avec le script d&apos;amorçage comme décrit dans le document <xref linkend="cha-ha-qdevice"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-installation">
    <title>Installation de <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase></title>
    <para>
      Les paquetages de configuration et de gestion de grappe avec High Availability Extension sont inclus dans le modèle d&apos;installation <literal>haute disponibilité</literal> (appelé <literal>sles_ha</literal> dans la ligne de commande). Ce modèle est disponible uniquement si <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> a été installé en tant qu&apos;extension de SUSE® Linux Enterprise Server.
    </para>
    <para>
      Pour plus d&apos;informations sur l&apos;installation des extensions, reportez-vous au manuel <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-add-ons.html"><citetitle>Deployment Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase></link> (Guide de déploiement de SUSE Linux Enterprise Server 15 SP4).
    </para>

    <procedure xml:id="pro-ha-inst-quick-pattern">
      <title>Installation du modèle <literal>Haute disponibilité</literal></title>
       <para>
        Si le modèle n&apos;est pas encore installé, procédez comme suit :
       </para>
      <step>
       <para>
        Installez-le via la ligne de commande à l&apos;aide de Zypper :</para>
<screen><prompt role="root">root # </prompt><command>zypper</command> install -t pattern ha_sles</screen>
      </step>
      <step>
       <para>
          Installez le modèle Haute disponibilité sur <emphasis>toutes</emphasis> les machines qui feront partie de votre grappe.
       </para>
       <note>
        <title>installation des paquetages logiciels sur toutes les parties</title>
        <para>
         Pour procéder à une installation automatisée de SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase> et de <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase>
         <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase>, utilisez AutoYaST afin de cloner des noeuds existants. Pour plus d&apos;informations, reportez-vous au <xref linkend="sec-ha-installation-autoyast"/>.
        </para>
       </note>
      </step>
      <step>
       <para>
         Enregistrez les machines sur le SUSE Customer Center. Pour plus d&apos;informations, reportez-vous au manuel <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-upgrade-offline.html#sec-update-registersystem"><citetitle>Upgrade Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase></link> (Guide de mise à niveau de SUSE Linux Enterprise Server 15 SP4).
       </para>
      </step>
    </procedure>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-sbd">
  <title>Utilisation de SBD comme mécanisme d&apos;isolation</title>
  

   <para>
    Si vous disposez d&apos;un stockage partagé, par exemple un SAN (sous-réseau de stockage), vous pouvez l&apos;utiliser pour éviter des vues de grappe divergentes. Pour ce faire, configurez SBD comme mécanisme d&apos;isolation de noeud. Un SBD utilise la prise en charge de la surveillance et l&apos;agent de ressource STONITH <literal>external/sbd</literal>.
   </para>

  <sect2 xml:id="sec-ha-inst-quick-sbd-req">
   <title>Configuration requise pour le SBD</title>
   <para>
    Lors de la configuration du premier noeud à l&apos;aide du script <command>crm cluster init</command>, vous pouvez décider d&apos;utiliser ou non un SBD. Si vous choisissez d&apos;utiliser un SBD, vous devez spécifier le chemin d&apos;accès au périphérique de stockage partagé. Par défaut, le script <command>crm cluster init</command> crée automatiquement une petite partition sur le périphérique à utiliser pour le SBD.
   </para>
   <para>Pour pouvoir utiliser un SBD, les conditions suivantes doivent être remplies :</para>

   <itemizedlist>
    <listitem>
     <para>Le chemin d&apos;accès au périphérique de stockage partagé doit être persistant et cohérent sur tous les noeuds de la grappe. Utilisez des noms de périphérique stables, tels que <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
     </para>
    </listitem>
    <listitem>
     <para> Le périphérique SBD <emphasis>ne doit pas</emphasis> utiliser LVM2 ou la technologie RAID basée sur un hôte, ni résider sur une instance DRBD*.
     </para>
    </listitem>
   </itemizedlist>

  <para>
   Pour des détails sur la configuration d&apos;un stockage partagé, reportez-vous au manuel <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html"><citetitle>Storage Administration Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP4</phrase></phrase></link> (Guide d&apos;administration du stockage de SUSE Linux Enterprise Server 15 SP4).
  </para>
  
  </sect2>

  <sect2 xml:id="sec-ha-inst-quick-sbd-setup">
   <title>Activation de la surveillance softdog pour SBD</title>
   
   <para>
    Dans SUSE Linux Enterprise Server (SLES), la prise en charge de la surveillance dans le kernel est activée par défaut : SLES inclut plusieurs modules de kernel qui fournissent des pilotes de surveillance propres au matériel. High Availability Extension utilise le daemon SBD comme composant logiciel pour <quote>alimenter</quote> le système de surveillance.
   </para>
   <para>
    La procédure suivante utilise la surveillance <systemitem>softdog</systemitem>.
   </para>

   
   <important>
    <title>limites de la surveillance softdog</title>
    <para>
     Le pilote softdog part du principe qu&apos;au moins un processeur est toujours en cours d&apos;exécution. Si tous les processeurs sont bloqués, le code dans le pilote softdog qui devrait redémarrer le système ne sera jamais exécuté. En revanche, les systèmes de surveillance matérielle continuent à travailler même si tous les processeurs sont bloqués.
    </para>
    <para>Avant d&apos;utiliser la grappe dans un environnement de production, il est vivement recommandé de remplacer le module <systemitem>softdog</systemitem> par le module matériel qui correspond le mieux à votre matériel.
    </para>
    <para>Toutefois, si aucun système de surveillance ne correspond à votre matériel, <systemitem class="resource">softdog</systemitem> peut être utilisé en tant que module de surveillance de kernel.</para>
   </important>

   <procedure xml:id="pro-ha-inst-quick-sbd-setup">
    <step>
     <para>
      Créez un stockage partagé persistant comme décrit dans la <xref linkend="sec-ha-inst-quick-sbd-req"/>.
     </para>
    </step>
    <step>
     <para>
      Activez la surveillance logicielle :
     </para>
     
     <screen><prompt role="root">root # </prompt><command>echo</command> softdog &gt; /etc/modules-load.d/watchdog.conf
<prompt role="root">root # </prompt><command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
      
     <para>Vérifiez si le module de surveillance logicielle est chargé correctement :
     </para>
     <screen><prompt role="root">root # </prompt><command>lsmod</command> | grep dog
softdog                16384  1</screen>
    </step>
  </procedure>

   <remark>toms 2018-04-05: we need to add a bit more info here how you do
    the tests and what to do when it fails.
    However, this needs some further info from our developers. Some info can
    be found in ha_storage_protection.xml.
    Usually it boils down to "sbd -d DEV list" and "sbd -d DEV message bob test"
   </remark>
   <para>
    Nous vous conseillons vivement de tester le bon fonctionnement du mécanisme d&apos;isolation de SBD afin d&apos;éviter un scénario de divergence. Ce type de test peut être exécuté en bloquant la communication de grappe Corosync.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-1st-node">
  <title>Configuration du premier noeud</title>
   <para>
   Configurez le premier noeud avec le script <command>crm cluster init</command>. Cette opération ne nécessite qu&apos;un minimum de temps et d&apos;effort manuel.
  </para>

  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-init">
   <title>Configuration du premier noeud (<systemitem class="server">alice</systemitem>) avec le script <command>crm cluster init</command></title>
   <step>
    <para>
     Connectez-vous en tant qu&apos;utilisateur <systemitem class="username">root</systemitem> à la machine physique ou virtuelle à utiliser comme noeud de grappe.
    </para>
   </step>
   <step>
    <para>
     Démarrez le script d&apos;amorçage en exécutant la commande suivante :
    </para>
    <screen><prompt role="root">root # </prompt><command>crm cluster init</command> --name <replaceable>CLUSTERNAME</replaceable></screen>
    <para>Remplacez la marque de réservation <replaceable>NOM_GRAPPE</replaceable> par un nom pertinent, comme l&apos;emplacement géographique de votre grappe (par exemple, <literal>amsterdam</literal>). Cela est particulièrement utile pour créer une grappe géographique ultérieurement, car cela simplifie l&apos;identification d&apos;un site.
    </para>
    <para>
     Si vous avez besoin de la multidiffusion plutôt que la monodiffusion (par défaut) pour la communication de votre grappe, utilisez l&apos;option <option>--multicast</option> (ou <option>-U</option>).
    </para>
    <para>
     Le script vérifie la configuration NTP et la présence d&apos;un service de surveillance matérielle. Il génère les clés SSH publiques et privées utilisées pour l&apos;accès SSH et la synchronisation Csync2, et démarre les services respectifs.
    </para>
    
   </step>
   <step>
    <para>
     Configurez la couche de communication de grappe (Corosync) :
    </para>
    <substeps>
     <step>
      <para>
       Spécifiez une adresse réseau pour la liaison. Par défaut, le script propose l&apos;adresse réseau <systemitem>eth0</systemitem>. Vous pouvez également spécifier une autre adresse réseau, par exemple <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Acceptez le port proposé (<literal>5405</literal>) ou entrez-en un autre.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
    Configurez le SBD comme mécanisme d&apos;isolation de noeud :</para>
    <substeps>
     <step>
      <para>Spécifiez <literal>y</literal> pour confirmer que vous souhaitez utiliser le SBD.</para>
     </step>
     <step>
      <para>Entrez un chemin d&apos;accès persistant à la partition du périphérique de bloc que vous souhaitez utiliser pour le SBD (voir <xref linkend="sec-ha-inst-quick-sbd"/>). Le chemin doit être cohérent sur tous les noeuds de la grappe.</para>
     </step>
    </substeps>
   </step>
   <step xml:id="st-crm-cluster-init-ip">
    
    <para>Configurez une adresse IP virtuelle pour l&apos;administration de la grappe avec Hawk2. (Nous utiliserons cette ressource IP virtuelle ultérieurement pour tester le bon fonctionnement du basculement).</para>
    <substeps>
     <step>
      <para>Spécifiez <literal>y</literal> pour confirmer que vous souhaitez configurer une adresse IP virtuelle.</para></step>
     <step>
      <para>Spécifiez une adresse IP inutilisée que vous souhaitez employer comme adresse IP d&apos;administration pour Hawk2 : <literal>192.168.2.1</literal>.
      </para>
      <para>Au lieu de vous connecter à un noeud de grappe individuel avec Hawk2, vous pouvez vous connecter à l&apos;adresse IP virtuelle.</para>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   Enfin, le script démarre le service Pacemaker pour mettre en ligne la grappe et activer Hawk2. L&apos;URL à utiliser pour Hawk2 apparaît à l&apos;écran.
  </para>

  <para>
   Vous disposez maintenant d&apos;une grappe opérationnelle comportant un seul noeud. Pour visualiser son état, procédez comme suit :
  </para>
  <procedure xml:id="pro-ha-inst-quick-hawk2-login">
   <title>Connexion à l&apos;interface Web de Hawk2</title>
   <step>
    <para> Sur une machine, démarrez un navigateur Web et assurez-vous que JavaScript et les cookies sont activés. </para>
   </step>
   <step>
    <para> Spécifiez comme URL l&apos;adresse IP ou le nom d&apos;hôte d&apos;un noeud de grappe qui exécute le service Web Hawk. Vous pouvez également spécifier l&apos;adresse IP virtuelle que vous avez configurée à l&apos;<xref linkend="st-crm-cluster-init-ip"/> de la <xref linkend="pro-ha-inst-quick-setup-crm-cluster-init"/> : </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
    <note>
     <title>avertissement de certificat</title>
     <para> Si un avertissement de certificat s&apos;affiche la première fois que vous tentez d&apos;accéder à l&apos;URL, un certificat auto-signé est utilisé. Par défaut, les certificats auto-signés ne sont pas considérés comme fiables. </para>
     <para> Demandez à votre opérateur de grappe de vous fournir les informations relatives au certificat afin de vérifier ce dernier. </para>
     <para> Pour continuer, vous pouvez ajouter une exception dans le navigateur afin d&apos;ignorer l&apos;avertissement. </para>
     
    </note>
   </step>
   <step>
    <para> Dans l&apos;écran de connexion de Hawk2, spécifiez le <guimenu>nom d&apos;utilisateur</guimenu> et le <guimenu>mot de passe</guimenu> de l&apos;utilisateur qui a été créé lors de la procédure d&apos;amorçage (utilisateur : <systemitem class="username">hacluster</systemitem>, mot de passe : <literal>Linux</literal>).</para>
    <important>
     <title>mot de passe sécurisé</title>
     <para>Remplacez dès que possible le mot de passe par défaut par un mot de passe sécurisé :
     </para>
     <screen><prompt role="root">root # </prompt><command>passwd</command> hacluster</screen>
    </important>
   </step>
   <step>
    <para>
     Cliquez sur <guimenu>Connexion</guimenu>. Une fois que vous êtes connecté, l&apos;interface Web Hawk2 affiche l&apos;écran d&apos;état par défaut, lequel permet de visualiser rapidement l&apos;état actuel de la grappe :
    </para>
    <figure xml:id="fig-ha-inst-quick-one-node-status">
     <title>état de la grappe à un noeud dans Hawk2</title>
     <mediaobject>
      <imageobject>
       <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-2nd-node">
  <title>Ajout du deuxième noeud</title>
  <para>
    Si vous disposez d&apos;une grappe à un noeud opérationnelle, ajoutez-y le deuxième noeud à l&apos;aide du script d&apos;amorçage <command>crm cluster join</command>, comme décrit dans la <xref linkend="pro-ha-inst-quick-setup-crm-cluster-join" xrefstyle="select:label nopage"/>. Le script nécessite uniquement un accès à un noeud de grappe existant et procède automatiquement à la configuration de base sur la machine actuelle. Pour plus de détails, reportez-vous à la page de manuel <command>crm cluster join</command>.
  </para>
  <para>
   Les scripts d&apos;amorçage prennent en charge la modification de la configuration spécifique à une grappe à deux noeuds, par exemple, SBD et Corosync.
  </para>
  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-join">
   <title>Ajout du deuxième noeud (<systemitem class="server">bob</systemitem>) à l&apos;aide du script <command>crm cluster join</command></title>
   <step>
    <para>
     Connectez-vous en tant qu&apos;utilisateur <systemitem class="username">root</systemitem> à la machine physique ou virtuelle censée rejoindre la grappe.
    </para>
   </step>
   <step>
    <para>
     Démarrez le script d&apos;amorçage en exécutant la commande suivante :
    </para>
<screen><prompt role="root">root # </prompt><command>crm cluster join</command></screen>
    <para>
     Si un service NTP n&apos;a pas été configuré pour se lancer au moment du démarrage, un message s&apos;affiche. Le script vérifie également la présence d&apos;un périphérique de surveillance du matériel (important si vous souhaitez configurer SBD). Vous êtes averti si aucun n&apos;est présent.
    </para>
   </step>
   <step>
    <para>
     Si vous décidez de continuer, le système vous invite à spécifier l&apos;adresse IP d&apos;un noeud existant. Entrez l&apos;adresse IP du premier noeud (<systemitem class="server">alice</systemitem>, <systemitem class="ipaddress">192.168.1.1</systemitem>).
    </para>
   </step>
   <step>
    <para>
     Si vous n&apos;avez pas encore configuré d&apos;accès SSH sans mot de passe entre les deux machines, le système vous invite à spécifier le mot de passe <systemitem class="username">root</systemitem> du noeud existant.
    </para>
    <para>
     Une fois connecté au noeud spécifié, le script copie la configuration Corosync, configure SSH et Csync2, et met en ligne la machine actuelle en tant que nouveau noeud de grappe. Par ailleurs, le script démarre le service requis pour Hawk2. 
    </para>
   </step>
  </procedure>
  <para>
   Vérifiez l&apos;état de la grappe dans Hawk2. Dans <menuchoice>
    <guimenu> État</guimenu>
    <guimenu> Noeuds</guimenu>
   </menuchoice> , vous devriez normalement voir deux noeuds présentant un état vert (voir <xref linkend="fig-ha-inst-quick-two-node-cluster"/>).
  </para>

  <figure xml:id="fig-ha-inst-quick-two-node-cluster">
   <title>état de la grappe à deux noeuds</title>
   <mediaobject>
    <imageobject>
     <imagedata width="80%" fileref="installquick-two-nodecluster-status.png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-test">
   <title>Test de la grappe</title>
   <para>
    La <xref linkend="sec-ha-inst-quick-test-resource-failover"/> est un test simple qui permet de vérifier si la grappe déplace l&apos;adresse IP virtuelle vers l&apos;autre noeud lorsque le noeud qui exécute actuellement la ressource est mis en mode <literal>veille</literal>.
   </para>
   <para>Toutefois, un test réaliste implique des cas et des scénarios d&apos;utilisation spécifiques, notamment la vérification du bon fonctionnement du mécanisme d&apos;isolation destiné à éviter une situation de vues de grappe divergentes. Si vous n&apos;avez pas configuré votre mécanisme d&apos;isolation correctement, la grappe ne fonctionnera pas convenablement.</para>
   <para>Avant d&apos;utiliser la grappe dans un environnement de production, testez-la entièrement en fonction de vos cas d&apos;utilisation ou à l&apos;aide du script <command>ha-cluster-preflight-check</command>.
    
   </para>
   <sect2 xml:id="sec-ha-inst-quick-test-resource-failover">
    <title>Test du basculement des ressources</title>
    <para>
     La procédure ci-après permet d&apos;effectuer un test rapide du basculement des ressources :
    </para>
   <remark>toms 2016-07-27: Fate#321073
    Tool for Standardize Testing of Basic Cluster Functionality</remark>
   <procedure xml:id="pro-ha-inst-quick-test">
    <title>test du basculement des ressources</title>
    <step>
     <para>
      Ouvrez un terminal et exécutez une commande ping sur l&apos;adresse <systemitem>192.168.2.1</systemitem>, autrement dit votre adresse IP virtuelle :
     </para>
     <screen><prompt role="root">root # </prompt><command>ping</command> 192.168.2.1</screen>
    </step>
    <step>
     <para>
      Connectez-vous à votre grappe, comme décrit dans la <xref linkend="pro-ha-inst-quick-hawk2-login"/>.
     </para>
    </step>
    <step>
     <para>
      Dans Hawk2 <menuchoice>
       <guimenu>État</guimenu>
       <guimenu>Ressources</guimenu>
      </menuchoice>, vérifiez sur quel noeud s&apos;exécute l&apos;adresse IP virtuelle (ressource <systemitem>admin_addr</systemitem>). Supposons que la ressource s&apos;exécute sur <systemitem class="server">alice</systemitem>.
      </para>
    </step>
    <step>
     <para>
      Mettez <systemitem class="server">alice</systemitem> en mode <guimenu>veille</guimenu> (voir <xref linkend="fig-ha-inst-quick-standby"/>).
     </para>
     <figure xml:id="fig-ha-inst-quick-standby">
      <title>noeud <systemitem class="server">alice</systemitem> en mode veille</title>
      <mediaobject>
       <imageobject>
        <imagedata width="60%" fileref="installquick-standby-node.png"/>
       </imageobject>
      </mediaobject>
     </figure>
     
    </step>
    <step>
     <para>
      Cliquez sur <menuchoice>
       <guimenu>État</guimenu>
       <guimenu>Ressources</guimenu>
      </menuchoice>. La ressource <systemitem>admin_addr</systemitem> a été migrée vers <systemitem class="server">bob</systemitem>.
     </para>
    </step>
   </procedure>
   <para>
    Pendant la migration, vous devriez normalement voir un flux continu de requêtes ping envoyées à l&apos;adresse IP virtuelle. Cela montre que la configuration de grappe et l&apos;adresse IP flottante fonctionnent correctement. Annulez la commande <command>ping</command> en appuyant sur <keycombo>
       <keycap function="control"/><keycap>C</keycap>
      </keycombo> .
   </para>
   </sect2>

   <sect2 xml:id="sec-ha-inst-quick-test-with-cluster-script">
    <title>Test à l&apos;aide de la commande ha-cluster-preflight-check</title>
    <para>
     La commande <command>ha-cluster-preflight-check</command> exécute des tests standardisés pour une grappe. Elle déclenche des défaillances de grappe et vérifie la configuration afin de repérer les problèmes. Avant d&apos;utiliser la grappe en production, il est recommandé d&apos;appliquer cette commande pour vous assurer que tout fonctionne comme prévu.
    </para>
    <para>
     La commande prend en charge les vérifications suivantes :
    </para>
    <itemizedlist>
     <listitem>
      <formalpara>
       <title>Vérification de l&apos;environnement <option>-e</option>/<option>--env-check</option></title>
       <para>
        Ce test vérifie les points suivants :
       </para>
      </formalpara>
      <itemizedlist>
       <listitem>
        <para> Les noms d&apos;hôtes peuvent-ils être résolus ? </para>
       </listitem>
       <listitem>
        <para>
         Le service de temps est-il activé et démarré ?
        </para>
       </listitem>
       <listitem>
        <para>
         Une surveillance (watchdog) est-elle configurée pour le noeud actuel ?
        </para>
       </listitem>
       <listitem>
        <para>
         Le service <command>firewalld</command> est-il démarré et les ports associés à la grappe sont-ils ouverts ?
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <formalpara>
       <title>Vérification de l&apos;état de la grappe <option>-c</option>/<option>--cluster-check</option></title>
       <para> Vérifie les différents états et services de la grappe. Ce test vérifie les points suivants :</para>
      </formalpara>
      <itemizedlist>
       <listitem>
        <para>
         Les services de grappe (Pacemaker/Corosync) sont-ils activés et en cours d&apos;exécution ?
        </para>
       </listitem>
       <listitem>
        <para>
         STONITH est-il activé ? Ce test vérifie également si les ressources associées à STONITH sont configurées et démarrées. Si vous avez configuré SBD, le service SBD est-il démarré ?
        </para>
       </listitem>
       <listitem>
        <para>
         La grappe comporte-t-elle un quorum ? Ce test affiche les noeuds DC et les noeuds qui sont en ligne, hors ligne et non nettoyés.
        </para>
       </listitem>
       <listitem>
        <para>
         Le système comporte-t-il des ressources démarrées, arrêtées ou ayant échoué ?
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <formalpara>
       <title>Vérification des grappes divergentes <option>--split-brain-iptables</option></title>
       <para>
        Ce test simule un scénario de grappes divergentes en bloquant le port Corosync. Il vérifie si un noeud peut être délimité comme prévu.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Destruction des daemons pour SBD, Corosync et Pacemaker <option>-kill-sbd</option>/<option>-kill-corosync</option>/<option>-kill-pacemakerd</option></title>
       <para>
        Après avoir exécuté ce test, vous trouverez un rapport dans <filename>/var/lib/ha-cluster-Preflight-Check</filename>. Ce rapport inclut une description de cas de test, la consignation des opérations et une explication sur les résultats possibles.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Vérification de la délimitation du noeud <option>--fence-node</option></title>
       <para>Ce test délimite le noeud spécifique transmis par la ligne de commande.</para>
      </formalpara>
     </listitem>
    </itemizedlist>
    <para>
     Par exemple, pour tester l&apos;environnement, exécutez :
    </para>
    <screen><prompt role="root">root # </prompt><command>crm_mon -1</command>
Stack: corosync
Current DC: alice (version ...) - partition with quorum
Last updated: Fri Mar 03 14:40:21 2020
Last change: Fri Mar 03 14:35:07 2020 by root via cibadmin on alice

2 nodes configured
1 resource configured

Online: [ alice bob ]
Active resources:

 stonith-sbd    (stonith:external/sbd): Started alice

<prompt role="root">root # </prompt><command>ha-cluster-preflight-check</command> -e
[2020/03/20 14:40:45]INFO: Checking hostname resolvable [Pass]
[2020/03/20 14:40:45]INFO: Checking time service [Fail]
 INFO: chronyd.service is available
 WARNING: chronyd.service is disabled
 WARNING: chronyd.service is not active
[2020/03/20 14:40:45]INFO: Checking watchdog [Pass]
[2020/03/20 14:40:45]INFO: Checking firewall [Fail]
 INFO: firewalld.service is available
 WARNING: firewalld.service is not active</screen>
    <para>
     Vous pouvez vérifier le résultat dans <filename>/var/log/ha-cluster-preflight-check.log</filename>.
    </para>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-moreinfo">
   <title>Pour en savoir plus</title>
    <para>
     Pour plus de documentation sur ce produit, reportez-vous à l&apos;adresse <link xlink:href="https://documentation.suse.com/sle-ha/"/>. Pour des informations concernant d&apos;autres tâches de configuration et d&apos;administration, reportez-vous au <link xlink:href="https://documentation.suse.com/sle-ha/html/SLE-HA-all/book-sleha-guide.html"><citetitle> Guide d&apos;administration</citetitle></link>.
    </para>
   </sect1>
 <xi:include href="common_copyright_quick.xml"/>
 <xi:include href="common_legal.xml"/>
</article>
