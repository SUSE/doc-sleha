<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:its="http://www.w3.org/2005/11/its" xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="article_installation.xml" version="5.2" xml:lang="pt-br" xml:id="article-installation">
 <title>Inicialização Rápida de Instalação e Configuração</title>
 <info>
  <productnumber>15 SP7</productnumber>
  <productname>SUSE Linux Enterprise High Availability</productname>
  <date><?dbtimestamp format="d de B de Y"?>
</date>
  <xi:include href="common_copyright_gfdl.xml"/>
  <abstract>
   <para>
     Este documento orienta você na configuração de um cluster muito básico de dois nós usando os scripts de boot incluídos no crm shell. Isso inclui a configuração de um endereço IP virtual como um recurso de cluster e o uso do SBD em armazenamento compartilhado como um mecanismo de fencing de nós.
   </para>
  </abstract>
  <dm:docmanager>
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <meta name="title" its:translate="yes">Inicialização Rápida de Instalação e Configuração</meta>
  <meta name="series" its:translate="no">Products &amp; Solutions</meta>
  <meta name="description" its:translate="yes">Como configurar um cluster básico de dois nós usando os scripts de boot incluídos no shell crm</meta>
  <meta name="social-descr" its:translate="yes">Configurar um cluster básico de dois nós</meta>
  <meta name="task" its:translate="no">
    <phrase>Installation</phrase>
    <phrase>Administration</phrase>
    <phrase>Clustering</phrase>
  </meta> <revhistory xml:id="rh-article-installation">
    <revision>
      <date>2025-06-17</date>
      <revdescription>
        <para>
          Updated for the initial release of SUSE Linux Enterprise High Availability 15 SP7.
        </para>
      </revdescription>
    </revision>
   </revhistory>
  </info>

  <sect1 xml:id="sec-ha-inst-quick-usage-scenario">
   <title>Cenário de uso</title>
   <para>
    Os procedimentos neste documento orientam na configuração mínima de um cluster de dois nós com as seguintes propriedades:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Dois nós: <systemitem class="server">alice</systemitem> (IP: <systemitem class="ipaddress">192.168.1.1</systemitem>) e <systemitem class="server">bob</systemitem> (IP: <systemitem class="ipaddress">192.168.1.2</systemitem>), conectados por rede.
     </para>
    </listitem>
    <listitem>
     <para>
      Um endereço IP virtual flutuante (<systemitem class="ipaddress">192.168.1.10</systemitem>) que permite aos clientes se conectarem ao serviço independentemente do nó em que ele está sendo executado. Esse endereço IP é usado para conexão com a ferramenta de gerenciamento gráfico Hawk2.
     </para>
    </listitem>
    <listitem>
     <para>Um dispositivo de armazenamento compartilhado usado como mecanismo de fencing SBD. Isso evita cenários split-brain.
     </para>
    </listitem>
    <listitem>
     <para>
      Failover de recursos de um nó para outro em caso de falha no host ativo (configuração de modo <emphasis>ativo/passivo</emphasis>).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Você pode usar o cluster de dois nós para fins de teste ou como uma configuração de cluster mínima que pode ser estendida posteriormente. Antes de usar o cluster em um ambiente de produção, consulte o <xref linkend="book-administration"/> para modificar o cluster de acordo com os seus requisitos.
   </para>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-req">
   <title>Requisitos do sistema</title>
   <para>
    Esta seção informa você sobre os principais requisitos do sistema para o cenário descrito na <xref linkend="sec-ha-inst-quick-usage-scenario" xrefstyle="select:label"/>. Para ajustar o cluster para uso em um ambiente de produção, consulte a lista completa no <xref linkend="cha-ha-requirements"/>.
   </para>

  <sect2 xml:id="vl-ha-inst-quick-req-hw">
   <title>Requisitos de hardware</title>
   <variablelist>
    <varlistentry>
     <term>Servidores</term>
     <listitem>
      <para>
       Dois servidores com software conforme especificado na <xref linkend="il-ha-inst-quick-req-sw"/>.
      </para>
      <para>
      Os servidores podem ser completamente vazios ou máquinas virtuais. Eles não exigem hardware idêntico (memória, espaço em disco, etc.), mas devem ter a mesma arquitetura. Clusters compatíveis com várias plataformas não são suportados.
     </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Canais de comunicação</term>
     <listitem>  <para>
       No mínimo, duas mídias de comunicação TCP/IP por nó do cluster. O equipamento de rede deve suportar os meios de comunicação que você deseja usar para comunicação do cluster: multicast ou unicast. A mídia de comunicação deve suportar uma taxa de dados de 100 Mbit/s ou superior. Para uma configuração de cluster compatível, são necessários dois ou mais caminhos de comunicação redundantes. Isso pode ser feito por meio de:</para>
       <itemizedlist>
        <listitem>
         <para>
          Ligação de Dispositivo de Rede (preferencial)
         </para>
        </listitem>
        <listitem>
         <para>
          um segundo canal de comunicação no Corosync
         </para>
        </listitem>
       </itemizedlist> </listitem>
    </varlistentry>
    <varlistentry>
     <term>Fencing de nó/STONITH</term>
     <listitem>
      <para>
       Um dispositivo de fencing de nó (STONITH) para evitar cenários split-brain. Ele pode ser um dispositivo físico (interruptor) ou um mecanismo como SBD (STONITH por disco) em combinação com um watchdog. O SBD pode ser usado com armazenamento compartilhado ou no modo sem disco. Este documento descreve o uso do SBD com armazenamento compartilhado. Os seguintes requisitos devem ser atendidos:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Um dispositivo de armazenamento compartilhado. Para obter informações sobre configuração de armazenamento compartilhado, consulte o <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html"> Guia de Administração de Armazenamento do SUSE Linux Enterprise Server</link>. Se você precisa apenas de armazenamento compartilhado básico para fins de teste, consulte o <xref linkend="ha-iscsi-for-sbd"/>.
        </para>
       </listitem>
       <listitem>
        <para>O caminho para o dispositivo de armazenamento compartilhado deve ser persistente e consistente em todos os nós no cluster. Use nomes de dispositivos estáveis, como <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
        </para>
       </listitem>
       <listitem>
        <para> O dispositivo SBD <emphasis>não deve</emphasis> usar RAID, LVM ou DRBD* baseado em host.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Para obter mais informações sobre o STONITH, consulte o <xref linkend="cha-ha-fencing"/>. Para obter mais informações sobre o SBD, consulte o <xref linkend="cha-ha-storage-protect"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
 </sect2>
  <sect2 xml:id="il-ha-inst-quick-req-sw">
   <title>Requisitos de software</title>
   <para>
   Todos os nós precisam, no mínimo, dos seguintes módulos e extensões:
  </para>

<itemizedlist>
   <listitem>
    <para>Módulo Basesystem 15 SP7</para>
   </listitem>
   <listitem>
    <para>Módulo Server Applications 15 SP7</para>
   </listitem>
   <listitem>
    <para>SUSE Linux Enterprise High Availability 15 SP7</para>
   </listitem>
  </itemizedlist>

  </sect2>
  <sect2 xml:id="vl-ha-inst-quick-req-other">
   <title>Outros requisitos e recomendações</title>
   <variablelist>
    <varlistentry>
     <term>Sincronização de Horário</term>  <listitem>
   <para>
     Os nós do cluster devem ser sincronizados com um servidor NTP fora do cluster. Desde o SUSE Linux Enterprise High Availability 15, chrony é a implementação padrão de NTP. Para obter mais informações, consulte o <link xlink:href="https://documentation.suse.com/sles-15/html/SLES-all/cha-ntp.html"> Guia de Administração do SUSE Linux Enterprise Server 15 SP7</link>.
    </para>
    <para>
     O cluster pode não funcionar apropriadamente se os nós não estiverem sincronizados ou, mesmo se estiverem sincronizados, mas tiverem fusos horários diferentes configurados. Além disso, os arquivos de registro e os relatórios do cluster são muito difíceis de analisar sem a sincronização. Se você usar os scripts de boot, será avisado caso o NTP ainda não tenha sido configurado.
    </para>
  </listitem> </varlistentry>
    <varlistentry>
     <term>Nome de host e endereço IP</term>
     <listitem>
      <itemizedlist>
       <listitem>
        <para> Use endereços IP estáticos. </para>
       </listitem>
       <listitem>
        <para>
         Apenas o endereço IP principal é suportado.
        </para>
       </listitem>
       <listitem>  <para>
     Liste todos os nós do cluster no arquivo <filename>/etc/hosts</filename> com o respectivo nome completo e abreviado do host. É essencial que os membros do cluster possam encontrar uns aos outros pelo nome. Se os nomes não estiverem disponíveis, haverá falha na comunicação interna do cluster.
   </para> </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SSH</term>
     <listitem>  <para>
    Todos os nós do cluster devem ser capazes de acessar uns aos outros por SSH. Ferramentas como o comando <command>crm report</command> (para solução de problemas) e o <guimenu>Explorador do Histórico</guimenu> do Hawk2 exigem acesso por SSH sem senha entre os nós; do contrário, elas apenas poderão coletar dados do nó atual.
  </para> <para> Se você usar os scripts de boot para configurar o cluster, as chaves SSH serão automaticamente criadas e copiadas. </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-bootstrap">
  <title>Visão geral dos scripts de boot</title>
  <para>
   Os comandos a seguir executam scripts de boot que exigem apenas um mínimo de intervenção manual e de tempo.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Com o <command>crm cluster init</command>, defina os parâmetros básicos necessários para a comunicação do cluster. Dessa forma, você tem um cluster de um nó em execução.
    </para>
   </listitem>
   <listitem>
    <para>
     Com o <command>crm cluster join</command>, adicione mais nós ao cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Com o <command>crm cluster remove</command>, remova nós do cluster.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   As opções definidas pelos scripts de boot podem não ser iguais às configurações padrão do Pacemaker. Você pode verificar quais configurações os scripts de boot mudaram em <filename>/var/log/crmsh/crmsh.log</filename>. As opções definidas durante o processo de boot podem ser modificadas mais tarde com o YaST ou crmsh.
  </para>
  <para>
   O script de boot <command>crm cluster init</command> verifica e configura os seguintes componentes:
  </para>

  <variablelist>
   <varlistentry>
    <term>NTP/Chrony</term>
    <listitem>
     <para>
      Verifica se o NTP/Chrony está configurado para ser iniciado no momento da inicialização. Do contrário, será exibida uma mensagem.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>Cria chaves SSH para login sem senha entre os nós do cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Csync2</term>
    <listitem>
     <para>
      Configura o Csync2 para replicar os arquivos de configuração para todos os nós em um cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Corosync</term>
    <listitem>
     <para>Configura o sistema de comunicação do cluster.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD/watchdog</term>
    <listitem>
     <para>Verifica se há um watchdog e pergunta se é para configurar o SBD como mecanismo de fencing de nó.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>IP flutuante virtual</term>
    <listitem>
     <para>Pergunta se é para configurar um endereço IP virtual para administração do cluster com o Hawk2.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Firewall</term>
    <listitem>
     <para>Abre as portas no firewall que são necessárias para a comunicação do cluster.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Nome do cluster</term>
    <listitem>
     <para>
       Define um nome para o cluster. Por padrão, <systemitem>hacluster</systemitem>. Isso é opcional para um cluster básico, mas é obrigatório ao usar o DLM. Um nome de cluster exclusivo também é útil para clusters geográficos. Geralmente, o nome do cluster reflete a localização geográfica e facilita distinguir um site dentro de um cluster Geográfico.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>QDevice/QNetd</term>
    <listitem>
     <para>
      Pergunta se você deseja configurar o QDevice/QNetd para participar das decisões de quorum. É recomendado usar o QDevice e o QNetd para clusters com um número par de nós e, especialmente, para clusters de dois nós.
     </para>
     <para>
      Essa configuração não é abordada aqui, mas você pode defini-la posteriormente, conforme descrito no <xref linkend="cha-ha-qdevice"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <note>
    <title>Configuração de cluster para plataformas diferentes</title>
    <para>
      O script <command>crm cluster init</command> detecta o ambiente do sistema (por exemplo, Microsoft Azure) e ajusta determinadas configurações do cluster com base no perfil desse ambiente. Para obter mais informações, consulte o arquivo <filename>/etc/crm/profiles.yml</filename>.
    </para>
  </note>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-installation">
    <title>Instalando os pacotes de alta disponibilidade</title>
    <para>
      Os pacotes para configurar e gerenciar um cluster estão incluídos no padrão de instalação <literal>High Availability</literal>. Esse padrão estará disponível apenas após a instalação do SUSE Linux Enterprise High Availability.
    </para>
    <para>
      Você pode se registrar no SUSE Customer Center e instalar o SUSE Linux Enterprise High Availability durante ou após a instalação do SUSE Linux Enterprise Server. Para obter mais informações, consulte o <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-register-sle.html"> Guia de Implantação</link> do SUSE Linux Enterprise Server.
    </para>
    <procedure xml:id="pro-ha-inst-quick-pattern">
      <title>Instalando o padrão High Availability</title>
      <step>
       <para>
        Instale o padrão de Alta Disponibilidade pela linha de comando:</para>
<screen><prompt role="root"># </prompt><command>zypper install -t pattern ha_sles</command></screen>
      </step>
      <step>
       <para>
          Instale o padrão Alta Disponibilidade em <emphasis>todas</emphasis> as máquinas que farão parte do cluster.
       </para>
       <note>
        <title>Instalando pacotes de software em todos os nós</title>
        <para>
          Para instalação automatizada do SUSE Linux Enterprise Server 15 SP7 e do SUSE Linux Enterprise High Availability 15 SP7, use o AutoYaST para clonar os nós existentes. Para obter mais informações, consulte <xref linkend="sec-ha-installation-autoyast"/>. 
        </para>
       </note>
      </step>
    </procedure>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-sbd">
  <title>Usando o SBD para fencing de nó</title>
   <para>
    Antes de configurar o SBD com o script de boot, você deve habilitar um watchdog em cada nó. O SUSE Linux Enterprise Server é fornecido com vários módulos de kernel que incluem drivers watchdog específicos do hardware. O SUSE Linux Enterprise High Availability usa o daemon SBD como o componente de software que <quote>alimenta</quote> o watchdog.
   </para>
   <para>
    O procedimento a seguir usa o watchdog do <systemitem>softdog</systemitem>.
   </para>

   
   <important>
    <title>Limitações do softdog</title>
    <para>
     O driver softdog supõe que pelo menos uma CPU ainda esteja em execução. Se todas as CPUs estiverem travadas, o código no driver softdog que deve reinicializar o sistema nunca será executado. Por outro lado, os watchdogs do hardware continuarão funcionando mesmo se todas as CPUs estiverem travadas.
    </para>
    <para>Antes de usar o cluster em um ambiente de produção, é altamente recomendável substituir o módulo <systemitem>softdog</systemitem> pelo módulo mais adequado ao seu hardware.
    </para>
    <para>No entanto, se nenhum watchdog corresponder ao seu hardware, o <systemitem class="resource">softdog</systemitem> poderá ser usado como o módulo watchdog do kernel.</para>
   </important>

   <procedure xml:id="pro-ha-inst-quick-sbd-setup">
    <title>Habilitando o watchdog do softdog para SBD</title>
    <step>
     <para>
      Em cada nó, habilite o watchdog do softdog:
     </para>
     
     <screen><prompt role="root"># </prompt><command>echo softdog &gt; /etc/modules-load.d/watchdog.conf</command>
<prompt role="root"># </prompt><command>systemctl restart systemd-modules-load</command></screen>
    </step>
    <step>
     <para>Teste se o módulo softdog foi carregado corretamente:
     </para>
     <screen><prompt role="root"># </prompt><command>lsmod | grep dog</command>
softdog           16384  1</screen>
    </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-1st-node">
  <title>Configurando o primeiro nó</title>
   <para>
   Configure o primeiro nó com o script <command>crm cluster init</command>. Isso exige apenas um mínimo de intervenção manual e de tempo.
  </para>

  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-init">
   <title>Configurando o primeiro nó (<systemitem class="server">alice</systemitem>) com <command>crm cluster init</command></title>
   <step>
    <para>
     Efetue login no primeiro nó do cluster como <systemitem class="username">root</systemitem> ou como um usuário com privilégios <command>sudo</command>.
    </para>
    <important>
     <title>Acesso à chave SSH</title>
     <para>
      O cluster usa o acesso SSH sem senha para comunicação entre os nós. O script <command>crm cluster init</command> procura e gera as chaves SSH, se ainda não existirem.
     </para>
     <para>
      Na maioria dos casos, as chaves SSH do usuário <systemitem class="username">root</systemitem> ou <command>sudo</command> devem existir (ou ser geradas) no nó.
     </para>
     <para>
      Como alternativa, as chaves SSH de um usuário <command>sudo</command> podem existir em uma máquina local e ser passadas para o nó por meio do encaminhamento do agente SSH. Isso requer uma configuração adicional que não está descrita para esta configuração mínima. Para obter mais informações, consulte <xref linkend="sec-ha-manual-config-crm-user-privileges"/>.
     </para>
    </important>
   </step>
   <step>
    <para>
     Inicie o script de boot:
    </para>
    <screen><prompt role="root"># </prompt><command>crm cluster init --name <replaceable>CLUSTERNAME</replaceable></command></screen>
    <para>Substitua o marcador <replaceable>CLUSTERNAME</replaceable> por um nome significativo, como a localização geográfica do seu cluster (por exemplo, <literal>amsterdam</literal>). Isso é útil principalmente para criar um cluster Geográfico no futuro, pois ele simplifica a identificação de um site.
    </para>
    <para>
     Se você precisa usar multicast em vez de unicast (padrão) para a comunicação com o cluster, use a opção <option>--multicast</option> (ou <option>-U</option>).
    </para>
    <para>
     O script verifica se há uma configuração de NTP/Chrony e um serviço watchdog de hardware. Se necessário, ele gera as chaves SSH públicas e privadas usadas para acesso SSH e sincronização com o Csync2 e inicia os respectivos serviços.
    </para>
   </step>
   <step>
    <para>
     Configure a camada de comunicação do cluster (Corosync):
    </para>
    <substeps>
     <step>
      <para>
       Digite um endereço de rede ao qual vincular. Por padrão, o script propõe o endereço de rede de <systemitem>eth0</systemitem>. Se preferir, digite um endereço de rede diferente, por exemplo, o endereço de <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Aceite a porta proposta (<literal>5405</literal>) ou insira uma diferente.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
    Configure o SBD como mecanismo de fencing de nó:</para>
    <substeps>
     <step>
      <para>Pressione <literal>y</literal> para confirmar que você deseja usar o SBD.</para>
     </step>
     <step>
      <para>Insira um caminho persistente para a partição do dispositivo de blocos que você deseja usar para o SBD. O caminho deve ser consistente em todos os nós no cluster.</para>
     </step>
    </substeps>
   </step>
   <step xml:id="st-crm-cluster-init-ip">
    <para>Configure um endereço IP virtual para administração do cluster com o Hawk2:</para>
    <substeps>
     <step>
      <para>Pressione <literal>y</literal> para confirmar que você deseja configurar um endereço IP virtual.</para></step>
     <step>
      <para>Digite um endereço IP não utilizado que você deseja usar como o IP de administração no Hawk2: <literal>192.168.1.10</literal>
      </para>
      <para>Em vez de efetuar login em um nó de cluster individual com o Hawk2, você pode se conectar ao endereço IP virtual.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Escolha se você deseja configurar o QDevice e o QNetd. Para a configuração mínima descrita neste documento, use o comando <literal>n</literal> para recusar por enquanto. Você pode configurar o QDevice e o QNetd mais tarde, conforme descrito no <xref linkend="cha-ha-qdevice"/>.
    </para>
   </step>
  </procedure>
  <para>
   Por fim, o script inicia os serviços do cluster para colocar o cluster online e habilitar o Hawk2. O URL a ser usado no Hawk2 é exibido na tela.
  </para>

  <para>
   Agora, você tem um cluster de um nó em execução. Para ver seu status, faça o seguinte:
  </para>
  <procedure xml:id="pro-ha-inst-quick-hawk2-login">
   <title>Efetuando login na interface da Web do Hawk2</title>
   <step>
    <para> Em qualquer máquina, inicie um browser da Web e verifique se o JavaScript e os cookies estão habilitados. </para>
   </step>
   <step>
    <para>Como URL, digite o endereço IP virtual que você configurou com o script de boot:</para>
    <screen>https://192.168.1.10:7630/</screen>
    <note>
     <title>Aviso de certificado</title>
     <para> Se um aviso de certificado for exibido quando você tentar acessar o URL pela primeira vez, um certificado autoassinado estará em uso. Os certificados autoassinados não são considerados confiáveis por padrão. </para>
     <para> Solicite ao operador do cluster os detalhes do certificado para verificá-lo. </para>
     <para> Para continuar mesmo assim, você pode adicionar uma exceção ao browser para ignorar o aviso. </para>
    </note>
   </step>
   <step>
    <para> Na tela de login do Hawk2, insira o <guimenu>Nome de usuário</guimenu> e a <guimenu>Senha</guimenu> do usuário que foi criado pelo script de boot (usuário <systemitem class="username">hacluster</systemitem>, senha <literal>linux</literal>).</para>
    <important>
     <title>Senha segura</title>
     <para>Substitua a senha padrão por uma segura assim que possível:
     </para>
     <screen><prompt role="root"># </prompt><command>passwd hacluster</command></screen>
    </important>
   </step>
   <step>
    <para>
     Clique em <guimenu>Entrar</guimenu>. A interface da Web do Hawk2 mostra a tela Status por padrão:
    </para>
    <figure xml:id="fig-ha-inst-quick-one-node-status">
     <title>Status do cluster de um nó no Hawk2</title>
     <mediaobject>
      <imageobject>
       <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
      </imageobject>
      <textobject role="description">
        <phrase> Este exemplo da tela Status mostra um recurso (<literal>admin-ip</literal>) com um indicador de status verde. </phrase>
      </textobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-2nd-node">
  <title>Adicionando o segundo nó</title>
  <para>
    Adicione um segundo nó ao cluster com o script de boot <command>crm cluster join</command>. O script precisa apenas de acesso a um nó do cluster existente para concluir a configuração básica na máquina atual automaticamente.
  </para>
  <para>
   Para obter mais informações, consulte o comando <command>crm cluster join --help</command>.
  </para>
  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-join">
   <title>Adicionando o segundo nó (<systemitem class="server">bob</systemitem>) com <command>crm cluster join</command></title>
   <step>
    <para>
     Efetue login no segundo nó como <systemitem class="username">root</systemitem> ou como um usuário com privilégios <command>sudo</command>.
    </para>
   </step>
   <step>
    <para>
     Inicie o script de boot:
    </para>
    <para>
     Se você configurar o primeiro nó como <systemitem class="username">root</systemitem>, poderá executar este comando sem parâmetros adicionais:
    </para>
<screen><prompt role="root"># </prompt><command>crm cluster join</command></screen>
    <para>
     Se você configurar o primeiro nó como um usuário <command>sudo</command>, deverá especificar esse usuário com a opção <option>-c</option>:
    </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster join -c <replaceable>USER</replaceable>@alice</command></screen>
    <para>
     Se o NTP/Chrony não está configurado para ser iniciado no momento da inicialização, uma mensagem é exibida. O script também verifica se há um dispositivo watchdog de hardware. Você receberá um aviso se não houver nenhum.
    </para>
   </step>
   <step>
    <para>
     Se você ainda não especificou <systemitem class="server">alice</systemitem> com <option>-c</option>, é solicitado o endereço IP do primeiro nó.
    </para>
   </step>
   <step>
    <para>
     Se você ainda não configurou o acesso SSH sem senha entre as duas máquinas, é solicitado a digitar a senha do primeiro nó.
    </para>
    <para>
     Após efetuar login no nó especificado, o script copiará a configuração do Corosync, configurará o SSH e o Csync2, colocará a máquina atual online como um novo nó do cluster e iniciará o serviço necessário para o Hawk2.
    </para>
   </step>
  </procedure>
  <para>
   Verifique o status do cluster no Hawk2. Em <menuchoice> <guimenu>Status</guimenu> <guimenu>Nós</guimenu> </menuchoice>, você deve ver dois nós com status verde:
  </para>

  <figure xml:id="fig-ha-inst-quick-two-node-cluster">
   <title>Status do cluster de dois nós</title>
   <mediaobject>
    <imageobject>
     <imagedata width="80%" fileref="installquick-two-nodecluster-status.png"/>
    </imageobject>
    <textobject role="description">
      <phrase> Este exemplo da tela Status mostra dois nós, cada um com o indicador de status verde. </phrase>
    </textobject>
   </mediaobject>
  </figure>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-test">
   <title>Testando o cluster</title>
   <para>
    Os testes a seguir podem ajudar você a identificar problemas com a configuração do cluster. No entanto, um teste realista envolve casos de uso e cenários específicos. Antes de usar o cluster em um ambiente de produção, teste-o completamente de acordo com os seus casos de uso.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      O comando <command>sbd -d <replaceable>DEVICE_NAME</replaceable> list</command> lista todos os nós visíveis ao SBD. Para a configuração descrita neste documento, a saída deve mostrar tanto <systemitem class="server">alice</systemitem> quanto <systemitem class="server">bob</systemitem>.
     </para>
    </listitem>
    <listitem>
     <para>
      A <xref linkend="sec-ha-inst-quick-test-resource-failover"/> apresenta um teste simples para verificar se o cluster move o endereço IP virtual para o outro nó caso o nó que atualmente executa o recurso esteja definido como <literal>standby</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec-ha-inst-quick-test-with-cluster-script"/> simula falhas de cluster e relata os resultados.
     </para>
    </listitem>
   </itemizedlist>
   <sect2 xml:id="sec-ha-inst-quick-test-resource-failover">
    <title>Testando o failover de recursos</title>
    <para>
     Como um teste rápido, o procedimento a seguir verifica os failovers de recursos:
    </para>
   <procedure xml:id="pro-ha-inst-quick-test">
    <title>Testando o failover de recursos</title>
    <step>
     <para>
      Abra um terminal e execute o ping de <systemitem>192.168.1.10</systemitem>, seu endereço IP virtual:
     </para>
     <screen><prompt role="root"># </prompt><command>ping 192.168.1.10</command></screen>
    </step>
    <step>
     <para>
      Efetue login no Hawk2.
     </para>
    </step>
    <step>
     <para>
      Em <menuchoice><guimenu>Status</guimenu><guimenu>Recursos</guimenu></menuchoice>, verifique em qual nó o endereço IP virtual (recurso <systemitem>admin_addr</systemitem>) está sendo executado. Esse procedimento pressupõe que o recurso esteja sendo executado em <systemitem class="server">alice</systemitem>.
      </para>
    </step>
    <step>
     <para>
      Defina o modo de <systemitem class="server">alice</systemitem> como <guimenu>Standby</guimenu>:
     </para>
     <figure xml:id="fig-ha-inst-quick-standby">
      <title>Nó <systemitem class="server">alice</systemitem> no modo standby</title>
      <mediaobject>
       <imageobject>
        <imagedata width="60%" fileref="installquick-standby-node.png"/>
       </imageobject>
       <textobject role="description">
         <phrase> Este exemplo da tela Status mostra dois nós. O nó bob tem um indicador de status verde. O nó alice tem um indicador de status de ponto de interrogação amarelo e o botão de alternância Standby mudou para vermelho. </phrase>
       </textobject>
      </mediaobject>
     </figure>
     
    </step>
    <step>
     <para>
      Clique em <menuchoice> <guimenu>Status</guimenu> <guimenu>Recursos</guimenu> </menuchoice>. O recurso <systemitem>admin_addr</systemitem> foi migrado para <systemitem class="server">bob</systemitem>.
     </para>
    </step>
   </procedure>
   <para>
    Durante a migração, é exibido um fluxo contínuo de pings para o endereço IP virtual. Isso mostra que a configuração do cluster e o IP flutuante funcionam corretamente. Cancele o comando <command>ping</command> com <keycombo> <keycap function="control"></keycap><keycap>C</keycap> </keycombo>.
   </para>
   </sect2>

   <sect2 xml:id="sec-ha-inst-quick-test-with-cluster-script">
    <title>Testando com o comando <command>crm cluster crash_test</command></title>
    <para>
     O comando <command>crm cluster crash_test</command> aciona falhas do cluster para detectar problemas. Antes de usar seu cluster na produção, é recomendável usar esse comando para verificar se tudo funciona conforme o esperado.
    </para>
    <para>
     O comando suporta as seguintes verificações:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>--split-brain-iptables</option></term>
      <listitem>
       <para>
        Simula um cenário split-brain bloqueando a porta do Corosync. Verifica se um nó pode ser delimitado conforme o esperado.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--kill-sbd</option>/<option>--kill-corosync</option>/<option>--kill-pacemakerd</option></term>
      <listitem>
       <para>
        Elimina os daemons para o SBD, Corosync e Pacemaker. Após executar um desses testes, você encontrará um relatório no diretório <filename>/var/lib/crmsh/crash_test/</filename>. O relatório inclui a descrição de um caso de teste, o registro das ações e uma explicação dos possíveis resultados.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--fence-node <replaceable>NODE</replaceable></option></term>
      <listitem>
       <para>
        Delimita o nó específico passado pela linha de comando.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Para obter mais informações, consulte a <command>crm cluster crash_test --help</command>. 
    </para>
    <example xml:id="ex-test-with-cluster-script">
     <title>Testando o cluster: fencing de nó</title>
<screen><prompt role="root"># </prompt><command>crm_mon -1</command>
Stack: corosync
Current DC: alice (version ...) - partition with quorum
Last updated: Fri Mar 03 14:40:21 2020
Last change: Fri Mar 03 14:35:07 2020 by root via cibadmin on alice

2 nodes configured
1 resource configured

Online: [ alice bob ]
Active resources:

 stonith-sbd    (stonith:external/sbd): Started alice

<prompt role="root"># </prompt><command>crm cluster crash_test</command><command> --fence-node bob</command>

==============================================
Testcase:          Fence node bob
Fence action:      reboot
Fence timeout:     60

!!! WARNING WARNING WARNING !!!
THIS CASE MAY LEAD TO NODE BE FENCED.
TYPE Yes TO CONTINUE, OTHER INPUTS WILL CANCEL THIS CASE [Yes/No](No): <command>Yes</command>
INFO: Trying to fence node "bob"
INFO: Waiting 60s for node "bob" reboot...
INFO: Node "bob" will be fenced by "alice"!
INFO: Node "bob" was successfully fenced by "alice"</screen>
    </example>
    <para>
      Para observar a mudança de status de <systemitem class="server">bob</systemitem> durante o teste, efetue login no Hawk2 e navegue até <menuchoice><guimenu>Status</guimenu> <guimenu>Nós</guimenu></menuchoice>.
     </para>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-next">
   <title>Próximas etapas</title>
   <para>
    Os scripts de boot são um meio rápido para configurar um cluster de Alta Disponibilidade básico que pode ser usado para fins de teste. No entanto, para expandir esse cluster em um cluster de Alta Disponibilidade funcional que possa ser usado em ambientes de produção, são recomendadas mais etapas.
   </para>
   <variablelist xml:id="vl-ha-inst-quick-next-rec">
    <title>Etapas recomendadas para concluir a configuração do cluster de alta disponibilidade</title>
    <varlistentry>
     <term>Adicionando mais nós</term>
     <listitem>
      <para>
       Adicione mais nós ao cluster usando um dos seguintes métodos:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Para nós individuais, use o script <command>crm cluster join</command> conforme descrito na <xref linkend="sec-ha-inst-quick-setup-2nd-node"/>.
        </para>
       </listitem>
       <listitem>
        <para>
         Para instalação em massa de vários nós, use o AutoYaST conforme descrito no <xref linkend="sec-ha-installation-autoyast"/>.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Um cluster regular pode conter até 32 nós. Com o serviço <systemitem class="daemon">pacemaker_remote</systemitem>, os clusters de Alta Disponibilidade podem ser estendidos para incluir nós adicionais além desse limite. Consulte o <xref linkend="article-pacemaker-remote"/> para obter mais detalhes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Habilitando um watchdog de hardware</term>
     <listitem>
      <para>
       Antes de usar o cluster em um ambiente de produção, substitua o módulo <literal>softdog</literal> pelo módulo mais adequado ao seu hardware. Para obter os detalhes, consulte a <xref linkend="sec-ha-storage-protect-watchdog"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
      <term>Adicionando outros dispositivos STONITH</term>
      <listitem>
        <para>
          Para cargas de trabalho críticas, é altamente recomendável usar dois ou três dispositivos STONITH:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              Para continuar usando o SBD, consulte o <xref linkend="cha-ha-storage-protect"/>.
            </para>
          </listitem>
          <listitem>
            <para>
              Em vez disso, para usar dispositivos STONITH físicos, consulte o <xref linkend="cha-ha-fencing"/>.
            </para>
          </listitem>
        </itemizedlist>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Configurando o QDevice</term>
      <listitem>
       <para>
        Se o cluster tiver um número par de nós, configure o QDevice e o QNetd para participar das decisões de quorum. O QDevice oferece um número configurável de votos, permitindo que um cluster suporte mais falhas de nó do que o permitido pelas regras de quorum padrão. Para obter os detalhes, consulte a <xref linkend="cha-ha-qdevice"/>.
       </para>
      </listitem>
     </varlistentry>
   </variablelist>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-moreinfo">
   <title>Para obter mais informações</title>
    <para>
     Há mais documentação para este produto disponível em <link xlink:href="https://documentation.suse.com/sle-ha/"></link>. Para ver outras tarefas de configuração e de administração, consulte o <link xlink:href="https://documentation.suse.com/sle-ha/html/SLE-HA-all/book-administration.html"> Guia de Administração</link> completo.
    </para>
   </sect1>
 <xi:include href="ha_iscsi_for_sbd.xml"/>
 <xi:include href="common_legal.xml"/>
</article>
