<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ha.geo.manage">
 <title>Managing &geo; Clusters</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer/>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release/>
   <dm:repository/>
  </dm:docmanager>
 </info>
 <para>
  Before booth can manage a certain ticket within the &geo; cluster, you
  initially need to grant it to a site manually&mdash;either with the booth
  command line client or with &hawk2;.
 </para>
 <sect1 xml:id="sec.ha.geo.manage.cli">
  <title>Managing Tickets From Command Line</title>
  <para> Use the <command>booth</command> command line tool to grant, list, or
   revoke tickets as described in <xref linkend="sec.ha.geo.manage.cli.booth"/>.
  </para>
  <warning>
   <title><command>crm_ticket</command> and
    <command>crm&nbsp;site&nbsp;ticket</command></title>
   <para> If the booth service is not running for any reasons, you can also
    manage tickets manually with <command>crm_ticket</command> or
    <command>crm&nbsp;site&nbsp;ticket</command>. Both commands are only
    available on cluster nodes. Use them with great care as they
     <emphasis>cannot</emphasis> verify if the same ticket is already granted
    elsewhere. For more information, read the man pages. </para>
   <para> As long as booth is up and running, only use the
    <command>booth</command> for manual intervention. </para>
  </warning>


<sect2 xml:id="sec.ha.geo.manage.cli.booth">
 <title>Overview of <command>booth</command> Commands</title>
  <para>
   The <command>booth</command> commands can be run on any machine in the cluster,
   not only the ones having the &boothd; running. The
   <command>booth</command> commands try to find the <quote>local</quote>
   cluster by looking at the booth configuration file and the locally defined IP
   addresses. If you do not specify a site which <command>booth</command> should
   connect to (using the <option>-s</option> option), it will always connect to
   the local site.
  </para>

  <variablelist>
   <varlistentry>
    <term>Listing All Tickets</term>
    <listitem>
     <screen>&prompt.root;<command>booth</command> list
ticket: &ticket1;, leader: none
ticket: &ticket2;, leader: 10.2.12.101, expires: 2014-08-13 10:28:57</screen>
     <para> If you do not specify a certain site with <option>-s</option>, the
      information about the tickets will be requested from the local booth
      instance. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Granting a Ticket to a Site</term>
    <listitem>
     <screen>&prompt.root;<command>booth</command> grant -s 192.168.201.100 &ticket1;
booth[27891]: 2014/08/13_10:21:23 info: grant request sent, waiting for the result ...
booth[27891]: 2014/08/13_10:21:23 info: grant succeeded!</screen>
     <para> In this case, <literal>&ticket1;</literal> will be granted to
      the site <literal>192.168.201.100</literal>. Without the
       <option>-s</option> option, booth would automatically connect to the
      current site (the site you are running the booth client on) and would
      request the <command>grant</command> operation. </para>
     <para> Before granting a ticket, the command executes a sanity check. If
      the same ticket is already granted to another site, you are warned about
      that and are prompted to revoke the ticket from the current site first.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Revoking a Ticket From a Site</term>
    <listitem>
     <screen>&prompt.root;<command>booth</command> revoke &ticket1;
booth[27900]: 2014/08/13_10:21:23 info: revoke succeeded!</screen>
     <para> Booth checks to which site the ticket is currently granted and
      requests the <command>revoke</command> operation for
       <literal>&ticket1;</literal>. The revoke operation will be executed
      immediately. </para>
     <para>The <command>grant</command> and (under certain circumstances),
      <command>revoke</command> operations may take a while to return a definite
      operation's outcome. The client waits for the result up to the ticket's
       <varname>timeout</varname> value before it gives up waiting. If the
       <option>-w</option> option was used, the client will wait indefinitely
      instead. Find the exact status in the log files or with the
      <command>crm_ticket -L</command> command.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Forcing a Grant Operation</term>
    <listitem>
     <screen>&prompt.root;<command>booth</command> grant -F &ticket1;</screen>
     <para>The result of this command depends on whether you use automatic or
      manual tickets.</para>
      <itemizedlist>
       <listitem>
       <formalpara>
        <title>Automatic Tickets</title>
        <para>As long as booth can make sure a ticket is granted to one site,
         you cannot grant the same ticket to another site, not even by using
         the <option>-F</option> option. However, in case of a split brain
         situation, booth might not be able to check if an automatic ticket is
         granted somewhere else. In that case, the &geo; cluster
         administrator can override the automatic process and manually grant
         the ticket to the site that is still up and running. In this
         situation, the <option>-F</option> options tells booth
          <emphasis>not</emphasis> to wait for a response from other,
         unreachable sites (so ignoring the parameters
         <parameter>expire</parameter> and <parameter>acquire-after</parameter>,
         if defined for this ticket). Instead, booth will immediately grant the
         ticket to the specified site.</para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara>
        <title>Manual Tickets</title>
        <para>When using <emphasis>manual</emphasis> tickets, <command>booth
         grant -F</command> makes booth grant the ticket immediately to the
         specified site.</para>
       </formalpara>
      </listitem>
     </itemizedlist>
     <warning>
      <title>Potential Loss of Data</title>
      <para>Before using <command>booth grant -F</command>, make sure that no
       other site (which is online) owns the same ticket. If the same ticket is
       granted to multiple sites, resources depending on the ticket might start
       on several sites in parallel. This results in concurrency violation and
       potential data corruption.</para>
      <para>As &geo; cluster administrator, you need to resolve a conflict
       between tickets once the other site is reachable again.</para>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>In the following sections, find some examples for managing tickets in different
   scenarios.</para>
 </sect2>
 <sect2 xml:id="sec.ha.geo.manage.cli.booth.auto-ticket.move">
  <title>Manually Moving an Automatic Ticket</title>
   <para>
    Assuming that you want to manually move <literal>&ticket1;</literal> from
    site <literal>&cluster1;</literal> (with the virtual IP
    <literal>192.168.201.100</literal>) to site <literal>&cluster2;</literal>
    (with the virtual IP <literal>192.168.202.100</literal>), proceed as
    follows:
<!--<remark>taroth 2018-06: the original introduction to the procedure was:

'Let us assume that the (manually managed) ticket ticket-nfs had been granted to site
amsterdam (with the virtual IP 192.168.201.100 . This site cannot be reached at the
moment. Site berlin (with the virtual IP 192.168.202.100 ) is still available.
If you manage to contact a local administrator on amsterdam , you can verify if the site is
really down. In that case, the ticket on amsterdam cannot be revoked and the resources
depending on this ticket cannot be started on berlin unless you manually grant the ticket
ticket-nfs to berlin.

ygao 2018-06-26: @taroth:How about to rephrase "In that case ...to cluster2" to be something like:
"Make sure the ticket, or basically the relevant resources, have been relinquished
 from cluster1. If we are sure now it's safe and ready to grant the ticket to cluster2
 and host the relevant resources there, proceed as follows:"

taroth 2018-07-03: @gyan: But how to make sure the ticket/resources have been
relinquished from cluster1 if the site is already down?
</remark>-->
   </para>
  <procedure>
   <step>
    <para> Log in to <literal>&cluster1;</literal>.</para>
   </step>
   <step>
    <para>
     Set <literal>&ticket1;</literal> to standby with the following command:
    </para>
<screen>&prompt.root;<command>crm_ticket</command> -t &ticket1; -s</screen>
   </step>
   <step>
    <para>
     Wait for any resources that depend on <literal>&ticket1;</literal> to be
     stopped or demoted cleanly.
    </para>
   </step>
  <!-- <step>
    <para>
     Revoke <literal>&ticket1;</literal> from site
     <literal>&cluster1;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> revoke -s 192.168.201.100 &ticket1;</screen>
   </step>-->
   <step>
    <para>
     <!--After the ticket has been revoked from its original site, -->Grant
     <literal>&ticket1;</literal> to the site <literal>&cluster2;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> grant -s 192.168.202.100 &ticket1;</screen>
   </step>
   <step>
    <para>
     Activate <literal>&ticket1;</literal> on site <literal>&cluster1;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> grant -a 192.168.202.100 &ticket1;</screen>
     <para>Resources depending on &ticket1; will automatically fail over to
       <literal>&cluster1;</literal> in case
       <literal>&cluster2;</literal> fails.</para>
   </step>
  </procedure>
 </sect2>
 <sect2 xml:id="sec.ha.geo.manage.cli.booth.man-ticket.move">
  <title>Moving a Manual Ticket</title>
   <para>
    Assuming that you want to manually move <literal>ticketnfs</literal> from
    site <literal>&cluster1;</literal> (with the virtual IP
    <literal>192.168.201.100</literal>) to site <literal>&cluster2;</literal>
    (with the virtual IP <literal>192.168.202.100</literal>), proceed as
    follows:
   </para>
  <procedure>
   <step>
    <para>Log in to <literal>&cluster1;</literal>.</para>
   </step>
   <step>
    <para>
     Set <literal>&ticket3;</literal> to standby with the following command:
    </para>
<screen>&prompt.root;<command>crm_ticket</command> -t &ticket3; -s</screen>
   </step>
   <step>
    <para>
     Wait for any resources that depend on <literal>&ticket3;</literal> to be
     stopped or demoted cleanly.
    </para>
   </step>
   <step>
    <para>
     Grant <literal>&ticket3;</literal>to the site <literal>&cluster2;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> grant -s 192.168.202.100 -F &ticket3;</screen>
   </step>
   <step>
    <para>
     Activate <literal>&ticket3;</literal> on site <literal>&cluster1;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> grant -a 192.168.202.100 &ticket1;</screen>
     <para>Resources depending on <literal>&ticket3;</literal> will automatically fail over to
       <literal>&cluster1;</literal> in case
       <literal>&cluster2;</literal> fails.</para>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec.ha.geo.manage.cli.booth.man-ticket.failover">
  <title>Failing Over a Manual Ticket</title>
   <para> Let us assume that the (manually managed) ticket
     <literal>&ticket3;</literal> had been granted to site
     <literal>&cluster1;</literal> (with the virtual IP
     <literal>192.168.201.100</literal>. This site cannot be reached at the
    moment. Site <literal>&cluster2;</literal> (with the virtual IP
     <literal>192.168.202.100</literal>) is still available. </para>
   <para> If you manage to contact a local administrator on
     <literal>&cluster1;</literal>, you can verify if the site is really down.
    In that case, the ticket on <literal>&cluster1;</literal> cannot be revoked
    and the resources depending on this ticket cannot be started on
     <literal>&cluster2;</literal> unless you manually grant the ticket
     <literal>&ticket3;</literal> to <literal>&cluster2;</literal>. To do so,
    proceed as follows: </para>
 <procedure>
   <step>
    <para> Log in to <literal>&cluster2;</literal>.</para>
   </step>
   <step>
    <para> Grant <literal>&ticket3;</literal>to site
      <literal>&cluster2;</literal> using the <option>-F</option> option: </para>
    <screen>&prompt.root;<command>booth</command> grant -F &ticket3;</screen>
    <para>You will see a warning that the same ticket might be granted to
     another site, but the command will be executed.</para>
   </step>
   <step>
    <para>Check the result with:</para>
    <screen>&prompt.root;<command>booth</command> list</screen>
    <para>It should show <literal>&cluster2;</literal> as ticket owner for
      <literal>&ticket3;</literal> now. All resources that depend on this
     ticket will be started on <literal>&cluster2;</literal>.</para>
   </step>
   <step>
    <para>Before trying to bring back <literal>&cluster1;</literal> into the
     &geo; cluster again, make sure to revoke <literal>&ticket3;</literal>
     from <literal>&cluster1;</literal>: </para>
    <screen>&prompt.root;<command>booth</command> revoke -s 192.168.201.100 &ticket3;</screen>
   </step>
  </procedure>
 </sect2>
</sect1>
 <xi:include href="geo_manage_hawk2_i.xml"/>
</chapter>
