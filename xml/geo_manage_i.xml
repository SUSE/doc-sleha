<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ha.geo.manage">
 <title>Managing &geo; Clusters</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer/>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release/>
   <dm:repository/>
  </dm:docmanager>
 </info>
 <para>
  Before booth can manage a certain ticket within the &geo; cluster, you
  initially need to grant it to a site manually&mdash;either with the booth
  command line client or with &hawk2;.
 </para>
 <sect1 xml:id="sec.ha.geo.manage.cli">
  <title>Managing Tickets From Command Line</title>
  <para> Use the <command>booth</command> command line tool to grant, list, or
   revoke tickets as described in <xref linkend="vl.ha.booth.client.cmds"/>. The
   <command>booth</command> commands can be run on any machine in the cluster,
   not only the ones having the &boothd; running. The
   <command>booth</command> commands try to find the <quote>local</quote>
   cluster by looking at the booth configuration file and the locally defined IP
   addresses. If you do not specify a site which <command>booth</command> should
   connect to (using the <option>-s</option> option), it will always connect to
   the local site. </para>
  <warning>
   <title><command>crm_ticket</command> and
    <command>crm&nbsp;site&nbsp;ticket</command></title>
   <para> If the booth service is not running for any reasons, you can also
    manage tickets manually with <command>crm_ticket</command> or
    <command>crm&nbsp;site&nbsp;ticket</command>. Both commands are only
    available on cluster nodes. Use them with great care as they
     <emphasis>cannot</emphasis> verify if the same ticket is already granted
    elsewhere. For more information, read the man pages. </para>
   <para> As long as booth is up and running, only use the
    <command>booth</command> for manual intervention. </para>
  </warning>

  <variablelist xml:id="vl.ha.booth.client.cmds">
   <title>Overview of <command>booth</command> Commands</title>
   <varlistentry>
    <term>Listing All Tickets</term>
    <listitem>
     <screen>&prompt.root;<command>booth</command> list
ticket: &ticket1;, leader: none
ticket: &ticket2;, leader: 10.2.12.101, expires: 2014-08-13 10:28:57</screen>
     <para> If you do not specify a certain site with <option>-s</option>, the
      information about the tickets will be requested from the local booth
      instance. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Granting a Ticket to a Site</term>
    <listitem>
     <screen>&prompt.root;<command>booth</command> grant -s 192.168.201.100 &ticket1;
booth[27891]: 2014/08/13_10:21:23 info: grant request sent, waiting for the result ...
booth[27891]: 2014/08/13_10:21:23 info: grant succeeded!</screen>
     <para> In this case, <literal>&ticket1;</literal> will be granted to
      the site <literal>192.168.201.100</literal>. Without the
       <option>-s</option> option, booth would automatically connect to the
      current site (the site you are running the booth client on) and would
      request the <command>grant</command> operation. </para>
     <para> Before granting a ticket, the command executes a sanity check. If
      the same ticket is already granted to another site, you are warned about
      that and are prompted to revoke the ticket from the current site first.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Revoking a Ticket From a Site</term>
    <listitem>
     <screen>&prompt.root;<command>booth</command> revoke &ticket1;
booth[27900]: 2014/08/13_10:21:23 info: revoke succeeded!</screen>
     <para> Booth checks to which site the ticket is currently granted and
      requests the <command>revoke</command> operation for
       <literal>&ticket1;</literal>. The revoke operation will be executed
      immediately. </para>
     <para>The <command>grant</command> and (under certain circumstances),
      <command>revoke</command> operations may take a while to return a definite
      operation's outcome. The client waits for the result up to the ticket's
       <varname>timeout</varname> value before it gives up waiting. If the
       <option>-w</option> option was used, the client will wait indefinitely
      instead. Find the exact status in the log files or with the
      <command>crm_ticket -L</command> command.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Forcing a Grant Operation</term>
    <listitem>
     <screen>&prompt.root;<command>booth</command> grant -F &ticket1;</screen>
     <para>The result of this command depends on whether you use automatic or
      manual tickets.</para>
      <itemizedlist>
       <listitem>
       <formalpara>
        <title>Automatic Tickets</title>
        <para>As long as booth can make sure a ticket is granted to one site,
         you cannot grant the same ticket to another site, not even by using
         the <option>-F</option> option. However, in case of a split brain
         situation, booth might not be able to check if an automatic ticket is
         granted somewhere else. In that case, the &geo; cluster
         administrator can override the automatic process and manually grant
         the ticket to the site that is still up and running. In this
         situation, the <option>-F</option> options tells booth
          <emphasis>not</emphasis> to wait for a response from other,
         unreachable sites (so ignoring the parameters
         <parameter>expire</parameter> and <parameter>acquire-after</parameter>,
         if defined for this ticket). Instead, booth will immediately grant the
         ticket to the specified site&mdash; see also <xref
         linkend="pro.ha.geo.manage.manual.tickets"/>.</para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara>
        <title>Manual Tickets</title>
        <para>When using <emphasis>manual</emphasis> tickets, <command>booth
         grant -F</command> makes booth grant the ticket immediately to the
         specified site.</para>
       </formalpara>
      </listitem>
     </itemizedlist>
     <warning>
      <title>Potential Loss of Data</title>
      <para>Before using <command>booth grant -F</command>, make sure that no
       other site (which is online) owns the same ticket. If the same ticket is
       granted to multiple sites, resources depending on the ticket might start
       on several sites in parallel. This results in concurrency violation and
       potential data corruption.</para>
      <para>As &geo; cluster administrator, you need to resolve a ticket
       conflict once the other site is reachable again.</para>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>In the following find some scenarios and procedures that guide you
   through managing tickets.</para>

<example xml:id="ex.ha.geo.auto-ticket.move">
  <title>Manually Moving an Automatic Ticket</title>
   <para>
    Assuming that you want to manually move <literal>&ticket1;</literal> from
    site <literal>&cluster1;</literal> (with the virtual IP
    <literal>192.168.201.100</literal>) to site <literal>&cluster2;</literal>
    (with the virtual IP <literal>192.168.202.100</literal>), proceed as
    follows:
   </para>
  <procedure>
   <step>
    <para> Log in to <literal>&cluster1;</literal>.</para>
   </step>
   <step>
    <para>
     Set <literal>&ticket1;</literal> to standby with the following command:
    </para>
<screen>&prompt.root;<command>crm_ticket</command> -t &ticket1; -s</screen>
   </step>
   <step>
    <para>
     Wait for any resources that depend on <literal>&ticket1;</literal> to be
     stopped or demoted cleanly.
    </para>
   </step>
  <!-- <step>
    <para>
     Revoke <literal>&ticket1;</literal> from site
     <literal>&cluster1;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> revoke -s 192.168.201.100 &ticket1;</screen>
   </step>-->
   <step>
    <para>
     <!--After the ticket has been revoked from its original site, -->Grant
     <literal>&ticket1;</literal> to the site <literal>&cluster2;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> grant -s 192.168.202.100 &ticket1;</screen>
   </step>
   <step>
    <para>
     Activate <literal>&ticket1;</literal> on site <literal>&cluster1;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> grant -a 192.168.202.100 &ticket1;</screen>
     <para>Resources depending on &ticket1; will automatically fail over to
       <literal>&cluster1;</literal> in case
       <literal>&cluster2;</literal> fails.</para>
   </step>
  </procedure>
</example>

<example xml:id="ex.ha.geo.man-ticket.move">
  <title>Moving a Manual Ticket</title>
   <para>
    Assuming that you want to manually move <literal>ticketnfs</literal> from
    site <literal>&cluster1;</literal> (with the virtual IP
    <literal>192.168.201.100</literal>) to site <literal>&cluster2;</literal>
    (with the virtual IP <literal>192.168.202.100</literal>), proceed as
    follows:
   </para>
  <procedure>
   <step>
    <para>Log in to <literal>&cluster1;</literal>.</para>
   </step>
   <step>
    <para>
     Set <literal>ticket-nfs</literal> to standby with the following command:
    </para>
<screen>&prompt.root;<command>crm_ticket</command> -t ticket-nfs -s</screen>
   </step>
   <step>
    <para>
     Wait for any resources that depend on <literal>ticket-nfs</literal> to be
     stopped or demoted cleanly.
    </para>
   </step>
   <step>
    <para>
     Grant <literal>ticket-nfs</literal>to the site <literal>&cluster2;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> grant -s 192.168.202.100 -F ticket-nfs</screen>
   </step>
   <step>
    <para>
     Activate <literal>ticket-nfs</literal> on site <literal>&cluster1;</literal> with:
    </para>
<screen>&prompt.root;<command>booth</command> grant -a 192.168.202.100 &ticket1;</screen>
     <para>Resources depending on <literal>ticket-nfs</literal> will automatically fail over to
       <literal>&cluster1;</literal> in case
       <literal>&cluster2;</literal> fails.</para>
   </step>
  </procedure>
</example>

<example xml:id="ex.ha.geo.man-ticket.failover">
  <title>Failing Over a Manual Ticket</title>
   <para> Let us assume that the (manually managed) ticket
     <literal>&ticket3;</literal> had been granted to site
     <literal>&cluster1;</literal> (with the virtual IP
     <literal>192.168.201.100</literal>. This site cannot be reached at the
    moment. Site <literal>&cluster2;</literal> (with the virtual IP
     <literal>192.168.202.100</literal>) is still available. </para>
   <para> If you manage to contact a local administrator on
     <literal>&cluster1;</literal>, you can verify if the site is really down.
    In that case, the ticket on <literal>&cluster1;</literal> cannot be revoked
    and the resources depending on this ticket cannot be started on
     <literal>&cluster2;</literal> unless you manually grant the ticket
     <literal>ticket-nfs</literal> to <literal>&cluster2;</literal>. To do so,
    proceed as follows: </para>
 <procedure>
   <step>
    <para> Log in to <literal>&cluster2;</literal>.</para>
   </step>
   <step>
    <para> Grant <literal>ticket-nfs</literal>to site
      <literal>&cluster2;</literal> using the <option>-F</option> option: </para>
    <screen>&prompt.root;<command>booth</command> grant -F ticket-nfs</screen>
    <para>You will see a warning that the same ticket might be granted to
     another site, but the command will be executed.</para>
   </step>
   <step>
    <para>Check the result with:</para>
    <screen>&prompt.root;<command>booth</command> list</screen>
    <para>It should show <literal>&cluster2;</literal> as ticket owner for
      <literal>ticket-nfs</literal> now. All resources that depend on this
     ticket will be started on <literal>&cluster2;</literal>.</para>
   </step>
   <step>
    <para> Before trying to get <literal>&cluster1;</literal> up and running
     again, set <literal>ticket-nfs</literal> to standby on <literal>&cluster2;</literal> (to
     prevent that resources will be started on both sites in parallel): </para>
    <screen>&prompt.root;<command>crm_ticket</command> -t ticket-nfs --standby</screen>
   </step>
   <step>
    <para> Wait for any resources that depend on <literal>ticket-nfs</literal>
     to be stopped or demoted cleanly. </para>
   </step>
   <step>
    <para>Get <literal>&cluster1;</literal> up and running again. If you want
     to keep the resources running on site <literal>&cluster2;</literal>,
     revoke <literal>ticket-nfs</literal> from <literal>&cluster1;</literal>: </para>
    <screen>&prompt.root;<command>booth</command> revoke -s 192.168.201.100 ticket-nfs</screen>
   </step>
   <step>
    <para>Log in to site <literal>&cluster2;</literal> and put
      <literal>ticket-nfs</literal> into active mode again:</para>
    <screen>&prompt.root;<command>crm_ticket</command> -t ticket-nfs --activate</screen>
    <para>All resources that depend on this ticket will be started on
      <literal>&cluster2;</literal> again. </para>
   </step>
  </procedure>
</example>
 </sect1>
 <xi:include href="geo_manage_hawk2_i.xml"/>
</chapter>
