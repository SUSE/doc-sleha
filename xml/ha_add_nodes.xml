<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-ha-add-nodes" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>Adding more nodes</title>
  <info>
    <abstract>
      <para>

      </para>
    </abstract>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker></dm:bugtracker>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  </info>

  <sect1 xml:id="sec-ha-install-node-join-bootstrap">
  <!-- Duplicated from xml/article_installation.xml -->
    <title>Adding nodes with <command>crm cluster join</command></title>
    <para>
      You can add more nodes to the cluster with the <command>crm cluster join</command> bootstrap script.
      The script only needs access to an existing cluster node, and completes the basic setup
      on the current machine automatically.
    </para>
    <para>
      For more information, run the <command>crm cluster join --help</command> command.
    </para>
    <procedure xml:id="pro-ha-install-node-join-bootstrap">
    <title>Adding nodes with <command>crm cluster join</command></title>
    <step>
      <para>
        Log in to a node as &rootuser;, or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Start the bootstrap script:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            If you set up the first node as &rootuser;, you can run this command with
            no additional parameters:
          </para>
<screen>&prompt.root;<command>crm cluster join</command></screen>
        </listitem>
        <listitem>
          <para>
            If you set up the first node as a <command>sudo</command> user, you must
            specify the user and node with the <option>-c</option> option:
          </para>
<screen>&prompt.user;<command>sudo crm cluster join -c <replaceable>USER</replaceable>@&node1;</command></screen>
        </listitem>
        <listitem>
          <para>
            If you set up the first node as a <command>sudo</command> user with SSH agent forwarding,
            use the following command:
          </para>
<screen>&prompt.user;<command>sudo --preserve-env=SSH_AUTH_SOCK crm cluster join --use-ssh-agent -c <replaceable>USER</replaceable>@&node1;</command></screen>
        </listitem>
      </itemizedlist>
      <para>
        If NTP is not configured to start at boot time, a message
        appears. The script also checks for a hardware watchdog device.
        You are warned if none is present.
      </para>
    </step>
    <step>
      <para>
      If you did not already specify <systemitem class="server">&node1;</systemitem>
      with <option>-c</option>, you will be prompted for the IP address of the first node.
      </para>
    </step>
    <step>
      <para>
        If you did not already configure passwordless SSH access between
        both machines, you will be prompted for the password of the first node.
      </para>
      <para>
        After logging in to the specified node, the script copies the
        &corosync; configuration, configures SSH and &csync;,
        brings the current machine online as a new cluster node, and
        starts the service needed for &hawk2;.
      </para>
    </step>
    </procedure>
    <para>
      Repeat this procedure for each node. You can check the status of the cluster at any time
      with the <command>crm status</command> command, or by logging in to &hawk2; and navigating to
      <menuchoice><guimenu>Status</guimenu><guimenu>Nodes</guimenu></menuchoice>.
    </para>
  </sect1>

  <sect1 xml:id="sec-ha-install-node-manual">
    <title>Adding nodes manually</title>
    <para>

    </para>
  </sect1>

  <sect1 xml:id="sec-ha-installation-autoyast">
  <title>Adding nodes with &ay;</title>

  <para>
   After you have installed and set up a two-node cluster, you can extend the
   cluster by cloning existing nodes with &ay; and adding the clones to the cluster.
  </para>

   <para>&ay; uses profiles that contains installation and configuration data.
   A profile tells &ay; what to install and how to configure the installed system to
   get a ready-to-use system in the end. This profile can then be used
   for mass deployment in different ways (for example, to clone existing
   cluster nodes).</para>

   <para>
    For detailed instructions on how to use &ay; in various scenarios,
    see the <link
     xlink:href="https://documentation.suse.com/sles-15/html/SLES-all/book-autoyast.html">
     &ayguide; for &sls; &productnumber;</link>.
   </para>

   <important>
    <title>Identical hardware</title>
    <para>
     <xref linkend="pro-ha-installation-clone-node"/> assumes you are rolling
     out &productname; &productnumber; to a set of machines with identical hardware
     configurations.
    </para>
    <para>
     If you need to deploy cluster nodes on non-identical hardware, refer to the
     &deploy; for &sls; &productnumber;,
     chapter <citetitle>Automated Installation</citetitle>, section
     <citetitle>Rule-Based Autoinstallation</citetitle>.
    </para>
   </important>

  <procedure xml:id="pro-ha-installation-clone-node">
   <title>Cloning a cluster node with &ay;</title>
   <step>
    <para>
     Make sure the node you want to clone is correctly installed and
     configured. For details, see the &haquick; or
     <xref linkend="cha-ha-ycluster"/>.
    </para>
   </step>
   <step>
    <para>
     Follow the description outlined in the &sle;
     &productnumber; &deploy; for simple mass
     installation. This includes the following basic steps:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Creating an &ay; profile. Use the &ay; GUI to create and modify
       a profile based on the existing system configuration. In &ay;,
       choose the <guimenu>&ha;</guimenu> module and click the
       <guimenu>Clone</guimenu> button. If needed, adjust the configuration
       in the other modules and save the resulting control file as XML.
      </para>
      <para>
       If you have configured DRBD, you can select and clone this module in
       the &ay; GUI, too.
      </para>
     </step>
     <step>
      <para>
       Determining the source of the &ay; profile and the parameter to
       pass to the installation routines for the other nodes.
      </para>
     </step>
     <step>
      <para>
       Determining the source of the &sls; and &productname;
       installation data.
      </para>
     </step>
     <step>
      <para>
       Determining and setting up the boot scenario for autoinstallation.
      </para>
     </step>
     <step>
      <para>
       Passing the command line to the installation routines, either by
       adding the parameters manually or by creating an
       <filename>info</filename> file.
      </para>
     </step>
     <step>
      <para>
       Starting and monitoring the autoinstallation process.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <para>
   After the clone has been successfully installed, execute the following
   steps to make the cloned node join the cluster:
  </para>

  <procedure xml:id="pro-ha-installation-clone-start">
   <title>Bringing the cloned node online</title>
   <step>
    <para>
     Transfer the key configuration files from the already configured nodes
     to the cloned node with &csync; as described in
     <xref linkend="sec-ha-installation-setup-csync2"/>.
    </para>
   </step>
   <step>
    <para>
     To bring the node online, start the cluster services on the cloned
     node as described in <xref linkend="sec-ha-installation-start"/>.
    </para>
   </step>
  </procedure>

  <para>
   The cloned node now joins the cluster because the
   <filename>/etc/corosync/corosync.conf</filename> file has been applied to
   the cloned node via &csync;. The CIB is automatically synchronized
   among the cluster nodes.
  </para>
 </sect1>

</chapter>
