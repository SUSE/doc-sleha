<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<!--
   toms 2015-10-26: After it has been clarified that the crmsh cluster
      scripts are the preferred method for cluster resources, move the
      contents into art_sle_ha_nfs_quick.xml and remove this file.

      At the time of writing, it was added as it was not sure which
      route (manual or with cluster scripts) should we take.

   To make the cluster scripts fly, the following bugs should be fixed:
    bsc#951132, bsc#951564, bsc#951954, bsc#952002, bsc#952234,
    bsc#952428, bsc#952444, bsc#952581, bsc#952600, bsc#952670,
    bsc#952775
-->

<sect1 version="5.0" xml:id="sec-haqs-nfs-clusterscript"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>Setting up NFS server with cluster scripts</title>
  <para>This section describes the initial configuration of a highly
    available NFS export in the context of the &pace; cluster manager
    with &crmshell; cluster scripts. Furthermore, it is assumed that
    you create DRBD on empty disks (or partitions).
  </para>
  <para>Cluster scripts provide an integrated way to perform common
    tasks across the cluster. This section uses cluster scripts extensively by
    performing the <command>crm script run</command> command. Use the
    <command>verify</command> subcommand to check your arguments before you
    execute it. For example, to verify the <literal>sbd</literal> primitive
    and its parameters, run:
  </para>
  <screen>&prompt.root;<command>crm</command> script verify sbd id=sdb.&node1; node=178325803 sbd_device=/dev/sdb</screen>

  <para>For more information refer to <xref linkend="sec-ha-manual-config-clusterscripts"/>.</para>

<!--
  <table xml:id="tab-haqs-params">
    <title>Overview of parameters for cluster scripts</title>
    <tgroup cols="3">
      <colspec colname="c1" colwidth="1*"/>
      <colspec colname="c2" colwidth="2*"/>
      <colspec colname="c3" colwidth="2*"/>
      <thead>
        <row>
          <entry>Parameter</entry>
          <entry>Value for &node1;</entry>
          <entry>Value for &node2;</entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry align="center" namest="c1" nameend="c3"><emphasis role="strong">DRBD</emphasis></entry>
        </row>
        <row>
          <entry align="right">DRBD resource name</entry>
          <entry align="center" namest="c2" nameend="c3">
            <systemitem class="resource">nfs</systemitem>
          </entry>
        </row>
        <row>
          <entry align="right">Adress:Port</entry>
          <entry><systemitem class="ipaddress">10.0.42.1:&drbd.port;</systemitem></entry>
          <entry><systemitem class="ipaddress">10.0.42.2:&drbd.port;</systemitem></entry>
        </row>
        <row>
          <entry align="right">Device</entry>
          <entry align="center" namest="c2" nameend="c3"><filename>/dev/drbd0</filename></entry>
        </row>
        <row>
          <entry align="right">Disk</entry>
          <entry align="center" namest="c2" nameend="c3"><filename>/dev/sda1</filename></entry>
        </row>
        <row>
          <entry align="right">Meta-disk</entry>
          <entry align="center" namest="c2" nameend="c3"><systemitem>internal</systemitem></entry>
        </row>
        <!-\- ================================================= -\->
        <row>
          <entry align="center" namest="c1" nameend="c3"><emphasis role="strong">LVM</emphasis></entry>
        </row>
        <row>
          <entry align="right">DRBD resource path</entry>
          <entry align="center" namest="c2" nameend="c3"><filename>/dev/drbd/by-res/nfs/0</filename></entry>
        </row>
        <row>
          <entry align="right">LVM volume group name</entry>
          <entry align="center" namest="c2" nameend="c3"><systemitem>nfs</systemitem></entry>
        </row>
        <row>
          <entry align="right">File system</entry>
          <entry align="center" namest="c2" nameend="c3"><systemitem>ext3</systemitem></entry>
        </row>
        <row>
          <entry align="right">File system</entry>
          <entry align="center" namest="c2" nameend="c3"><systemitem>ext3</systemitem></entry>
        </row>
        <!-\- ================================================= -\->
        <row>
          <entry align="center" namest="c1" nameend="c3"><emphasis role="strong">File System Resources</emphasis></entry>
        </row>
        <row>
          <entry align="right">Devices</entry>
          <entry align="center" namest="c2" nameend="c3"><filename>/dev/nfs/sales</filename>, <filename>/dev/nfs/devel</filename></entry>
        </row>
        <!-\- ================================================= -\->
        <row>
          <entry align="center" namest="c1" nameend="c3"><emphasis role="strong">NFS export resources</emphasis></entry>
        </row>
        <row>
          <entry align="right">Rootfs directory</entry>
          <entry align="center" namest="c2" nameend="c3"><filename>/srv/nfs</filename></entry>
        </row>
        <row>
          <entry align="right"><quote>sales</quote> directory</entry>
          <entry align="center" namest="c2" nameend="c3"><filename>/srv/nfs/sales</filename></entry>
        </row>
        <row>
          <entry align="right"><quote>develop</quote> directory</entry>
          <entry align="center" namest="c2" nameend="c3"><filename>/srv/nfs/devel</filename></entry>
        </row>
        <!-\- ================================================= -\->
        <row>
          <entry align="center" namest="c1" nameend="c3"><emphasis role="strong">Floating IP address</emphasis></entry>
        </row>
        <row>
          <entry align="right">IP address</entry>
          <entry align="center" namest="c2" nameend="c3"><systemitem class="ipaddress">10.9.9.180</systemitem></entry>
        </row>
      </tbody>
    </tgroup>
  </table>
-->

  <!-- Prerequisites:
  * Initial Cluster Setup (id=sec_ha_quick_nfs_initial_setup)
  * Creating a Basic Pacemaker Configuration (id=sec_ha_quick_nfs_initial_pacemaker)
  -->

  <sect2 xml:id="sec-haqs-nfs-sbd">
    <title>SBD device</title>
    <para>This setup uses the most simple SBD (<emphasis>STONITH Based
        Death</emphasis>) setup. It is appropriate for clusters
      where all of your data is on the same shared storage. The shared
      storage segment <emphasis>must not</emphasis> use host-based RAID,
      LVM2, nor DRBD*. See <xref linkend="cha-ha-storage-protect"/> and
        <xref linkend="cha-ha-fencing"/> for further details. Proceed as
      follows: </para>

    <procedure>
      <step>
        <para>Determine a dedicated device or partition as SBD device.</para>
      </step>
      <step>
        <para>Get the identifier of your node where this SBD device should
          be run:</para>
        <screen>&prompt.root;<command>crm</command> configure show type:node
node <emphasis role="strong">178325803</emphasis>: &node1;
node 178326059: &node2;</screen>
        <para>In this case, our primary node is &node1;, so the
          identifier is <literal>178325803</literal>.</para>
      </step>
      <step>
        <para>Run the <command>sbd</command> cluster script:</para>
        <screen>&prompt.root;<command>crm</command> script run sbd id=sdb.&node1; node=178325803 sbd_device=/dev/sdb</screen>
      </step>
    </procedure>
    <para>After you have finished this procedure, <command>crm status</command>
      gives you this result:</para>
    <screen>sdb.&node1;   (stonith:external/sbd): Started &node1;</screen>
  </sect2>

  <sect2 xml:id="sec-haqs-nfs-drbd-config">
    <title>DRBD configuration</title>
    <para>This section explains, how to create the DRBD configuration file
      and to synchronizes it to all nodes:</para>
    <procedure xml:id="pro-haqs-nfs-prep-drbd">
      <step>
        <para> Create the file <filename>/etc/drbd.d/nfs.res</filename>
          with the following contents and replace
            <filename>/dev/sda1</filename> with the correct partition. </para>
        <screen>resource nfs {
    device /dev/drbd0;
    disk /dev/sda1;
    meta-disk internal;
    on &node1; {
      address 10.0.42.1:&drbd.port;;
    }
    on &node2; {
      address 10.0.42.2:&drbd.port;;
    }
}</screen>
        <para>If you have different partitions on each node, remove the
          line with the <literal>disk</literal> keyword, insert them on
          both nodes after the <literal>address</literal> line, and
          replace it with your correct partitions.</para>
      </step>
      <step>
        <para> Open <filename>/etc/csync2/csync2.cfg</filename> and
          check, if the following two lines exist: </para>
        <screen>include /etc/drbd.conf;
include /etc/drbd.d;</screen>
        <para> If not, add them to the file. </para>
      </step>
      <step>
        <para> Copy the file to the other nodes: </para>
        <screen>&prompt.root;<command>csync2</command> -xv</screen>
        <para> For information about &csync;, refer to <xref
            linkend="sec-ha-installation-setup-csync2"/>. </para>
      </step>
    </procedure>
  </sect2>

  <sect2 xml:id="sec-haqs-nfs-lvm-config">
    <title>LVM configuration</title>
    <para>To use LVM with DRBD, it is necessary to change some options
      in the LVM configuration file
        (<filename>/etc/lvm/lvm.conf</filename>) and to remove stale
      cache entries on the nodes. This can be done manually (see the
      following procedure) or use &yast;'s DRBD module. In the &yast;
      DRBD module, to make the automatically change inactive, disable
      the checkbox <guimenu>Modify LVM Device filter
        Automatically</guimenu> and change filter manually. In the
        <guimenu>LVM Configuration</guimenu> entry you can enable or
      disable the LVM cache. </para>
    <para>To remove the cache manually, do the following:</para>
    <procedure>
      <step>
        <para> Open <filename>/etc/lvm/lvm.conf</filename> on your
          primary node in a text editor. </para>
      </step>
      <step>
        <para>Search for the line starting with
            <literal>filter</literal> and edit it as follows: </para>
        <screen>filter = [ "r|/dev/sda.*|" ]</screen>
        <para> This masks the underlying block device from the list of
          devices LVM scans for Physical Volume signatures. This way,
          LVM is instructed to read Physical Volume signatures from DRBD
          devices, rather than from the underlying backing block
          devices. </para>
        <para> However, if you are using LVM
            <emphasis>exclusively</emphasis> on your DRBD devices, then
          you may also specify the LVM filter as such: </para>
        <screen>filter = [ "a|/dev/drbd.*|", "r|.*|" ]</screen>
      </step>
      <step>
        <para> In addition, disable the LVM cache by setting: </para>
        <screen>write_cache_state = 0</screen>
      </step>
      <step>
        <para> Save your changes to the file. </para>
      </step>
      <step>
        <para> Delete <filename>/etc/lvm/cache/.cache</filename> to
          remove any stale cache entries. </para>
      </step>
      <step>
        <para> Use &csync; to replicate these changes to peer node.
        </para>
      </step>
    </procedure>
  </sect2>

  <sect2 xml:id="sec-haqs-nfs-drbd-setup">
    <title>DRBD setup</title>
    <para>This section assumes that you have created the DRBD configuration file
      from <xref linkend="sec-haqs-nfs-drbd-config"/>. It explains how you
      finalize the DRBD setup by keeping the DRBD nodes in sync.</para>
    <!-- toms 2015-10-27: Not sure which one is the best solution -->
    <!--
      <para>In case you have used a disk with DRBD before and want to erase its
      metainformation, use this command:
    </para>
    <screen>&prompt.root;<command>dd</command> if=/dev/zero of=<replaceable>DEVICE</replaceable> bs=1M count=128
&prompt.root;<command>drbdmeta</command> /dev/drbd0 v08 /dev/sda internal wipe-md</screen>
    -->

    <procedure>
     <step>
      <para>If you use a firewall in your cluster, open port
            &drbd.port; in your firewall configuration. </para>
     </step>
     <step>
      <para>
       Execute the following commands on <emphasis>both</emphasis> nodes (in
       our example, &node1; and &node2;):
      </para>
<screen>&prompt.root;<command>drbdadm</command> create-md nfs
&prompt.root;<command>drbdadm</command> up nfs</screen>
      <para>
       This initializes the meta data storage and has to be done only once.
       The disk state from <command>drbdadm status</command> is
        <literal>Inconsistent</literal> and is expected at this
        point.
      </para>
     </step>
     <step>
       <para>Create a new UUID on &node1; to shorten the initial resyncronisation of
          DRBD resource:</para>
       <screen>&prompt.root;<command>drbdadm</command> new-current-uuid --zeroout-devices nfs/0</screen>
     </step>
     <step>
       <para>Switch to &node1; and make this side primary:</para>
       <screen>&prompt.root;<command>drbdadm</command> primary nfs</screen>
       <para>To enforce synchronization, use the <option>--force</option>
         option after <command>primary</command>.
       </para>
     </step>
      <step>
       <para>Watch the DRBD status by entering the following on each node:</para>
       <screen>&prompt.root;<command>cat</command> /proc/drbd</screen>
       <remark>toms 2016-08-04: try to avoid /proc/drbd</remark>
       <para> You should get something like this: </para>
          <screen>[... version string omitted ...]
 m:res  cs         ro                   ds                         p  mounted  fstype
 0:r0   Connected  Secondary/Secondary  Inconsistent/Inconsistent  C
[... further lines omitted ...]</screen>
     </step>
     <step>
      <para>
        Start the resync process on your intended primary node (&node1; in this case):
      </para>
<screen>&prompt.root;<command>drbdadm</command> -- --overwrite-data-of-peer primary nfs</screen>
     </step>
    </procedure>
    <para>After you have finished this procedure, the status in the
      <literal>ds</literal> row (<quote>disk status</quote>) must be
      <literal>UpToDate</literal> on both nodes:
      <filename>/proc/drbd</filename>
    </para>
    <screen>&prompt.root;<command>cat</command> /proc/drbd
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:524236 nr:0 dw:0 dr:525148 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0</screen>
  </sect2>

  <sect2 xml:id="sec-haqs-nfs-drbd-res">
    <title>DRBD resource</title>
    <para>It is necessary to configure a DRBD resource to hold
      your data. This resource will act as the physical volume of an LVM
      volume group to be created later. This section assums that the LVM
      volume group is to be called <literal>nfs</literal>. Hence, the
      DRBD resource uses that same name.</para>
    <procedure>
      <step>
        <para>Load the DRBD kernel module to make the proc file
          <filename>/proc/drbd</filename> available:</para>
        <screen>&prompt.root;<command>modprobe</command> drbd</screen>
      </step>
      <step>
        <para>If you want to have the kernel module for DRBD permanent,
          follow these steps: </para>
        <substeps>
          <step>
            <step>
              <para>Add the following line in <filename>/etc/csync2/csync2.cfg</filename>:</para>
              <screen>include /etc/dracut.conf.d/drbd.conf</screen>
            </step>
            <para>Create a file
                <filename>/etc/dracut.conf.d/drbd.conf</filename>. The
                <filename class="extension">.conf</filename> extension
              is required, but you can freely choose the base name.
            </para>
          </step>
          <step>
            <para>Insert your DRBD driver without the <filename
                class="extension">.ko</filename> extension and save the
              file: </para>
            <screen># DRBD configuration
add_drivers+=drbd</screen>
          </step>
          <step>
            <para>Use &csync; to replicate these changes to peer node.</para>
          </step>
          <step>
            <para>Build the initrd:</para>
            <screen>&prompt.root;<command>mkinitrd</command></screen>
          </step>
          <step>
            <para>Reboot.</para>
          </step>
        </substeps>
      </step>
      <step>
        <para>Create a DRBD resource:</para>
        <screen>&prompt.root;<command>crm</command> script run drbd id=drbd drbd_resource=nfs drbdconf=/etc/drbd.d/nfs.res</screen>
      </step>
    </procedure>

    <para>After you have finished this procedure, <command>crm status</command>
      gives you this result:</para>
    <screen>Master/Slave Set: ms-drbd [drbd]
     Masters: [ &node1; ]
     Slaves: [ &node2; ]</screen>
  </sect2>

  <sect2 xml:id="sec-haqs-nfs-lvm">
    <title>LVM</title>
    <para>After you have successfully setup DRBD and LVM configuration
      files and the DRBD resource, now you can prepare the physical
      volume, create an LVM volume group with logical volumes and create
      file systems on the logical volumes:</para>
    <important>
      <title>Automatic synchronization</title>
      <para>
        Execute all of the following steps only on the node where your
        resource is currently in the primary role. It is
        <emphasis>not</emphasis> necessary to repeat the commands on the DRBD
        peer node as the changes are automatically synchronized.
      </para>
    </important>
    <procedure>
      <step>
        <para> Initialize the DRBD resource as an LVM physical
          volume:</para>
        <screen>&prompt.root;<command>pvcreate</command> /dev/drbd/by-res/nfs/0
  Physical volume "/dev/drbd/by-res/nfs/0" successfully created</screen>
      </step>
      <step>
        <para> Create an LVM volume group that includes this physical
          volume: </para>
        <screen>&prompt.root;<command>vgcreate</command> nfs /dev/drbd/by-res/nfs/0
  Volume group "nfs" successfully created</screen>
      </step>
      <step>
        <para> Create logical volumes in the volume group. This example
          assumes two logical volumes of 20 GB each, named
            <literal>sales</literal> and <literal>devel</literal>: </para>
        <screen>&prompt.root;<command>lvcreate</command> -n sales -L 20G nfs
  Logical volume "sales" created.
&prompt.root;<command>lvcreate</command> -n devel -L 20G nfs
  Logical volume "devel" created.</screen>
      </step>
      <step>
        <para> Activate the volume group:</para>
        <screen>&prompt.root;<command>vgchange</command> -ay nfs</screen>
      </step>
      <step>
        <para>Create file systems on the new logical volumes. This
          example assumes <literal>ext3</literal> as the file system
          type: </para>
        <screen>&prompt.root;<command>mkfs.ext3</command> /dev/nfs/sales
&prompt.root;<command>mkfs.ext3</command> /dev/nfs/devel</screen>
      </step>
      <step>
        <para>Finally, create an LVM resource:</para>
        <screen>&prompt.root;<command>crm</command> script run lvm id=lvm volgrpname=nfs</screen>
      </step>
    </procedure>
    <remark>toms 2015-10-28: should we explain/use lvmdiskscan, pvscan,
      vgscan, or lvscan?
    </remark>
    <para>After you have finished this procedure, check the physical
      volumes, volume groups, and logical volumes with:</para>
    <screen>&prompt.root;<command>pvdisplay</command>
&prompt.root;<command>vgdisplay</command>
&prompt.root;<command>lvdisplay</command></screen>
    <para>As DRBD is active/passive, you will find the LVM devices only
      on the first, primary node.</para>
  </sect2>

  <sect2 xml:id="sec-haqs-nfs-fs">
    <title>File system resources</title>
    <para>This section creates the respective file systems for
        <literal>sales</literal> and <literal>devel</literal>  and
      groups them:</para>
    <procedure>
      <step>
        <para>Create the <quote>sales</quote> file system resource:</para>
        <screen>&prompt.root;<command>crm</command> script run filesystem id=fs-sales device=/dev/nfs/sales directory=/srv/nfs/sales fstype=ext3</screen>
      </step>
      <step>
        <para>Create the <quote>devel</quote> file system resource:</para>
        <screen>&prompt.root;<command>crm</command> script run filesystem id=fs-devel device=/dev/nfs/devel directory=/srv/nfs/devel fstype=ext3</screen>
      </step>
      <step>
        <para>Group the LVM, sales, and devel resources:</para>
        <screen>&prompt.root;<command>crm</command> configure group g-nfs lvm fs-sales fs-devel</screen>
      </step>
    </procedure>
    <para>After you have finished this procedure, check the resource
      group with <command>crm status</command>:</para>
    <screen>Resource Group: g-nfs
     lvm        (ocf:heartbeat:LVM):   Started &node1;
     fs-sales   (ocf:heartbeat:Filesystem):    Started &node1;
     fs-devel   (ocf:heartbeat:Filesystem):    Started &node1;</screen>
  </sect2>
  <sect2 xml:id="sec-haqs-nfs-exportfs">
    <title>NFS export resources</title>
    <para> After your DRBD, LVM, and file system resources are working properly,
    continue with the resources managing your NFS exports. To create highly
    available NFS export resources, use the <literal>exportfs</literal>
    resource type.</para>
    <procedure>
      <step>
        <para>If clients exclusively use NFSv3 to connect to the server, you do not
     need this resource. In this case, continue with <xref
       linkend="st-haqs-nfs-exportfs-non-root"/>, otherwise proceed:</para>
        <substeps>
          <step>
            <para>Create the root of the virtual NFSv4 file system:</para>
            <remark>toms 2015-10-26: How do you derive clientspec?</remark>
            <screen>&prompt.root;<command>crm</command> script run exportfs id=exportfs-root directory=/srv/nfs options="rw,crossmnt" clientspec="10.161.60.0/255.255.255.0" fsid=0</screen>
          </step>
          <step>
            <para>Clone the resource:</para>
            <screen>&prompt.root;<command>crm</command> configure clone cl-exportfs-root exportfs-root</screen>
            <para>This resource does not hold any actual NFS-exported data, merely the
              empty directory (<filename>/srv/nfs</filename>) that the other NFS
              exports are mounted into. Since there is no shared data involved
              here, we can safely <emphasis>clone</emphasis> this resource.</para>
          </step>
          <step>
            <para> Since any data should be exported only on nodes where
              this clone has been properly started, add the following
              constraints to the configuration:</para>
            <screen>&prompt.root;<command>crm</command> order o-root_before_nfs Mandatory: cl-exportfs-root g-nfs:start
&prompt.root;<command>crm</command> colocation c-nfs_on_root inf: g-nfs cl-exportfs-root</screen>
          </step>
          <step>
            <para>Check the output of the <command>exportfs -v</command>
              command to verify this.</para>
          </step>
        </substeps>
      </step>
      <step xml:id="st-haqs-nfs-exportfs-non-root">
        <para>Create NFS exports for <quote>sales</quote> and <quote>develop</quote>:</para>
        <screen>&prompt.root;<command>crm</command> script run exportfs id=export-sales directory=/srv/nfs/sales/ \
    options="rw,mountpoint" clientspec="10.161.60.0/255.255.255.0" fsid=1
&prompt.root;<command>crm</command> script run exportfs id=export-devel directory=/srv/nfs/devel/ \
    options="rw,mountpoint" clientspec="10.161.60.0/255.255.255.0" fsid=2</screen>
      </step>
      <step>
        <para>
       After you have created these resources, add them to the existing
       <literal>g-nfs</literal> resource group:
      </para>
<screen>&prompt.root;<command>crm</command> configure edit g-nfs</screen>
        <para>It should look like this:</para>
        <screen>group g-nfs lvm fs-sales fs-devel exportfs-sales exportfs-devel</screen>
      </step>
    </procedure>
    <para>After you have finished this procedures, <command>exportfs</command> <option>-v</option>
      gives you this result:</para>
    <screen>/srv/nfs/devel  10.161.60.0/255.255.255.0(rw,wdelay,root_squash,no_subtree_check,fsid=2,mountpoint,sec=sys,rw,root_squash,no_all_squash)
/srv/nfs        10.161.60.0/255.255.255.0(rw,wdelay,crossmnt,root_squash,no_subtree_check,fsid=0,sec=sys,rw,root_squash,no_all_squash)
/srv/nfs/sales  10.161.60.0/255.255.255.0(rw,wdelay,root_squash,no_subtree_check,fsid=1,mountpoint,sec=sys,rw,root_squash,no_all_squash)</screen>
  </sect2>

  <sect2 xml:id="ex-haqs-nfs-virtualip">
    <title>Resource for floating IP address</title>
    <para>To enable smooth and seamless failover, your NFS clients will
      be connecting to the NFS service via a floating cluster IP
      address, rather than via any of the hosts' physical IP
      addresses.</para>
    <procedure>
      <step>
        <para>Create a virtual, floating IPv4 address:</para>
        <screen>&prompt.root;<command>crm</command> script run virtual-ip id=vip-nfs ip=10.9.9.180 cidr_netmask=24</screen>
      </step>
      <step>
        <para> Add the IP address to the resource group (like you did
          with the <literal>exportfs</literal> resources): </para>
<screen>&prompt.root;<command>crm</command> configure edit g-nfs</screen>
     <para>
      This is the final setup of the resource group:
     </para>
<screen>group g-nfs \
  lvm fs-devel fs-sales \
  exportfs-devel exportfs-sales \
  vip-nfs</screen>
      </step>
    </procedure>
    <para>
      Confirm that the cluster IP is running correctly:
     </para>
<screen>&prompt.root;<command>ip</command> address show</screen>
     <para>
      The cluster IP should be added as a <literal>secondary</literal>
      address to whatever interface is connected to the
      <literal>10.9.9.0/24</literal> subnet.
     </para>

    <remark>toms 2015-10-27: Add full example of the "crm configure show"
      output.</remark>
  </sect2>
</sect1>
