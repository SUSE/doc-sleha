<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-gfs2">
 <title>GFS2</title>
 <info>
      <abstract>
        <para>
    Global File System 2 or GFS2 is a shared disk file system for Linux
    computer clusters. GFS2 allows all nodes to have direct concurrent
    access to the same shared block storage. GFS2 has no disconnected
    operating mode, and no client or server roles. All nodes in a GFS2
    cluster function as peers. GFS2 supports up to 32 cluster nodes. Using
    GFS2 in a cluster requires hardware to allow access to the shared
    storage, and a lock manager to control access to the storage.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>

    <sect1 xml:id="sec-ha-gfs2-utils">
  <title>GFS2 packages and management utilities</title>

  <para>
   To use GFS2, make sure
   <package>gfs2-utils</package> and a matching
   <package>gfs2-kmp-*</package> package for your
   Kernel is installed on each node of the cluster.
  </para>

  <para>
   The <package>gfs2-utils</package> package provides
   the following utilities for management of GFS2 volumes. For syntax
   information, see their man pages.
  </para>
  <variablelist>
   <varlistentry>
    <term>fsck.gfs2</term>
    <listitem>
     <para>
      Checks the file system for errors and optionally repairs errors.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>gfs2_jadd</term>
    <listitem>
     <para>
      Adds additional journals to a GFS2 file system.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>gfs2_grow</term>
    <listitem>
     <para>
      Grow a GFS2 file system.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mkfs.gfs2</term>
    <listitem>
     <para>
      Create a GFS2 file system on a device, usually a shared device or
      partition.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>tunegfs2</term>
    <listitem>
     <para>
      Allows viewing and manipulating the GFS2 file system parameters such
      as <varname>UUID</varname>, <varname>label</varname>,
      <varname>lockproto</varname> and <varname>locktable</varname>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ha-gfs2-create-service">
  <title>Configuring GFS2 services and a &stonith; resource</title>

  <para>
   Before you can create GFS2 volumes, you must configure DLM and a
   &stonith; resource.
  </para>

  <procedure xml:id="pro-gfs2-stonith">
   <title>Configuring a &stonith; resource</title>
   <note>
    <title>&stonith; device needed</title>
    <para>
     You need to configure a fencing device. Without a &stonith;
     mechanism (like <literal>external/sbd</literal>) in place the
     configuration fails.
    </para>
   </note>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Create an SBD partition as described in
     <xref linkend="pro-ha-storage-protect-sbd-create"/>.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm configure</command>.
    </para>
   </step>
   <step>
    <para>
     Configure <literal>external/sbd</literal> as the fencing device:
    </para>
<screen>&prompt.crm.conf;<command>primitive sbd_stonith stonith:external/sbd \
    params pcmk_delay_max=30 meta target-role="Started"</command></screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
   </step>
  </procedure>

  <para>
    For details on configuring the resource for DLM, see <xref
     linkend="sec-ha-storage-generic-dlm-config"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-gfs2-create">
  <title>Creating GFS2 volumes</title>

  <para>
   After you have configured DLM as cluster resources as described in
   <xref linkend="sec-ha-gfs2-create-service"/>, configure your system to
   use GFS2 and create GFS2 volumes.
  </para>

  <note>
   <title>GFS2 volumes for application and data files</title>
   <para>
    We recommend that you generally store application files and data files
    on different GFS2 volumes. If your application volumes and data volumes
    have different requirements for mounting, it is mandatory to store them
    on different volumes.
   </para>
  </note>

  <para>
   Before you begin, prepare the block devices you plan to use for your GFS2
   volumes. Leave the devices as free space.
  </para>

  <para>
   Then create and format the GFS2 volume with the
   <command>mkfs.gfs2</command> as described in
   <xref linkend="pro-gfs2-volume"/>. The most important parameters for the
   command are listed below. For
   more information and the command syntax, refer to the
   <command>mkfs.gfs2</command> man page.
  </para>
  <variablelist>
   <varlistentry>
    <term>Lock Protocol Name (<option>-p</option>)</term>
    <listitem>
     <para>
      The name of the locking protocol to use. Acceptable locking
      protocols are lock_dlm (for shared storage) or, if you are using GFS2
      as a local file system (1 node only), you can specify the
      lock_nolock protocol. If this option is not specified, lock_dlm
      protocol is assumed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Lock Table Name (<option>-t</option>)</term>
    <listitem>
     <para>
      The lock table field appropriate to the lock module you are using. It is
      <replaceable>clustername</replaceable>:<replaceable>fsname</replaceable>.
      The <replaceable>clustername</replaceable> value must match that in the
      cluster configuration file, &corosync.conf;. Only members of this
      cluster are permitted to use this file system.
      The <replaceable>fsname</replaceable> value is a unique file system name used
      to distinguish this GFS2 file system from others created (1 to 16 characters).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Number of Journals (<option>-j</option>)</term>
    <listitem>
     <para>
      The number of journals for gfs2_mkfs to create. You need at least
      one journal per machine that will mount the file system. If this
      option is not specified, one journal is created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <procedure xml:id="pro-gfs2-volume">
   <title>Creating and formatting a GFS2 volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.gfs2</command>
     utility. For information about the syntax for this command, refer to
     the <command>mkfs.gfs2</command> man page.
    </para>
    <para>
     For example, to create a new GFS2 file system
     that supports up to 32 cluster nodes,
     use the following command:
    </para>
<screen>&prompt.root;<command>mkfs.gfs2 -t hacluster:mygfs2 -p lock_dlm -j 32 /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
    <para>
     The <systemitem class="server">hacluster</systemitem> name relates to
     the entry <option>cluster_name</option> in the file
     <filename>/etc/corosync/corosync.conf</filename> (this is the default).
    </para>
    <para>
      Always use a stable device name (for example:
      <filename>/dev/disk/by-id/scsi-ST2000DM001-0123456_Wabcdefg</filename>).
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-gfs2-mount">
  <title>Mounting GFS2 volumes</title>

  <para>
   You can either mount a GFS2 volume manually or with the cluster manager,
   as described in <xref linkend="pro-gfs2-mount-cluster"/>.
  </para>

  <procedure xml:id="pro-gfs2-mount-manual">
   <title>Manually mounting a GFS2 volume</title>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the
     <command>mount</command> command.
    </para>
   </step>
  </procedure>

  <warning>
   <title>Manually mounted GFS2 devices</title>
   <para>
    If you mount the GFS2 file system manually for testing purposes, make
    sure to unmount it again before starting to use it via cluster resources.
   </para>
  </warning>

  <procedure xml:id="pro-gfs2-mount-cluster">
   <title>Mounting a GFS2 volume with the cluster manager</title>
   <para>
    To mount a GFS2 volume with the &ha; software, configure an OCF file
    system resource in the cluster. The following procedure uses the
    <command>crm</command> shell to configure the cluster resources.
    Alternatively, you can also use &hawk2; to configure the resources.
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm configure</command>.
    </para>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the GFS2 file system on every node in the
     cluster:
    </para>
<screen>&prompt.crm.conf;<command>primitive gfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>" directory="/mnt/shared" fstype="gfs2" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Started"</command></screen>
   </step>
   <step>
    <para>
     Add the <literal>gfs2-1</literal> primitive
     to the <literal>g-storage</literal> group you created in
     <xref linkend="pro-dlm-resources"/>.
    </para>
<screen>&prompt.crm.conf;<command>modgroup g-storage add gfs2-1</command></screen>
    <para>
     Because of the base group's internal colocation and ordering,
     the <systemitem class="resource">gfs2-1</systemitem> resource can only start
     on nodes that also have a <literal>dlm</literal> resource already running.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-migrate-ocfs2-gfs2">
  <title>Migrating from &ocfs; to GFS2</title>
  <para>
    &ocfs; is deprecated in &productname; &productnumber;. It will not be supported in future
    releases.
  </para>
  <note>
    <title>No reflink support in GFS2</title>
    <para>
      Unlike &ocfs;, GFS2 does <emphasis>not</emphasis> support the reflink feature.
    </para>
  </note>
  <para>
    This procedure shows one method of migrating from &ocfs; to GFS2. It assumes
    you have a single &ocfs; volume that is part of the <literal>g-storage</literal> group.
  </para>
  <para>
    The steps for preparing new block storage and backing up data depend on your specific setup.
    See the relevant documentation if you need more details.
  </para>
  <para>
    You only need to perform this procedure on <emphasis>one</emphasis> of the cluster nodes.
  </para>
  <warning>
    <title>Test this procedure first</title>
    <para>
      Thoroughly test this procedure in a test environment before performing it in a
      production environment.
    </para>
  </warning>

  <procedure xml:id="pro-migrate-ocfs2-gfs2">
   <title>Migrating from &ocfs; to GFS2</title>
   <step>
    <para>
     Prepare a block device for the GFS2 volume.
    </para>
    <para>
      To check the disk space required, run <command>df -h</command> on the &ocfs; mount point.
      For example:
    </para>
<screen>&prompt.root;<command>df -h /mnt/shared/</command>
Filesystem     Size  Used Avail Use% Mounted on
/dev/sdb        10G  2.3G  7.8G  23% /mnt/shared</screen>
    <para>
      Make a note of the disk name under <literal>Filesystem</literal>. This will be useful
      later to help check if the migration worked.
    </para>
    <note>
      <title>&ocfs; disk usage</title>
      <para>
        Because some &ocfs; system files can hold disk space instead of returning it to the
        global bitmap file, the actual disk usage might be less than the amount shown in the
        <command>df -h</command> output.
      </para>
    </note>
   </step>
   <step>
    <para>
      Install the GFS2 packages on all nodes in the cluster. You can do this on all nodes at once
      with the following command:
    </para>
<screen>&prompt.root;<command>crm cluster run "zypper install -y gfs2-utils gfs2-kmp-default"</command></screen>
  </step>
  <step>
    <para>
      Create and format the GFS2 volume using the <command>mkfs.gfs2</command> utility. For information
      about the syntax for this command, refer to the <command>mkfs.gfs2</command> man page.
     </para>
     <tip>
       <title>Key differences between <command>mkfs.ofs2</command> and <command>mkfs.gfs2</command></title>
       <itemizedlist>
         <listitem>
           <para>
             &ocfs; uses <literal>-C</literal> to specify the cluster size and <literal>-b</literal>
             to specify the block size. GFS2 also specifies the block size with <literal>-b</literal>,
             but has no cluster size setting so does not use <literal>-C</literal>.
           </para>
         </listitem>
         <listitem>
           <para>
             &ocfs; specifies the number of nodes with <literal>-N</literal>. GFS2 specifies the
             number of nodes with <literal>-j</literal>.
           </para>
         </listitem>
       </itemizedlist>
      </tip>
     <para>
      For example, to create a new GFS2 file system that supports up to 32 cluster nodes,
      use the following command:
     </para>
<screen>&prompt.root;<command>mkfs.gfs2 -t <replaceable>CLUSTERNAME</replaceable>:mygfs2 -p lock_dlm -j 32 /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
     <para>
      <literal>CLUSTERNAME</literal> must be the same as the entry <option>cluster_name</option>
      in the file <filename>/etc/corosync/corosync.conf</filename>. The default is
      <literal>hacluster</literal>.
     </para>
     <para>
       Always use a stable device name for devices shared between cluster nodes.
     </para>
   </step>
   <step>
    <para>
     Put the cluster into maintenance mode:
    </para>
<screen>&prompt.root;<command>crm maintenance on</command></screen>
   </step>
   <step>
     <para>
       Back up the &ocfs; volume's data.
     </para>
   </step>
   <step>
     <para>
       Start the &crmshell; in interactive mode:
     </para>
<screen>&prompt.root;<command>crm configure</command></screen>
   </step>
   <step>
    <para>
      Delete the &ocfs; resource:
    </para>
<screen>&prompt.crm;<command>delete ocfs2-1</command></screen>
  </step>
  <step>
    <para>
      Mount the GFS2 file system on every node in the cluster, using the <emphasis>same</emphasis>
      mount point that was used for the &ocfs; file system:
    </para>
<screen>&prompt.crm;<command>primitive gfs2-1 ocf:heartbeat:Filesystem \
params device="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>" directory="/mnt/shared" fstype="gfs2" \
op monitor interval="20" timeout="40" \
op start timeout="60" op stop timeout="60" \
meta target-role="Started"</command></screen>
  </step>
  <step>
    <para>
      Add the GFS2 primitive to the <literal>g-storage</literal> group:
    </para>
<screen>&prompt.crm;<command>modgroup g-storage add gfs2-1</command></screen>
  </step>
  <step>
    <para>
      Review your changes with <command>show</command>.
    </para>
  </step>
  <step>
    <para>
      If everything is correct, submit your changes with <command>commit</command> and leave the
      crm live configuration with <command>quit</command>.
    </para>
  </step>
   <step>
     <para>
       Take the cluster out of maintenance mode:
     </para>
<screen>&prompt.root;<command>crm maintenance off</command></screen>
   </step>
   <step>
     <para>
       Check the status of the cluster, with expanded details about the group
       <literal>g-storage</literal>:
     </para>
<screen>&prompt.root;<command>crm status detail</command></screen>
     <para>
      The group should now include the primitive resource <literal>gfs2-1</literal>.
     </para>
   </step>
   <step>
     <para>
       Run <command>df -h</command> on the mount point to make sure the disk name changed:
     </para>
<screen>&prompt.root;<command>df -h /mnt/shared/</command>
Filesystem     Size  Used Avail Use% Mounted on
/dev/sdc        10G  290M  9.8G   3% /mnt/shared</screen>
     <para>
      If the output shows the wrong disk, the new <literal>gfs2-1</literal> resource might be
      restarting. This issue should resolve itself if you wait a short time and then run the
      command again.
     </para>
   </step>
   <step>
    <para>
      Restore the data from the backup to the GFS2 volume.
    </para>
    <note>
      <title>GFS2 disk usage</title>
      <para>
        Even after restoring the data, the GFS2 volume might not use as much disk space as the
        &ocfs; volume.
      </para>
    </note>
  </step>
  <step>
    <para>
      To make sure that the data appears correctly, check the contents of the mount point.
      For example:
    </para>
<screen>&prompt.root;<command>ls -l /mnt/shared/</command></screen>
    <para>
      You can also run this command on other nodes to make sure the data is being shared correctly.
    </para>
  </step>
  <step>
    <para>
      If required, you can now remove the &ocfs; disk.
    </para>
  </step>
  </procedure>
 </sect1>
</chapter>
