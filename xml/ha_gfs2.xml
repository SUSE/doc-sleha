<?xml version="1.0"?>
<!DOCTYPE chapter [
  <!ENTITY % entities SYSTEM "generic-entities.ent">
  %entities;
]>

<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-gfs2">
 <title>GFS2</title>
 <info xmlns:d="http://docbook.org/ns/docbook">
      <abstract>
        <para>
    Global File System 2 or GFS2 is a shared disk file system for Linux
    computer clusters. GFS2 allows all nodes to have direct concurrent
    access to the same shared block storage. GFS2 has no disconnected
    operating mode, and no client or server roles. All nodes in a GFS2
    cluster function as peers. GFS2 supports up to 32 cluster nodes. Using
    GFS2 in a cluster requires hardware to allow access to the shared
    storage, and a lock manager to control access to the storage.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    <revhistory xml:id="rh-cha-ha-gfs2">
   <revision>
     <date>2025-05-13</date>
     <revdescription>
       <para/>
     </revdescription>
   </revision>
 </revhistory>
</info>

    <sect1 xml:id="sec-ha-gfs2-utils">
  <title>GFS2 packages and management utilities</title>

  <para>
   To use GFS2, make sure
   <package>gfs2-utils</package> and a matching
   <package>gfs2-kmp-*</package> package for your
   Kernel is installed on each node of the cluster.
  </para>

  <para>
   The <package>gfs2-utils</package> package provides
   the following utilities for management of GFS2 volumes. For syntax
   information, see their man pages.
  </para>
  <variablelist>
   <varlistentry>
    <term>fsck.gfs2</term>
    <listitem>
     <para>
      Checks the file system for errors and optionally repairs errors.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>gfs2_jadd</term>
    <listitem>
     <para>
      Adds additional journals to a GFS2 file system.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>gfs2_grow</term>
    <listitem>
     <para>
      Grow a GFS2 file system.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mkfs.gfs2</term>
    <listitem>
     <para>
      Create a GFS2 file system on a device, usually a shared device or
      partition.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>tunegfs2</term>
    <listitem>
     <para>
      Allows viewing and manipulating the GFS2 file system parameters such
      as <varname>UUID</varname>, <varname>label</varname>,
      <varname>lockproto</varname> and <varname>locktable</varname>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ha-gfs2-create-service">
  <title>Configuring GFS2 services and a &stonith; resource</title>

  <para>
   Before you can create GFS2 volumes, you must configure DLM and a
   &stonith; resource.
  </para>

  <procedure xml:id="pro-gfs2-stonith">
   <title>Configuring a &stonith; resource</title>
   <note>
    <title>&stonith; device needed</title>
    <para>
     You need to configure a fencing device. Without a &stonith;
     mechanism (like <literal>external/sbd</literal>) in place the
     configuration fails.
    </para>
   </note>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Create an SBD partition as described in
     <xref linkend="pro-ha-storage-protect-sbd-create"/>.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm configure</command>.
    </para>
   </step>
   <step>
    <para>
     Configure <literal>external/sbd</literal> as the fencing device:
    </para>
<screen>&prompt.crm.conf;<command>primitive sbd_stonith stonith:external/sbd \
    params pcmk_delay_max=30 meta target-role="Started"</command></screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
   </step>
  </procedure>

  <para>
    For details on configuring the resource for DLM, see <xref linkend="sec-ha-storage-generic-dlm-config"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-gfs2-create">
  <title>Creating GFS2 volumes</title>

  <para>
   After you have configured DLM as cluster resources as described in
   <xref linkend="sec-ha-gfs2-create-service"/>, configure your system to
   use GFS2 and create GFS2 volumes.
  </para>

  <note>
   <title>GFS2 volumes for application and data files</title>
   <para>
    We recommend that you generally store application files and data files
    on different GFS2 volumes. If your application volumes and data volumes
    have different requirements for mounting, it is mandatory to store them
    on different volumes.
   </para>
  </note>

  <para>
   Before you begin, prepare the block devices you plan to use for your GFS2
   volumes. Leave the devices as free space.
  </para>

  <para>
   Then create and format the GFS2 volume with the
   <command>mkfs.gfs2</command> as described in
   <xref linkend="pro-gfs2-volume"/>. The most important parameters for the
   command are listed below. For
   more information and the command syntax, refer to the
   <command>mkfs.gfs2</command> man page.
  </para>
  <variablelist>
   <varlistentry>
    <term>Lock Protocol Name (<option>-p</option>)</term>
    <listitem>
     <para>
      The name of the locking protocol to use. Acceptable locking
      protocols are lock_dlm (for shared storage) or, if you are using GFS2
      as a local file system (1 node only), you can specify the
      lock_nolock protocol. If this option is not specified, lock_dlm
      protocol is assumed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Lock Table Name (<option>-t</option>)</term>
    <listitem>
     <para>
      The lock table field appropriate to the lock module you are using. It is
      <replaceable>clustername</replaceable>:<replaceable>fsname</replaceable>.
      The <replaceable>clustername</replaceable> value must match that in the
      cluster configuration file, &corosync.conf;. Only members of this
      cluster are permitted to use this file system.
      The <replaceable>fsname</replaceable> value is a unique file system name used
      to distinguish this GFS2 file system from others created (1 to 16 characters).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Number of Journals (<option>-j</option>)</term>
    <listitem>
     <para>
      The number of journals for gfs2_mkfs to create. You need at least
      one journal per machine that will mount the file system. If this
      option is not specified, one journal is created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <procedure xml:id="pro-gfs2-volume">
   <title>Creating and formatting a GFS2 volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.gfs2</command>
     utility. For information about the syntax for this command, refer to
     the <command>mkfs.gfs2</command> man page.
    </para>
    <para>
     For example, to create a new GFS2 file system
     that supports up to 32 cluster nodes,
     use the following command:
    </para>
<screen>&prompt.root;<command>mkfs.gfs2 -t <replaceable>CLUSTERNAME:FSNAME</replaceable> -p lock_dlm -j 32 /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
    <para>
      <literal>CLUSTERNAME</literal> must be the same as the entry <option>cluster_name</option>
      in the file <filename>/etc/corosync/corosync.conf</filename>. The default is
      <literal>hacluster</literal>.
    </para>
    <para>
      <literal>FSNAME</literal> is used to identify this file system and must therefore be unique.
    </para>
    <para>
      Always use a stable device name for devices shared between cluster nodes.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-gfs2-mount">
  <title>Mounting GFS2 volumes</title>

  <para>
   You can either mount a GFS2 volume manually or with the cluster manager,
   as described in <xref linkend="pro-gfs2-mount-cluster"/>.
  </para>
  <para>
    To mount multiple GFS2 volumes, see <xref linkend="pro-gfs2-mount-multiple"/>.
   </para>

  <procedure xml:id="pro-gfs2-mount-manual">
   <title>Manually mounting a GFS2 volume</title>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the
     <command>mount</command> command.
    </para>
   </step>
  </procedure>

  <warning>
   <title>Manually mounted GFS2 devices</title>
   <para>
    If you mount the GFS2 file system manually for testing purposes, make
    sure to unmount it again before starting to use it via cluster resources.
   </para>
  </warning>

  <procedure xml:id="pro-gfs2-mount-cluster">
   <title>Mounting a GFS2 volume with the cluster manager</title>
   <para>
    To mount a GFS2 volume with the &ha; software, configure a file
    system resource in the cluster. The following procedure uses the
    <command>crm</command> shell to configure the cluster resources.
    Alternatively, you can also use &hawk2; to configure the resources as
    described in <xref linkend="sec-ha-gfs2-rsc-hawk2"/>.
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm configure</command>.
    </para>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the GFS2 file system on every node in the
     cluster:
    </para>
<screen>&prompt.crm.conf;<command>primitive gfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>" directory="/mnt/shared" fstype="gfs2" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Started"</command></screen>
   </step>
   <step>
    <para>
     Add the <literal>gfs2-1</literal> primitive
     to the <literal>g-storage</literal> group you created in
     <xref linkend="pro-dlm-resources"/>.
    </para>
<screen>&prompt.crm.conf;<command>modgroup g-storage add gfs2-1</command></screen>
    <para>
     Because of the base group's internal colocation and ordering,
     the <systemitem class="resource">gfs2-1</systemitem> resource can only start
     on nodes that also have a <literal>dlm</literal> resource already running.
    </para>
    <important>
      <title>Do not use a group for multiple GFS2 resources</title>
      <para>
       Adding multiple GFS2 resources to a group creates a dependency between
       the GFS2 volumes. For example, if you created a group with
       <command>crm configure group g-storage dlm gfs2-1 gfs2-2</command>,
       then stopping <literal>gfs2-1</literal> also stops
       <literal>gfs2-2</literal>, and starting <literal>gfs2-2</literal>
       also starts <literal>gfs2-1</literal>.
      </para>
      <para>
       To use multiple GFS2 resources in the cluster, use colocation and order
       constraints as described in <xref linkend="pro-gfs2-mount-multiple"/>.
      </para>
     </important>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
   </step>
  </procedure>
  <procedure xml:id="pro-gfs2-mount-multiple">
    <title>Mounting multiple GFS2 volumes with the cluster resource manager</title>
    <para>
     To mount multiple GFS2 volumes in the cluster, configure a file system
     resource for each volume, and colocate them with the <literal>dlm</literal>
     resource you created in <xref linkend="pro-dlm-multiple-resources"/>.
    </para>
    <important role="compact">
     <para>
      Do <emphasis>not</emphasis> add multiple GFS2 resources to a group with DLM.
      This creates a dependency between the GFS2 volumes. For example, if
      <literal>gfs2-1</literal> and <literal>gfs2-2</literal> are in the same group,
      then stopping <literal>gfs2-1</literal> also stops <literal>gfs2-2</literal>.
     </para>
    </important>
    <step>
     <para>
      Log in to a node as &rootuser; or equivalent.
     </para>
    </step>
    <step>
     <para>
      Run <command>crm configure</command>.
     </para>
    </step>
    <step>
     <para>
      Create the primitive for the first GFS2 volume:
     </para>
<screen>&prompt.crm.conf;<command>primitive gfs2-1 Filesystem \
  params directory="/srv/gfs2-1" fstype=gfs2 device="/dev/disk/by-id/<replaceable>DEVICE_ID1</replaceable>" \
  op monitor interval=20 timeout=40 \
  op start timeout=60 interval=0 \
  op stop timeout=60 interval=0</command></screen>
    </step>
    <step>
     <para>
      Create the primitive for the second GFS2 volume:
     </para>
<screen>&prompt.crm.conf;<command>primitive gfs2-2 Filesystem \
  params directory="/srv/gfs2-2" fstype=gfs2 device="/dev/disk/by-id/<replaceable>DEVICE_ID2</replaceable>" \
  op monitor interval=20 timeout=40 \
  op start timeout=60 interval=0 \
  op stop timeout=60 interval=0</command></screen>
    </step>
    <step>
     <para>
      Clone the GFS2 resources so that they can run on all nodes:
     </para>
<screen>&prompt.crm.conf;<command>clone cl-gfs2-1 gfs2-1 meta interleave=true</command>
&prompt.crm.conf;<command>clone cl-gfs2-2 gfs2-2 meta interleave=true</command></screen>
    </step>
    <step>
     <para>
      Add a colocation constraint for both GFS2 resources so that they can only
      run on nodes where DLM is also running:
     </para>
<screen>&prompt.crm.conf;<command>colocation col-gfs2-with-dlm inf: ( cl-gfs2-1 cl-gfs2-2 ) cl-dlm</command></screen>
    </step>
    <step>
     <para>
      Add an order constraint for both GFS2 resources so that they can only
      start after DLM is already running:
     </para>
<screen>&prompt.crm.conf;<command>order o-dlm-before-gfs2 Mandatory: cl-dlm ( cl-gfs2-1 cl-gfs2-2 )</command></screen>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with <command>commit</command>
      and leave the crm live configuration with <command>quit</command>.
     </para>
    </step>
   </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-gfs2-rsc-hawk2">
  <title>Configuring GFS2 resources with &hawk2;</title>
  <para>
   Instead of configuring the DLM and the file system resource for GFS2
   manually with the &crmshell;, you can also use the GFS2 template
   in &hawk2;'s <guimenu>Setup Wizard</guimenu>.
  </para>
  <important>
   <title>Differences between manual configuration and &hawk2;</title>
   <para>
    The GFS2 template in the <guimenu>Setup Wizard</guimenu> does
    <emphasis>not</emphasis> include the configuration of a &stonith;
    resource. If you use the wizard, you still need to create an SBD
    device on the shared storage and configure a &stonith; resource as
    described in <xref linkend="pro-gfs2-stonith"/>.
   </para>
   <para>
    Using the GFS2 template in the &hawk2; <guimenu>Setup
    Wizard</guimenu> also leads to a slightly different resource
    configuration than the manual configuration described in
    <xref linkend="pro-dlm-resources"/> and
    <xref linkend="pro-gfs2-mount-cluster"/>.
   </para>
  </important>
  <procedure xml:id="pro-ha-gfs2-rsc-hawk">
   <title>Configuring GFS2 resources with &hawk2;'s <guimenu>Wizard</guimenu></title>
   <step>
    <para>
     Log in to &hawk2;:
    </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
   </step>
   <step>
    <para>
     In the left navigation bar, select <menuchoice><guimenu>Configuration</guimenu>
    <guimenu>Wizards</guimenu></menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Expand the <guimenu>File System</guimenu> category and select
     <literal>GFS2 File System (Cloned)</literal>.
    </para>
   </step>
   <step>
    <para>
     Follow the instructions on the screen. If you need information about an
     option, click it to display a short help text in &hawk2;. After the last
     configuration step, <guimenu>Verify</guimenu> the values you have entered.
    </para>
    <para>
     The wizard displays the configuration snippet that will be applied to
     the CIB and any additional changes, if required.
    </para>
    <figure>
     <title>&hawk2; summary screen of GFS2 CIB changes</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk2-wizard-gfs2-verify.png" width="70%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk2-wizard-gfs2-verify.png" width="70%"/>
      </imageobject>
      <textobject role="description">
        <phrase>
          A summary screen showing the changes to be applied to the CIB for
          the GFS2 resource.
        </phrase>
      </textobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     Check the proposed changes. If everything is according to your wishes,
     apply the changes.
    </para>
    <para>
     A message on the screen shows if the action has been successful.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-migrate-ocfs2-gfs2">
  <title>Migrating from &ocfs; to GFS2</title>
  <para>
    &ocfs; is deprecated in &productname; &productnumber;. It will not be supported in future
    releases.
  </para>
  <note>
    <title>No reflink support in GFS2</title>
    <para>
      Unlike &ocfs;, GFS2 does <emphasis>not</emphasis> support the reflink feature.
    </para>
  </note>
  <para>
    This procedure shows one method of migrating from &ocfs; to GFS2. It assumes
    you have a single &ocfs; volume that is part of the <literal>g-storage</literal> group.
  </para>
  <para>
    The steps for preparing new block storage and backing up data depend on your specific setup.
    See the relevant documentation if you need more details.
  </para>
  <para>
    You only need to perform this procedure on <emphasis>one</emphasis> of the cluster nodes.
  </para>
  <warning>
    <title>Test this procedure first</title>
    <para>
      Thoroughly test this procedure in a test environment before performing it in a
      production environment.
    </para>
  </warning>

  <procedure xml:id="pro-migrate-ocfs2-gfs2">
   <title>Migrating from &ocfs; to GFS2</title>
   <step>
    <para>
     Prepare a block device for the GFS2 volume.
    </para>
    <para>
      To check the disk space required, run <command>df -h</command> on the &ocfs; mount point.
      For example:
    </para>
<screen>&prompt.root;<command>df -h /mnt/shared/</command>
Filesystem     Size  Used Avail Use% Mounted on
/dev/sdb        10G  2.3G  7.8G  23% /mnt/shared</screen>
    <para>
      Make a note of the disk name under <literal>Filesystem</literal>. This will be useful
      later to help check if the migration worked.
    </para>
    <note>
      <title>&ocfs; disk usage</title>
      <para>
        Because some &ocfs; system files can hold disk space instead of returning it to the
        global bitmap file, the actual disk usage might be less than the amount shown in the
        <command>df -h</command> output.
      </para>
    </note>
   </step>
   <step>
    <para>
      Install the GFS2 packages on all nodes in the cluster. You can do this on all nodes at once
      with the following command:
    </para>
<screen>&prompt.root;<command>crm cluster run "zypper install -y gfs2-utils gfs2-kmp-default"</command></screen>
  </step>
  <step>
    <para>
      Create and format the GFS2 volume using the <command>mkfs.gfs2</command> utility. For information
      about the syntax for this command, refer to the <command>mkfs.gfs2</command> man page.
     </para>
     <tip>
       <title>Key differences between <command>mkfs.ofs2</command> and <command>mkfs.gfs2</command></title>
       <itemizedlist>
         <listitem>
           <para>
             &ocfs; uses <literal>-C</literal> to specify the cluster size and <literal>-b</literal>
             to specify the block size. GFS2 also specifies the block size with <literal>-b</literal>,
             but has no cluster size setting so does not use <literal>-C</literal>.
           </para>
         </listitem>
         <listitem>
           <para>
             &ocfs; specifies the number of nodes with <literal>-N</literal>. GFS2 specifies the
             number of nodes with <literal>-j</literal>.
           </para>
         </listitem>
       </itemizedlist>
      </tip>
     <para>
      For example, to create a new GFS2 file system that supports up to 32 cluster nodes,
      use the following command:
     </para>
<screen>&prompt.root;<command>mkfs.gfs2 -t <replaceable>CLUSTERNAME:FSNAME</replaceable> -p lock_dlm -j 32 /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
     <para>
       <literal>CLUSTERNAME</literal> must be the same as the entry <option>cluster_name</option>
       in the file <filename>/etc/corosync/corosync.conf</filename>. The default is
       <literal>hacluster</literal>.
     </para>
     <para>
       <literal>FSNAME</literal> is used to identify this file system and must therefore be unique.
     </para>
     <para>
       Always use a stable device name for devices shared between cluster nodes.
     </para>
   </step>
   <step>
    <para>
     Put the cluster into maintenance mode:
    </para>
<screen>&prompt.root;<command>crm maintenance on</command></screen>
   </step>
   <step>
     <para>
       Back up the &ocfs; volume's data.
     </para>
   </step>
   <step>
     <para>
       Start the &crmshell; in interactive mode:
     </para>
<screen>&prompt.root;<command>crm configure</command></screen>
   </step>
   <step>
    <para>
      Delete the &ocfs; resource:
    </para>
<screen>&prompt.crm;<command>delete ocfs2-1</command></screen>
  </step>
  <step>
    <para>
      Mount the GFS2 file system on every node in the cluster, using the <emphasis>same</emphasis>
      mount point that was used for the &ocfs; file system:
    </para>
<screen>&prompt.crm;<command>primitive gfs2-1 ocf:heartbeat:Filesystem \
params device="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>" directory="/mnt/shared" fstype="gfs2" \
op monitor interval="20" timeout="40" \
op start timeout="60" op stop timeout="60" \
meta target-role="Started"</command></screen>
  </step>
  <step>
    <para>
      Add the GFS2 primitive to the <literal>g-storage</literal> group:
    </para>
<screen>&prompt.crm;<command>modgroup g-storage add gfs2-1</command></screen>
  </step>
  <step>
    <para>
      Review your changes with <command>show</command>.
    </para>
  </step>
  <step>
    <para>
      If everything is correct, submit your changes with <command>commit</command> and leave the
      crm live configuration with <command>quit</command>.
    </para>
  </step>
   <step>
     <para>
       Take the cluster out of maintenance mode:
     </para>
<screen>&prompt.root;<command>crm maintenance off</command></screen>
   </step>
   <step>
     <para>
       Check the status of the cluster, with expanded details about the group
       <literal>g-storage</literal>:
     </para>
<screen>&prompt.root;<command>crm status detail</command></screen>
     <para>
      The group should now include the primitive resource <literal>gfs2-1</literal>.
     </para>
   </step>
   <step>
     <para>
       Run <command>df -h</command> on the mount point to make sure the disk name changed:
     </para>
<screen>&prompt.root;<command>df -h /mnt/shared/</command>
Filesystem     Size  Used Avail Use% Mounted on
/dev/sdc        10G  290M  9.8G   3% /mnt/shared</screen>
     <para>
      If the output shows the wrong disk, the new <literal>gfs2-1</literal> resource might be
      restarting. This issue should resolve itself if you wait a short time and then run the
      command again.
     </para>
   </step>
   <step>
    <para>
      Restore the data from the backup to the GFS2 volume.
    </para>
    <note>
      <title>GFS2 disk usage</title>
      <para>
        Even after restoring the data, the GFS2 volume might not use as much disk space as the
        &ocfs; volume.
      </para>
    </note>
  </step>
  <step>
    <para>
      To make sure that the data appears correctly, check the contents of the mount point.
      For example:
    </para>
<screen>&prompt.root;<command>ls -l /mnt/shared/</command></screen>
    <para>
      You can also run this command on other nodes to make sure the data is being shared correctly.
    </para>
  </step>
  <step>
    <para>
      If required, you can now remove the &ocfs; disk.
    </para>
  </step>
  </procedure>
 </sect1>
</chapter>
