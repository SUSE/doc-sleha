<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 version="5.0" xml:id="cha-ha-ycluster">
<?dbfo-need height="20em"?>
 <title>Using the &yast; cluster module</title>
 <info>
  <abstract>
   <para>The &yast; cluster module allows you to set up a cluster manually
    or to modify options for an existing cluster.
   </para>
  </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-installation-terms">
  <title>Definition of terms</title>

  <para>
   Several key terms used in the &yast; cluster module and in this chapter are
   defined below.
  </para>

  <variablelist>
   <varlistentry>
    <term>Bind network address (<systemitem>bindnetaddr</systemitem>)
    </term>
    <listitem>
     <para>
      &def-bindnetaddr; To simplify sharing configuration files across
      the cluster, &corosync; uses network interface netmask to mask only
      the address bits that are used for routing the network. For example,
      if the local interface is <literal>192.168.5.92</literal> with netmask
      <literal>255.255.255.0</literal>, set
      <systemitem>bindnetaddr</systemitem> to
      <literal>192.168.5.0</literal>. If the local interface is
      <literal>192.168.5.92</literal> with netmask
      <literal>255.255.255.192</literal>, set
      <systemitem>bindnetaddr</systemitem> to
      <literal>192.168.5.64</literal>.
     </para>
     <para> If <systemitem>nodelist</systemitem> with
       <systemitem>ringX_addr</systemitem> is explicitly configured in
       <filename>/etc/corosync/corosync.conf</filename>,
       <systemitem>bindnetaddr</systemitem> is not strictly required. </para>
     <note>
      <title>Network address for all nodes</title>
      <para>
       As the same &corosync; configuration is used on all nodes,
       make sure to use a network address as
       <systemitem>bindnetaddr</systemitem>, not the address of a specific
       network interface.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><systemitem class="resource">conntrack</systemitem> Tools</term>
    <listitem>
     <para>
      &def-conntrack; For detailed information, refer to
      <link xlink:href="https://conntrack-tools.netfilter.org/"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&csync;</term>
    <listitem>
     <para>
      &def-csync2; &csync; can handle any number of hosts, sorted into
      synchronization groups. Each synchronization group has its own list of
      member hosts and its include/exclude patterns that define which Ô¨Åles
      should be synchronized in the synchronization group. The groups, the
      host names belonging to each group, and the include/exclude rules for
      each group are specified in the &csync; configuration file,
      <filename>/etc/csync2/csync2.cfg</filename>.
     </para>
     <para>
      For authentication, &csync; uses the IP addresses and pre-shared
      keys within a synchronization group. You need to generate one key file
      for each synchronization group and copy it to all group members.
     </para>
     <para>
      For more information about &csync;, refer to
      <link xlink:href="https://oss.linbit.com/csync2/paper.pdf"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Existing cluster</term>
    <listitem>
     <para>
      &def-existing-cluster;
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
     <term>Heuristics</term>
     <listitem>
       <para>
        &qdevice; supports a set of commands (<quote>heuristics</quote>).
        The commands are executed locally on start-up of cluster services,
        cluster membership change, successful connection to the &qnet; server,
        or, optionally, at regular times.
     </para>
     <para>
        Only if all commands are executed successfully are the heuristics
        considered to have passed; otherwise, they failed. The heuristics' result is
        sent to the &qnet; server, where it is used in calculations to determine
        which partition should be quorate.
       </para>
     </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multicast</term>
    <listitem>
     <para>
      &def-multicast;
     </para>
     <note>
      <title>Switches and multicast</title>
      <para>
       To use multicast for cluster communication, make sure
       your switches support multicast.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-mcastaddr">
    <term>Multicast address (<systemitem>mcastaddr</systemitem>)
   </term>
    <listitem>
     <para>
      &def-mcastaddr; If IPv6 networking is used, node IDs must be
      specified. You can use any multicast address in your private network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multicast port (<systemitem>mcastport</systemitem>)</term>
    <listitem>
     <para>
      &def-mcastport; &corosync; uses two ports: the specified
      <literal>mcastport</literal> for receiving multicast, and
      <literal>mcastport -1</literal> for sending multicast.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
     <term>&qdevice;</term>
     <listitem>
       <para>
        A systemd service (a daemon) on each cluster node running together with &corosync;.
        This is the client of the &qnet; server. Its primary use is to allow a cluster
        to sustain more node failures than standard quorum rules allow.
     </para>
     <para>
        &qdevice; is designed to work with different arbitrators. However, currently,
        only &qnet; is supported.
       </para>
     </listitem>
   </varlistentry>
   <varlistentry>
     <term>&qnet;</term>
     <listitem>
       <para>
        A systemd service (a daemon, the <quote>&qnet; server</quote>), which is not part
        of the cluster. The systemd service provides a vote to the &qdevice; daemon.
     </para>
     <para>
        To improve security, &qnet; can work with TLS for client certificate checking.
       </para>
     </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-rrp">
    <term>Redundant Ring Protocol (RRP)</term>
    <listitem>
     <para>
      &def-rrp; A logical token-passing ring is imposed on all
      participating nodes to deliver messages in a reliable and sorted
      manner. A node is allowed to broadcast a message only if it holds the
      token.
     </para>
     <para>
      When having defined redundant communication channels in &corosync;,
      use RRP to tell the cluster how to use these interfaces. RRP can have
      three modes (<literal>rrp_mode</literal>):
     </para>
     <itemizedlist>
      <listitem>
       <para>
        If set to <literal>active</literal>, &corosync; uses both
        interfaces actively. However, this mode is deprecated.
       </para>
      </listitem>
      <listitem>
       <para>
        If set to <literal>passive</literal>, &corosync; sends messages
        alternatively over the available networks.
       </para>
      </listitem>
      <listitem>
       <para>
        If set to <literal>none</literal>, RRP is disabled.
       </para>
      </listitem>
     </itemizedlist>
     <!--With RRP, two physically separate networks are used for communication. In case one
      network fails, the cluster nodes can still communicate via the other network.-->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Unicast</term>
    <listitem>
     <para>
      &def-unicast;
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec-ha-setup-yast-overview">
   <title>Starting the &yast; <guimenu>Cluster</guimenu> module</title>
   <para>
    Start &yast; and select <menuchoice> <guimenu>&ha;</guimenu>
    <guimenu>Cluster</guimenu> </menuchoice>. Alternatively, start the
    module from the command line:
   </para>
  <screen>&prompt.root;<command>yast2 cluster</command></screen>
  <para>
    If you start the cluster module for the first time, it appears as a
    wizard, guiding you through all the steps necessary for basic setup.
    Otherwise, select the categories on the left panel to access the
    configuration options for each step.
  </para>
  <para>
   The following list shows an overview of the available screens in the
   &yast; cluster module. It also mentions whether the screen contains parameters that
   are <emphasis>required</emphasis> for successful cluster setup or whether its
   parameters are <emphasis>optional</emphasis>. If you are following a guided
   first-time setup, the screens appear in the order shown in this list.
  </para>
  <variablelist>
   <varlistentry>
    <term>Communication channels (required)</term>
    <listitem>
     <para> Allows you to define one or two communication channels for
      communication between the cluster nodes. As transport protocol,
      either use multicast (UDP) or unicast (UDPU). For details, see
      <xref linkend="sec-ha-installation-setup-channels"/>.</para>
     <important>
      <title>Redundant communication paths</title>
      <para>For a supported cluster setup, two or more redundant communication
       paths are required. The preferred way is to use network device bonding.
       If this is impossible, you must define a second communication
       channel in &corosync;.</para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
     <term>&corosync; &qdevice; (optional but recommended for clusters with an even number of nodes)</term>
     <listitem>
       <para>
        Allows you to configure &qdevice; as a client of a &qnet; server to
        participate in quorum decisions. This is recommended for clusters with
        an even number of nodes, and especially for two-node clusters.
        For details, see <xref linkend="sec-ha-installation-setup-qdevice"/>.
       </para>
     </listitem>
   </varlistentry>
   <varlistentry>
      <term>Security (optional but recommended)</term>
      <listitem>
       <para>Allows you to define the authentication settings for the cluster.
        HMAC/SHA1 authentication requires a shared secret used
        to protect and authenticate messages. For details, see
        <xref linkend="sec-ha-installation-setup-security"/>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
       <term>Configure &csync; (optional but recommended)</term>
       <listitem>
     <para>
      &csync; helps you to keep track of configuration changes and to
      keep files synchronized across the cluster nodes. If you are using
      &yast; to set up the cluster for the first time, we strongly recommend
      configuring &csync;. If you do not use &csync;, you must manually copy
      all configuration files from the first node to the rest of the nodes in
      the cluster. For details, see <xref linkend="sec-ha-installation-setup-csync2"/>.
     </para>
       </listitem>
      </varlistentry>
      <varlistentry>
        <term>Configure conntrackd (optional)</term>
        <listitem>
         <para>
          Allows you to configure the user space
          <systemitem class="daemon">conntrackd</systemitem>. Use the conntrack
          tools for <emphasis>stateful</emphasis> packet inspection for iptables.
          For details, see <xref linkend="sec-ha-installation-setup-conntrackd"/>.
         </para>
        </listitem>
      </varlistentry>
   <varlistentry>
    <term>Service (required)</term>
    <listitem>
     <para>
      Allows you to configure the service for bringing the cluster node online.
      Define whether to start the cluster services at boot time and whether to open the
      ports in the firewall that are needed for communication between the nodes.
      For details, see <xref linkend="sec-ha-installation-setup-services"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
   <note>
    <title>Settings in the &yast; <guimenu>Cluster</guimenu> module</title>
     <para>Certain settings in the &yast; cluster module apply only to the
      current node. Other settings may automatically be transferred to all nodes
      with &csync;. Find detailed information about this in the following
      sections.
    </para>
   </note>
  </sect1>

  <sect1 xml:id="sec-ha-installation-setup-channels">
   <title>Defining the communication channels</title>
   <para>
    For successful communication between the cluster nodes, define at least
    one communication channel. All settings defined in the &yast;
    <guimenu>Communication Channels</guimenu>
    screen are written to &corosync.conf;. Find example
    files for a multicast and a unicast setup in
    <filename>/usr/share/doc/packages/corosync/</filename>.
   </para>
   <para>If you are using IPv4 addresses, node IDs are optional. If you are using
    IPv6 addresses, node IDs are required. Instead of specifying IDs manually
    for each node, the &yast; cluster module contains an option to automatically
    generate a unique ID for every cluster node.</para>
    <para>
      As transport protocol, either use multicast (UDP) or unicast (UDPU) as described
      in the following procedures:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          To configure multicast, use <xref linkend="pro-ha-installation-setup-channel1-udp"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          To configure unicast, use <xref linkend="pro-ha-installation-setup-channel1-udpu"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          If you also need to define a second, redundant channel, configure the
          first channel with one of the procedures above, then continue to
         <xref linkend="pro-ha-installation-setup-channel2"/>.
        </para>
      </listitem>
    </itemizedlist>
   <note>
    <title>Public clouds: use unicast</title>
    <para>
     For deploying &productname; on public cloud platforms, use unicast as
     the transport protocol. Multicast is generally not supported by the cloud
     platforms themselves.
    </para>
   </note>

   <procedure xml:id="pro-ha-installation-setup-channel1-udp">
    <title>Defining the first communication channel (multicast)</title>
    <para>
     When using multicast, the same <systemitem>bindnetaddr</systemitem>,
    <systemitem>mcastaddr</systemitem>, and <systemitem>mcastport</systemitem>
     is used for all cluster nodes. All nodes in the cluster know each
     other by using the same multicast address. For different clusters, use
     different multicast addresses.
    <!--taroth 2011-10-26: for the records a statement by lmb: Setting up two or more
     clusters that use the same multicast address, but a different port, also works,
     but is less efficient)-->
    </para>
    <step>
     <para>
       If you are modifying an existing cluster, switch to the
       <guimenu>Communication Channels</guimenu> category.
      </para>
      <para>
        If you are following the initial setup wizard, you do not need to
        switch categories manually.
      </para>
    </step>
    <step>
     <para>
      Set the <guimenu>Transport</guimenu> protocol to
      <literal>Multicast</literal>.
     </para>
    </step>
    <step>
     <para>
      Define the <guimenu>Bind Network Address</guimenu>. Set the value to
      the subnet you will use for cluster multicast.
     </para>
    </step>
    <step>
     <para>
      Define the <guimenu>Multicast Address</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Define the <guimenu>Port</guimenu>.
     </para>
    </step>
    <step>
     <para>
      To automatically generate a unique ID for every cluster node, keep
      <guimenu>Auto Generate Node ID</guimenu> enabled.
     </para>
    </step>
    <step>
     <para>
      Define a <guimenu>Cluster Name</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter the number of <guimenu>Expected Votes</guimenu>. This is
      important for &corosync; to calculate
      <xref linkend="gloss-quorum"/> in case of a partitioned cluster. By
      default, each node has <literal>1</literal> vote. The number of
      <guimenu>Expected Votes</guimenu> must match the number of nodes in
      your cluster.
     </para>
    </step>
    <step>
     <para>
      If you need to define a redundant communication channel in &corosync;,
      continue to <xref linkend="pro-ha-installation-setup-channel2"/>.
     </para>
     <para>
       Otherwise, confirm your changes with <guimenu>Next</guimenu> (setup wizard)
       or <guimenu>Finish</guimenu> (existing cluster).
     </para>
    </step>
   </procedure>

   <figure>
    <title>&yast; <guimenu>Cluster</guimenu>&mdash;multicast configuration</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast_cluster_comm_mcast.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast_cluster_comm_mcast.png" width="75%"/>
     </imageobject>
     <textobject role="description">
      <phrase>The Communication Channels screen shows the settings for configuring the
      &corosync; communication channel or channels. In this example, Multicast is selected.
      A bind network address and a multicast address have been added, but the member
      addresses for each node in the cluster are not required.</phrase>
    </textobject>
    </mediaobject>
   </figure>

   <procedure xml:id="pro-ha-installation-setup-channel1-udpu">
    <title>Defining the first communication channel (unicast)</title>
    <step>
     <para>
       If you are modifying an existing cluster, switch to the
       <guimenu>Communication Channels</guimenu> category.
     </para>
     <para>
      If you are following the initial setup wizard, you do not need
      to switch categories manually.
     </para>
    </step>
      <step>
       <para>
        Set the <guimenu>Transport</guimenu> protocol to
        <literal>Unicast</literal>.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Port</guimenu>.
       </para>
      </step>
      <step>
       <para>
        For unicast communication, &corosync; needs to know the IP
        addresses of all nodes in the cluster. For each node,
        select <guimenu>Add</guimenu> and enter the
        following details:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          <guimenu>IP Address</guimenu>
         </para>
        </listitem>
        <listitem>
         <para>
          <guimenu>Redundant IP Address</guimenu> (only required if you use
          a second communication channel in &corosync;)
         </para>
        </listitem>
        <listitem>
         <para>
          <guimenu>Node ID</guimenu> (only required if the option
          <guimenu>Auto Generate Node ID</guimenu> is disabled)
         </para>
        </listitem>
       </itemizedlist>
       <para>
        To modify or remove any addresses of cluster members, use the
        <guimenu>Edit</guimenu> or <guimenu>Del</guimenu> buttons.
       </para>
      </step>
    <step>
     <para>
      To automatically generate a unique ID for every cluster node keep
      <guimenu>Auto Generate Node ID</guimenu> enabled.
     </para>
    </step>
    <step>
     <para>
      Define a <guimenu>Cluster Name</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter the number of <guimenu>Expected Votes</guimenu>. This is
      important for &corosync; to calculate
      <xref linkend="gloss-quorum"/> in case of a partitioned cluster. By
      default, each node has <literal>1</literal> vote. The number of
      <guimenu>Expected Votes</guimenu> must match the number of nodes in
      your cluster.
     </para>
    </step>
    <step>
     <para>
      If you need to define a redundant communication channel in &corosync;,
      continue to <xref linkend="pro-ha-installation-setup-channel2"/>.
     </para>
     <para>
       Otherwise, confirm your changes with <guimenu>Next</guimenu> (setup wizard)
       or <guimenu>Finish</guimenu> (existing cluster).
     </para>
    </step>
   </procedure>
   <figure>
    <title>&yast; <guimenu>Cluster</guimenu>&mdash;unicast configuration</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast_cluster_comm_ucast.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast_cluster_comm_ucast.png" width="75%"/>
     </imageobject>
     <textobject role="description">
      <phrase>
       The Communication Channels screen shows the settings for configuring the
       &corosync; communication channel or channels. In this example, Unicast is selected.
       A bind network address and a multicast address are not required, but member
       addresses for each node in the cluster must be added.
      </phrase>
    </textobject>
    </mediaobject>
   </figure>

  <procedure xml:id="pro-ha-installation-setup-channel2">
   <title>Defining a redundant communication channel</title>
   <para>
    If network device bonding cannot be used for any reason, the second
    best choice is to define a redundant communication channel (a second
    ring) in &corosync;. That way, two physically separate networks can
    be used for communication. If one network fails, the cluster nodes
    can still communicate via the other network.
   </para>
   <para>The additional communication channel in
    &corosync; forms a second token-passing ring. The first channel you
    configured is the primary ring and gets the ring number
   <literal>0</literal>. The second ring (redundant channel) gets the ring number
    <literal>1</literal>.
   </para>
   <important>
    <title>Redundant rings and <filename>/etc/hosts</filename></title>
    <para> If multiple rings are configured in &corosync;, each node can
     have multiple IP addresses. This needs to be reflected in the
      <filename>/etc/hosts</filename> file of all nodes. </para>
   </important>
   <step>
    <para>
    Configure the first channel as described in <xref linkend="pro-ha-installation-setup-channel1-udp"/>
    or <xref linkend="pro-ha-installation-setup-channel1-udpu"/>.
     </para>
   </step>
   <step>
    <para> Activate <guimenu>Redundant Channel</guimenu>. The redundant channel
     must use the same protocol as the first communication channel you defined.
    </para>
   </step>
   <step>
    <para> If you use multicast, enter the following parameters: the
      <guimenu>Bind Network Address</guimenu> to use, the <guimenu>Multicast
      Address</guimenu> and the <guimenu>Port</guimenu> for the
     redundant channel. </para>
    <para> If you use unicast, define the following parameters: the
      <guimenu>Bind Network Address</guimenu> to use, and the <guimenu>Port</guimenu>.
     Under <guimenu>Member Address</guimenu>, <guimenu>Edit</guimenu> each entry to add
     a <guimenu>Redundant IP</guimenu> for each node that will be part of the cluster.</para>
   </step>
   <step>
    <para>To tell &corosync; how and when to use the different channels,
     select the <guimenu>rrp_mode</guimenu> to use:</para>
    <itemizedlist>
     <listitem>
      <para> If only one communication channel is defined,
        <guimenu>rrp_mode</guimenu> is automatically disabled (value
        <literal>none</literal>).</para>
     </listitem>
     <listitem>
      <para> If set to <literal>active</literal>, &corosync; uses both
       interfaces actively. However, this mode is deprecated.</para>
     </listitem>
     <listitem>
      <para> If set to <literal>passive</literal>, &corosync; sends messages
       alternatively over the available networks. </para>
     </listitem>
    </itemizedlist>
    <para>When RRP is used, &productname; monitors the status of the current
     rings and automatically re-enables redundant rings after faults.</para>
    <para>Alternatively, check the ring status manually with
     <command>corosync-cfgtool</command>. View the available options with
      <option>-h</option>. </para>
   </step>
   <step>
    <para> Confirm your changes with <guimenu>Next</guimenu> (setup wizard)
       or <guimenu>Finish</guimenu> (existing cluster). </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-installation-setup-qdevice">
   <title>Configuring an arbitrator for quorum decisions</title>
   <para>
     &qdevice; and &qnet; participate in quorum decisions. With assistance from
     the arbitrator <systemitem class="daemon">corosync-qnetd</systemitem>,
     <systemitem class="daemon">corosync-qdevice</systemitem> provides a
     configurable number of votes, allowing a cluster to sustain more node
     failures than the standard quorum rules allow. We recommend deploying
     <systemitem class="daemon">corosync-qnetd</systemitem> and
     <systemitem class="daemon">corosync-qdevice</systemitem> for clusters
     with an even number of nodes, and especially for two-node clusters.
     For more information, see <xref linkend="cha-ha-qdevice"/>.
   </para>
   <itemizedlist>
     <title>Requirements</title>
     <listitem>
       <para>
         Before you configure &qdevice;, you must set up a &qnet; server.
         See <xref linkend="sec-ha-qdevice-setup-qnetd"/>.
       </para>
     </listitem>
   </itemizedlist>
   <procedure xml:id="pro-ha-installation-setup-qdevice">
     <title>Configuring &qdevice; and &qnet;</title>
     <step>
       <para>
        If you are modifying an existing cluster, switch to the
        <guimenu>&corosync; &qdevice;</guimenu> category.
      </para>
      <para>
        If you are following the initial setup wizard, you do not need
        to switch categories manually.
      </para>
     </step>
     <step>
       <para>
         Activate <guimenu>Enable &corosync; &qdevice;</guimenu>.
       </para>
     </step>
     <step>
       <para>
         In the <guimenu>&qnet; server host</guimenu> field, enter the IP address
         or host name of the &qnet; server.
       </para>
     </step>
     <step>
       <para>
         Select the mode for <guimenu>TLS</guimenu>:
       </para>
       <itemizedlist>
         <listitem>
           <para>
             Use <guimenu>off</guimenu> if TLS is not required and should not be tried.
           </para>
         </listitem>
         <listitem>
           <para>
             Use <guimenu>on</guimenu> to attempt to connect with TLS, but connect without TLS
             if it is not available.
           </para>
         </listitem>
         <listitem>
           <para>
             Use <guimenu>required</guimenu> to make TLS mandatory. &qdevice; will exit with
             an error if TLS is not available.
           </para>
         </listitem>
       </itemizedlist>
     </step>
     <step>
       <para>
         Accept the default values for <guimenu>Algorithm</guimenu> and
         <guimenu>Tie breaker</guimenu>. If you need to change these values,
         you can do so with the command <command>crm cluster init qdevice</command>
         after you finish setting up the cluster.
       </para>
     </step>
     <step>
       <para>
         Select the <guimenu>Heuristics Mode</guimenu>:
       </para>
       <itemizedlist>
         <listitem>
           <para>
             Use <guimenu>off</guimenu> to disable heuristics.
           </para>
         </listitem>
         <listitem>
           <para>
             Use <guimenu>on</guimenu> to run heuristics on a regular basis, as set by the
             <guimenu>Heuristics Interval</guimenu>.
           </para>
         </listitem>
         <listitem>
           <para>
             Use <guimenu>sync</guimenu> to only run heuristics during startup, when cluster
             membership changes, and on connection to &qnet;.
           </para>
         </listitem>
       </itemizedlist>
     </step>
     <step>
       <para>
         If you set the <guimenu>Heuristics Mode</guimenu> to <guimenu>on</guimenu>
         or <guimenu>sync</guimenu>, add your heuristics commands to the
         <guimenu>Heuristics Executables</guimenu> list:
       </para>
       <substeps>
         <step>
           <para>
             Select <guimenu>Add</guimenu>. A new window opens.
           </para>
         </step>
         <step>
           <para>
             Enter an <guimenu>Execute Name</guimenu> for the command.
           </para>
         </step>
         <step>
           <para>
             Enter the command in the <guimenu>Execute Script</guimenu> field. This can be a
             single command or the path to a script, and can be written in any language
             such as Shell, Python, or Ruby.
           </para>
         </step>
         <step>
           <para>
             Select <guimenu>OK</guimenu> to close the window.
           </para>
         </step>
       </substeps>
     </step>
     <step>
       <para>
         Confirm your changes with <guimenu>Next</guimenu> (setup wizard)
         or <guimenu>Finish</guimenu> (existing cluster).
       </para>
     </step>
   </procedure>
   <figure xml:id="fig-ha-installation-setup-qdevice">
     <title>&yast; <guimenu>Cluster</guimenu>&mdash;&corosync; &qdevice;</title>
     <mediaobject>
       <imageobject role="fo">
         <imagedata fileref="yast_cluster_qdevice.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
         <imagedata fileref="yast_cluster_qdevice.png" width="75%"/>
       </imageobject>
       <textobject role="description">
         <phrase>The &corosync; &qdevice; screen shows the settings for configuring &qdevice;.
         The <guimenu>Enable &corosync; &qdevice;</guimenu> check box is activated,
         and the cursor is in the <guimenu>&qnet; server host</guimenu> field.</phrase>
       </textobject>
     </mediaobject>
   </figure>
 </sect1>

  <sect1 xml:id="sec-ha-installation-setup-security">
   <title>Defining authentication settings</title>
   <para>
    To define the authentication settings for the cluster, you can use HMAC/SHA1
    authentication. This requires a shared secret used
    to protect and authenticate messages. The authentication key (password)
    you specify is used on all nodes in the cluster.
   </para>
  <procedure xml:id="pro-ha-installation-setup-security">
   <title>Enabling secure authentication</title>
   <step>
     <para>
      If you are modifying an existing cluster, switch to the
      <guimenu>Security</guimenu> category.
     </para>
     <para>
      If you are following the initial setup wizard, you do not need
      to switch categories manually.
     </para>
   </step>
   <step>
    <para> Activate <guimenu>Enable Security Auth</guimenu>. </para>
   </step>
   <step>
    <para> For a newly created cluster, select <guimenu>Generate Auth Key
      File</guimenu>. An authentication key is created and written to
      <filename>/etc/corosync/authkey</filename>. </para>
    <para> If you want the current machine to join an existing cluster, do not
     generate a new key file. Instead, copy the
      <filename>/etc/corosync/authkey</filename> from one of the nodes to the
     current machine (either manually or with &csync;). </para>
   </step>
   <step>
    <para> Confirm your changes with <guimenu>Next</guimenu> (setup wizard)
       or <guimenu>Finish</guimenu> (existing cluster). &yast; writes the configuration to
      <filename>/etc/corosync/corosync.conf</filename>. </para>
   </step>
  </procedure>

   <figure>
    <title>&yast; <guimenu>Cluster</guimenu>&mdash;security</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast_cluster_security.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast_cluster_security.png" width="75%"/>
     </imageobject>
     <textobject role="description">
      <phrase>
        The Security screen shows the settings for configuring &corosync;'s authentication.
        The Enable Security Auth checkbox is activated, and you can select the type of
        Crypto Hash and Crypto Cipher. There is also an option to click Generate Auth Key File.
      </phrase>
    </textobject>
    </mediaobject>
   </figure>
  </sect1>

  <sect1 xml:id="sec-ha-installation-setup-csync2">
   <title>Configuring &csync; to synchronize files</title>
   <para>
    Instead of copying the configuration files to all nodes
    manually, use the <command>csync2</command> tool for replication across
    all nodes in the cluster. &csync; helps you to keep track of configuration changes
    and to keep files synchronized across the cluster nodes:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You can define a list of files that are important for operation.
     </para>
    </listitem>
    <listitem>
     <para>
      You can show changes to these files (against the other cluster nodes).
     </para>
    </listitem>
    <listitem>
     <para>
      You can synchronize the configured files with a single command.
     </para>
    </listitem>
    <listitem>
     <para>
      With a simple shell script in <filename>~/.bash_logout</filename>, you
      can be reminded about unsynchronized changes before logging out of the
      system.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Find detailed information about &csync; at
    <link xlink:href="https://oss.linbit.com/csync2/"/> and
    <link xlink:href="https://oss.linbit.com/csync2/paper.pdf"/>.
   </para>
   <important>
    <title>Pushing synchronization after any changes</title>
    <para>
      &csync; only pushes changes. It does <emphasis>not</emphasis> continuously
      synchronize files between the machines. Each time you update files that need
      to be synchronized, you need to push the changes to the other machines.
      Using <command>csync2</command> to push changes is described later, after
      the cluster configuration with &yast; is complete.
    </para>
   </important>
   <procedure xml:id="pro-ha-installation-setup-csync2-yast">
    <title>Configuring &csync; with &yast;</title>
    <step>
     <para>
      If you are modifying an existing cluster, switch to the
      <guimenu>Configure &csync;</guimenu> category.
     </para>
     <para>
      If you are following the initial setup wizard, you do not need
      to switch categories manually.
     </para>
    </step>
    <step>
     <para> To specify the synchronization group, select <guimenu>Add</guimenu>
      in the <guimenu>Sync Host</guimenu> group and enter the local host names
      of all nodes in your cluster. For each node, you must use exactly the
      strings that are returned by the <command>hostname</command> command. </para>
     <tip>
      <title>Host name resolution</title>
      <para> If host name resolution does not work properly in your
       network, you can also specify a combination of host name and IP address
       for each cluster node. To do so, use the string
        <replaceable>HOSTNAME@IP</replaceable> such as
        <literal>&node1;@&wsIip;</literal>, for example. &csync;
       then uses the IP addresses when connecting. </para>
     </tip>
    </step>
    <step xml:id="step-csync2-generate-key">
     <para> Click <guimenu>Generate Pre-Shared-Keys</guimenu> to create a key
      file for the synchronization group. The key file is written to
       <filename>/etc/csync2/key_hagroup</filename>. After it has been created,
      it must be copied manually to all members of the cluster. </para>
    </step>
    <step>
     <para> To populate the <guimenu>Sync File</guimenu> list with the files
      that usually need to be synchronized among all nodes, select <guimenu>Add
       Suggested Files</guimenu>. </para>
    </step>
    <step>
     <para> To <guimenu>Edit</guimenu>, <guimenu>Add</guimenu> or
       <guimenu>Remove</guimenu> files from the list of files to be synchronized,
      use the respective buttons. You must enter the absolute path for each
      file. </para>
    </step>
    <step>
     <para> Activate &csync; by selecting <guimenu>Turn &csync;
       ON</guimenu>. This enables &csync; to start
      automatically at boot time. </para>
    </step>
    <step>
     <para>Confirm your changes with <guimenu>Next</guimenu> (setup wizard)
       or <guimenu>Finish</guimenu> (existing cluster). &yast; writes the &csync;
      configuration to <filename>/etc/csync2/csync2.cfg</filename>.</para>
    </step>
   </procedure>
   <figure>
    <title>&yast; <guimenu>Cluster</guimenu>&mdash;&csync;</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast_cluster_sync.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast_cluster_sync.png" width="75%"/>
     </imageobject>
     <textobject role="description">
      <phrase>
        The Configure &csync; screen shows the settings to configure the transfer of files
        between nodes. On the left is the Sync Host list, where you need to add all
        the cluster nodes. On the right is the Sync File list, which is automatically
        populated with configuration files. There are also options to select
        <literal>Generate Pre-Shared-Keys</literal> and <literal>Turn &csync; ON</literal>.
      </phrase>
    </textobject>
    </mediaobject>
   </figure>
  </sect1>

  <sect1 xml:id="sec-ha-installation-setup-conntrackd">
   <title>Synchronizing connection status between cluster nodes</title>
   <para>
    To enable <emphasis>stateful</emphasis> packet inspection for iptables,
    configure and use the conntrack tools. This requires the following basic
    steps:
   </para>

   <procedure xml:id="pro-ha-installation-setup-conntrackd">
    <title>Configuring the <systemitem class="resource">conntrackd</systemitem> with &yast;</title>
    <para>
     Use the &yast; cluster module to configure the user space
     <systemitem class="daemon">conntrackd</systemitem> (see <xref
      linkend="fig-ha-installation-setup-conntrackd"/>).  It needs a
     dedicated network interface that is not used for other communication
     channels. The daemon can be started via a resource agent afterward.
    </para>
    <step>
     <para>
      If you are modifying an existing cluster, switch to the
      <guimenu>Configure conntrackd</guimenu> category.
     </para>
     <para>
      If you are following the initial setup wizard, you do not need
      to switch categories manually.
     </para>
    </step>
    <step>
     <para>
      Select a <guimenu>Dedicated Interface</guimenu> for synchronizing the
      connection status. The IPv4 address of the selected interface is
      automatically detected and shown in &yast;. It must already be
      configured and it must support multicast.
      <!--taroth 2011-11-09: for the records, this has nothing to do with the
       corosync conf-->
     </para>
    </step>
    <step>
     <para>
      Define the <guimenu>Multicast Address</guimenu> to be used for
      synchronizing the connection status.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Group Number</guimenu>, define a numeric ID for the group
      to synchronize the connection status to.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Generate /etc/conntrackd/conntrackd.conf</guimenu> to
      create the configuration file for
      <systemitem class="daemon">conntrackd</systemitem>.
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module.
     </para>
    </step>
    <step>
     <para>
      Confirm your changes with <guimenu>Next</guimenu> (setup wizard)
       or <guimenu>Finish</guimenu> (existing cluster).
     </para>
    </step>
   </procedure>
      <para>
    After having configured the conntrack tools, you can use them for &lvs;
    (see <xref linkend="cha-ha-lb" xrefstyle="select:title"/>).
   </para>
   <figure xml:id="fig-ha-installation-setup-conntrackd">
    <title>&yast; <guimenu>Cluster</guimenu>&mdash;<systemitem class="resource">conntrackd</systemitem></title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast_cluster_conntrackd.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast_cluster_conntrackd.png" width="75%"/>
     </imageobject>
     <textobject role="description">
      <phrase>
        The Configure conntrackd screen shows the settings for configuring the conntrack tools.
        You can select a Dedicated Interface, enter a multicast address and group number, and
        generate the <filename>/etc/conntrackd/conntrackd.conf</filename> file.
      </phrase>
    </textobject>
    </mediaobject>
   </figure>
  </sect1>

  <sect1 xml:id="sec-ha-installation-setup-services">
   <title>Configuring services</title>
   <para>
    In the &yast; cluster module, define whether to start certain services
    on a node at boot time. You can also use the module to start and stop
    the services manually. To bring the cluster nodes online and start the
    cluster resource manager, &pace; must be running as a service.
   </para>
   <para>
    The configuration in this section only applies to the current machine,
    not to all cluster nodes.
   </para>
   <procedure xml:id="pro-ha-installation-setup-services">
    <title>Enabling the cluster services</title>
    <step>
     <para>
      If you are modifying an existing cluster, switch to the
      <guimenu>Service</guimenu> category. You can use the options in this
      section to start and stop cluster services on this node.
     </para>
     <para>
      If you are following the initial setup wizard, you do not need to switch
      categories manually. You will start the cluster services later, so you can
      skip straight to <xref linkend="yast-cluster-service-firewall"/>.
     </para>
    </step>
    <step>
     <para>
      To start the cluster services each time this cluster node is booted, select
      <guimenu>Enable cluster</guimenu>. If you select <guimenu>Disable cluster</guimenu>,
      you must start the cluster services manually each time this node is booted.
     </para>
    </step>
    <step>
     <para>
      To start or stop the cluster services immediately, select <guimenu>Start Now</guimenu>
      or <guimenu>Stop Now</guimenu>.
     </para>
    </step>
    <step>
      <para>
        To start or stop &qdevice; immediately, select <guimenu>Start Now</guimenu> or
        <guimenu>Stop Now</guimenu>.
      </para>
    </step>
    <step xml:id="yast-cluster-service-firewall">
     <para>
      To open the ports in the firewall that are needed for cluster
      communication, activate <guimenu>Open Port in Firewall</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Confirm your changes with <guimenu>Next</guimenu> (setup wizard)
      or <guimenu>Finish</guimenu> (existing cluster).
     </para>
     <para>
      If you are following the initial setup wizard, this completes the initial
      configuration and exits &yast;. Continue to <xref linkend="sec-ha-setup-yast-csync2-sync"/>.
   </para>
    </step>
   </procedure>
   <figure>
    <title>&yast; <guimenu>Cluster</guimenu>&mdash;services</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast_cluster_services.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast_cluster_services.png" width="75%"/>
     </imageobject>
     <textobject role="description">
      <phrase>
        The Service screen lets you start or stop services. You can enable or disable the
        cluster starting at boot time, start or stop &pace; and &corosync;, and start or stop
        &qdevice;. You can also open the required firewall port.
      </phrase>
    </textobject>
    </mediaobject>
   </figure>
  </sect1>

  <sect1 xml:id="sec-ha-setup-yast-csync2-sync">
   <title>Transferring the configuration to all nodes</title>
   <para>
    After the cluster configuration with &yast; is complete, use <command>csync2</command>
    to copy the configuration files to the rest of the cluster nodes. To receive the files,
    nodes must be included in the <guimenu>Sync Host</guimenu> group you configured in
    <xref linkend="pro-ha-installation-setup-csync2-yast"/>.
   </para>
   <para> Before running &csync; for the first time, you need to make the
    following preparations: </para>
   <procedure>
    <title>Preparing for initial synchronization with &csync;</title>
    <step>
      <para>
        Make sure passwordless SSH is configured between the nodes. This is required for
        cluster communication.
      </para>
    </step>
    <step>
     <para>
      Copy the file <filename>/etc/csync2/csync2.cfg</filename> manually
      to <emphasis>all</emphasis> nodes in the cluster.
     </para>
    </step>
    <step>
     <para> Copy the file <filename>/etc/csync2/key_hagroup</filename> manually
      to <emphasis>all</emphasis> nodes in the cluster. It is needed for
      authentication by &csync;. However, do <emphasis>not</emphasis>
      regenerate the file on the other nodes&mdash;it needs to be the same
      file on all nodes. </para>
    </step>
    <step>
     <para>Run the following command on all nodes to enable and start the service now: </para>
     <screen>&prompt.root;<command>systemctl enable --now csync2.socket</command></screen>
    </step>
   </procedure>

   <para>
    Use the following procedure to transfer the configuration files to all cluster nodes:
   </para>
   <procedure xml:id="pro-ha-installation-setup-csync2-start">
    <title>Synchronizing changes with &csync;</title>
    <step>
     <para>To synchronize all files once, run the following
      command on the machine that you want to copy the configuration
       <emphasis>from</emphasis>: </para>
     <screen>&prompt.root;<command>csync2 -xv</command></screen>
     <para> This synchronizes all the files once by pushing them to the
      other nodes. If all files are synchronized successfully, &csync;
      finishes with no errors. </para>
     <para> If one or several files that are to be synchronized have been
      modified on other nodes (not only on the current one), &csync;
      reports a conflict with an output similar to the one below: </para>
     <screen>While syncing file /etc/corosync/corosync.conf:
ERROR from peer hex-14: File is also marked dirty here!
Finished with 1 errors.</screen>
    </step>
    <step>
     <para> If you are sure that the file version on the current node is the
       <quote>best</quote> one, you can resolve the conflict by forcing this
      file and resynchronizing: </para>
     <screen>&prompt.root;<command>csync2 -f /etc/corosync/corosync.conf</command>
&prompt.root;<command>csync2 -xv</command></screen>
    </step>
   </procedure>
   <para> For more information on the &csync; options, run</para>
   <screen>&prompt.root;<command>csync2 --help</command></screen>
   <important>
    <title>Pushing synchronization after any changes</title>
    <para> &csync; only pushes changes. It does <emphasis>not</emphasis>
     continuously synchronize files between the machines. </para>
    <para> Each time you update files that need to be synchronized, you need to
     push the changes to the other machines by running <command>csync2&nbsp;-xv</command>
    on the machine where you did the changes. If you run
     the command on any of the other machines with unchanged files, nothing
     happens. </para>
   </important>
  </sect1>

  <sect1 xml:id="sec-ha-installation-start">
   <title>Bringing the cluster online</title>
   <para>
    After the initial cluster configuration is done, start the cluster
    services on all cluster nodes to bring the stack online:
   </para>
   <procedure xml:id="pro-ha-installation-start-cluster">
    <title>Starting cluster services and checking the status</title>
    <step>
     <para>
      Log in to an existing node.
     </para>
    </step>
    <step>
     <para>
      Start the cluster services on all cluster nodes:
     </para>
<screen>&prompt.root;<command>crm cluster start --all</command></screen>
     <para>
       This command requires passwordless SSH access between the nodes. You can also
       start individual nodes with <command>crm cluster start</command>.
     </para>
    </step>
    <step>
     <para>
      Check the cluster status with the
      <command>crm&nbsp;status</command> command. If all nodes are
      online, the output should be similar to the following:
     </para>
<screen>&prompt.root;<command>crm status</command>
Cluster Summary:
  * Stack: corosync
  * Current DC: &node1; (version ...) - partition with quorum
  * Last updated: ...
  * Last change:  ... by hacluster via crmd on &node2;
  * 2 nodes configured
  * 1 resource instance configured

Node List:
  * Online: [ &node1; &node2; ]
...</screen>
     <para>
      This output indicates that the cluster resource manager is started and
      is ready to manage resources.
     </para>
    </step>
   </procedure>
   <para>
     To be supported, a &sleha; cluster <emphasis>must</emphasis> have &stonith; (node fencing)
     enabled. A node fencing mechanism can be either a physical device (a power switch)
     or a mechanism like SBD in combination with a watchdog. Before you continue using
     the cluster, configure one or more &stonith; devices as described in
     <xref linkend="cha-ha-fencing"/> or <xref linkend="cha-ha-storage-protect"/>.
   </para>
  </sect1>
</chapter>
