<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-ha-monitor-clusters" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Monitoring Clusters</title>
 <info>
  <abstract>
   <para>
    This chapter describes how to monitor a cluster's health and view its history.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <xi:include href="ha_hawk2_monitor_i.xml"/>
 <xi:include href="ha_hawk2_health_i.xml"/>
 <xi:include href="ha_hawk2_history_i.xml"/>

 <sect1 xml:id="sec-ha-config-basics-monitor-health">
  <title>Monitoring System Health</title>

  <para>
   To prevent a node from running out of disk space and thus being unable to
   manage any resources that have been assigned to it, the &hasi;
   provides a resource agent,
   <systemitem>ocf:pacemaker:SysInfo</systemitem>. Use it to monitor a
   node's health with regard to disk partitions.
   The SysInfo RA creates a node attribute named
   <literal>#health_disk</literal> which will be set to
   <literal>red</literal> if any of the monitored disks' free space is below
   a specified limit.
  </para>

  <para>
   To define how the CRM should react in case a node's health reaches a
   critical state, use the global cluster option
   <systemitem>node-health-strategy</systemitem>.
  </para>

  <procedure xml:id="pro-ha-health-monitor">
   <title>Configuring System Health Monitoring</title>
   <para>
    To automatically move resources away from a node in case the node runs
    out of disk space, proceed as follows:
   </para>
   <step>
    <para>
     Configure an <systemitem>ocf:pacemaker:SysInfo</systemitem> resource:
    </para>
<screen><?dbsuse-fo font-size="0.71em"?>primitive sysinfo ocf:pacemaker:SysInfo \
     params disks="/tmp /var"<co xml:id="co-disks"/> min_disk_free="100M"<co xml:id="co-min-disk-free"/> disk_unit="M"<co xml:id="co-disk-unit"/> \<!--delay="30s"<co id="co-delay"/>\-->
     op monitor interval="15s"</screen>
    <calloutlist>
     <callout arearefs="co-disks">
      <para>
       Which disk partitions to monitor. For example,
       <filename>/tmp</filename>, <filename>/usr</filename>,
       <filename>/var</filename>, and <filename>/dev</filename>. To specify
       multiple partitions as attribute values, separate them with a blank.
      </para>
      <note>
       <title><filename>/</filename> File System Always Monitored</title>
       <para>
        You do not need to specify the root partition
        (<filename>/</filename>) in <literal>disks</literal>. It is always
        monitored by default.
       </para>
      </note>
     </callout>
     <callout arearefs="co-min-disk-free">
      <para>
       The minimum free disk space required for those partitions.
       Optionally, you can specify the unit to use for measurement (in the
       example above, <literal>M</literal> for megabytes is used). If not
       specified, <systemitem>min_disk_free</systemitem> defaults to the
       unit defined in the <systemitem>disk_unit</systemitem> parameter.
      </para>
     </callout>
     <callout arearefs="co-disk-unit">
      <para>
       The unit in which to report the disk space.
      </para>
     </callout>
    </calloutlist>
   </step>
   <step>
    <para>
     To complete the resource configuration, create a clone of
     <systemitem>ocf:pacemaker:SysInfo</systemitem> and start it on each
     cluster node.
    </para>
   </step>
   <step>
    <para>
     Set the <systemitem>node-health-strategy</systemitem> to
     <literal>migrate-on-red</literal>:
    </para>
<screen>property node-health-strategy="migrate-on-red"</screen>
    <para>
     In case of a <systemitem>#health_disk</systemitem> attribute set to
     <literal>red</literal>, the <systemitem
      class="daemon">pacemaker-schedulerd</systemitem> adds <literal>-INF</literal>
     to the resources' score for that node. This will cause any resources to
     move away from this node. The &stonith; resource will be the last
     one to be stopped but even if the &stonith; resource is not running
     anymore, the node can still be fenced. Fencing has direct access to the
     CIB and will continue to work.
    </para>
   </step>
  </procedure>

  <para>
   After a node's health status has turned to <literal>red</literal>, solve
   the issue that led to the problem. Then clear the <literal>red</literal>
   status to make the node eligible again for running resources. Log in to
   the cluster node and use one of the following methods:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Execute the following command:
    </para>
<screen>&prompt.root;<command>crm node status-attr <replaceable>NODE</replaceable> delete #health_disk</command></screen>
   </listitem>
   <listitem>
    <para>
     Restart the cluster services on that node.
    </para>
   </listitem>
   <listitem>
    <para>
     Reboot the node.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The node will be returned to service and can run resources again.
  </para>
 </sect1>

</chapter>
