<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-ha-monitor-clusters" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Monitoring clusters</title>
 <info>
  <abstract>
   <para>
    This chapter describes how to monitor a cluster's health and view its history.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <xi:include href="ha_hawk2_monitor_i.xml"/>

 <xi:include href="ha_hawk2_health_i.xml"/>

 <sect1 xml:id="sec-ha-manual-config-cli-health">
  <title>Getting health status</title>
  <remark>toms 2014-05-08: FATE#316464</remark>
  <para>
   The <quote>health</quote> status of a cluster or node can be displayed
   with so called <emphasis>scripts</emphasis>. A script can perform
   different tasks&mdash;they are not targeted to health. However, for
   this subsection, we focus on how to get the health status.
  </para>
  <para>
   To get all the details about the <command>health</command> command, use
   <command>describe</command>:
  </para>
<screen>&prompt.root;<command>crm</command> script describe health</screen>
  <para>
   It shows a description and a list of all parameters and their default
   values. To execute a script, use <command>run</command>:
  </para>
  <remark>toms 2014-05-08: see bnc#876882</remark>
<screen>&prompt.root;<command>crm</command> script run health</screen>
  <para>
   If you prefer to run only one step from the suite, the
   <command>describe</command> command lists all available steps in the
   <citetitle>Steps</citetitle> category.
  </para>
  <para>
   For example, the following command executes the first step of the
   <command>health</command> command. The output is stored in the
   <filename>health.json</filename> file for further investigation:
  </para>
<screen>&prompt.root;<command>crm</command> script run health
   statefile='health.json'</screen>
  <para>It is also possible to run the above commands with
   <command>crm cluster health</command>.</para>
  <para>
   For additional information regarding scripts, see
   <link xlink:href="http://crmsh.github.io/scripts/"/>.
  </para>
 </sect1>

 <xi:include href="ha_hawk2_history_i.xml"/>

 <sect1 xml:id="sec-ha-config-crm-history">
  <title>Retrieving history information</title>
  <para>
   Investigating the cluster history is a complex task. To simplify this
   task, &crmsh; contains the <command>history</command> command with its
   subcommands. It is assumed SSH is configured correctly.
  </para>
  <para>
   Each cluster moves states, migrates resources, or starts important
   processes. All these actions can be retrieved by subcommands of
   <command>history</command>. <!--FIXME: HAWK1 - add history explorer
    in Hawk2 and re-add link-->
    <!--Alternatively, use &hawk2; as explained in
   <xref linkend="pro-ha-config-hawk-history-explorer"/>.-->
  </para>
  <para>
   By default, all <command>history</command> commands look at the events of
   the last hour. To change this time frame, use the
   <command>limit</command> subcommand. The syntax is:
  </para>
<screen>&prompt.root;<command>crm</command> history
&prompt.crm.hist;<command>limit</command> <replaceable>FROM_TIME</replaceable> [<replaceable>TO_TIME</replaceable>]</screen>
  <para>
   Some valid examples include:
  </para>
  <variablelist>
   <varlistentry>
    <term><command>limit</command><literal>4:00pm</literal>
    </term>
    <term><command>limit</command><literal>16:00</literal>
    </term>
    <listitem>
     <para>
      Both commands mean the same, today at 4pm.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>limit</command><literal>2012/01/12 6pm</literal>
    </term>
    <listitem>
     <para>
      January 12th 2012 at 6pm
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>limit</command><literal>"Sun 5 20:46"</literal>
    </term>
    <listitem>
     <para>
      In the current year of the current month at Sunday the 5th at 8:46pm
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
<!-- On SLE 11 HA SP2:
    # rpm -q python-dateutil
python-dateutil-1.4.1-1.20
  -->
  <para>
   Find more examples and how to create time frames at
   <link xlink:href="http://labix.org/python-dateutil"/>.
  </para>
  <para>
   The <command>info</command> subcommand shows all the parameters which are
   covered by the <command>crm report</command>:
  </para>
<screen>&prompt.crm.hist;<command>info</command>
Source: live
Period: 2012-01-12 14:10:56 - end
Nodes: &node1;
Groups:
Resources:</screen>
  <para>
   To limit <command>crm report</command> to certain parameters view the
   available options with the subcommand <command>help</command>.
  </para>
  <para>
   To narrow down the level of detail, use the subcommand
   <command>detail</command> with a level:
  </para>
<screen>&prompt.crm.hist;<command>detail</command> 1</screen>
  <para>
   The higher the number, the more detailed your report will be. Default is
   <literal>0</literal> (zero).
  </para>
  <para>
   After you have set above parameters, use <command>log</command> to show
   the log messages.
  </para>
  <para>
   To display the last transition, use the following command:
  </para>
<screen>&prompt.crm.hist;<command>transition</command> -1
INFO: fetching new logs, please wait ...</screen>
  <para>
   This command fetches the logs and runs <command>dotty</command> (from the
   <package>graphviz</package> package) to show the
   transition graph. The shell opens the log file which you can browse with
   the <keycap function="down"/> and <keycap function="up"/> cursor keys.
  </para>
  <para>
   If you do not want to open the transition graph, use the
   <option>nograph</option> option:
  </para>
<screen>&prompt.crm.hist;<command>transition</command> -1 nograph</screen>
 </sect1>

 <sect1 xml:id="sec-ha-config-basics-monitor-health">
  <title>Monitoring system health with the <literal>SysInfo</literal> resource agent</title>
  <para>
   To prevent a node from running out of disk space and thus being unable to
   manage any resources that have been assigned to it, the &hasi;
   provides a resource agent,
   <systemitem>ocf:pacemaker:SysInfo</systemitem>. Use it to monitor a
   node's health with regard to disk partitions.
   The SysInfo RA creates a node attribute named
   <literal>#health_disk</literal> which will be set to
   <literal>red</literal> if any of the monitored disks' free space is below
   a specified limit.
  </para>
  <para>
   To define how the CRM should react in case a node's health reaches a
   critical state, use the global cluster option
   <systemitem>node-health-strategy</systemitem>.
  </para>
  <procedure xml:id="pro-ha-health-monitor">
   <title>Configuring system health monitoring</title>
   <para>
    To automatically move resources away from a node in case the node runs
    out of disk space, proceed as follows:
   </para>
   <step>
    <para>
     Configure an <systemitem>ocf:pacemaker:SysInfo</systemitem> resource:
    </para>
<screen><?dbsuse-fo font-size="0.71em"?>primitive sysinfo ocf:pacemaker:SysInfo \
     params disks="/tmp /var"<co xml:id="co-disks"/> min_disk_free="100M"<co xml:id="co-min-disk-free"/> disk_unit="M"<co xml:id="co-disk-unit"/> \<!--delay="30s"<co id="co-delay"/>\-->
     op monitor interval="15s"</screen>
    <calloutlist>
     <callout arearefs="co-disks">
      <para>
       Which disk partitions to monitor. For example,
       <filename>/tmp</filename>, <filename>/usr</filename>,
       <filename>/var</filename>, and <filename>/dev</filename>. To specify
       multiple partitions as attribute values, separate them with a blank.
      </para>
      <note>
       <title><filename>/</filename> file system always monitored</title>
       <para>
        You do not need to specify the root partition
        (<filename>/</filename>) in <literal>disks</literal>. It is always
        monitored by default.
       </para>
      </note>
     </callout>
     <callout arearefs="co-min-disk-free">
      <para>
       The minimum free disk space required for those partitions.
       Optionally, you can specify the unit to use for measurement (in the
       example above, <literal>M</literal> for megabytes is used). If not
       specified, <systemitem>min_disk_free</systemitem> defaults to the
       unit defined in the <systemitem>disk_unit</systemitem> parameter.
      </para>
     </callout>
     <callout arearefs="co-disk-unit">
      <para>
       The unit in which to report the disk space.
      </para>
     </callout>
    </calloutlist>
   </step>
   <step>
    <para>
     To complete the resource configuration, create a clone of
     <systemitem>ocf:pacemaker:SysInfo</systemitem> and start it on each
     cluster node.
    </para>
   </step>
   <step>
    <para>
     Set the <systemitem>node-health-strategy</systemitem> to
     <literal>migrate-on-red</literal>:
    </para>
<screen>property node-health-strategy="migrate-on-red"</screen>
    <para>
     In case of a <systemitem>#health_disk</systemitem> attribute set to
     <literal>red</literal>, the <systemitem
      class="daemon">pacemaker-schedulerd</systemitem> adds <literal>-INF</literal>
     to the resources' score for that node. This will cause any resources to
     move away from this node. The &stonith; resource will be the last
     one to be stopped but even if the &stonith; resource is not running
     anymore, the node can still be fenced. Fencing has direct access to the
     CIB and will continue to work.
    </para>
   </step>
  </procedure>
  <para>
   After a node's health status has turned to <literal>red</literal>, solve
   the issue that led to the problem. Then clear the <literal>red</literal>
   status to make the node eligible again for running resources. Log in to
   the cluster node and use one of the following methods:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Execute the following command:
    </para>
<screen>&prompt.root;<command>crm</command> node status-attr <replaceable>NODE</replaceable> delete #health_disk</screen>
   </listitem>
   <listitem>
    <para>
     Restart the cluster services on that node.
    </para>
   </listitem>
   <listitem>
    <para>
     Reboot the node.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The node will be returned to service and can run resources again.
  </para>
 </sect1>
</chapter>
