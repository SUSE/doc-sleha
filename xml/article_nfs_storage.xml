<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<article version="5.0" xml:lang="en" xml:id="article-nfs-storage"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&nfsquick;</title>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp format="B d, Y"?></date>
  <xi:include href="common_copyright_gfdl.xml"/>
  <abstract>
   <para>
    &abstract-nfsquick;
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-ha-quick-nfs-usagescenario">
  <title>Usage scenario</title>
  <para>
   This document will help you set up a highly available NFS server.
   The cluster used for the highly available NFS storage has the
   following properties:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Two nodes: <systemitem class="server">&node1;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.1</systemitem>)
     and <systemitem class="server">&node2;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.2</systemitem>),
     connected to each other via network.
    </para>
   </listitem>
   <listitem>
    <para>
     Two floating, virtual IP addresses (<systemitem class="ipaddress"
      >&nfs-vip-hawk;</systemitem> and <systemitem class="ipaddress"
      >&nfs-vip-exports;</systemitem>), allowing clients to connect to
     the service no matter which physical node it is running on.
     One IP address is used for cluster administration with &hawk2;, the other
     IP address is used exclusively for the NFS exports.
    </para>
   </listitem>
   <listitem>
    <para>
     SBD used as a &stonith; fencing device to avoid split-brain scenarios.
     &stonith; is mandatory for the HA cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Failover of resources from one node to the other if the active host breaks
     down (<emphasis>active/passive</emphasis> setup).
    </para>
   </listitem>
   <listitem>
    <para>
     Local storage on each node. The data is synchronized between the
     nodes using DRBD on top of LVM.
    </para>
   </listitem>
   <listitem>
    <para>
      A file system exported through NFS.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   After installing and setting up the basic two-node cluster, and extending it
   with storage and cluster resources for NFS, you will have a highly
   available NFS storage server.
  </para>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-installation">
  <title>Installing a basic two-node cluster</title>
  <para>
   Before you proceed, install and set up a basic two-node cluster. This task is
   described in <xref linkend="article-installation"/>. The &haquick; describes
   how to use the &crmshell; to set up a
   cluster with minimal effort.
  </para>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-lvm">
   <title>Creating LVM devices</title>
   <para>LVM (<emphasis>Logical Volume Manager</emphasis>) enables
    flexible distribution of hard disk space over several file
    systems.
   </para>
   <procedure>
    <title>Creating LVM devices for DRBD</title>
    <step>
     <para>
      Create an LVM physical volume and replace <filename>/dev/sdb1</filename>
      with your corresponding device for LVM:</para>
     <screen>&prompt.root;<command>pvcreate /dev/sdb1</command></screen>
    </step>
    <step>
     <para>Create an LVM volume group <systemitem>nfs</systemitem>
            that includes this physical volume: </para>
     <screen>&prompt.root;<command>vgcreate nfs /dev/sdb1</command></screen>
    </step>
    <step>
      <para>
       Create a logical volume named <systemitem>exportfs</systemitem> in the
       volume group <systemitem>nfs</systemitem>:
      </para>
      <screen>&prompt.root;<command>lvcreate -n exportfs -L 20G nfs</command></screen>
     </step>
     <step>
      <para>
       Create a second logical volume, named <systemitem>state</systemitem>,
       in the volume group <systemitem>nfs</systemitem>:
      </para>
      <screen>&prompt.root;<command>lvcreate -n state -L 20G nfs</command></screen>
     </step>
     <step>
      <para>
       Activate the volume group: </para>
<screen>&prompt.root;<command>vgchange -ay nfs</command></screen>
     </step>
   </procedure>
   <para>
    You should now see the following devices on the system:
    <filename>/dev/nfs/exportfs</filename> and <filename>/dev/nfs/state</filename>.
   </para>
  </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-drbd-device">
   <title>Creating DRBD devices</title>
   <para>
    This section describes how to set up DRBD devices on top of LVM.
    Using LVM as a back-end of DRBD has some benefits:
   </para>
  <itemizedlist>
   <listitem>
    <para>Easier setup than with LVM on top of DRBD.</para>
   </listitem>
   <listitem>
    <para>Easier administration in case the LVM disks need to be resized or
     more disks are added to the volume group.
    </para>
   </listitem>
  </itemizedlist>
  <para>
    The following procedures will result in two DRBD devices: one device
    for the NFS file system, and a second device to share the NFS file states.
   </para>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-config">
    <title>Creating DRBD configurations</title>
    <para>
     For consistency reasons, it is highly recommended to follow this advice:
    </para>
    <itemizedlist>
     <listitem>
      <para>Use the directory <filename>/etc/drbd.d/</filename> for your
      configurations.</para>
     </listitem>
     <listitem>
      <para>Name the files according to the purpose of the resources.
      </para>
     </listitem>
     <listitem>
      <para>Put your resource configurations in files with a <filename
       class="extension">.res</filename> extension.
      </para>
     </listitem>
    </itemizedlist>
    <procedure>
     <title>Creating DRBD configurations</title>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/nfs_exportfs.res</filename> with the
        following contents:
      </para>
<screen>resource nfs_exportfs {
   device /dev/drbd1 minor 1; <co xml:id="co-ha-quick-nfs-drbd-device"/>
   disk   /dev/nfs/exportfs; <co xml:id="co-ha-quick-nfs-drbd-disk"/>
   meta-disk internal; <co xml:id="co-ha-quick-nfs-drbd-metadisk"/>

   net {
      protocol  C; <co xml:id="co-ha-quick-nfs-drbd-protocol"/>
      fencing resource-and-stonith; <co xml:id="co-ha-quick-nfs-fencing-policy"/>
   }

   handlers { <co xml:id="co-ha-quick-nfs-fencing-handlers"/>
      fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
      after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
      # ...
   }

   connection-mesh { <co xml:id="co-ha-quick-nfs-connectionmesh"/>
      hosts     &node1; &node2;;
   }
   on &node1; { <co xml:id="co-ha-quick-nfs-drbd-on"/>
      address   &subnetI;.1:&drbd.port;;
      node-id   0;
   }
   on &node2; { <xref linkend="co-ha-quick-nfs-drbd-on"/>
      address   &subnetI;.2:&drbd.port;;
      node-id   1;
   }
}</screen>
      <calloutlist>
       <callout arearefs="co-ha-quick-nfs-drbd-device">
        <para>The DRBD device that applications will access.</para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-disk">
        <para>The lower-level block device used by DRBD to store the actual
         data. This is the LVM device that was created in <xref
          linkend="sec-ha-quick-nfs-lvm"/>.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-metadisk">
        <para>Where the metadata format is stored. Using
         <literal>internal</literal>, the metadata is stored together with
         the user data on the same device. See the man page for further
         information.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-protocol">
        <para>The specified protocol to be used for this connection. For protocol
         <literal>C</literal>, a write is considered to be complete when
         it has reached all disks, be they local or remote.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-fencing-policy">
        <para>
         Specifies the fencing policy. For clusters with a &stonith; device
         configured, use <literal>resource-and-stonith</literal>.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-fencing-handlers">
        <para>
         Enables resource-level fencing. If the DRBD replication link
         becomes disconnected, &pace; tries to promote the DRBD resource
         to another node. During this process, the scripts were called.
         See <xref linkend="sec-ha-drbd-fencing"/> for more
         information.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-connectionmesh">
        &drbd-connection-mesh;
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-on">
        <para>Contains the IP address and a unique identifier for each node.</para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/nfs_state.res</filename> with the
       same contents as <filename>/etc/drbd.d/nfs_exportfs.res</filename>, but
       change the following lines:
      </para>
<screen>resource nfs_<emphasis role="bold">state</emphasis> {
   device /dev/drbd<emphasis role="bold">2</emphasis> minor <emphasis role="bold">2</emphasis>;
   disk   /dev/nfs/<emphasis role="bold">state</emphasis>;</screen>
     </step>
     <step>
      <para>
       Open <filename>/etc/csync2/csync2.cfg</filename> and check whether the
       following two lines exist:
      </para>
<screen>include /etc/drbd.conf;
include /etc/drbd.d/*.res;</screen>
      <para>
       If not, add them to the file.
      </para>
     </step>
     <step>
      <para>
       Copy the file to the other nodes:
      </para>
<screen>&prompt.root;<command>csync2 -xv</command></screen>
      <para>
       For information about &csync;, refer to
       <xref linkend="sec-ha-installation-setup-csync2"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-activate">
    <title>Activating the DRBD devices</title>
    <para>
     After preparing the DRBD configurations, activate the devices:
    </para>
    <procedure>
     <title>Activating DRBD devices</title>
     <step>
      <para>
       If you use a firewall in your cluster, open port
       <systemitem>&drbd.port;</systemitem> in your firewall configuration.
      </para>
     </step>
     <step>
      <para>The first time you do this, execute the following
        commands on <emphasis>both</emphasis> nodes (in our example, <systemitem>&node1;</systemitem>
        and <systemitem>&node2;</systemitem>):
      </para>
<screen>&prompt.root;<command>drbdadm create-md nfs_exportfs</command>
&prompt.root;<command>drbdadm up nfs_exportfs</command></screen>
      <para>
       This initializes the metadata storage and creates the DRBD device.
      </para>
     </step>
     <step>
      <para>
       If the DRBD devices on all nodes have the same data, skip
       the initial resynchronization. Use the following command:
      </para>
      <screen>&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap nfs_exportfs/0</command></screen>
     </step>
     <step>
       <para>Make <systemitem>&node1;</systemitem> primary:</para>
       <screen>&prompt.root;<command>drbdadm primary --force nfs_exportfs</command></screen>
     </step>
      <step>
       <para>Check the DRBD status:</para>
       <screen>&prompt.root;<command>drbdadm status nfs_exportfs</command></screen>
       <para>This returns the following message:</para>
          <screen>nfs_exportfs role:Primary
  disk:UpToDate
  &node1; role:Secondary
    peer-disk:UpToDate</screen>
     </step>
     <step>
      <para>
       Repeat these steps for the <literal>nfs_state</literal> configuration.
      </para>
     </step>
    </procedure>
    <para>
     You can access the DRBD resources on the block devices
     <filename>/dev/drbd1</filename> and <filename>/dev/drbd2</filename>.

     </para>
     <para>
      For more information about DRBD, refer to <xref linkend="cha-ha-drbd"/>.
     </para>
   </sect2>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-createfs">
    <title>Creating the file system</title>
    <para>
     After activating the DRBD devices, create the NFS file system on
     <filename>/dev/drbd1</filename>:
    </para>
    <screen>&prompt.root;<command>mkfs.ext4 /dev/drbd1</command></screen>
   </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-initial-pacemaker">
  <title>Adjusting Pacemaker's configuration</title>
  &failback-nodes;
  <para>
    To adjust the option, open the &crmshell; as &rootuser; (or any
    non-&rootuser; user that is part of the
    <systemitem class="groupname">haclient</systemitem> group) and run the
    following commands:
   </para>
   <screen>&prompt.root;<command>crm configure</command>
&prompt.crm.conf;<command>rsc_defaults resource-stickiness="200"</command>
&prompt.crm.conf;<command>commit</command></screen>

   <para>
    For more information about global cluster options, refer to
    <xref linkend="sec-ha-config-basics-global-options"/>.
   </para>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-resources">
  <title>Creating cluster resources</title>
  <para>
   The following procedures describe how to configure the resources required
   for a highly available NFS cluster.
  </para>
  <variablelist>
   <title>Overview of cluster resources</title>
   <varlistentry>
    <term>DRBD primitive and promotable clone resources</term>
    <listitem>
     <para>
      These resources are used to replicate data. The promotable clone resource
      is switched to and from the primary and secondary roles as deemed necessary
      by the cluster resource manager.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>File system resources</term>
    <listitem>
     <para>
      These resources manage the NFS file system and file states.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS kernel server resource</term>
    <listitem>
     <para>
      This resource manages the NFS server daemon.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS exports</term>
    <listitem>
     <para>
      This resource is used to export the <filename>/srv/nfs/exportfs</filename>
      directory to clients.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual IP address</term>
    <listitem>
     <para>
      The initial installation creates an administrative virtual IP address for &hawk2;.
      Create another virtual IP address exclusively for NFS exports. This makes it
      easier to apply security restrictions later.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <itemizedlist>
    <title>Example NFS scenario</title>
    <listitem>
        <para>The following configuration examples assume that
         <systemitem class="ipaddress">&nfs-vip-exports;</systemitem> is the virtual
         IP address to use for an NFS server which serves clients in the
         <systemitem class="ipaddress">&subnetI;.x/24</systemitem> subnet.</para>
    </listitem>
    <listitem>
        <para>The service exports data served from
         <literal>/srv/nfs/exportfs</literal>. </para>
    </listitem>
    <listitem>
        <para>Into this export directory, the cluster mounts
            <literal>ext4</literal> file systems from the DRBD device
         <filename>/dev/drbd1</filename>.
         This DRBD device sits on top of an LVM logical volume named
         <literal>/dev/nfs/exportfs</literal>.
        </para>
    </listitem>
    <listitem>
     <para>
      A second DRBD device, <literal>/dev/drbd2</literal>, is used to share the
      NFS file states from <filename>/var/lib/nfs</filename>. This DRBD device
      sits on top of an LVM logical volume named <literal>/dev/nfs/state</literal>.
     </para>
    </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-ha-quick-nfs-resources-drbd">
   <title>Creating DRBD primitive and promotable clone resources</title>
   <para>
    First, create cluster resources to manage the DRBD devices, and promotable
    clones to allow these resources to run on both nodes.
   </para>
   <procedure>
    <title>Creating DRBD resources for NFS</title>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive shell:
     </para>
<screen>&prompt.root;<command>crm configure</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the DRBD resource <literal>nfs_exportfs</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive drbd-nfs_exportfs ocf:linbit:drbd \
  params drbd_resource="nfs_exportfs" \
  op monitor interval="15" role="Promoted" \
  op monitor interval="30" role="Unpromoted"</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the DRBD resource <literal>nfs_state</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive drbd-nfs_state ocf:linbit:drbd \
  params drbd_resource="nfs_state" \
  op monitor interval="15" role="Promoted" \
  op monitor interval="30" role="Unpromoted"</command></screen>
    </step>
    <step>
     <para>
      Create a promotable clone for the <literal>drbd-nfs_exportfs</literal>
      primitive:
     </para>
<screen>&prompt.crm.conf;<command>clone cl-nfs_exportfs drbd-nfs_exportfs \
  meta promotable="true" promoted-max="1" promoted-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</command></screen>
    </step>
    <step>
     <para>
      Create a promotable clone for the <literal>drbd-nfs_state</literal>
      primitive:
     </para>
<screen>&prompt.crm.conf;<command>clone cl-nfs_state drbd-nfs_state \
  meta promotable="true" promoted-max="1" promoted-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    Pacemaker activates the DRBD resources on both nodes and promotes
    them to the primary role on one of the nodes. Check the state of the
    cluster with the <command>crm status</command> command, or run
    <command>drbdadm status</command>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-lvm">
   <title>Creating NFS file system resources</title>
   <para>
    Next, create cluster resources to manage the NFS file system and file states.
    <emphasis>Do not</emphasis> commit this configuration until after you add
    the colocation and order constraints.
   </para>
   <procedure>
    <title>Creating file system resources for NFS</title>
    <step>
     <para>
      Create a file system type resource for the NFS file system on
      <literal>/dev/drbd1</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive fs-nfs_exportfs Filesystem \
  params device=/dev/drbd1 directory=/srv/nfs/exportfs fstype=ext4 \
  op monitor interval="30s"</command></screen>
    </step>
    <step>
     <para>
      Create a file system type resource for the NFS file states on
      <literal>/dev/drbd2</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive fs-nfs_state Filesystem \
  params device=/dev/drbd2 directory=/var/lib/nfs fstype=ext4 \
  op monitor interval="30s"</command></screen>
    </step>
    <step>
     <para>
      Add a colocation constraint to make sure that the two resources
      always start on the same node:
     </para>
<screen>&prompt.crm.conf;<command>colocation co-nfs_exportfs-with-nfs_state \
  inf: fs-nfs_exportfs fs-nfs_state</command></screen>
    </step>
    <step>
     <para>
      Add colocation constraints to make sure that each resource always starts
      on the node where the related DRBD promotable clone is in the primary role:
     </para>
<screen>&prompt.crm.conf;<command>colocation co-nfs_exportfs-on-drbd \
  inf: fs-nfs_exportfs cl-nfs_exportfs:Promoted</command>
&prompt.crm.conf;<command>colocation co-nfs_state-on-drbd \
  inf: fs-nfs_state cl-nfs_state:Promoted</command></screen>
    </step>
    <step>
     <para>
      Add order constraints to make sure the DRBD promotable clones always start
      before the file system resources:
     </para>
<screen>&prompt.crm.conf;<command>order o-drbd-before-nfs_exportfs \
  Mandatory: cl-nfs_exportfs:promote fs-nfs_exportfs:start</command>
&prompt.crm.conf;<command>order o-drbd-before-nfs_state \
  Mandatory: cl-nfs_state:promote fs-nfs_state:start</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    Pacemaker mounts <literal>/dev/drbd1</literal> to <filename>/srv/nfs/exportfs</filename>,
    and <literal>/dev/drbd2</literal> to <filename>/var/lib/nfs</filename>. Confirm this
    with <command>mount</command>, or by looking at <filename >/proc/mounts</filename>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-nffserver-exportfs-vip">
   <title>Creating NFS supporting resources</title>
   <para>
    Finally, create cluster resources for the NFS server daemon, the NFS exports,
    and the virtual IP address. <emphasis>Do not</emphasis> commit this configuration
    until after you add the colocation and order constraints.
   </para>
   <procedure>
    <title>Creating supporting resources for NFS</title>
    <step>
     <para>
      Create a primitive to manage the NFS server daemon:
     </para>
<screen>&prompt.crm.conf;<command>primitive p-nfsserver nfsserver \
  params nfs_server_scope=SUSE \
  op monitor interval=10s timeout=20s \
  meta target-role=Started</command></screen>
    </step>
     <step>
      <para>
       Create a primitive for the NFS exports:
      </para>
<screen>&prompt.crm.conf;<command>primitive p-nfs_exportfs exportfs \
  params directory="/srv/nfs/exportfs" \
  options="rw,mountpoint" clientspec="*" fsid=100 \
  op monitor interval=30s \
  meta target-role=Started</command></screen>
     </step>
     <step>
      <para>
       Confirm that the NFS exports are set up properly:
      </para>
<screen>&prompt.root;<command>exportfs -v</command>
/srv/nfs/exportfs   <replaceable>IP_ADDRESS_OF_CLIENT</replaceable>(<replaceable>OPTIONS</replaceable>)</screen>
     </step>
    <step>
     <para>
      Create a primitive for the virtual IP address:
     </para>
<screen>&prompt.crm.conf;<command>primitive vip_nfs IPaddr2 \
  params ip=&nfs-vip-exports; \
  op monitor interval=10 timeout=20 \
  meta target-role=Started</command></screen>
    </step>
    <step>
     <para>
      Add a colocation constraint to make sure that the NFS server daemon,
      the NFS exports, and the virtual IP address all start on the same node
      as the NFS file system resources:
     </para>
<screen>&prompt.crm.conf;<command>colocation co-nfs_resources-with-filesystem \
  inf: vip_nfs p-nfs_exportfs p-nfsserver ( fs-nfs_exportfs fs-nfs_state )</command></screen>
    </step>
    <step>
     <para>
      Add an order constraint to make sure that the file system resources always start
      before the NFS server daemon, the NFS exports, and finally the virtual IP address:
     </para>
<screen>&prompt.crm.conf;<command>order o-filesystem-before-nfs_resources \
  Mandatory: ( fs-nfs_exportfs fs-nfs_state ) p-nfsserver p-nfs_exportfs vip_nfs</command></screen>
    </step>
    <step>
     <para>
      Commit these changes:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-use">
  <title>Using the NFS service</title>
  <para>
   This section outlines how to use the highly available NFS service from an
   NFS client.
  </para>
  <para>
   To connect to the NFS service, make sure to use the <emphasis>virtual IP
   address</emphasis> to connect to the cluster rather than a physical IP
   configured on one of the cluster nodes' network interfaces. For compatibility
   reasons, use the <emphasis>full</emphasis> path of the NFS export on the server.
  </para>

  <para>In its simplest form, the command to mount the NFS export looks like
   this:</para>
  <screen>&prompt.root;<command>mount -t nfs &nfs-vip-exports;:/srv/nfs/work /home/work</command></screen>
  <para>
   To configure a specific transport protocol (<option>proto</option>)
   and maximum read and write request sizes (<option>rsize</option> and
    <option>wsize</option>), use:
  </para>
  <screen>&prompt.root;<command>mount -o rsize=32768,wsize=32768 \
    &nfs-vip-exports;:/srv/nfs/work /home/work</command></screen>
  <para>
   If you need to be compatible with NFS version&nbsp;3, include the value
   <option>vers=3</option> after the <option>-o</option> option.
  </para>
  <para>
   For further NFS mount options, consult the <command>nfs</command> man page.
  </para>
 </sect1>
 <xi:include href="common_legal.xml"/>
 </article>
