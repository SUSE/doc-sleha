<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<article version="5.0" xml:lang="en" xml:id="article-nfs-storage"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&nfsquick;</title>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp format="B d, Y"?></date>
  <xi:include href="common_copyright_gfdl.xml"/>
  <abstract>
   <para>
    &abstract-nfsquick;
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-ha-quick-nfs-usagescenario">
  <title>Usage scenario</title>
  <para>
   This document will help you set up a highly available NFS server.
   The cluster used for the highly available NFS storage has the
   following properties:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Two nodes: <systemitem class="server">&node1;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.1</systemitem>)
     and <systemitem class="server">&node2;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.2</systemitem>),
     connected to each other via network.
    </para>
   </listitem>
   <listitem>
    <para>
     Two floating, virtual IP addresses (<systemitem class="ipaddress"
      >&nfs-vip-hawk;</systemitem> and <systemitem class="ipaddress"
      >&nfs-vip-exports;</systemitem>), allowing clients to connect to
     the service no matter which physical node it is running on.
     One IP address is used for cluster administration with &hawk2;, the other
     IP address is used exclusively for the NFS exports.
    </para>
   </listitem>
   <listitem>
    <para>
     SBD used as a &stonith; fencing device to avoid split-brain scenarios.
     &stonith; is mandatory for the HA cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Failover of resources from one node to the other if the active host breaks
     down (<emphasis>active/passive</emphasis> setup).
    </para>
   </listitem>
   <listitem>
    <para>
     Local storage on each node. The data is synchronized between the
     nodes using DRBD on top of LVM.
    </para>
   </listitem>
   <listitem>
    <para>
      A file system exported through NFS.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   After installing and setting up the basic two-node cluster, and extending it
   with storage and cluster resources for NFS, you will have a highly
   available NFS storage server.
  </para>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-installation">
  <title>Installing a basic two-node cluster</title>
  <para>
   Before you proceed, install and set up a basic two-node cluster. This task is
   described in <xref linkend="article-installation"/>. The &haquick; describes
   how to use the &crmshell; to set up a
   cluster with minimal effort.
  </para>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-lvm">
   <title>Creating LVM devices</title>
   <para>LVM (<emphasis>Logical Volume Manager</emphasis>) enables
    flexible distribution of hard disk space over several file
    systems.
   </para>
   <procedure>
    <title>Creating LVM devices for DRBD</title>
    <step>
     <para>
      Create an LVM physical volume and replace <filename>/dev/sdb1</filename>
      with your corresponding device for LVM:</para>
     <screen>&prompt.root;<command>pvcreate /dev/sdb1</command></screen>
    </step>
    <step>
     <para>Create an LVM volume group <systemitem>nfs</systemitem>
            that includes this physical volume: </para>
     <screen>&prompt.root;<command>vgcreate nfs /dev/sdb1</command></screen>
    </step>
    <step>
      <para>
       Create a logical volume named <systemitem>share</systemitem> in the
       volume group <systemitem>nfs</systemitem>:
      </para>
      <screen>&prompt.root;<command>lvcreate -n share -L 20G nfs</command></screen>
     </step>
     <step>
      <para>
       Create a second logical volume, named <systemitem>state</systemitem>,
       in the volume group <systemitem>nfs</systemitem>:
      </para>
      <screen>&prompt.root;<command>lvcreate -n state -L 4G nfs</command></screen>
     </step>
     <step>
      <para>
       Activate the volume group: </para>
<screen>&prompt.root;<command>vgchange -ay nfs</command></screen>
     </step>
   </procedure>
   <para>
    You should now see the following devices on the system:
    <filename>/dev/nfs/share</filename> and <filename>/dev/nfs/state</filename>.
   </para>
  </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-drbd-device">
   <title>Creating DRBD devices</title>
   <para>
    This section describes how to set up DRBD devices on top of LVM.
    Using LVM as a back-end of DRBD has some benefits:
   </para>
  <itemizedlist>
   <listitem>
    <para>Easier setup than with LVM on top of DRBD.</para>
   </listitem>
   <listitem>
    <para>Easier administration in case the LVM disks need to be resized or
     more disks are added to the volume group.
    </para>
   </listitem>
  </itemizedlist>
  <para>
    The following procedures will result in two DRBD devices: one device for the
    <literal>ext4</literal> file system that will be exported, and a second device
    to track the NFS client states.
   </para>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-config">
    <title>Creating the DRBD configurations</title>
    <para>
     For consistency reasons, it is highly recommended to follow this advice:
    </para>
    <itemizedlist>
     <listitem>
      <para>Use the directory <filename>/etc/drbd.d/</filename> for your
      configurations.</para>
     </listitem>
     <listitem>
      <para>Name the files according to the purpose of the resources.
      </para>
     </listitem>
     <listitem>
      <para>Put your resource configurations in files with a <filename
       class="extension">.res</filename> extension.
      </para>
     </listitem>
    </itemizedlist>
    <procedure>
     <title>Creating DRBD configurations</title>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/nfs-share.res</filename> with the
        following contents:
      </para>
<screen>resource nfs-share {
   device /dev/drbd1 minor 1; <co xml:id="co-ha-quick-nfs-drbd-device"/>
   disk   /dev/nfs/share; <co xml:id="co-ha-quick-nfs-drbd-disk"/>
   meta-disk internal; <co xml:id="co-ha-quick-nfs-drbd-metadisk"/>

   net {
      protocol  C; <co xml:id="co-ha-quick-nfs-drbd-protocol"/>
      fencing resource-and-stonith; <co xml:id="co-ha-quick-nfs-fencing-policy"/>
   }

   handlers { <co xml:id="co-ha-quick-nfs-fencing-handlers"/>
      fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
      after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
      # ...
   }

   connection-mesh { <co xml:id="co-ha-quick-nfs-connectionmesh"/>
      hosts     &node1; &node2;;
   }
   on &node1; { <co xml:id="co-ha-quick-nfs-drbd-on"/>
      address   &subnetI;.1:&drbd.port;;
      node-id   0;
   }
   on &node2; { <xref linkend="co-ha-quick-nfs-drbd-on"/>
      address   &subnetI;.2:&drbd.port;;
      node-id   1;
   }
}</screen>
      <calloutlist>
       <callout arearefs="co-ha-quick-nfs-drbd-device">
        <para>The DRBD device that applications will access.</para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-disk">
        <para>The lower-level block device used by DRBD to store the actual
         data. This is the LVM device that was created in <xref
          linkend="sec-ha-quick-nfs-lvm"/>.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-metadisk">
        <para>Where the metadata format is stored. Using
         <literal>internal</literal>, the metadata is stored together with
         the user data on the same device. See the man page for further
         information.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-protocol">
        <para>The specified protocol to be used for this connection. For protocol
         <literal>C</literal>, a write is considered to be complete when
         it has reached all disks, be they local or remote.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-fencing-policy">
        <para>
         Specifies the fencing policy. For clusters with a &stonith; device
         configured, use <literal>resource-and-stonith</literal>.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-fencing-handlers">
        <para>
         Enables resource-level fencing. If the DRBD replication link
         becomes disconnected, &pace; tries to promote the DRBD resource
         to another node. During this process, the scripts were called.
         See <xref linkend="sec-ha-drbd-fencing"/> for more
         information.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-connectionmesh">
        &drbd-connection-mesh;
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-on">
        <para>Contains the IP address and a unique identifier for each node.</para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/nfs-state.res</filename> with the
       same contents as <filename>/etc/drbd.d/nfs-share.res</filename>, but
       change the following lines:
      </para>
<screen>resource nfs_<emphasis role="bold">state</emphasis> {
   device /dev/drbd<emphasis role="bold">2</emphasis> minor <emphasis role="bold">2</emphasis>;
   disk   /dev/nfs/<emphasis role="bold">state</emphasis>;</screen>
     </step>
     <step>
      <para>
       Open <filename>/etc/csync2/csync2.cfg</filename> and check whether the
       following two lines exist:
      </para>
<screen>include /etc/drbd.conf;
include /etc/drbd.d/*.res;</screen>
      <para>
       If not, add them to the file.
      </para>
     </step>
     <step>
      <para>
       Copy the file to the other nodes:
      </para>
<screen>&prompt.root;<command>csync2 -xv</command></screen>
      <para>
       For information about &csync;, refer to
       <xref linkend="sec-ha-installation-setup-csync2"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-activate">
    <title>Activating the DRBD devices</title>
    <para>
     After preparing the DRBD configurations, activate the devices:
    </para>
    <procedure>
     <title>Activating DRBD devices</title>
     <step>
      <para>
       If you use a firewall in your cluster, open port
       <systemitem>&drbd.port;</systemitem> in your firewall configuration.
      </para>
     </step>
     <step>
      <para>
       Initialize the metadata storage. Run these commands on
       <emphasis>both</emphasis> nodes:
      </para>
<screen>&prompt.root;<command>drbdadm create-md nfs-share</command>
&prompt.root;<command>drbdadm create-md nfs-state</command></screen>
     </step>
     <step>
      <para>
       Create the DRBD devices. Run these commands on
       <emphasis>both</emphasis> nodes:
      </para>
<screen>&prompt.root;<command>drbdadm up nfs-share</command>
&prompt.root;<command>drbdadm up nfs-state</command></screen>
     </step>
     <step>
      <para>
       The devices do not have data yet, so
       you can run these commands to skip the initial synchronization:
      </para>
<screen>&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap nfs-share/1</command>
&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap nfs-state/2</command></screen>
     </step>
     <step>
       <para>Make <systemitem>&node1;</systemitem> primary:</para>
<screen>&prompt.root;<command>drbdadm primary --force nfs-share</command>
&prompt.root;<command>drbdadm primary --force nfs-state</command></screen>
     </step>
      <step>
       <para>Check the DRBD status of <literal>nfs-share</literal>:</para>
<screen>&prompt.root;<command>drbdadm status nfs-share</command></screen>
       <para>This returns the following message:</para>
<screen>nfs-share role:Primary
  disk:UpToDate
  &node1; role:Secondary
    peer-disk:UpToDate</screen>
     <para>
      Checking the status of <literal>nfs-state</literal> should return the same message.
     </para>
     </step>
    </procedure>
    <para>
     You can access the DRBD resources on the block devices
     <filename>/dev/drbd1</filename> and <filename>/dev/drbd2</filename>.
     </para>
   </sect2>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-createfs">
    <title>Creating the file systems</title>
    <para>
     After activating the DRBD devices, create file systems on them:

    </para>
    <procedure>
     <title>Creating file systems for DRBD</title>
     <step>
      <para>
       Create an <literal>ext4</literal> file system on <filename>/dev/drbd1</filename>:
      </para>
      <screen>&prompt.root;<command>mkfs.ext4 /dev/drbd1</command></screen>
     </step>
     <step>
      <para>
       Create an <literal>ext4</literal> file system on <filename>/dev/drbd2</filename>:
      </para>
      <screen>&prompt.root;<command>mkfs.ext4 /dev/drbd2</command></screen>
     </step>
    </procedure>
    <warning>
     <title>Low lease time can cause loss of file state</title>
     <para>
      NFS clients regularly renew their state with the NFS server. If the lease time
      is too low, operating system or network delays can cause the timer to expire
      before the renewal is complete, leading to loss of file state and I/O errors.
     </para>
     <para>
      <literal>NFSV4LEASETIME</literal> is set on the NFS server in the file
      <filename>/etc/sysconfig/nfs</filename>. The default is 90 seconds.
      If lowering the lease time is necessary, we recommend a value of 60 or
      higher. We strongly discourage values lower than 30.
     </para>
    </warning>
   </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-resources">
  <title>Creating cluster resources</title>
  <para>
   The following procedures describe how to configure the resources required
   for a highly available NFS cluster.
  </para>
  <variablelist>
   <title>Overview of cluster resources</title>
   <varlistentry>
    <term>DRBD primitive and promotable clone resources</term>
    <listitem>
     <para>
      These resources are used to replicate data. The promotable clone resource
      is switched to and from the primary and secondary roles as deemed necessary
      by the cluster resource manager.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>File system resources</term>
    <listitem>
     <para>
      These resources manage the file system that will be exported, and the
      file system that will track NFS client states.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS kernel server resource</term>
    <listitem>
     <para>
      This resource manages the NFS server daemon.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS exports</term>
    <listitem>
     <para>
      This resource is used to export the directory <filename>/srv/nfs/share</filename>
      to clients.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual IP address</term>
    <listitem>
     <para>
      The initial installation creates an administrative virtual IP address for &hawk2;.
      Create another virtual IP address exclusively for NFS exports. This makes it
      easier to apply security restrictions later.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <itemizedlist>
    <title>Example NFS scenario</title>
    <listitem>
        <para>The following configuration examples assume that
         <systemitem class="ipaddress">&nfs-vip-exports;</systemitem> is the virtual
         IP address to use for an NFS server which serves clients in the
         <systemitem class="ipaddress">&subnetI;.x/24</systemitem> subnet.</para>
    </listitem>
    <listitem>
        <para>The service exports data served from
         <literal>/srv/nfs/share</literal>. </para>
    </listitem>
    <listitem>
        <para>Into this export directory, the cluster mounts an
            <literal>ext4</literal> file system from the DRBD device
         <filename>/dev/drbd1</filename>.
         This DRBD device sits on top of an LVM logical volume named
         <literal>/dev/nfs/share</literal>.
        </para>
    </listitem>
    <listitem>
     <para>
      A second DRBD device, <literal>/dev/drbd2</literal>, is used to share the
      NFS client states from <filename>/var/lib/nfs</filename>. This DRBD device
      sits on top of an LVM logical volume named <literal>/dev/nfs/state</literal>.
     </para>
    </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-ha-quick-nfs-resources-drbd">
   <title>Creating DRBD primitive and promotable clone resources</title>
   <para>
    First, create cluster resources to manage the DRBD devices, and promotable
    clones to allow these resources to run on both nodes.
   </para>
   <procedure>
    <title>Creating DRBD resources for NFS</title>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive shell:
     </para>
<screen>&prompt.root;<command>crm configure</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the DRBD device <literal>nfs-share</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive drbd_nfs-share ocf:linbit:drbd \
  params drbd_resource="nfs-share" \
  op monitor interval="15" role="Promoted" \
  op monitor interval="30" role="Unpromoted"</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the DRBD device <literal>nfs-state</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive drbd_nfs-state ocf:linbit:drbd \
  params drbd_resource="nfs-state" \
  op monitor interval="15" role="Promoted" \
  op monitor interval="30" role="Unpromoted"</command></screen>
    </step>
    <step>
     <para>
      Create a promotable clone for the <literal>drbd_nfs-share</literal>
      primitive:
     </para>
<screen>&prompt.crm.conf;<command>clone cl_nfs-share drbd_nfs-share \
  meta promotable="true" promoted-max="1" promoted-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</command></screen>
    </step>
    <step>
     <para>
      Create a promotable clone for the <literal>drbd_nfs-state</literal>
      primitive:
     </para>
<screen>&prompt.crm.conf;<command>clone cl_nfs-state drbd_nfs-state \
  meta promotable="true" promoted-max="1" promoted-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    Pacemaker activates the DRBD resources on both nodes and promotes
    them to the primary role on one of the nodes. Check the state of the
    cluster with the <command>crm status</command> command, or run
    <command>drbdadm status</command>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-lvm">
   <title>Creating file system resources</title>
   <para>
    Next, create cluster resources to manage the file systems for export and
    state tracking. <emphasis>Do not</emphasis> commit this configuration until
    after you add the colocation and order constraints.
   </para>
   <procedure>
    <title>Creating file system resources for NFS</title>
    <step>
     <para>
      Create a primitive for the file system to be exported on
      <literal>/dev/drbd1</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive fs_nfs-share Filesystem \
  params device=/dev/drbd1 directory=/srv/nfs/share fstype=ext4 \
  op monitor interval="30s"</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the NFS client states on <literal>/dev/drbd2</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive fs_nfs-state Filesystem \
  params device=/dev/drbd2 directory=/var/lib/nfs fstype=ext4 \
  op monitor interval="30s"</command></screen>
    </step>
    <step>
     <para>
      Add a colocation constraint to make sure that the two resources
      always start on the same node:
     </para>
<screen>&prompt.crm.conf;<command>colocation co_nfs-share-with-nfs-state \
  inf: fs_nfs-share fs_nfs-state</command></screen>
    </step>
    <step>
     <para>
      Add colocation constraints to make sure that each resource always starts
      on the node where the related DRBD promotable clone is in the primary role:
     </para>
<screen>&prompt.crm.conf;<command>colocation co_nfs-share-on-drbd \
  inf: fs_nfs-share cl_nfs-share:Promoted</command>
&prompt.crm.conf;<command>colocation co_nfs-state-on-drbd \
  inf: fs_nfs-state cl_nfs-state:Promoted</command></screen>
    </step>
    <step>
     <para>
      Add order constraints to make sure the DRBD promotable clones always start
      before the file system resources:
     </para>
<screen>&prompt.crm.conf;<command>order o_drbd-before-nfs-share \
  Mandatory: cl_nfs-share:promote fs_nfs-share:start</command>
&prompt.crm.conf;<command>order o_drbd-before-nfs-state \
  Mandatory: cl_nfs-state:promote fs_nfs-state:start</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    Pacemaker mounts <literal>/dev/drbd1</literal> to <filename>/srv/nfs/share</filename>,
    and <literal>/dev/drbd2</literal> to <filename>/var/lib/nfs</filename>. Confirm this
    with <command>mount</command>, or by looking at <filename >/proc/mounts</filename>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-nffserver-exportfs-vip">
   <title>Creating NFS supporting resources</title>
   <para>
    Finally, create cluster resources for the NFS server daemon, the NFS exports,
    and the virtual IP address. <emphasis>Do not</emphasis> commit this configuration
    until after you add the colocation and order constraints.
   </para>
   <procedure>
    <title>Creating supporting resources for NFS</title>
    <step>
     <para>
      Create a primitive to manage the NFS server daemon:
     </para>
<screen>&prompt.crm.conf;<command>primitive p_nfsserver nfsserver \
  params nfs_server_scope=SUSE \
  op monitor interval=10s timeout=20s</command></screen>
    </step>
     <step>
      <para>
       Create a primitive for the NFS exports:
      </para>
<screen>&prompt.crm.conf;<command>primitive p_exportfs exportfs \
  params directory="/srv/nfs/share" \
  options="rw,mountpoint" clientspec="*" fsid=100 \
  op monitor interval=30s</command></screen>
      <important>
       <title>Do not set <literal>wait_for_leasetime_on_stop=true</literal></title>
       <para>
        Setting this option to <literal>true</literal>
        in a highly available NFS setup can cause unnecessary delays and loss of locks.
       </para>
       <para>
        The default value for <literal>wait_for_leasetime_on_stop</literal> is
        <literal>false</literal>. There is no need to set it to <literal>true</literal>
        when <filename>/var/lib/nfs</filename> and <literal>p_nfsserver</literal>
        are configured as described in this guide.
       </para>
      </important>
     </step>
     <step>
      <para>
       Confirm that the NFS exports are set up properly:
      </para>
<screen>&prompt.root;<command>exportfs -v</command>
/srv/nfs/share   <replaceable>IP_ADDRESS_OF_CLIENT</replaceable>(<replaceable>OPTIONS</replaceable>)</screen>
     </step>
    <step>
     <para>
      Create a primitive for the virtual IP address:
     </para>
<screen>&prompt.crm.conf;<command>primitive vip_nfs IPaddr2 \
  params ip=&nfs-vip-exports; \
  op monitor interval=10 timeout=20</command></screen>
    </step>
    <step>
     <para>
      Add a colocation constraint to make sure that the NFS server daemon,
      the NFS exports, and the virtual IP address all start on the same node
      as the file system resources:
     </para>
<screen>&prompt.crm.conf;<command>colocation co_nfs-resources-with-filesystem \
  inf: vip_nfs p_exportfs p_nfsserver ( fs_nfs-share fs_nfs-state )</command></screen>
    </step>
    <step>
     <para>
      Add an order constraint to make sure that the file system resources always start
      before the NFS server daemon, the NFS exports, and finally the virtual IP address:
     </para>
<screen>&prompt.crm.conf;<command>order o_filesystem-before-nfs-resources \
  Mandatory: ( fs_nfs-share fs_nfs-state ) p_nfsserver p_exportfs vip_nfs</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-use">
  <title>Using the NFS service</title>
  <para>
   This section outlines how to use the highly available NFS service from an
   NFS client.
  </para>
  <para>
   To connect to the NFS service, make sure to use the <emphasis>virtual IP
   address</emphasis> to connect to the cluster rather than a physical IP
   configured on one of the cluster nodes' network interfaces. For compatibility
   reasons, use the <emphasis>full</emphasis> path of the NFS export on the server.
  </para>
  <para>
   The command to mount the NFS export looks like this:
  </para>
<screen>&prompt.root;<command>mount &nfs-vip-exports;:/srv/nfs/share /home/share</command></screen>
  <para>
   If you need to configure other mount options, such as a specific transport protocol
   (<option>proto</option>), maximum read and write request sizes (<option>rsize</option>
   and <option>wsize</option>), or a specific NFS version (<option>vers</option>),
   use the <option>-o</option> option.
  </para>
  <para>
   For example:
  </para>
<screen>&prompt.root;<command>mount -o proto=tcp,rsize=32768,wsize=32768,vers=3 \
&nfs-vip-exports;:/srv/nfs/share /home/share</command></screen>
  <para>
   For further NFS mount options, refer to the <command>nfs</command> man page.
  </para>
  <note>
   <title>Loopback mounts</title>
   <para>
    Loopback mounts are only supported for NFS version 3, <emphasis>not</emphasis>
    NFS version 4. For more information, refer to
    <link xlink:href="https://www.suse.com/support/kb/doc/?id=000018709"/>.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-quick-nfs-more-info">
  <title>For more information</title>
  <itemizedlist>
   <listitem>
    <para>
     For more details about the steps in this guide, refer to
     <link xlink:href="https://www.suse.com/support/kb/doc/?id=000020396"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more information about NFS and LVM, refer to
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html">
     &storage_guide; for &sles;</link>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more information about DRBD, refer to <xref linkend="cha-ha-drbd"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <xi:include href="common_legal.xml"/>
 </article>
