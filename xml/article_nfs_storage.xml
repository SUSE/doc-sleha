<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<article version="5.0" xml:lang="en" xml:id="article-nfs-storage"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&nfsquick;</title>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp format="B d, Y"?></date>
  <xi:include href="common_copyright_gfdl.xml"/>
  <abstract>
   <para>
    &abstract-nfsquick;
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-ha-quick-nfs-usagescenario">
  <title>Usage scenario</title>
  <para>
   This document will help you set up a highly available NFS server.
   The cluster used for the highly available NFS storage has the
   following properties:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Two nodes: <systemitem class="server">&node1;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.1</systemitem>)
     and <systemitem class="server">&node2;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.2</systemitem>),
     connected to each other via network.
    </para>
   </listitem>
   <listitem>
    <para>
     Two floating, virtual IP addresses (<systemitem class="ipaddress"
      >&nfs-vip-hawk;</systemitem> and <systemitem class="ipaddress"
      >&nfs-vip-exports;</systemitem>), allowing clients to connect to
     the service no matter which physical node it is running on.
     One IP address is used for cluster administration with &hawk2;, the other
     IP address is used exclusively for the NFS exports.
    </para>
   </listitem>
   <listitem>
    <para>
     SBD used as a &stonith; fencing device to avoid split-brain scenarios.
     &stonith; is mandatory for the HA cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Failover of resources from one node to the other if the active host breaks
     down (<emphasis>active/passive</emphasis> setup).
    </para>
   </listitem>
   <listitem>
    <para>
     Local storage on each node. The data is synchronized between the
     nodes using DRBD on top of LVM.
    </para>
   </listitem>
   <listitem>
    <para>
      A file system exported through NFS.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   After installing and setting up the basic two-node cluster, and extending it
   with storage and cluster resources for NFS, you will have a highly
   available NFS storage server.
  </para>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-installation">
  <title>Installing a basic two-node cluster</title>
  <para>
   Before you proceed, install and set up a basic two-node cluster. This task is
   described in <xref linkend="article-installation"/>. The &haquick; describes
   how to use the &crmshell; to set up a
   cluster with minimal effort.
  </para>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-lvm">
   <title>Creating LVM devices</title>
   <para>LVM (<emphasis>Logical Volume Manager</emphasis>) enables
    flexible distribution of hard disk space over several file
    systems.
   </para>
   <para>
    Use <command>crm cluster run</command> to create the LVM devices on both nodes at once.
   </para>
   <procedure>
    <title>Creating LVM devices for DRBD</title>
    <step>
     <para>
      Create an LVM physical volume and replace <filename>/dev/sdb1</filename>
      with your corresponding device for LVM:</para>
     <screen>&prompt.root;<command>crm cluster run "pvcreate /dev/sdb1"</command></screen>
    </step>
    <step>
     <para>Create an LVM volume group <systemitem>nfs</systemitem>
            that includes this physical volume: </para>
     <screen>&prompt.root;<command>crm cluster run "vgcreate nfs /dev/sdb1"</command></screen>
    </step>
    <step>
      <para>
       Create a logical volume named <systemitem>share</systemitem> in the
       volume group <systemitem>nfs</systemitem>:
      </para>
      <screen>&prompt.root;<command>crm cluster run "lvcreate -n share -L 20G nfs"</command></screen>
     </step>
     <step>
      <para>
       Create a second logical volume, named <systemitem>state</systemitem>,
       in the volume group <systemitem>nfs</systemitem>:
      </para>
      <screen>&prompt.root;<command>crm cluster run "lvcreate -n state -L 4G nfs"</command></screen>
     </step>
     <step>
      <para>
       Activate the volume group: </para>
<screen>&prompt.root;<command>crm cluster run "vgchange -ay nfs"</command></screen>
     </step>
   </procedure>
   <para>
    You should now see the following devices on the system:
    <filename>/dev/nfs/share</filename> and <filename>/dev/nfs/state</filename>.
   </para>
  </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-drbd-device">
   <title>Creating DRBD devices</title>
   <para>
    This section describes how to set up DRBD devices on top of LVM.
    Using LVM as a back-end of DRBD has some benefits:
   </para>
  <itemizedlist>
   <listitem>
    <para>Easier setup than with LVM on top of DRBD.</para>
   </listitem>
   <listitem>
    <para>Easier administration in case the LVM disks need to be resized or
     more disks are added to the volume group.
    </para>
   </listitem>
  </itemizedlist>
  <para>
    The following procedures will result in two DRBD devices: one device for the
    <literal>ext4</literal> file system that will be exported, and a second device
    to track the NFS client states.
   </para>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-config">
    <title>Creating the DRBD configuration</title>
    <para>
     For consistency reasons, it is highly recommended to follow this advice:
    </para>
    <itemizedlist>
     <listitem>
      <para>Use the directory <filename>/etc/drbd.d/</filename> for your
      configuration.</para>
     </listitem>
     <listitem>
      <para>Name the file according to the purpose of the resource.
      </para>
     </listitem>
     <listitem>
      <para>Put your resource configuration in a file with a
       <filename class="extension">.res</filename> extension.
      </para>
     </listitem>
    </itemizedlist>
    <procedure>
     <title>Creating a DRBD configuration</title>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/nfs.res</filename> with the
        following contents:
      </para>
<screen>resource nfs {
   volume 1 { <co xml:id="co-ha-quick-nfs-drbd-volume"/>
      device           /dev/drbd1 minor 1; <co xml:id="co-ha-quick-nfs-drbd-device"/>
      disk             /dev/nfs/state; <co xml:id="co-ha-quick-nfs-drbd-disk"/>
      meta-disk        internal; <co xml:id="co-ha-quick-nfs-drbd-metadisk"/>
   }
   volume 1 {
      device           /dev/drbd2 minor 2;
      disk             /dev/nfs/share;
      meta-disk        internal;
   }

   net {
      protocol  C; <co xml:id="co-ha-quick-nfs-drbd-protocol"/>
      fencing resource-and-stonith; <co xml:id="co-ha-quick-nfs-fencing-policy"/>
   }

   handlers { <co xml:id="co-ha-quick-nfs-fencing-handlers"/>
      fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
      after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
   }

   connection-mesh { <co xml:id="co-ha-quick-nfs-connectionmesh"/>
      hosts     &node1; &node2;;
   }
   on &node1; { <co xml:id="co-ha-quick-nfs-drbd-on"/>
      address   &subnetI;.1:&drbd.port;;
      node-id   0;
   }
   on &node2; { <xref linkend="co-ha-quick-nfs-drbd-on"/>
      address   &subnetI;.2:&drbd.port;;
      node-id   1;
   }
}</screen>
      <calloutlist>
       <callout arearefs="co-ha-quick-nfs-drbd-volume">
        <para>The volume number for each DRBD device you want to create.</para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-device">
        <para>The DRBD device that applications will access.</para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-disk">
        <para>The lower-level block device used by DRBD to store the actual
         data. This is the LVM device that was created in <xref
          linkend="sec-ha-quick-nfs-lvm"/>.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-metadisk">
        <para>Where the metadata format is stored. Using
         <literal>internal</literal>, the metadata is stored together with
         the user data on the same device. See the man page for further
         information.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-protocol">
        <para>The specified protocol to be used for this connection. For protocol
         <literal>C</literal>, a write is considered to be complete when
         it has reached all disks, be they local or remote.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-fencing-policy">
        <para>
         Specifies the fencing policy. For clusters with a &stonith; device
         configured, use <literal>resource-and-stonith</literal>.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-fencing-handlers">
        <para>
         Enables resource-level fencing. If the DRBD replication link
         becomes disconnected, &pace; tries to promote the DRBD resource
         to another node. During this process, the scripts were called.
         See <xref linkend="sec-ha-drbd-fencing"/> for more
         information.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-connectionmesh">
        &drbd-connection-mesh;
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-on">
        <para>Contains the IP address and a unique identifier for each node.</para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Open <filename>/etc/csync2/csync2.cfg</filename> and check whether the
       following two lines exist:
      </para>
<screen>include /etc/drbd.conf;
include /etc/drbd.d/*.res;</screen>
      <para>
       If not, add them to the file.
      </para>
     </step>
     <step>
      <para>
       Copy the file to the other nodes:
      </para>
<screen>&prompt.root;<command>csync2 -xv</command></screen>
      <para>
       For information about &csync;, refer to
       <xref linkend="sec-ha-installation-setup-csync2"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-activate">
    <title>Activating the DRBD devices</title>
    <para>
     After preparing the DRBD configuration, activate the devices:
    </para>
    <procedure>
     <title>Activating DRBD devices</title>
     <step>
      <para>
       If you use a firewall in the cluster, open port
       <systemitem>&drbd.port;</systemitem> in the firewall configuration.
      </para>
     </step>
     <step>
      <para>
       Initialize the metadata storage:
      </para>
<screen>&prompt.root;<command>crm cluster run "drbdadm create-md nfs"</command></screen>
     </step>
     <step>
      <para>
       Create the DRBD devices:
      </para>
<screen>&prompt.root;<command>crm cluster run "drbdadm up nfs"</command></screen>
     </step>
     <step>
      <para>
       The devices do not have data yet, so you can run these commands to
       skip the initial synchronization:
      </para>
<screen>&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap nfs/1</command>
&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap nfs/2</command></screen>
     </step>
     <step>
       <para>Make <systemitem>&node1;</systemitem> primary:</para>
<screen>&prompt.root;<command>drbdadm primary --force nfs</command></screen>
     </step>
      <step>
       <para>Check the DRBD status of <literal>nfs</literal>:</para>
<screen>&prompt.root;<command>drbdadm status nfs</command></screen>
       <para>This returns the following message:</para>
<screen>nfs role:Primary
  volume:1 disk:UpToDate
  volume:2 disk:UpToDate
  &node2; role:Secondary
    volume:1 peer-disk:UpToDate
    volume:2 peer-disk:UpToDate</screen>
     </step>
    </procedure>
    <para>
     You can access the DRBD resources on the block devices
     <filename>/dev/drbd1</filename> and <filename>/dev/drbd2</filename>.
     </para>
   </sect2>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-createfs">
    <title>Creating the file systems</title>
    <para>
     After activating the DRBD devices, create file systems on them:

    </para>
    <procedure>
     <title>Creating file systems for DRBD</title>
     <step>
      <para>
       Create an <literal>ext4</literal> file system on <filename>/dev/drbd1</filename>:
      </para>
      <screen>&prompt.root;<command>mkfs.ext4 /dev/drbd1</command></screen>
     </step>
     <step>
      <para>
       Create an <literal>ext4</literal> file system on <filename>/dev/drbd2</filename>:
      </para>
      <screen>&prompt.root;<command>mkfs.ext4 /dev/drbd2</command></screen>
     </step>
    </procedure>
   </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-resources">
  <title>Creating cluster resources</title>
  <para>
   The following procedures describe how to configure the resources required
   for a highly available NFS cluster.
  </para>
  <variablelist>
   <title>Overview of cluster resources</title>
   <varlistentry>
    <term>DRBD primitive and promotable clone resources</term>
    <listitem>
     <para>
      These resources are used to replicate data. The promotable clone resource
      is switched to and from the primary and secondary roles as deemed necessary
      by the cluster resource manager.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>File system resources</term>
    <listitem>
     <para>
      These resources manage the file system that will be exported, and the
      file system that will track NFS client states.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS kernel server resource</term>
    <listitem>
     <para>
      This resource manages the NFS server daemon.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS exports</term>
    <listitem>
     <para>
      This resource is used to export the directory <filename>/srv/nfs/share</filename>
      to clients.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual IP address</term>
    <listitem>
     <para>
      The initial installation creates an administrative virtual IP address for &hawk2;.
      Create another virtual IP address exclusively for NFS exports. This makes it
      easier to apply security restrictions later.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <itemizedlist>
    <title>Example NFS scenario</title>
    <listitem>
        <para>The following configuration examples assume that
         <systemitem class="ipaddress">&nfs-vip-exports;</systemitem> is the virtual
         IP address to use for an NFS server which serves clients in the
         <systemitem class="ipaddress">&subnetI;.x/24</systemitem> subnet.</para>
    </listitem>
    <listitem>
        <para>The service exports data served from
         <literal>/srv/nfs/share</literal>. </para>
    </listitem>
    <listitem>
        <para>Into this export directory, the cluster mounts an
            <literal>ext4</literal> file system from the DRBD device
         <filename>/dev/drbd2</filename>.
         This DRBD device sits on top of an LVM logical volume named
         <literal>/dev/nfs/share</literal>.
        </para>
    </listitem>
    <listitem>
     <para>
      The DRBD device <literal>/dev/drbd1</literal> is used to share the
      NFS client states from <filename>/var/lib/nfs</filename>. This DRBD device
      sits on top of an LVM logical volume named <literal>/dev/nfs/state</literal>.
     </para>
    </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-ha-quick-nfs-resources-drbd">
   <title>Creating DRBD primitive and promotable clone resources</title>
   <para>
    Create a cluster resource to manage the DRBD devices, and a promotable
    clone to allow this resource to run on both nodes.
   </para>
   <procedure>
    <title>Creating a DRBD resource for NFS</title>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive shell:
     </para>
<screen>&prompt.root;<command>crm configure</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the DRBD configuration <literal>nfs</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive drbd_nfs ocf:linbit:drbd \
  params drbd_resource="nfs" \
  op monitor interval=15 role=Promoted timeout=20 \
  op monitor interval=30 role=Unpromoted timeout=20 \
  op start timeout=240 interval=0s \
  op promote timeout=90 interval=0s \
  op demote timeout=90 interval=0s \
  op stop timeout=100 interval=0s</command></screen>
    </step>
    <step>
     <para>
      Create a promotable clone for the <literal>drbd_nfs</literal> primitive:
     </para>
<screen>&prompt.crm.conf;<command>clone cl_nfs drbd_nfs \
  meta promotable="true" promoted-max="1" promoted-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true" interleave=true</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    Pacemaker activates the DRBD resources on both nodes and promotes
    them to the primary role on one of the nodes. Check the state of the
    cluster with the <command>crm status</command> command, or run
    <command>drbdadm status</command>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-lvm">
   <title>Creating file system resources</title>
   <para>
    Create cluster resources to manage the file systems for export and
    state tracking. <emphasis>Do not</emphasis> commit this configuration until
    after you add the colocation and order constraints.
   </para>
   <procedure>
    <title>Creating file system resources for NFS</title>
    <step>
     <para>
      Create a primitive for the NFS client states on <literal>/dev/drbd1</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive fs_nfs-state Filesystem \
  params device=/dev/drbd1 directory=/var/lib/nfs fstype=ext4 \
  op monitor interval=30s timeout=40s \
  op start timeout=60s interval=0s \
  op stop timeout=60s interval=0s</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the file system to be exported on
      <literal>/dev/drbd2</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive fs_nfs-share Filesystem \
  params device=/dev/drbd2 directory=/srv/nfs/share fstype=ext4 \
  op monitor interval=30s timeout=40s \
  op start timeout=60s interval=0s \
  op stop timeout=60s interval=0s</command></screen>
    </step>
    <step>
     <para>
      Add both of these resources to a resource group named <literal>g_nfs</literal>:
     </para>
<screen>&prompt.crm.conf;<command>group g_nfs fs_nfs-state fs_nfs-share</command></screen>
     <para>
      Resources start in the order they are added to the group, and stop in the reverse order.
     </para>
    </step>
    <step>
     <para>
      Add a colocation constraint to make sure that the resource group always
      starts on the node where the DRBD promotable clone is in the primary role:
     </para>
<screen>&prompt.crm.conf;<command>colocation co_nfs-on-drbd inf: g_nfs cl_nfs:Promoted</command></screen>
    </step>
    <step>
     <para>
      Add an order constraint to make sure the DRBD promotable clone always
      starts before the resource group:
     </para>
<screen>&prompt.crm.conf;<command>order o_drbd-before-nfs Mandatory: cl_nfs:promote g_nfs:start</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    Pacemaker mounts <literal>/dev/drbd1</literal> to <filename>/var/lib/nfs</filename>,
    and <literal>/dev/drbd2</literal> to <filename>srv/nfs/share</filename>. Confirm this
    with <command>mount</command>, or by looking at <filename >/proc/mounts</filename>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-nfsserver">
   <title>Creating an NFS kernel server resource</title>
   <para>
    Create a cluster resource to manage the NFS server daemon.
   </para>
   <para>
    Make sure the package <package>nfs-kernel-server</package> is installed on
    both nodes before performing this procedure.
   </para>
   <procedure>
    <title>Creating an NFS kernel server resource</title>
    <step>
     <para>
      Create a primitive to manage the NFS server daemon:
     </para>
<screen>&prompt.crm.conf;<command>primitive p_nfsserver nfsserver \
  params nfs_server_scope=SUSE \
  op monitor timeout=20s interval=10s \
  op start timeout=40s interval=0s \
  op stop timeout=20s interval=0s</command></screen>
     <warning>
      <title>Low lease time can cause loss of file state</title>
      <para>
       NFS clients regularly renew their state with the NFS server. If the lease time
       is too low, operating system or network delays can cause the timer to expire
       before the renewal is complete, leading to loss of file state and I/O errors.
      </para>
      <para>
       <literal>NFSV4LEASETIME</literal> is set on the NFS server in the file
       <filename>/etc/sysconfig/nfs</filename>. The default is 90 seconds.
       If lowering the lease time is necessary, we recommend a value of 60 or
       higher. We strongly discourage values lower than 30.
      </para>
     </warning>
    </step>
    <step>
     <para>
      Append this resource to the existing <literal>g_nfs</literal> resource group:
     </para>
<screen>&prompt.crm.conf;<command>modgroup g_nfs add p_nfsserver</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-nfsexport">
   <title>Creating an NFS export resource</title>
   <para>
    Create a cluster resource to manage the NFS exports.
   </para>
   <procedure>
    <title>Creating an NFS export resource</title>
     <step>
      <para>
       Create a primitive for the NFS exports:
      </para>
<screen>&prompt.crm.conf;<command>primitive p_exportfs exportfs \
  params directory="/srv/nfs/share" \
  options="rw,mountpoint" clientspec="*" fsid=100 \
  op monitor interval=30s timeout=90s \
  op start timeout=40s interval=0s \
  op stop timeout=120s interval=0s</command></screen>
      <para>
       The value of <literal>op monitor timeout</literal> must be higher
       than the value of <literal>stonith-timeout</literal>. To find the
       <literal>stonith-timeout</literal> value, run <command>crm configure show</command>
       and look under the <literal>property</literal> section.
      </para>
      <important>
       <title>Do not set <literal>wait_for_leasetime_on_stop=true</literal></title>
       <para>
        Setting this option to <literal>true</literal> in a highly available
        NFS setup can cause unnecessary delays and loss of locks.
       </para>
       <para>
        The default value for <literal>wait_for_leasetime_on_stop</literal> is
        <literal>false</literal>. There is no need to set it to <literal>true</literal>
        when <filename>/var/lib/nfs</filename> and <literal>p_nfsserver</literal>
        are configured as described in this guide.
       </para>
      </important>
     </step>
     <step>
      <para>
       Append this resource to the existing <literal>g_nfs</literal> resource group:
      </para>
 <screen>&prompt.crm.conf;<command>modgroup g_nfs add p_exportfs</command></screen>
     </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
    <step>
     <para>
      Confirm that the NFS exports are set up properly:
     </para>
<screen>&prompt.root;<command>exportfs -v</command>
/srv/nfs/share   <replaceable>IP_ADDRESS_OF_CLIENT</replaceable>(<replaceable>OPTIONS</replaceable>)</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-vip">
   <title>Creating a virtual IP address for NFS exports</title>
   <para>
    Create a cluster resource to manage the virtual IP address for the NFS exports.
   </para>
   <procedure>
    <title>Creating virtual IP address for NFS exports</title>
    <step>
     <para>
      Create a primitive for the virtual IP address:
     </para>
<screen>&prompt.crm.conf;<command>primitive vip_nfs IPaddr2 \
  params ip=&nfs-vip-exports; \
  op monitor interval=10 timeout=20 \
  op start timeout=20s interval=0s \
  op stop timeout=20s interval=0s</command></screen>
    </step>
    <step>
     <para>
      Append this resource to the existing <literal>g_nfs</literal> resource group:
     </para>
<screen>&prompt.crm.conf;<command>modgroup g_nfs add vip_nfs</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-use">
  <title>Using the NFS service</title>
  <para>
   This section outlines how to use the highly available NFS service from an
   NFS client.
  </para>
  <para>
   To connect to the NFS service, make sure to use the <emphasis>virtual IP
   address</emphasis> to connect to the cluster rather than a physical IP
   configured on one of the cluster nodes' network interfaces. For compatibility
   reasons, use the <emphasis>full</emphasis> path of the NFS export on the server.
  </para>
  <para>
   The command to mount the NFS export looks like this:
  </para>
<screen>&prompt.root;<command>mount &nfs-vip-exports;:/srv/nfs/share /home/share</command></screen>
  <para>
   If you need to configure other mount options, such as a specific transport protocol
   (<option>proto</option>), maximum read and write request sizes (<option>rsize</option>
   and <option>wsize</option>), or a specific NFS version (<option>vers</option>),
   use the <option>-o</option> option.
  </para>
  <para>
   For example:
  </para>
<screen>&prompt.root;<command>mount -o proto=tcp,rsize=32768,wsize=32768,vers=3 \
&nfs-vip-exports;:/srv/nfs/share /home/share</command></screen>
  <para>
   For further NFS mount options, refer to the <command>nfs</command> man page.
  </para>
  <note>
   <title>Loopback mounts</title>
   <para>
    Loopback mounts are only supported for NFS version 3, <emphasis>not</emphasis>
    NFS version 4. For more information, refer to
    <link xlink:href="https://www.suse.com/support/kb/doc/?id=000018709"/>.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-quick-nfs-more-info">
  <title>For more information</title>
  <itemizedlist>
   <listitem>
    <para>
     For more details about the steps in this guide, refer to
     <link xlink:href="https://www.suse.com/support/kb/doc/?id=000020396"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more information about NFS and LVM, refer to
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html">
     &storage_guide; for &sles;</link>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more information about DRBD, refer to <xref linkend="cha-ha-drbd"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <xi:include href="common_legal.xml"/>
 </article>
