<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-fencing">
<!-- http://hg.clusterlabs.org/pacemaker/stable-1.0/file/tip/doc/crm_fencing.txt -->
 <title>Fencing and &stonith;</title>
 <info>
      <abstract>
        <para>
    Fencing is a very important concept in computer clusters for HA (High
    Availability). A cluster sometimes detects that one of the nodes is
    behaving strangely and needs to remove it. This is called
    <emphasis>fencing</emphasis> and is commonly done with a &stonith;
    resource. Fencing may be defined as a method to bring an HA cluster to a
    known state.
   </para>
        <para>
    Every resource in a cluster has a state attached. For example:
    <quote>resource r1 is started on &node1;</quote>. In an HA cluster, such
    a state implies that <quote>resource r1 is stopped on all nodes except
    &node1;</quote>, because the cluster must make sure that every resource
    may be started on only one node. Every node must report every change
    that happens to a resource. The cluster state is thus a collection of
    resource states and node states.
   </para>
        <para>
    When the state of a node or resource cannot be established with
    certainty, fencing comes in. Even when the cluster is not aware of what
    is happening on a given node, fencing can ensure that the node does not
    run any important resources.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <!-- http://hg.clusterlabs.org/pacemaker/stable-1.0/file/tip/doc/crm_fencing.txt -->
 
 <sect1 xml:id="sec-ha-fencing-classes">
  <title>Classes of fencing</title>

  <para>
   There are two classes of fencing: resource level and node level fencing.
   The latter is the primary subject of this chapter.
  </para>

  <variablelist>
   <varlistentry>
    <term>Resource level fencing</term>
    <listitem>
     <para>Resource level fencing ensures exclusive access to a given resource.
      Common examples of this are changing the zoning of the node from a SAN fiber
      channel switch (thus locking the node out of access to its disks) or methods
      like SCSI reserve. For examples, refer to <xref
      linkend="sec-ha-storage-protect-rsc-fencing"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Node level fencing</term>
    <listitem>
     <para>
      Node level fencing prevents a failed node from accessing shared resources
      entirely. This is usually done in a simple and abrupt way: reset or power
      off the node.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ha-fencing-nodes">
  <title>Node level fencing</title>

  <para>
   In a Pacemaker cluster, the implementation of node level fencing is &stonith;
   (Shoot The Other Node in the Head). The &hasi;
   includes the <command>stonith</command> command line tool, an extensible
   interface for remotely powering down a node in the cluster. For an
   overview of the available options, run <command>stonith --help</command>
   or refer to the man page of <command>stonith</command> for more
   information.
  </para>

  <sect2 xml:id="sec-ha-fencing-nodes-devices">
   <title>&stonith; devices</title>
   <para>
    To use node level fencing, you first need to have a fencing device. To
    get a list of &stonith; devices which are supported by the &hasi;, run
    one of the following commands on any of the nodes:
   </para>
<screen>&prompt.root;stonith -L</screen>
   <para>
    or
   </para>
<screen>&prompt.root;crm ra list stonith</screen>
   <para>
    &stonith; devices may be classified into the following categories:
   </para>
   <variablelist>
    <varlistentry>
     <term>Power Distribution Units (PDU)</term>
     <listitem>
      <para>
       Power Distribution Units are an essential element in managing power
       capacity and functionality for critical network, server and data
       center equipment. They can provide remote load monitoring of
       connected equipment and individual outlet power control for remote
       power recycling.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Uninterruptible Power Supplies (UPS)</term>
     <listitem>
      <para>
       A stable power supply provides emergency power to connected equipment
       by supplying power from a separate source if a utility
       power failure occurs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Blade power control devices</term>
     <listitem>
      <para>
       If you are running a cluster on a set of blades, then the power
       control device in the blade enclosure is the only candidate for
       fencing. Of course, this device must be capable of managing single
       blade computers.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Lights-out devices</term>
     <listitem>
      <para>
       Lights-out devices (IBM RSA, HP iLO, Dell DRAC) are becoming
       increasingly popular and may even become standard in off-the-shelf
       computers. However, they are inferior to UPS devices, because they
       share a power supply with their host (a cluster node). If a node
       stays without power, the device supposed to control it would be
       useless. In that case, the CRM would continue its attempts to fence
       the node indefinitely while all other resource operations would wait
       for the fencing/&stonith; operation to complete.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Testing devices</term>
     <listitem>
      <para>
       Testing devices are used exclusively for testing purposes. They are
       usually more gentle on the hardware. Before the cluster goes into
       production, they must be replaced with real fencing devices.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The choice of the &stonith; device depends mainly on your budget and the
    kind of hardware you use.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-fencing-nodes-implementation">
   <title>&stonith; implementation</title>
   <para>
    The &stonith; implementation of &productnamereg; consists of two
    components:
   </para>
   <variablelist>
    <varlistentry>
     <term>pacemaker-fenced</term>
     <listitem>
      <para>
       <systemitem class="daemon">pacemaker-fenced</systemitem> is a daemon which can be accessed by local processes or over
       the network. It accepts the commands which correspond to fencing
       operations: reset, power-off, and power-on. It can also check the
       status of the fencing device.
      </para>
      <para>
       The <systemitem class="daemon">pacemaker-fenced</systemitem> daemon runs on every node in the &ha; cluster. The
       <systemitem class="resource">pacemaker-fenced</systemitem> instance running on the DC node receives a fencing request
       from the <systemitem class="daemon">pacemaker-controld</systemitem>. It
       is up to this and other <systemitem class="daemon">pacemaker-fenced</systemitem> programs to carry
       out the desired fencing operation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&stonith; plug-ins</term>
     <listitem>
      <para>
       For every supported fencing device there is a &stonith; plug-in which
       is capable of controlling said device. A &stonith; plug-in is the
       interface to the fencing device. The &stonith; plug-ins contained in
       the <package>cluster-glue</package> package reside in
       <filename>/usr/lib64/stonith/plugins</filename> on each node.
       (If you installed the
       <package>fence-agents</package> package, too,
       the plug-ins contained there are installed in
       <filename>/usr/sbin/fence_*</filename>.) All &stonith; plug-ins look
       the same to <systemitem class="daemon">pacemaker-fenced</systemitem>,
       but are quite different on the other side, reflecting the nature of the
       fencing device.
      </para>
      <para>
       Some plug-ins support more than one device. A typical example is
       <literal>ipmilan</literal> (or <literal>external/ipmi</literal>)
       which implements the IPMI protocol and can control any device which
       supports this protocol.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ha-fencing-config">
  <title>&stonith; resources and configuration</title>

  <para>
   To set up fencing, you need to configure one or more &stonith;
   resources&mdash;the <systemitem class="daemon">pacemaker-fenced</systemitem> daemon requires no configuration. All
   configuration is stored in the CIB. A &stonith; resource is a resource of
   class <literal>stonith</literal> (see
   <xref linkend="sec-ha-config-basics-raclasses"/>). &stonith; resources
   are a representation of &stonith; plug-ins in the CIB. Apart from the
   fencing operations, the &stonith; resources can be started, stopped and
   monitored, like any other resource. Starting or stopping &stonith;
   resources means loading and unloading the &stonith; device driver on a
   node. Starting and stopping are thus only administrative operations and
   do not translate to any operation on the fencing device itself. However,
   monitoring does translate to logging it to the device (to verify that the
   device will work in case it is needed). When a &stonith; resource fails
   over to another node it enables the current node to talk to the &stonith;
   device by loading the respective driver.
  </para>

  <para>
   &stonith; resources can be configured like any other resource. For
   details how to do so with your preferred cluster management tool:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &hawk2;: <xref linkend="sec-conf-hawk2-rsc-stonith"/>
    </para>
   </listitem>
   <listitem>
    <para>
     &crmsh;: <xref linkend="sec-ha-manual-create-stonith"/>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The list of parameters (attributes) depends on the respective &stonith;
   type. To view a list of parameters for a specific device, use the
   <command>stonith</command> command:
  </para>

<screen>stonith -t <replaceable>stonith-device-type</replaceable> -n</screen>

  <para>
   For example, to view the parameters for the <literal>ibmhmc</literal>
   device type, enter the following:
  </para>

<screen>stonith -t ibmhmc -n</screen>

<!--    <note>
    <para> It is easy to guess the class of a fencing device from the
     set of attribute names. </para>
   </note>-->

  <para>
   To get a short help text for the device, use the <option>-h</option>
   option:
  </para>

<screen>stonith -t <replaceable>stonith-device-type</replaceable> -h</screen>

  <sect2 xml:id="sec-ha-fencing-config-examples">
   <title>Example &stonith; resource configurations</title>
   <para>
    In the following, find some example configurations written in the syntax
    of the <command>crm</command> command line tool. To apply them, put the
    sample in a text file (for example, <filename>sample.txt</filename>) and
    run:
   </para>
<screen>&prompt.root;<command>crm</command> &lt; sample.txt</screen>
   <para>
    For more information about configuring resources with the
    <command>crm</command> command line tool, refer to
    <xref linkend="cha-ha-manual-config"/>.
   </para>
   <example>
    <title>Configuration of an IBM RSA lights-out device</title>
    <para>
     An IBM RSA lights-out device might be configured like this:
    </para>
<screen>configure
primitive st-ibmrsa-1 stonith:external/ibmrsa-telnet \
params nodename=&node1; ip_address=192.168.0.101 \
username=USERNAME password=PASSW0RD
primitive st-ibmrsa-2 stonith:external/ibmrsa-telnet \
params nodename=&node2; ip_address=192.168.0.102 \
username=USERNAME password=PASSW0RD
location l-st-&node1; st-ibmrsa-1 -inf: &node1;
location l-st-&node2; st-ibmrsa-2 -inf: &node2;
commit</screen>
    <para>
     In this example, location constraints are used for the following
     reason: There is always a certain probability that the &stonith;
     operation is going to fail. Therefore, a &stonith; operation on the
     node which is the executioner as well is not reliable. If the node is
     reset, it cannot send the notification about the fencing operation
     outcome. The only way to do that is to assume that the operation is
     going to succeed and send the notification beforehand. But if the
     operation fails, problems could arise. Therefore, by convention,
     <systemitem class="daemon">pacemaker-fenced</systemitem> refuses to terminate its host.
    </para>
   </example>
   <example>
    <title>Configuration of a UPS fencing device</title>
    <para>
     The configuration of a UPS type fencing device is similar to the
     examples above. The details are not covered here. All UPS devices
     employ the same mechanics for fencing. How the device is accessed
     varies. Old UPS devices only had a serial port, usually connected at
     1200baud using a special serial cable. Many new ones still have a
     serial port, but often they also use a USB or Ethernet interface. The
     kind of connection you can use depends on what the plug-in supports.
    </para>
    <para>
     For example, compare the <literal>apcmaster</literal> with the
     <literal>apcsmart</literal> device by using the <command>stonith
     -t</command> <replaceable>stonith-device-type</replaceable> -n command:
    </para>
<screen>stonith -t apcmaster -h</screen>
    <para>
     returns the following information:
    </para>
<screen>STONITH Device: apcmaster - APC MasterSwitch (via telnet)
NOTE: The APC MasterSwitch accepts only one (telnet)
connection/session a time. When one session is active,
subsequent attempts to connect to the MasterSwitch will fail.
For more information see http://www.apc.com/
List of valid parameter names for apcmaster STONITH device:
        ipaddr
        login
        password
For Config info [-p] syntax, give each of the above parameters in order as
the -p value.
Arguments are separated by white space.
Config file [-F] syntax is the same as -p, except # at the start of a line
denotes a comment</screen>
    <para>
     With
    </para>
<screen>stonith -t apcsmart -h</screen>
    <para>
     you get the following output:
    </para>
<screen>STONITH Device: apcsmart - APC Smart UPS
(via serial port - NOT USB!). 
Works with higher-end APC UPSes, like
Back-UPS Pro, Smart-UPS, Matrix-UPS, etc.
(Smart-UPS may have to be &gt;= Smart-UPS 700?).
See http://www.networkupstools.org/protocols/apcsmart.html
for protocol compatibility details.
For more information see http://www.apc.com/
List of valid parameter names for apcsmart STONITH device:
ttydev
hostlist</screen>
    <para>
     The first plug-in supports APC UPS with a network port and telnet
     protocol. The second plug-in uses the APC SMART protocol over the
     serial line, which is supported by many APC UPS product
     lines.
    </para>
   </example>
   <example xml:id="ex-ha-fencing-kdump">
    <title>Configuration of a Kdump device</title>
    <para>Kdump belongs to the <xref linkend="sec-ha-fencing-special"
    xrefstyle="select:title"/> and is in fact the opposite of a fencing device.
     The plug-in checks if a Kernel dump is in progress on a node. If so, it
     returns true, and acts <emphasis>as if</emphasis> the node has been fenced.
    </para>
    <para>
     The Kdump plug-in must be used in concert with another, real &stonith;
     device, for example, <literal>external/ipmi</literal>. For the fencing
     mechanism to work properly, you must specify that Kdump is checked before
     a real &stonith; device is triggered. Use <command>crm configure
     fencing_topology</command> to specify the order of the fencing devices as
     shown in the following procedure.
   </para>
   <procedure>
    <step>
    <para>
     Use the <literal>stonith:fence_kdump</literal> resource agent (provided
     by the package <package>fence-agents</package>)
     to monitor all nodes with the Kdump function enabled. Find a
     configuration example for the resource below:
    </para>
<screen>configure
  primitive st-kdump stonith:fence_kdump \
    params nodename="&node1; "\ <co xml:id="co-ha-fenc-kdump-nodename"/>
    pcmk_host_check="static-list" \
    pcmk_reboot_action="off" \
    pcmk_monitor_action="metadata" \
    pcmk_reboot_retries="1" \
    timeout="60"
commit</screen>
    <calloutlist>
     <callout arearefs="co-ha-fenc-kdump-nodename">
      <para>
       Name of the node to be monitored. If you need to monitor more than one
       node, configure more &stonith; resources. To prevent a specific node
       from using a fencing device, add location constraints.
      </para>
     </callout>
    </calloutlist>
    <para>
     The fencing action will be started after the timeout of the resource.
    </para>
   </step>
   <step>
    <para>
     In <filename>/etc/sysconfig/kdump</filename> on each node, configure
     <literal>KDUMP_POSTSCRIPT</literal> to send a notification to all nodes
     when the Kdump process is finished. For example:
    </para>
<screen>/usr/lib/fence_kdump_send -i <replaceable>INTERVAL</replaceable> -p <replaceable>PORT</replaceable> -c 1 &node1; &node2; &node3; [...]</screen>
    <para>
     The node that does a Kdump will restart automatically after Kdump has
     finished.
    </para>
   </step>
   <step>
    <para>
     Write a new <filename>initrd</filename> to include the library <literal>fence_kdump_send</literal>
     with network enabled. Use the <option>-f</option> option to overwrite
     the existing file, so the new file will be used for the next boot process:
    </para>
<screen>&prompt.root;dracut -f -a kdump</screen>
   </step>
   <step>
    <para>
     Open a port in the firewall for the <literal>fence_kdump</literal> resource.
     The default port is <literal>7410</literal>.
    </para>
   </step>
   <step>
    <para>
     To achieve that Kdump is checked before triggering a real fencing
     mechanism (like <literal>external/ipmi</literal>),
     use a configuration similar to the following:</para>
    <screen>fencing_topology \
  &node1;: kdump-node1 ipmi-node1 \
  &node2;: kdump-node2 ipmi-node2</screen>
    <para>For more details on <option>fencing_topology</option>:
    </para>
   <screen>crm configure help fencing_topology</screen>
  </step>
 </procedure>
 </example>
   </sect2>
 </sect1>
 <sect1 xml:id="sec-ha-fencing-monitor">
  <title>Monitoring fencing devices</title>

  <para>
   Like any other resource, the &stonith; class agents also support the
   monitoring operation for checking status.
  </para>

<!--fate#310010-->

  <note>
   <title>Monitoring &stonith; resources</title>
   <para>
    Monitor &stonith; resources regularly, yet sparingly. For most devices a
    monitoring interval of at least 1800 seconds (30 minutes) should
    suffice.
   </para>
  </note>

  <para>
   Fencing devices are an indispensable part of an HA cluster, but the less
   you need to use them, the better. Power management equipment is often
   affected by too much broadcast traffic. Some devices cannot handle more
   than ten or so connections per minute. Some get confused if two clients
   try to connect at the same time. Most cannot handle more than one session
   at a time.
  </para>

  <para>
   Checking the status of fencing devices once every few hours should
   usually be enough. The probability that a fencing operation needs to be
   performed and the power switch fails is low.
  </para>

  <para>
   For detailed information on how to configure monitor operations, refer to
<!--
   <xref linkend="pro-ha-config-gui-parameters"/> for the GUI approach or to-->
   <xref linkend="sec-ha-manual-config-monitor"/> for the command line
   approach.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-fencing-special">
  <title>Special fencing devices</title>

  <para>
   In addition to plug-ins which handle real &stonith; devices, there are
   special purpose &stonith; plug-ins.
  </para>

  <warning>
   <title>For testing only</title>
   <para>
    Some &stonith; plug-ins mentioned below are for demonstration and
    testing purposes only. Do not use any of the following devices in
    real-life scenarios because this may lead to data corruption and
    unpredictable results:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>external/ssh </literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>ssh </literal>
     </para>
    </listitem>
<!--<listitem>
     <para>
      <literal>null</literal>
     </para>
    </listitem>-->
   </itemizedlist>
  </warning>

  <variablelist>
<!--Fate#317135-->
   <varlistentry xml:id="vle-fence-kdump">
    <term><literal>fence_kdump</literal>
    </term>
    <listitem>
     <para>
      This plug-in checks if a Kernel dump is in progress on a node. If so,
      it returns <literal>true</literal>, and acts as if the node has been
      fenced. The node cannot run any resources during the dump anyway. This
      avoids fencing a node that is already down but doing a dump, which
      takes some time. The plug-in must be used in concert with another,
      real &stonith; device.
     </para>
     <para>
      For configuration details, see <xref linkend="ex-ha-fencing-kdump"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>external/sbd</literal>
    </term>
    <listitem>
     <para>
      This is a self-fencing device. It reacts to a so-called <quote>poison
      pill</quote> which can be inserted into a shared disk. On
      shared-storage connection loss, it stops the node from operating.
      Learn how to use this &stonith; agent to implement storage-based
      fencing in
      <xref linkend="cha-ha-storage-protect" xrefstyle="select:label nopage"/>,
      <xref linkend="pro-ha-storage-protect-fencing"/>. See also
      <link xlink:href="http://www.linux-ha.org/wiki/SBD_Fencing"/> for more
      details.
     </para>
<!--latest feedback by Fabian Herrschel/lmb, also ack'ed by emap -->
     <important>
      <title><literal>external/sbd</literal> and DRBD</title>
      <para>
       The <literal>external/sbd</literal> fencing mechanism requires that
       the SBD partition is readable directly from each node. Thus, a DRBD*
       device must not be used for an SBD partition.
      </para>
      <para>
       However, you can use the fencing mechanism for a DRBD cluster,
       provided the SBD partition is located on a shared disk that is not
       mirrored or replicated.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>external/ssh</literal>
    </term>
    <listitem>
     <para>
      Another software-based <quote>fencing</quote> mechanism. The nodes
      must be able to log in to each other as &rootuser; without passwords.
      It takes a single parameter, <literal>hostlist</literal>, specifying
      the nodes that it will target. As it is not able to reset a truly
      failed node, it must not be used for real-life clusters&mdash;for
      testing and demonstration purposes only. Using it for shared storage
      would result in data corruption.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>meatware</literal>
    </term>
    <listitem>
     <para>
      <literal>meatware</literal> requires help from the user to operate.
      Whenever invoked, <literal>meatware</literal> logs a CRIT severity
      message which shows up on the node's console. The operator then
      confirms that the node is down and issues a
      <command>meatclient(8)</command> command. This tells
      <literal>meatware</literal> to inform the cluster that the node should
      be considered dead. See
      <filename>/usr/share/doc/packages/cluster-glue/README.meatware</filename>
      for more information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>suicide</literal>
    </term>
    <listitem>
     <para>
      This is a software-only device, which can reboot a node it is running
      on, using the <command>reboot</command> command. This requires action
      by the node's operating system and can fail under certain
      circumstances. Therefore avoid using this device whenever possible.
      However, it is safe to use on one-node clusters.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Diskless SBD</term>
    <listitem>
     <para>&sbd-diskless;</para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   <literal>suicide</literal>
<!--and <literal>null</literal> -->
   is the only exception to the <quote>I do not shoot my host</quote> rule.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-fencing-recommend">
  <title>Basic recommendations</title>

  <para>
   Check the following list of recommendations to avoid common mistakes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Do not configure several power switches in parallel.
    </para>
   </listitem>
   <listitem>
    <para>
     To test your &stonith; devices and their configuration, pull the plug
     once from each node and verify that fencing the node does takes place.
    </para>
   </listitem>
   <listitem>
    <para>
     Test your resources under load and verify the timeout values are
     appropriate. Setting timeout values too low can trigger (unnecessary)
     fencing operations. For details, refer to
     <xref linkend="sec-ha-config-basics-timeouts"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Use appropriate fencing devices for your setup. For details, also refer
     to <xref linkend="sec-ha-fencing-special"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Configure one or more &stonith; resources. By default, the global
     cluster option <literal>stonith-enabled</literal> is set to
     <literal>true</literal>. If no &stonith; resources have been defined,
     the cluster will refuse to start any resources.
    </para>
   </listitem>
   <listitem>
    <para>
     Do not set the global cluster option
     <systemitem>stonith-enabled</systemitem> to <literal>false</literal>
     for the following reasons:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Clusters without &stonith; enabled are not supported.
      </para>
     </listitem>
     <listitem>
      <para>
       DLM/OCFS2 will block forever waiting for a fencing operation that
       will never happen.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Do not set the global cluster option
     <systemitem>startup-fencing</systemitem> to <literal>false</literal>.
     By default, it is set to <literal>true</literal> for the following
     reason: If a node is in an unknown state during cluster start-up, the
     node will be fenced once to clarify its status.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-ha-fencing-more">
  <title>For more information</title>

  <variablelist>
   <varlistentry>
    <term><filename>/usr/share/doc/packages/cluster-glue</filename>
    </term>
    <listitem>
     <para>
      In your installed system, this directory contains README files for
      many &stonith; plug-ins and devices.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://www.linux-ha.org/wiki/STONITH"/>
    </term>
    <listitem>
     <para>
      Information about &stonith; on the home page of The High
      Availability Linux Project.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://www.clusterlabs.org/pacemaker/doc/"/>
    </term>
    <listitem>
     <itemizedlist>
<!-- taroth 2015-05-06: commenting for now since it is a little bit outdated
       (clones are still used in stonith configuration examples)
       <listitem>
       <para>
       <citetitle>Configuring Fencing with crmsh</citetitle>: Information about
       fencing on the home page of the Pacemaker Project.
       </para>
       </listitem>-->
      <listitem>
       <para>
        &paceex;: Explains the concepts used to configure Pacemaker.
        Contains comprehensive and very detailed information for reference.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html"/>
    </term>
    <listitem>
     <para>
      Article explaining the concepts of split brain, quorum and fencing in
      HA clusters.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
