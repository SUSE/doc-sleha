<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<!--
  TODO:
  * SBD configuration
  * crm shell -> Hawk2
  * sharing paragraphs through entities (system requirements)
  * spell check and style check
  * technical proofread by Kristoffer
-->

<?provo dirname="install_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art.ha.install.quick"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
<?suse-quickstart columns="no" version="2"?>
  <!--https://fate.suse.com/320823: [DOC] HA quick start document-->
  <title>&instquick;</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp?></date>
  <xi:include href="ha_authors.xml"/>
  <abstract>
   <para>
    &abstract-instquick;
   </para>
  </abstract>
  <dm:docmanager>
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:product>SUSE Linux Enterprise High Availability Extension 12
     SP2</dm:product>
    <dm:component>Documentation</dm:component>
   </dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

  <sect1 xml:id="sec.ha.inst.quick.usage-scenario">
   <title>Usage Scenario</title>
   <para>
    The procedures in this document will lead to a minimal setup of a two-node
    cluster with the following properties:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Two nodes: <systemitem class="server">&node1;</systemitem> (IP: <systemitem
       class="ipaddress">&subnetI;.1</systemitem>)
      and <systemitem class="server">&node2;</systemitem> (IP: <systemitem
       class="ipaddress">&subnetI;.2</systemitem>),
      connected to each other via network.
     </para>
    </listitem>
    <listitem>
     <para>
      A floating, virtual IP address (<systemitem class="ipaddress"
       >&subnetII;.1</systemitem>) which
      allows clients to connect to the service no matter which physical node it
      is running on.
     </para>
    </listitem>
    <listitem>
     <para>A shared storage device, used as SBD fencing mechanism.
      This avoids split brain scenarios.
     </para>
    </listitem>
    <listitem>
     <para>
      Failover of resources from one node to the other if the active host breaks
      down (<emphasis>active/passive</emphasis> setup).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    After setup of the cluster with the bootstrap scripts, we will monitor
    the cluster with the graphical &haweb; (&hawk;), one of the cluster management
    tools included with &productnamereg;. As a basic test if failover of resources
    works, we will put one of the nodes into standby mode and check if the
    virtual IP address is migrated to the second node.
   </para>
   <para>
    You can use the two-node cluster for testing purposes or as a minimal
    cluster configuration that you can extended later on. Before using the
    cluster in a production environment, modify it according to your
    requirements.
   </para>
  </sect1>

  <sect1 xml:id="sec.ha.inst.quick.req">
   <title>System Requirements</title>
   <para>
    This section informs you about the key system requirements for the
    scenario described in <xref linkend="sec.ha.inst.quick.usage-scenario"/>.
    If you want to adjust the cluster for use in a production environment,
    read the full list of <citetitle>System Requirements and
    Recommendations</citetitle> in the <citetitle>&admin;</citetitle>
    for &productnamereg;: <link
    xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_requirements.html"/>.
   </para>
  <variablelist xml:id="vl.ha.inst.quick.req.hw">
   <title>Hardware Requirements</title>
   <varlistentry>
    <term>Servers</term>
    <listitem>
    <para>
     Two servers with software as specified in <xref
     linkend="il.ha.inst.quick.req.sw"/>.
     </para>
     &sys-req-hw-nodes;
     <!--<para>
     The servers can be bare metal or virtual machines. They do not require
     identical hardware (memory, disk space, etc.), but they must have the
     same architecture. Cross-platform clusters are not supported.
     </para>-->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Communication Channels</term>
   <listitem>
    &sys-req-hw-comm-channels;
   </listitem>
   </varlistentry>
   <varlistentry>
    <term>Node Fencing/&stonith;</term>
   <listitem>
    &sys-req-hw-stonith;
   </listitem>
   </varlistentry>
  </variablelist>

   <para>
    On all nodes that will be part of the cluster the following software
    must be installed:
   </para>

  <itemizedlist xml:id="il.ha.inst.quick.req.sw">
   <title>Software Requirements</title>
    <listitem>
     &sys-req-sw-sles;
    </listitem>
    <listitem>
     &sys-req-sw-sleha;
    </listitem>
   </itemizedlist>

  <variablelist xml:id="vl.ha.inst.quick.req.other">
   <title>Other Requirements and Recommendations</title>
   <varlistentry>
    <term>Time Synchronization</term>
    &sys-req-other-ntp;
   </varlistentry>
   <varlistentry>
    <term>Host Name and IP Address</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        Use static IP addresses.
       </para>
      </listitem>
      <listitem>
       &sys-req-other-etc-hosts;
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.ha.req.ssh">
    <term>SSH</term>
    <listitem>
     &sys-req-other-ssh;
     <para>
      If you use the bootstrap scripts for setting up the cluster, the SSH keys
      will automatically created and copied.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  </sect1>

 <sect1 xml:id="sec.ha.inst.quick.bootstrap">
  <title>Overview of the Bootstrap Scripts</title>
  <para>
   All commands from the <package>ha-cluster-bootstrap</package> package
   execute bootstrap scripts that require only a minimum of time and manual
   intervention.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     With <command>ha-cluster-init</command>, define the basic parameters needed
     for cluster communication. The leaves you with a running one-node cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>ha-cluster-join</command>, add more nodes to your cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>ha-cluster-remove</command>, remove nodes from your cluster.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   All bootstrap scripts log to <filename>/var/log/ha-cluster-bootstrap.log</filename>.
   Check this file for any details of the bootstrap process. Any options set
   during the bootstrap process can be modified later with the
   &yast; cluster module. See
   <link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/sec_ha_installation_setup_manual.html"/>
   for details.
  </para>
  <para>Each script comes with a man page covering the range of functions, the
   script's options, and an overview of the files the script can create and modify.
  </para>
  <para>
   The bootstrap script <command>ha-cluster-init</command> checks and
   configures the following components:
  </para>

  <variablelist>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      If NTP has not been configured to start at boot time, a message appears.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>It creates SSH keys for passwordless login between cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&csync;</term>
    <listitem>
     <para>
      It configures &csync; to replicate configuration files across all nodes
      in a cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&corosync;</term>
    <listitem>
     <para>It configures the cluster communication system.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD/Watchdog</term>
    <listitem>
     <para>It checks if a watchdog exists and asks you if to configure SBD
      as node fencing mechanism.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual Floating IP</term>
    <listitem>
     <para>It asks you if to configure a virtual IP address for cluster
      administration with &hawk2;.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Firewall</term>
    <listitem>
     <para>It opens the ports in the firewall that are needed for cluster communication.</para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

  <sect1 xml:id="sec.ha.inst.quick.installation">
    <title>Installing &sls; and &ha; Extension</title>
    <para>
      The packages for configuring and managing a cluster with the
      &hasi; are included in the <literal>&ha;</literal> installation
      pattern. This pattern is only available after &productname; has been
      installed as an extension to &slsreg;.
    </para>
    <para>
      For information on how to install
      extensions, see the <citetitle>&sle; &productnumber;
      &deploy;</citetitle>:
     <link
      xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
    </para>

    <procedure xml:id="pro.ha.inst.quick.pattern">
      <title>Installing the &ha; Pattern</title>
     <para>If the pattern is not installed yet, proceed as follows:</para>
      <step>
        <para>
          Start &yast; and select <menuchoice>
            <guimenu>Software</guimenu> <guimenu>Software Management</guimenu>
          </menuchoice>.
        </para>
      </step>
      <step>
        <para>
         Click the <guimenu>Patterns</guimenu> tab and activate the <guimenu>High
            Availability</guimenu> pattern in the pattern list.
          <!--SLE HA GEO: <guimenu>GEO Clustering for High Availability</guimenu>-->
        </para>
      </step>
      <step>
        <para>
          Click <guimenu>Accept</guimenu> to start installing the packages.
        </para>
      </step>
      <step>
       <para>
          Install the &ha; pattern on <emphasis>all</emphasis> machines that
          will be part of your cluster.
       </para>
       <note>
        <title>Installing Software Packages on All Parties</title>
        <para>
         For an automated installation of &sls; &productnumber; and &productname;
         &productnumber; use &ay; to clone existing nodes. For more information
         see <link
          xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/sec_ha_installation_autoyast.html"/>.
        </para>
       </note>
      </step>
     <step>
      <para>
       Register the machines at &scc;. Find more information at
       <link xlink:href="https://www.suse.com/doc/sles-12/book_sle_deployment/data/sec_update_registersystem.html"/>.
      </para>
     </step>
    </procedure>
  </sect1>

 <sect1 xml:id="sec.ha.inst.quick.sbd">
  <title>Using SBD as Fencing Mechanism</title>
  <!-- For more information, see:
   * https://trello.com/c/rclDZHPR
   * http://www.linux-ha.org/wiki/SBD_Fencing
  -->
   <!--taroth 2016-07-21: the following is copied from
    /usr/sbin/ha-cluster-init: If you have shared storage, for example a SAN or
    iSCSI target, you can use it to avoid split brain scenarios-->
   <para>
    If you have shared storage, for example, a SAN (Storage Area Network),
    you can use it to avoid split brain scenarios by configuring SBD
    as node fencing mechanism. SBD uses watchdog support
    and the <literal>external/sbd</literal> &stonith; resource agent.
   </para>

  <sect2 xml:id="sec.ha.inst.quick.sbd.req">
   <title>Requirements for SBD</title>
   <para>
    During setup of the first node with <command>ha-cluster-init</command>, you
    can decide if to use SBD. If yes, you need to enter the path to the shared
    storage device. By default, <command>ha-cluster-init</command> will automatically
    create a small partition on the device to be used for SBD.
   </para>
   <para>To use SBD, the following requirements must be met:</para>

   <itemizedlist>
    <listitem>
     <para>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
     </para>
    </listitem>
    <listitem>
     <para> The SBD device <emphasis>must not</emphasis>
      use host-based RAID, cLVM2, nor reside on a DRBD* instance.
     </para>
    </listitem>
   </itemizedlist>

  <para>
   For details how to set up shared storage, refer to the <citetitle>&storage;</citetitle>
   for &sls; &productnumber;: <link
    xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/stor_admin.html"/>
  </para>
  <!--<remark>toms, 2016-07-30: (kai) miss a link to set up FC storage</remark>
  <remark>toms 2016-08-01: we don't have yet doc for FC storage.</remark>
  <variablelist>
   <varlistentry>
    <term>iSCSI</term>
    <listitem>
     <para><link xlink:href="https://www.suse.com/doc/sles-12/stor_admin/data/cha_iscsi.html"/></para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>FCoE</term>
   <listitem>
    <para><link xlink:href="https://www.suse.com/doc/sles-12/stor_admin/data/cha_fcoe.html"/></para>
   </listitem>
   </varlistentry>
   <varlistentry>
    <term>FC</term>
    <listitem>
     <para><remark>taroth 2016-08-04: the following was based a proposal by kdupke,
     replace with something more appropriate in the future (as soon as we have
     covered this in the storage guide)</remark>
      No special configuration needed, configure it as other FC storage.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>-->
  </sect2>

  <sect2 xml:id="sec.ha.inst.quick.sbd.setup">
   <title>Setting Up Softdog Watchdog and SBD</title>
   <!--Taken from ha_storage_protection.xml (pro.ha.storage.protect.watchdog)?-->
   <para>
    In &sls;, watchdog support in the kernel is enabled by default: It ships
    with several kernel modules that provide hardware-specific
    watchdog drivers. The &hasi; uses the SBD daemon as software component
    that <quote>feeds</quote> the watchdog.
   </para>
   <para>
    It is highly recommended to use the hardware watchdog that best fits your
    hardware.
   </para>

   <note>
    <title>Softdog Not Supported</title>
    <para>
     &suse; does not support <systemitem>softdog</systemitem> for
     production. Use <systemitem>softdog</systemitem> for testing purposes
     only. Before using the cluster in production environment, replace the
     <systemitem>softdog</systemitem> module with the respective hardware
     module. For details, refer to <!-- toms 2016-08-01:
      TODO for SP3: Use xref instead of link: <xref linkend="pro.ha.storage.protect.watchdog"/>.
     -->
     <link xlink:href="https://www.suse.com/doc/sle-ha-12/book_sleha/data/sec_ha_storage_protect_fencing.html#pro_ha_storage_protect_watchdog"/>.
    </para>
   </note>

   <para>
    The following procedure uses the <systemitem>softdog</systemitem>
    watchdog:
   </para>

   <procedure xml:id="pro.ha.inst.quick.sbd.setup">
    <step>
     <para>
      Create a persistent, shared storage as described in <xref
       linkend="sec.ha.inst.quick.sbd.req"/>.
     </para>
    </step>
    <step>
     <para>
      Enable the softdog watchdog:
     </para>
     <!-- See /usr/lib/ha-cluster-functions -->
     <screen>&prompt.root;<command>echo</command> softdog > /etc/modules-load.d/watchdog.conf
&prompt.root;<command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
      <remark>toms, 2016-07-30: (kai) doesn't ha_init check for a loaded watchdog?
       If so, this step can be skipped</remark>
      <remark>toms 2016-08-01: Tanja and I checked that, and according to
       ha-cluster-init this is NOT the case. We think, this hint is useful.
      </remark>
     <para>Test if the softdog module is loaded correctly:
     </para>
     <screen>&prompt.root;<command>lsmod</command> | grep dog
softdog                16384  1</screen>
    </step>
    <step>
     <para>
      On <systemitem class="server">&node2;</systemitem>, start SBD to listen
      on the SBD device:
     </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> watch</screen>
    </step>
    <step>
     <para>
      On <systemitem class="server">&node1;</systemitem>, send a test message:
     </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> message &node2; test<!--
Sep 22 17:01:00 &node1; sbd: [13412]: info: Received command test from &node1;--></screen>
     <remark>toms 2016-07-22: What to do when the test message fails? How to debug?</remark>
    </step>
    <step>
     <para>
      On <systemitem class="server">&node2;</systemitem>, check the status with
      <command>systemctl</command> and you should see the received message:
     </para>
     <screen>&prompt.root;<command>systemctl</command> status sbd
[...]
info: Received command test from &node1; on disk <replaceable>SBD</replaceable></screen>
    </step>
    <step>
     <para>
      Stop watching the SBD device on <systemitem class="server">&node2;</systemitem> with:
     </para>
     <screen>&prompt.root;<command>systemctl</command> stop sbd</screen>
    </step>
   </procedure>

   <para>
    Testing the SBD fencing mechanism for proper function in case of a split brain
    situation is highly recommended. Such a test can be done by blocking the &corosync;
    cluster communication.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.inst.quick.setup.1st-node">
  <title>Setting Up the First Node</title>
  <remark>toms 2016-08-01: @taroth: introducery para </remark>
  <para>
   Set up the first node with the <package>ha-cluster-init</package> script. This
   requires only a minimum of time and manual intervention.
  </para>

  <procedure xml:id="pro.ha.inst.quick.setup.ha-cluster-init">
   <title>Setting Up the First Node (<systemitem class="server">&node1;</systemitem>) with
    <command>ha-cluster-init</command></title>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine you want to
     use as cluster node.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
    <screen>&prompt.root;<command>ha-cluster-init</command></screen>
    <para>
     The scripts checks for NTP configuration and a hardware watchdog service.
     It generates the public and private SSH keys used for SSH access and
     &csync; synchronization and starts the respective services.
    </para>
    <!--<itemizedlist>
     <listitem>
      <para>Checks for a hardware watchdog device.</para>
     </listitem>
     <listitem>
      <para>
       Generate the public and private SSH keys, used for SSH access and
       &csync; syncronization.
      </para>
     </listitem>
     <listitem>
      <para>Start the SSH and &csync; services.</para>
     </listitem>
    </itemizedlist>-->
   </step>
   <step>
    <para>
     Configure the cluster communication layer (&corosync;):
    </para>
    <substeps>
     <step>
      <para>
       Enter a network address to bind to. By default, the script will
       propose the network address of <systemitem>eth0</systemitem>.
       Alternatively, enter a different network address, for example the
       address of <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast address. The script proposes a random address that
       you can use as default. Of course, your particular network need to
       support this multicast address.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast port. The script proposes <literal>5405</literal>
       as default.
      </para>
     </step>
    </substeps>
    <para>
     Finally, the script will start the &pace; service to bring the
     one-node cluster online and enable &hawk2;.
     The URL to use for &hawk2; is displayed on the screen.
    </para>
   </step>
   <step>
    <para>
    Set up SBD as node fencing mechanism:</para>
    <substeps>
     <step>
      <para>Confirm with <literal>y</literal> that you want to use SBD.</para>
     </step>
     <step>
      <para>Enter a persistent path to the partition of your block device that
       you want to use for SBD, see <xref linkend="sec.ha.inst.quick.sbd"/>.
       The path must be consistent across all nodes in the cluster.</para>
     </step>
    </substeps>
   </step>
   <step xml:id="st.ha-cluster-init.ip">
    <!-- taroth 2015-09-22: fate#318549: cluster-init: configure virtual IP for HAWK  -->
    <para>Configure a virtual IP address for cluster administration with
    &hawk2;. (We will use this virtual IP resource for testing successful
    failover later on).</para>
    <substeps>
     <step>
      <para>Confirm with <literal>y</literal> that you want to configure a
      virtual IP address.</para></step>
     <step>
      <para>Enter an unused IP address that you want to use as administration IP
       for &hawk2;: <literal>&subnetII;.1</literal>
      </para>
      <para>Instead of logging in to an individual cluster node with &hawk2;,
       you can connect to the virtual IP address.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     For any details of the setup process, check
     <filename>/var/log/ha-cluster-bootstrap.log</filename>.
    </para>
   </step>
   <step>
    <!--<remark>toms 2016-07-18: from FATE#320823, c#3</remark>-->
    <para>Check the &corosync; configuration file for the options specific to a
     two-node cluster:</para>
    <remark>toms, 2016-07-30: (kai) ??? Either this is in the RPM, or done by
      ha-init, or a bug we have to fix. I assume that these lines are there by
      default.</remark>
    <remark>toms 2016-08-01: Currently, this isn't the case. Not sure if this
     will be finished for SP2. Better we leave it there as this is important.
     For SP3, we should remove it.
    </remark>
    <substeps>
     <step><para>Open <filename>/etc/corosync/corosync.conf</filename> and locate
      the <literal>quorum</literal>
      section.</para>
     </step>
     <step><para>Check if the following key-value pairs exist:
     </para>
      <screen>quorum {
     provider: corosync_votequorum
     expected_votes: 2
     two_node: 1
     wait_for_all: 1
     }</screen>
     </step>
     <step>
      <para>If not, add the lines above. </para>
      <para> All of those parameters are needed for a two-node cluster setup.
      </para></step>
    </substeps>
   </step>
  </procedure>

  <para>
   You now have a running one-node cluster. To view it's status, proceed as follows:
  </para>

  <procedure xml:id="pro.ha.inst.quick.hawk2.login">
   <title>Logging In to the &hawk2; Web Interface</title>
   <step>
    <para> On any machine, start a Web browser and make sure that JavaScript and
     cookies are enabled. </para>
   </step>
   <step>
    <para> As URL, enter the IP address or host name of any cluster node running
     the &hawk; Web service. Alternatively, enter the address of the virtual
     IP address that you configured in <xref linkend="st.ha-cluster-init.ip"/>
     of <xref linkend="pro.ha.inst.quick.setup.ha-cluster-init"/>: </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
    <note>
     <title>Certificate Warning</title>
     <para> If a certificate warning appears when you try to access the URL for
      the first time, a self-signed certificate is in use. Self-signed
      certificates are not considered trustworthy by default. </para>
     <para> Ask your cluster operator for the certificate details to verify the
      certificate. </para>
     <para> To proceed anyway, you can add an exception in the browser to bypass
      the warning. </para>
     <!--FIXME: HAWK1 - see https://bugzilla.suse.com/show_bug.cgi?id=979095
      <para> For information on how to replace the self-signed certificate with a
      certificate signed by an official Certificate Authority, refer to
      <xref linkend="vle.ha.hawk.certificate"/>.</para>-->
    </note>
   </step>
   <step>
    <important>
     <title>Secure Password</title>
     <para>
      The bootstrap procedure creates a Linux user named
      <systemitem class="username">hacluster</systemitem> with the password
      <literal>linux</literal>. <!--You need it for logging in to &hawk2;.-->
      Replace the default password with a secure one as soon as possible:
     </para>
     <screen>&prompt.root;<command>passwd</command> hacluster</screen>
    </important>
    <para> On the &hawk2; login screen, enter the
      <guimenu>Username</guimenu> and <guimenu>Password</guimenu> of the
      <systemitem class="username">hacluster</systemitem> user. The user
      can be any other user that is a member of the <systemitem class="groupname"
      >haclient</systemitem> group. </para>
   </step>
   <step>
    <para>
     Click <guimenu>Log In</guimenu>. After login, the &hawk2; Web interface
     shows the Status screen by default, displaying the current cluster
     status at a glance:
    </para>
    <figure xml:id="fig.ha.inst.quick.one-node-status">
     <title>Status of the One-Node Cluster in &hawk2;</title>
     <mediaobject>
      <imageobject>
       <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.ha.inst.quick.setup.2nd-node">
  <title>Adding the Second Node</title>
  <para>
    If you have a one-node cluster up and running, add the second cluster
    node with the <command>ha-cluster-join</command> bootstrap
    script, as described in <xref linkend="pro.ha.inst.quick.setup.ha-cluster-join"/>.
    The script only needs access to an existing cluster node and
    will complete the basic setup on the current machine automatically.
    For details, refer to the <command>ha-cluster-join</command> man page.
  </para>
  <procedure xml:id="pro.ha.inst.quick.setup.ha-cluster-join">
   <title>Adding the Second Node (<systemitem class="server">&node2;</systemitem>) with
    <command>ha-cluster-join</command></title>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine supposed to
     join the cluster.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen>&prompt.root;<command>ha-cluster-join</command></screen>
    <para>
     If NTP has not been configured to start at boot time, a message
     appears. The script also checks for a hardware watchdog device (which
     is important in case you want to configure SBD) and warns you if none
     is present.
    </para>
   </step>
   <step>
    <para>
     If you decide to continue anyway, you will be prompted for the IP
     address of an existing node. Enter the IP address of the first node
     (<systemitem class="server">&node1;</systemitem>, <systemitem
      class="ipaddress">&subnetI;.1</systemitem>).
    </para>
   </step>
   <step>
    <para>
     If you have not already configured a passwordless SSH access between
     both machines, you will also be prompted for the &rootuser; password
     of the existing node.
    </para>
    <para>
     After logging in to the specified node, the script will copy the
     &corosync; configuration, configure SSH and &csync;, and will
     bring the current machine online as new cluster node. Apart from that,
     it will start the service needed for &hawk2;. <!--
     If you have configured shared storage with OCFS2, it will also
     automatically create the mount point directory for the OCFS2 file system.
     -->
    </para>
   </step>
   <step>
    <para>
     For details of the process, check
     <filename>/var/log/ha-cluster-bootstrap.log</filename>.
    </para>
   </step>
   <step>
    <remark>toms 2016-07-27: Fate#321076 Support Two-Node Cluster Setup in
     ha-cluster-init, ha-cluster-join, and/or ha-cluster-remove</remark>
    <para>
     Set the <parameter>pcmk_delay_max</parameter> to the SBD resource:
    </para>
    <screen>&prompt.root;<command>crm</command> resource param stonith-sbd set pcmk_delay_max 10</screen>
    <para>
     In a two-node cluster, it occurs quite frequently that both nodes will
     try to fence each other in case of split brain situation. To avoid
     this double fencing, add the above parameter to the configuration of
     the &stonith; resource. This gives servers with a working network card
     a better chance to survive.
     <remark>toms 2016-08-01: (lars) How does that work? I thought this is
      only a random	fencing delay, that makes one node shooting faster than
      the other.</remark>
    </para>
   </step>
  </procedure>

  <para>
   Check the cluster status in &hawk2;. Under <menuchoice>
    <guimenu>Status</guimenu>
    <guimenu>Nodes</guimenu>
   </menuchoice> you should see two nodes with a green status (see
   <xref linkend="fig.ha.inst.quick.two-node-cluster"/>).
  </para>

  <figure xml:id="fig.ha.inst.quick.two-node-cluster">
   <title>Status of the Two-Node Cluster</title>
   <mediaobject>
    <imageobject>
     <imagedata width="80%" fileref="installquick-two-nodecluster-status.png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   In case you need to remove and replace one of the nodes with another one,
   use the <command>ha-cluster-remove</command> script with the
   <option>-c</option>
   option:
  </para>
  <screen>&prompt.root;<command>ha-cluster-remove</command> <option>-c</option> <replaceable>IP_ADDR_OR_HOSTNAME</replaceable></screen>
 </sect1>

  <sect1 xml:id="sec.ha.inst.quick.test">
   <title>Testing the Cluster</title>
   <para>
    In case of a split brain situation, the cluster (or SBD) has to fence
    exactly one node.
    If you have not correctly set up SBD, the whole cluster will not work
    properly.
    To make sure the whole cluster configuration is correct, use the
    following procedure to test it:
   </para>
   <remark>toms 2016-07-27: Fate#321073
    Tool for Standardize Testing of Basic Cluster Functionality</remark>
   <procedure xml:id="pro.ha.inst.quick.test">
    <step>
     <para>
      Open a terminal and ping <systemitem>&subnetII;.1</systemitem>,
      your virtual IP address:
     </para>
     <screen>&prompt.root;<command>ping</command> &subnetII;.1</screen>
    </step>
    <step>
     <para>
      Log in to your cluster as described in <xref
       linkend="pro.ha.inst.quick.hawk2.login"/>.
     </para>
    </step>
    <step>
     <para>
      In &hawk2; <menuchoice>
       <guimenu>Status</guimenu>
       <guimenu>Resources</guimenu>
      </menuchoice>,
      check which node the virtual IP address (resource
      <systemitem>admin_addr</systemitem>) is running on. <!--As we did not
      configure any preferences, it will be an arbitrary node.-->
      We assume the resource is running on <systemitem class="server">&node1;</systemitem>.
      </para>
    </step>
    <step>
     <para>
      Put <systemitem class="server">&node1;</systemitem> into
      <guimenu>Standby</guimenu> mode (see <xref linkend="fig.ha.inst.quick.standby"/>).
     </para>
     <figure xml:id="fig.ha.inst.quick.standby">
      <title><systemitem class="server">&node1;</systemitem> in Standby Mode</title>
      <mediaobject>
       <imageobject>
        <imagedata width="60%" fileref="installquick-standby-node.png"/>
       </imageobject>
      </mediaobject>
     </figure>
     <!--
     <remark>toms 2016-07-18: Should we add an alternative method? I've tested
      it with "crm node standby NODE" to put the node "offline" temporarily.
      Sometimes an administrator doesn't want to go to a server room, so it
      could be more convenient.
     </remark>
     <remark>taroth 2016-07-20: good idea, need to discuss it with krig</remark>-->
    </step>
    <step>
     <para>
      Click <menuchoice>
       <guimenu>Status</guimenu>
       <guimenu>Resources</guimenu>
      </menuchoice>. The resource <systemitem>admin_addr</systemitem>
      has been migrated to <systemitem class="server">&node2;</systemitem>.
     </para>
    </step>
   </procedure>
   <para>
    During the migration, you should see an uninterrupted flow of pings to
    the virtual IP address. This shows that the cluster setup and the floating
    IP work correctly. Cancel the <command>ping</command> command with
    <keycombo>
       <keycap function="control"/><keycap>C</keycap>
      </keycombo>.
   </para>
   <para>
    The above procedure is a simple test to check if the cluster moves
    the virtual IP address to the other node. A realistic test involves
    specific use cases and scenarios. Before using the cluster in a production
    environment, test it thoroughly according to your use cases.
    <!-- toms 2016-08-01: For SP3, refer to the "Testing Cluster Guide" or
         to Lars Pinnes iptable rules
    -->
   </para>
  </sect1>

  <sect1 xml:id="sec.ha.inst.quick.moreinfo">
   <title>For More Information</title>
   <para>
    Find more documentation for this product at <link
     xlink:href="http://www.suse.com/documentation/sle-ha-12"/>. The
    documentation also includes a comprehensive &admin; for
    &productname;. Refer to it for further configuration and administration
    tasks.
   </para>
  </sect1>
 <xi:include href="common_legal.xml"/>
</article>
