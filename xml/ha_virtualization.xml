<?xml version="1.0"?>
<!DOCTYPE chapter [
  <!ENTITY % entities SYSTEM "generic-entities.ent">
  %entities;
]>

<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?><chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="cha-ha-virtualization" xml:lang="en" version="5.1">
  <title>&ha; for virtualization</title>
  <info xmlns:d="http://docbook.org/ns/docbook">
    <abstract>
      <para>
        This chapter explains how to configure virtual machines as highly available cluster resources.
      </para>
    </abstract>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker/>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  <revhistory xml:id="rh-cha-ha-virtualization">
   <revision>
     <date>2024-01-16</date>
     <revdescription>
       <para/>
     </revdescription>
   </revision>
 </revhistory>
</info>

  <sect1 xml:id="sec-ha-virtualization-overview">
    <title>Overview</title>
      <para>
      Virtual machines can take different roles in a &ha; cluster:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          A virtual machine can be managed by the cluster as a resource, without the cluster
          managing the services that run on the virtual machine. In this case, the VM is opaque
          to the cluster. This is the scenario described in this document.
        </para>
      </listitem>
      <listitem>
        <para>
          A virtual machine can be a cluster resource and run &pmremote;,
          which allows the cluster to manage services running on the virtual machine.
          In this case, the VM is a guest node and is transparent to the cluster.
          For this scenario, see <xref linkend="sec-ha-pmremote-install-virt-guest-nodes"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          A virtual machine can run a full cluster stack. In this case, the VM is a regular
          cluster node and is not managed by the cluster as a resource. For this scenario,
          see <xref linkend="article-installation"/>.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      The following procedures describe how to set up highly available virtual machines on
      block storage, with another block device used as an &ocfs; volume to store the VM lock files
      and XML configuration files. The virtual machines and the &ocfs; volume are configured as
      resources managed by the cluster, with resource constraints to ensure that the
      lock file directory is always available before a virtual machine starts on any node.
      This prevents the virtual machines from starting on multiple nodes.
    </para>
  </sect1>

  <sect1 xml:id="sec-ha-virtualization-requirements">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          A running &ha; cluster with at least two nodes and a fencing device such as SBD.
        </para>
      </listitem>
      <listitem>
        <para>
          Passwordless &rootuser; SSH login between the cluster nodes.
        </para>
      </listitem>
      <listitem>
        <para>
          A network bridge on each cluster node, to be used for installing and running the VMs.
          This must be separate from the network used for cluster communication and management.
        </para>
      </listitem>
      <listitem>
        <para>
          Two or more shared storage devices (or partitions on a single shared device),
          so that all cluster nodes can access the files and storage required by the VMs:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              A device to use as an &ocfs; volume, which will store the VM lock files and XML configuration files.
              Creating and mounting the &ocfs; volume is explained in the following procedure.
            </para>
          </listitem>
          <listitem>
            <para>
              A device containing the VM installation source (such as an ISO file or disk image).
            </para>
          </listitem>
          <listitem>
            <para>
              Depending on the installation source, you might also need another device for the
              VM storage disks.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          To avoid I/O starvation, these devices must be separate from the shared device used for SBD.
        </para>
      </listitem>
      <listitem>
        <para>
          Stable device names for all storage paths, for example,
          <filename>/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></filename>.
          A shared storage device might have mismatched <filename>/dev/sdX</filename> names on
          different nodes, which will cause VM migration to fail.
        </para>
      </listitem>
    </itemizedlist>
  </sect1>

  <sect1 xml:id="sec-ha-virtualization-configuring-cluster-resources">
    <title>Configuring cluster resources to manage the lock files</title>
    <para>
      Use this procedure to configure the cluster to manage the virtual machine lock files.
      The lock file directory must be available on all nodes so that the cluster is aware of the
      lock files no matter which node the VMs are running on.
    </para>
    <para>
      You only need to run the following commands on one of the cluster nodes.
    </para>
    <procedure xml:id="pro-ha-virtualization-configuring-cluster-resources">
      <title>Configuring cluster resources to manage the lock files</title>
      <step>
        <para>
          Create an &ocfs; volume on one of the shared storage devices:
        </para>
<screen>&prompt.root;<command>mkfs.ocfs2 /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
      </step>
      <step>
        <para>
          Run <command>crm configure</command> to start the <command>crm</command> interactive shell.
        </para>
      </step>
      <step>
        <para>
          Create a primitive resource for DLM:
        </para>
<screen>&prompt.crm.conf;<command>primitive dlm ocf:pacemaker:controld \
  op monitor interval=60 timeout=60</command></screen>
      </step>
      <step>
        <para>
          Create a primitive resource for the &ocfs; volume:
        </para>
<screen>&prompt.crm.conf;<command>primitive ocfs2 Filesystem \
  params device="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>" directory="/mnt/shared" fstype=ocfs2 \
  op monitor interval=20 timeout=40</command></screen>
      </step>
      <step>
        <para>
          Create a group for the DLM and &ocfs; resources:
        </para>
<screen>&prompt.crm.conf;<command>group g-virt-lock dlm ocfs2</command></screen>
      </step>
      <step>
        <para>
          Clone the group so that it runs on all nodes:
        </para>
<screen>&prompt.crm.conf;<command>clone cl-virt-lock g-virt-lock \
  meta interleave=true</command></screen>
      </step>
      <step>
        <para>
          Review your changes with <command>show</command>.
        </para>
      </step>
      <step>
        <para>
          If everything is correct, submit your changes with <command>commit</command>
          and leave the crm live configuration with <command>quit</command>.
        </para>
      </step>
      <step>
        <para>
          Check the status of the group clone. It should be running on all nodes:
        </para>
<screen>&prompt.root;<command>crm status</command>
[...]
Full List of Resources:
[...]
  * Clone Set: cl-virt-lock [g-virt-lock]:
    * Started: [ &node1; &node2; ]</screen>
      </step>
    </procedure>
    <para>

    </para>
  </sect1>

  <sect1 xml:id="sec-ha-virtualization-preparing-the-cluster-nodes">
    <title>Preparing the cluster nodes to host virtual machines</title>
    <para>
      Use this procedure to install and start the required virtualization services, and to
      configure the nodes to store the VM lock files on the shared &ocfs; volume.
    </para>
    <para>
      This procedure uses <command>crm cluster run</command> to run commands on all
      nodes at once. If you prefer to manage each node individually, you can omit the
      <command>crm cluster run</command> portion of the commands.
    </para>
    <procedure xml:id="pro-ha-virtualization-preparing-the-cluster-nodes">
      <title>Preparing the cluster nodes to host virtual machines</title>
      <step>
        <para>
          Install the virtualization packages on all nodes in the cluster:
        </para>
<screen>&prompt.root;<command>crm cluster run "zypper install -y -t pattern kvm_server kvm_tools"</command></screen>
      </step>
      <step>
        <para>
          On one node, find and enable the <literal>lock_manager</literal> setting in the file
          <filename>/etc/libvirt/qemu.conf</filename>:
        </para>
<screen>lock_manager = "lockd"</screen>
      </step>
      <step>
        <para>
          On the same node, find and enable the <literal>file_lockspace_dir</literal> setting in the
          file <filename>/etc/libvirt/qemu-lockd.conf</filename>, and change the value to point to
          a directory on the &ocfs; volume:
        </para>
<screen>file_lockspace_dir = "/mnt/shared/lockd"</screen>
      </step>
      <step>
        <para>
          Copy these files to the other nodes in the cluster:
        </para>
<screen>&prompt.root;<command>crm cluster copy /etc/libvirt/qemu.conf</command>
&prompt.root;<command>crm cluster copy /etc/libvirt/qemu-lockd.conf</command></screen>
      </step>
      <step>
        <para>
          Enable and start the <literal>libvirtd</literal> service on all nodes in the cluster:
        </para>
<screen>&prompt.root;<command>crm cluster run "systemctl enable --now libvirtd"</command></screen>
        <para>
          This also starts the <literal>virtlockd</literal> service.
        </para>
      </step>
    </procedure>
  </sect1>

  <sect1 xml:id="sec-ha-virtualization-adding-virtual-machines">
    <title>Adding virtual machines as cluster resources</title>
    <para>
      Use this procedure to add virtual machines to the cluster as cluster resources, with
      resource constraints to ensure the VMs can always access the lock files. The lock files are
      managed by the resources in the group <literal>g-virt-lock</literal>, which is available on
      all nodes via the clone <literal>cl-virt-lock</literal>.
    </para>
    <procedure xml:id="pro-ha-virtualization-adding-virtual-machines">
      <title>Adding virtual machines as cluster resources</title>
      <step>
        <para>
          Install your virtual machines on one of the cluster nodes, with the following restrictions:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              The installation source and storage must be on shared devices.
            </para>
          </listitem>
          <listitem>
            <para>
              Do not configure the VMs to start on host boot.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          For more information, see
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-virtualization.html">
          <citetitle>&virtual;</citetitle> for &sles;</link>.
        </para>
      </step>
      <step>
        <para>
          If the virtual machines are running, shut them down. The cluster will start the VMs
          after you add them as resources.
        </para>
      </step>
      <step>
        <para>
          Dump the XML configuration to the &ocfs; volume. Repeat this step for each VM:
        </para>
<screen>&prompt.root;<command>virsh dumpxml <replaceable>VM1</replaceable> &gt; /mnt/shared/<replaceable>VM1</replaceable>.xml</command></screen>
        <important role="compact">
          <para>
            Make sure the XML files do not contain any references to unshared local paths.
          </para>
        </important>
      </step>
      <step>
        <para>
          Run <command>crm configure</command> to start the <command>crm</command> interactive shell.
        </para>
      </step>
      <step>
        <para>
          Create primitive resources to manage the virtual machines. Repeat this step for each VM:
        </para>
<screen>&prompt.crm.conf;<command>primitive <replaceable>VM1</replaceable> VirtualDomain \
  params config="/mnt/shared/<replaceable>VM1</replaceable>.xml" remoteuri="qemu+ssh://%n/system" \
  meta allow-migrate=true \
  op monitor timeout=30s interval=10s</command></screen>
        <para>
          The option <literal>allow-migrate=true</literal> enables live migration. If the value is
          set to <literal>false</literal>, the cluster migrates the VM by shutting it down on
          one node and restarting it on another node.
        </para>
        <para>
          If you need to set utilization attributes to help place VMs based on their load impact,
          see <xref linkend="sec-ha-config-basics-utilization"/>.
        </para>
      </step>
      <step>
        <para>
          Create a colocation constraint so that the virtual machines can only start on nodes where
          <literal>cl-virt-lock</literal> is running:
        </para>
<screen>&prompt.crm.conf;<command>colocation col-fs-virt inf: ( <replaceable>VM1 VM2 VMX</replaceable> ) cl-virt-lock</command></screen>
      </step>
      <step>
        <para>
          Create an ordering constraint so that <literal>cl-virt-lock</literal> always starts before
          the virtual machines:
        </para>
<screen>&prompt.crm.conf;<command>order o-fs-virt Mandatory: cl-virt-lock ( <replaceable>VM1 VM2 VMX</replaceable> )</command></screen>
      </step>
      <step>
        <para>
          Review your changes with <command>show</command>.
        </para>
      </step>
      <step>
        <para>
          If everything is correct, submit your changes with <command>commit</command>
          and leave the crm live configuration with <command>quit</command>.
        </para>
      </step>
      <step>
        <para>
          Check the status of the virtual machines:
        </para>
<screen>&prompt.root;<command>crm status</command>
[...]
Full List of Resources:
[...]
  * Clone Set: cl-virt-lock [g-virt-lock]:
    * Started: [ &node1; &node2; ]
  * VM1 (ocf::heartbeat:VirtualDomain): Started &node1;
  * VM2 (ocf::heartbeat:VirtualDomain): Started &node1;
  * VMX (ocf::heartbeat:VirtualDomain): Started &node1;</screen>
      </step>
    </procedure>
    <para>
      The virtual machines are now managed by the &ha; cluster, and can migrate between the cluster nodes.
    </para>
    <important>
      <title>Do not manually start or stop cluster-managed VMs</title>
      <para>
        After adding virtual machines as cluster resources, do not manage them manually.
        Only use the cluster tools as described in <xref linkend="cha-ha-manage-resources"/>.
      </para>
      <para>
        To perform maintenance tasks on cluster-managed VMs, see <xref linkend="sec-ha-maint-overview"/>.
      </para>
    </important>
  </sect1>

  <sect1 xml:id="sec-ha-virtualization-testing">
    <title>Testing the setup</title>
    <para>
      Use the following tests to confirm that the virtual machine &ha; setup works as expected.
    </para>
    <important role="compact">
      <para>
        Perform these tests in a test environment, not a production environment.
      </para>
    </important>
    <procedure>
      <title>Verifying that the VM resource is protected across cluster nodes</title>
      <step>
        <para>
          The virtual machine <literal>VM1</literal> is running on node <literal>&node1;</literal>.
        </para>
      </step>
      <step>
        <para>
          On node <literal>&node2;</literal>, try to start the VM manually with
          <command>virsh start VM1</command>.
        </para>
      </step>
      <step>
        <para>
          <emphasis role="bold">Expected result:</emphasis> The <command>virsh</command> command
          fails. <literal>VM1</literal> cannot be started manually on <literal>&node2;</literal>
          when it is running on <literal>&node1;</literal>.
        </para>
      </step>
    </procedure>
    <procedure>
      <title>Verifying that the VM resource can live migrate between cluster nodes</title>
      <step>
        <para>
          The virtual machine <literal>VM1</literal> is running on node <literal>&node1;</literal>.
        </para>
      </step>
      <step>
        <para>
          Open two terminals.
        </para>
      </step>
      <step>
        <para>
          In the first terminal, connect to <literal>VM1</literal> via SSH.
        </para>
      </step>
      <step>
        <para>
          In the second terminal, try to migrate <literal>VM1</literal> to node
          <literal>&node2;</literal> with <command>crm resource move VM1 bob</command>.
        </para>
      </step>
      <step>
        <para>
          Run <command>crm_mon -r</command> to monitor the cluster status until it
          stabilizes. This might take a short time.
        </para>
      </step>
      <step>
        <para>
          In the first terminal, check whether the SSH connection to <literal>VM1</literal>
          is still active.
        </para>
      </step>
      <step>
        <para>
          <emphasis role="bold">Expected result:</emphasis> The cluster status shows that
          <literal>VM1</literal> has started on <literal>&node2;</literal>. The SSH connection
          to <literal>VM1</literal> remains active during the whole migration.
        </para>
      </step>
    </procedure>
    <procedure>
      <title>Verifying that the VM resource can migrate to another node when the current node reboots</title>
      <step>
        <para>
          The virtual machine <literal>VM1</literal> is running on node <literal>&node2;</literal>.
       </para>
      </step>
      <step>
        <para>
          Reboot <literal>&node2;</literal>.
        </para>
      </step>
      <step>
        <para>
          On node <literal>&node1;</literal>, run <command>crm_mon -r</command> to
          monitor the cluster status until it stabilizes. This might take a short time.
        </para>
      </step>
      <step>
        <para>
          <emphasis role="bold">Expected result:</emphasis> The cluster status shows that
          <literal>VM1</literal> has started on <literal>&node1;</literal>.
        </para>
      </step>
    </procedure>
    <procedure>
      <title>Verifying that the VM resource can fail over to another node when the current node crashes</title>
      <step>
        <para>
          The virtual machine <literal>VM1</literal> is running on node <literal>&node1;</literal>.
        </para>
      </step>
      <step>
        <para>
          Simulate a crash on <literal>&node1;</literal> by forcing the machine off or
          unplugging the power cable.
        </para>
      </step>
      <step>
        <para>
          On node <literal>&node2;</literal>, run <command>crm_mon -r</command> to
          monitor the cluster status until it stabilizes. VM failover after a node crashes
          usually takes longer than VM migration after a node reboots.
        </para>
      </step>
      <step>
        <para>
          <emphasis role="bold">Expected result:</emphasis> After a short time, the cluster status
          shows that <literal>VM1</literal> has started on <literal>&node2;</literal>.
        </para>
      </step>
    </procedure>
  </sect1>

</chapter>
