<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ha-maintenance">
 <title>Executing maintenance tasks</title>
 <info>
  <abstract>
   <para>
    To perform maintenance tasks on the cluster nodes, you might need to stop
    the resources running on that node, to move them, or to shut down or reboot
    the node. It might also be necessary to temporarily take over the control of
    resources from the cluster, or even to stop the cluster service while resources
    remain running.
   </para>
   <para>
    This chapter explains how to manually take down a cluster node without
    negative side-effects. It also gives an overview of different options the
    cluster stack provides for executing maintenance tasks.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer/>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release/>
   <dm:repository/>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-ha-maint-outline">
  <title>Preparing and finishing maintenance work</title>
  <para>
   Use the following commands to start, stop, or view the status of the cluster:
  </para>
  <variablelist>
   <varlistentry>
    <term><command>crm cluster start [--all]</command></term>
    <listitem>
     <para>Start the cluster services on one node or all nodes</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>crm cluster stop [--all]</command></term>
    <listitem>
     <para>Stop the cluster services on one node or all nodes</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>crm cluster restart [--all]</command></term>
    <listitem>
     <para>Restart the cluster services on one node or all nodes</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>crm cluster status</command></term>
    <listitem>
     <para>View the status of the cluster stack</para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   Execute the above commands as user &rootuser;, or as a user with the required privileges.
  </para>
  <para>
   When you shut down or reboot a cluster node (or stop the cluster services on a
   node), the following processes will be triggered:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     The resources that are running on the node will be stopped or moved off
     the node.
    </para>
   </listitem>
   <listitem>
    <para>
     If stopping a resource fails or times out, the &stonith; mechanism
     will fence the node and shut it down.
    </para>
   </listitem>
  </itemizedlist>
  <warning>
   <title>Risk of data loss</title>
   <para>
    If you need to do testing or maintenance work, follow the general steps
    below.
   </para>
   <para>
    Otherwise, you risk unwanted side effects, like resources not starting in an
    orderly fashion, unsynchronized CIBs across the cluster nodes, or even data loss.
   </para>
   <procedure>
   <step>
    <para>
     Before you start, choose the appropriate option from <xref linkend="sec-ha-maint-overview"/>.
    </para>
   </step>
   <step>
    <para>
     Apply this option with &hawk2; or &crmsh;.
    </para>
   </step>
   <step>
    <para>
     Execute your maintenance task or tests.
    </para>
   </step>
   <step>
    <para>
     After you have finished, put the resource, node or cluster back to
     <quote>normal</quote> operation.
    </para>
   </step>
  </procedure>
 </warning>
 </sect1>

 <sect1 xml:id="sec-ha-maint-overview">
  <title>Different options for maintenance tasks</title>

  <para>
   &pace; offers the following options for performing system maintenance:
  </para>

  <variablelist>
   <varlistentry xml:id="vle-ha-maint-mode-cluster">
    <!--<term>Putting the cluster in maintenance mode</term>-->
    <term><xref linkend="sec-ha-maint-mode-cluster" xrefstyle="select:title"/></term>
    <listitem>
     <para>
      The global cluster property <literal>maintenance-mode</literal> puts all
      resources into maintenance state at once. The cluster stops monitoring them
      and becomes oblivious to their status. Note that only the resource management
      by &pace; is disabled. &corosync; and SBD are still functional. Use maintenance
      mode for any tasks involving cluster resources. For any tasks involving
      infrastructure such as storage or networking, the safest method is to stop
      the cluster services completely. See <xref linkend="sec-ha-maint-shutdown-node"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-maint-mode-node">
    <!--<term>Putting a node in maintenance mode</term>-->
    <term><xref linkend="sec-ha-maint-mode-node" xrefstyle="select:title"/></term>
    <listitem>
     <para>
      This option allows you to put all resources running on a specific node into
      maintenance state at once. The cluster will cease monitoring them and thus
      become oblivious to their status.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-maint-node-standby">
    <!--<term>Putting a node in standby mode</term>-->
    <term><xref linkend="sec-ha-maint-node-standby" xrefstyle="select:title"/></term>
    <listitem>
     <para>
      A node that is in standby mode can no longer run resources. Any resources
      running on the node will be moved away or stopped (if no other node
      is eligible to run the resource). Also, all monitoring operations will be
      stopped on the node (except for those with
      <literal>role="Stopped"</literal>).
     </para>
     <para>
      You can use this option if you need to stop a node in a cluster while
      continuing to provide the services running on another node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-maint-shutdown-node">
    <!--<term>Stopping the cluster services on a node</term>-->
    <term><xref linkend="sec-ha-maint-shutdown-node" xrefstyle="select:title"/></term>
    <listitem>
     <para>
      This option stops all of the cluster services on a single node. Any resources
      running on the node will be moved away or stopped (if no other node is
      eligible to run the resource). If stopping a resource fails or times out,
      the node will be fenced.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-maint-mode-rsc">
    <!--<term>Putting a resource in maintenance mode</term>-->
    <term><xref linkend="sec-ha-maint-mode-rsc" xrefstyle="select:title"/></term>
    <listitem>
     <para>
      When this mode is enabled for a resource, no monitoring operations will be
      triggered for the resource.
     </para>
     <para>
      Use this option if you need to manually touch the service that is managed
      by this resource and do not want the cluster to run any monitoring
      operations for the resource during that time.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-maint-rsc-unmanaged">
    <!--<term>Putting a resource into unmanaged mode</term>-->
    <term><xref linkend="sec-ha-maint-rsc-unmanaged" xrefstyle="select:title"/></term>
     <listitem>
     <para>
      The <option>is-managed</option> meta attribute allows you to temporarily
      <quote>release</quote> a resource from being managed by the cluster
      stack. This means you can manually touch the service that is managed by
      this resource (for example, to adjust any components). However, the
      cluster will continue to <emphasis>monitor</emphasis> the resource and to
      report any failures.
     </para>
     <para>
      If you want the cluster to also cease <emphasis>monitoring</emphasis> the
      resource, use the per-resource maintenance mode instead (see
      <xref
      linkend="vle-ha-maint-mode-rsc"/>).
     </para>
    </listitem>
   </varlistentry>

  </variablelist>
 </sect1>

 <sect1 xml:id="sec-ha-maint-mode-cluster">
  <title>Putting the cluster into maintenance mode</title>
  <warning>
   <title>Maintenance mode only disables &pace;</title>
   <para>
    When putting a cluster into maintenance mode, only the resource management
    by &pace; is disabled. &corosync; and SBD are still functional. Depending
    on your maintenance tasks, this might lead to fence operations.
   </para>
   <para>
    Use maintenance mode for any tasks involving cluster resources. For any tasks
    involving infrastructure such as storage or networking, the safest method is to stop
    the cluster services completely. See <xref linkend="sec-ha-maint-shutdown-node"/>.
   </para>
  </warning>
   <para>
    To put the cluster into maintenance mode on the &crmshell;, use the following command:
   </para>
<screen>&prompt.root;<command>crm configure property maintenance-mode=true</command></screen>
   <para>
    To put the cluster back to normal mode after your maintenance work is done, use
    the following command:
   </para>
<screen>&prompt.root;<command>crm configure property maintenance-mode=false</command></screen>

<procedure xml:id="pro-ha-maint-mode-cluster-hawk2">
   <title>Putting the cluster into maintenance mode with &hawk2;</title>
   <step>
    <para>
     Start a Web browser and log in to the cluster as described in
     <xref linkend="sec-conf-hawk2-login"/>.
    </para>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Cluster
     Configuration</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In the <guimenu>CRM Configuration</guimenu> group, select the
     <guimenu>maintenance-mode</guimenu> attribute from the empty drop-down box
     and click the plus icon to add it.
    </para>
   </step>
   <step>
    <para>
     To set <literal>maintenance-mode=true</literal>, activate the check box
     next to <literal>maintenance-mode</literal> and confirm your changes.
    </para>
   </step>
   <step>
    <para>
     After you have finished the maintenance task for the whole cluster,
     deactivate the check box next to the <literal>maintenance-mode</literal>
     attribute.
    </para>
    <para>
     From this point on, &ha; will take over cluster management again.
    </para>
   </step>
  </procedure>
</sect1>

 <sect1 xml:id="sec-ha-maint-mode-node">
  <title>Putting a node into maintenance mode</title>
   <para>
    To put a node into maintenance mode on the &crmshell;, use the following command:
   </para>
<screen>&prompt.root;<command>crm node maintenance <replaceable>NODENAME</replaceable></command></screen>
   <para>
    To put the node back to normal mode after your maintenance work is done, use
    the following command:
   </para>
<screen>&prompt.root;<command>crm node ready <replaceable>NODENAME</replaceable></command></screen>

  <procedure xml:id="pro-ha-maint-mode-nodes-hawk2">
   <title>Putting a node into maintenance mode with &hawk2;</title>
   <step>
    <para>
     Start a Web browser and log in to the cluster as described in
     <xref linkend="sec-conf-hawk2-login"/>.
    </para>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Cluster Status</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In one of the individual nodes' views, click the wrench icon next to the
     node and select <guimenu>Maintenance</guimenu>.
    </para>
   </step>
   <step>
    <para>
      After you have finished your maintenance task, click the wrench icon next to the node
     and select <guimenu>Ready</guimenu>.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-maint-node-standby">
  <title>Putting a node into standby mode</title>
   <para>
    To put a node into standby mode on the &crmshell;, use the following command:
   </para>
<screen>&prompt.root;<command>crm node standby <replaceable>NODENAME</replaceable></command></screen>
   <para>
    To bring the node back online after your maintenance work is done, use the following command:
   </para>
<screen>&prompt.root;<command>crm node online <replaceable>NODENAME</replaceable></command></screen>

<procedure xml:id="pro-ha-maint-node-standby-hawk2">
  <title>Putting a node into standby mode with &hawk2;</title>
   <step>
    <para>
     Start a Web browser and log in to the cluster as described in
     <xref linkend="sec-conf-hawk2-login"/>.
    </para>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Cluster Status</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In one of the individual nodes' views, click the wrench icon next to the
     node and select <guimenu>Standby</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Finish the maintenance task for the node.
    </para>
   </step>
   <step>
    <para>
     To deactivate the standby mode, click the wrench icon next to the node
     and select <guimenu>Ready</guimenu>.
    </para>
   </step>
  </procedure>
</sect1>

<sect1 xml:id="sec-ha-maint-shutdown-node">
 <title>Stopping the cluster services on a node</title>
 <para>
  To move the services off the node in an orderly fashion
  before shutting down or rebooting the node, proceed as follows:
 </para>
 <procedure xml:id="pro-ha-maint-shutdown-node">
  <title>Manually rebooting a cluster node</title>
  <step>
   <para>
    On the node you want to reboot or shut down, log in as &rootuser; or
    equivalent.
   </para>
  </step>
  <step>
   <para>
    Put the node into <literal>standby</literal> mode:
   </para>
<screen>&prompt.root;<command>crm -w node standby</command></screen>
   <para>
    That way, services can migrate off the node without being limited by the
    shutdown timeout of the cluster services.
   </para>
  </step>
  <step>
   <para>
    Check the cluster status:
   </para>
<screen>&prompt.root;<command>crm status</command></screen>
   <para>
    It shows the respective node in <literal>standby</literal> mode:
   </para>
<screen>[...]
Node <replaceable>&node2;</replaceable>: standby
[...]</screen>
  </step>
  <step>
   <para>
    Stop the cluster services on that node:
   </para>
<screen>&prompt.root;<command>crm cluster stop</command></screen>
  </step>
  <step>
   <para>
    Reboot the node.
   </para>
  </step>
 </procedure>

 <para>
  To check if the node joins the cluster again:
 </para>

 <procedure>
  <step>
   <para>
    Log in to the node as &rootuser; or equivalent.
   </para>
  </step>
  <step>
   <para>
    Check if the cluster services have started:
   </para>
<screen>&prompt.root;<command>crm cluster status</command></screen>
   <para>
    If not, start them:
   </para>
<screen>&prompt.root;<command>crm cluster start</command></screen>
  </step>
  <step>
   <para>
    Check the cluster status:
   </para>
<screen>&prompt.root;<command>crm status</command></screen>
   <para>
    It should show the node coming online again.
   </para>
  </step>
 </procedure>
</sect1>

 <sect1 xml:id="sec-ha-maint-mode-rsc">
  <title>Putting a resource into maintenance mode</title>
   <para>
    To put a resource into maintenance mode on the &crmshell;, use the following command:</para>
<screen>&prompt.root;<command>crm resource maintenance <replaceable>RESOURCE_ID</replaceable> true</command></screen>
   <para>
    To put the resource back into normal mode after your maintenance work is done, use
    the following command:
   </para>
<screen>&prompt.root;<command>crm resource maintenance <replaceable>RESOURCE_ID</replaceable> false</command></screen>

<procedure xml:id="pro-ha-maint-mode-rsc-hawk2">
   <title>Putting a resource into maintenance mode with &hawk2;</title>
   <step>
    <para>
     Start a Web browser and log in to the cluster as described in
     <xref linkend="sec-conf-hawk2-login"/>.
    </para>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Resources</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Select the resource you want to put in maintenance mode or unmanaged mode,
     click the wrench icon next to the resource and select <guimenu>Edit
     Resource</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Open the <guimenu>Meta Attributes</guimenu> category.
    </para>
   </step>
   <step>
    <para>
     From the empty drop-down list, select the <guimenu>maintenance</guimenu>
     attribute and click the plus icon to add it.
    </para>
   </step>
   <step>
    <para>
     Activate the check box next to <literal>maintenance</literal> to set the
     maintenance attribute to <literal>yes</literal>.
    </para>
   </step>
   <step>
    <para>
     Confirm your changes.
    </para>
   </step>
   <step>
    <para>
     After you have finished the maintenance task for that resource, deactivate
     the check box next to the <literal>maintenance</literal> attribute for
     that resource.
    </para>
    <para>
     From this point on, the resource will be managed by the &ha; software
     again.
    </para>
   </step>
  </procedure>
</sect1>

<sect1 xml:id="sec-ha-maint-rsc-unmanaged">
  <title>Putting a resource into unmanaged mode</title>
   <para>
    To put a resource into unmanaged mode on the &crmshell;, use the following command:</para>
<screen>&prompt.root;<command>crm resource unmanage <replaceable>RESOURCE_ID</replaceable></command></screen>
   <para>
    To put it into managed mode again after your maintenance work is done, use the
    following command:
   </para>
<screen>&prompt.root;<command>crm resource manage <replaceable>RESOURCE_ID</replaceable></command></screen>

 <procedure xml:id="pro-ha-maint-rsc-unmanaged-hawk2">
   <title>Putting a resource into unmanaged mode with &hawk2;</title>
   <step>
    <para>
     Start a Web browser and log in to the cluster as described in
     <xref
      linkend="sec-conf-hawk2-login"/>.
    </para>
   </step>
   <step>
    <para>
     From the left navigation bar, select <guimenu>Status</guimenu> and go to
     the <guimenu>Resources</guimenu> list.
    </para>
   </step>
   <step>
    <para>
     In the <guimenu>Operations</guimenu> column, click the arrow down icon
     next to the resource you want to modify and select
     <guimenu>Edit</guimenu>.
    </para>
    <para>
     The resource configuration screen opens.
    </para>
   </step>
   <step>
    <para>
     Below <guimenu>Meta Attributes</guimenu>, select the
     <guimenu>is-managed</guimenu> entry from the empty drop-down box.
    </para>
   </step>
   <step>
    <para>
     Set its value to <literal>No</literal> and click <guimenu>Apply</guimenu>.
    </para>
   </step>
   <step>
    <para>
     After you have finished your maintenance task, set
     <guimenu>is-managed</guimenu> to <literal>Yes</literal> (which is the
     default value) and apply your changes.
    </para>
    <para>
     From this point on, the resource will be managed by the &ha; software
     again.
    </para>
   </step>
  </procedure>
 </sect1>

<sect1 xml:id="sec-ha-maint-shutdown-node-maint-mode">
 <title>Rebooting a cluster node while in maintenance mode</title>
    <note>
      <title>Implications</title>
      <para>
       If the cluster or a node is in maintenance mode, you can use tools external
       to the cluster stack (for example, <command>systemctl</command>) to manually
       operate the components that are managed by the cluster as resources.
       The &ha; software will not monitor them or attempt to restart them.
      </para>
      <para>
       If you stop the cluster services on a node, all daemons and processes
       (originally started as &pace;-managed cluster resources) will continue
       to run.
      </para>
      <para>
       If you attempt to start cluster services on a node while the cluster or
       node is in maintenance mode, &pace; will initiate a single one-shot monitor
       operation (a <quote>probe</quote>) for every resource to evaluate which
       resources are currently running on that node. However, it will take no
       further action other than determining the resources' status.
      </para>
     </note>

   <procedure xml:id="pro-ha-maint-reboot-node">
    <title>Rebooting a cluster node while the cluster or node is in maintenance mode</title>
    <step>
    <para>
     On the node you want to reboot or shut down, log in as &rootuser; or
     equivalent.
    </para>
   </step>
   <step>
    <para>
     If you have a DLM resource (or other resources depending on DLM), make
     sure to explicitly stop those resources before stopping the cluster services:
    </para>
<screen>&prompt.crm.res;<command>stop <replaceable>RESOURCE_ID</replaceable></command></screen>
    <para>
     The reason is that stopping &pace; also stops the &corosync; service on
     whose membership and messaging services DLM depends. If &corosync; stops,
     the DLM resource will assume a split brain scenario and trigger a fencing
     operation.
    </para>
   </step>
   <step>
    <para>
     Stop the cluster services on that node:
    </para>
<screen>&prompt.root;<command>crm cluster stop</command></screen>
   </step>
   <step>
    <para>
     Shut down or reboot the node.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
