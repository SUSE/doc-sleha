<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--
* FATE#320599: Pacemaker Remote
* See https://trello.com/c/tO9cNZRv for SP3
-->
<?provo dirname="nfs_quick/"?>

<article version="5.0" xml:lang="en" xml:id="art_sle_ha_pmremote"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>&pcmkremquick;</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
   <productnumber>&productnumber;</productnumber>
   <productname>&productname;</productname>
   <date><?dbtimestamp format="m/d/Y X"?></date>
   <xi:include href="ha_authors.xml"/>
   <abstract>
    <para>
     This document guides you through the setup of a cluster with a remote
     node or a guest node, managed by &pace; and &pmremote;.
     <emphasis>Remote</emphasis> in the &pmremote; term does not mean physical
     distance, but <quote>non-membership</quote> of a cluster.
    </para>
   </abstract>
   <dm:docmanager>
    <dm:bugtracker>
     <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
     <dm:product>SUSE Linux Enterprise High Availability Extension 12
      SP3</dm:product>
     <dm:component>Documentation</dm:component>
    </dm:bugtracker>
    <dm:translation>yes</dm:translation>
   </dm:docmanager>
   <xi:include href="copyright_techguides.xml" />
  </info>

 <sect1 xml:id="sec.ha.pmremote.usage">
  <title>Usage Scenario</title>
  <para>
   The procedures in this guide describe the process of setting up a minimal
   cluster with the following characteristics:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Two cluster nodes running &productname; 12 GA or higher. In this guide,
     their host names are <systemitem class="domainname">&node1;</systemitem>
     and <systemitem class="domainname">&node2;</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     Failover of resources from one node to the other if the active host
     breaks down (active/passive setup).
    </para>
   </listitem>
   <listitem>
    <para>
     Depending on the setup, your cluster will end up with one of the following
     nodes:
    </para>
    <itemizedlist>
     <listitem>
      <para>One remote node running &pmremote;. In this guide, the
       guest node is named <systemitem class="domainname">&node3;</systemitem>,
       or,</para>
     </listitem>
     <listitem>
      <para>
       One guest node running &pmremote;. In this guide, the guest
       node is named <systemitem class="domainname">&node4;</systemitem>.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &pace; to manage guest nodes and remote nodes.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.ha.pmremote.concept">
   <title>Conceptual Overview and Terminology</title>
   <para>
    A regular cluster may contain up to 32 nodes. With the &pmremote; service,
    &ha; clusters can be extended to include additional nodes beyond this
    limit.
   </para>
   <para>
    The &pmremote; service can be operated as a physical node (called remote
    node) or as a virtual node (called guest node).
    Unlike normal cluster nodes, both remote and guest nodes are managed by
    the cluster as resources.
    As such, they are not bound to the 32&nbsp;node limitation of the cluster
    stack. However, from the resource management point of view, they behave
    as regular cluster nodes.
   </para>
   <para>
    Remote nodes do not need to have the full cluster stack installed, as
    they only run the &pmremote; service. The service acts as a proxy,
    allowing the cluster stack on the <quote>regular</quote> cluster nodes
    to connect to the service.
    Thus, the node that runs the &pmremote; service is
    effectively integrated into the cluster as a remote node (see <xref
     linkend="vl.ha.pmremote.terminology"/>).
   </para>
   <variablelist xml:id="vl.ha.pmremote.terminology">
    <title>Terminology</title>
    <varlistentry>
     <term>Cluster Node</term>
     <listitem>
      <para>
       A node that runs the complete cluster stack components along with the
       complete &ha; &corosync; stack (see <xref
        linkend="fig.ha.pmremote.regular-cluster-stack"/>). </para>
      <figure xml:id="fig.ha.pmremote.regular-cluster-stack">
       <title>Regular Cluster Stack (Two-Node Cluster)</title>
       <mediaobject>
        <imageobject>
         <imagedata align="center" fileref="ha_pmremote-regular.svg" width="50%"/>
        </imageobject>
       </mediaobject>
      </figure>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Pacemaker Remote (&systemd; service: &pmremote;)</term>
     <listitem>
      <para>
       A service daemon that makes it possible to use a node as a &pace; node
       without deploying the full cluster stack. Note that &pmremote; is the
       name of the systemd service. However, the name of the daemon
       is &pmrm_daemon; (with a trailing d after its name).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Remote Node</term>
     <listitem>
      <para>
       A physical machine that runs the &pmremote; daemon. A special
       resource (<systemitem>ocf:pacemaker:remote</systemitem>) is required
       to be running on one of the cluster nodes to manage communication
       between the cluster node and the remote node (see use case&nbsp;1 in
       <xref linkend="tab.ha.pmremote.usecases"/>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Guest Node</term>
     <listitem>
      <para>
       A virtual machine that runs the &pmremote; daemon.
       A guest node is created using a resource agent such as
       <systemitem>ocf:pacemaker:VirtualDomain</systemitem> with the
       <option>remote-node</option> meta attribute (see use case&nbsp;2
       in <xref linkend="tab.ha.pmremote.usecases"/>)
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <table xml:id="tab.ha.pmremote.usecases">
    <title>Common Use Cases of &pmremote;</title>
    <tgroup cols="2">
     <colspec colwidth="1*"/>
     <colspec colwidth="1*"/>
     <thead>
      <row>
       <entry align="center">Use Case 1</entry>
       <entry align="center">Use Case 2</entry>
      </row>
      <row>
       <entry>Cluster Stack with Physical &pmremote; Node</entry>
       <entry>Cluster Stack with Virtual &pmremote; Guest Nodes</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry valign="middle" align="center"><informalfigure>
        <mediaobject>
         <imageobject>
          <imagedata fileref="ha_pmremote-phys-remote-nodes.svg" width="95%"/>
         </imageobject>
        </mediaobject>
       </informalfigure></entry>
       <entry valign="middle" align="center"><informalfigure>
        <mediaobject>
         <imageobject>
          <imagedata fileref="ha_pmremote-virt-guest-nodes.svg" width="95%"/>
         </imageobject>
        </mediaobject>
       </informalfigure></entry>
      </row>
      <row>
       <entry>See <xref linkend="sec.ha.pmremote.install.phys-remote-nodes"/></entry>
       <entry>See <xref linkend="sec.ha.pmremote.install.virt-guest-nodes"/></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <para>
    For a physical machine that contains several virtual guest nodes, the
    process is as follows:
   </para>
   <orderedlist>
    <listitem>
     <para>On the cluster node, virtual machines are launched by &pace;.</para>
    </listitem>
    <listitem>
     <para>The cluster connects to the &pmremote; service of the virtual
      machines.</para>
    </listitem>
    <listitem>
     <para>The virtual machines are integrated into the cluster by &pmremote;.
     </para>
    </listitem>
   </orderedlist>
   <para>
    A regular cluster node may perform the following tasks:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Run cluster resources.
     </para>
    </listitem>
    <listitem>
     <para>Run all command-line tools, such as <command>crm</command>,
      <command>crm_mon</command>.
     </para>
    </listitem>
    <listitem>
     <para>Execute fencing actions.</para>
    </listitem>
    <listitem>
     <para>
      Count toward cluster quorum.
     </para>
    </listitem>
    <listitem>
     <para>
      Serve as the cluster's designated coordinator (DC).
     </para>
    </listitem>
   </itemizedlist>

   <para>
    Remote nodes and guest nodes can run cluster resources and most command
    line tools. However, they have the following limitations:</para>
   <itemizedlist>
    <listitem>
     <para>
      They cannot execute fencing actions.
     </para>
    </listitem>
    <listitem>
     <para>
      They do not affect quorum.
     </para>
    </listitem>
    <listitem>
     <para>
      They cannot serve as Designated Coordinator (DC).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    It is important to distinguish between several roles that a virtual
    machine can take in the &ha; cluster:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      A virtual machine can run a full cluster stack. In this case, the
      virtual machine is a regular cluster node and is not itself managed
      by the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      A virtual machine can be managed by the cluster as a resource, without
      the cluster being aware of the services that run inside the virtual
      machine. In this case, the virtual machine is opaque to the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      A virtual machine can be a cluster resource, and run &pmremote;,
      which allows the cluster to manage services inside the virtual machine.
      In this case, the virtual machine is a guest node and is transparent
      to the cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect1>

 <sect1 xml:id="sec.ha.pmremote.install.phys-remote-nodes">
  <title>Use Case 1: Setting Up a Cluster with Remote Nodes</title>
  <para>
   Before you proceed, install and set up a basic two-node cluster. This task
   is described in <citetitle>&instquick;</citetitle>. This will lead to a
   two-node cluster with the two physical hosts <systemitem
    class="domainname">&node1;</systemitem> and <systemitem
     class="domainname">&node2;</systemitem>.
  </para>
  <para>
   In the following example setup, a remote node
   <systemitem class="domainname">&node3;</systemitem> is used:
  </para>
  <orderedlist>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.uc1.virt-auth-keys"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.uc1.remote.nodes"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.uc1.integrate"/></para>
   </listitem>
  </orderedlist>

  <sect2 xml:id="sec.ha.pmremote.config.uc1.virt-auth-keys">
   <title>Configuring Authentication Key</title>
   <para>
    On the cluster node <systemitem class="domainname">&node1;</systemitem>
    proceed as follows:
   </para>
   <procedure>
      <step>
       <para>Create a specific authentication key for the &pmremote;
        service: </para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0755 /etc/pacemaker
&prompt.root;<command>dd</command> if=/dev/urandom of=/etc/pacemaker/authkey bs=4k count=1</screen>
       <para>
        It needs to be synchronized across all cluster nodes and remote nodes
        with &csync;.
        The key for the &pmremote; service is different from the cluster
        authentication key that you create in the &yast; cluster module.</para>
      </step>
      <step>
       <para>In <filename>/etc/csync2/csync2.cfg</filename>, add the
        following line:</para>
       <screen>include /etc/pacemaker/authkey</screen>
      </step>
      <step>
       <para>
        List all cluster and remote nodes in the &csync; configuration file.
       </para>
      </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.ha.pmremote.config.uc1.remote.nodes">
   <title>Configuring Remote Nodes</title>
   <para>
    The following procedure prepares your cluster node
    <systemitem class="domainname">&node3;</systemitem>:
   </para>
   <procedure>
    <step>
     <para>On <systemitem class="domainname"
      >&node3;</systemitem>, proceed as follows:</para>
     <substeps>
      <step>
       <para>In the firewall settings, open the TCP port 3121 for
        &pmremote;.</para>
      </step>
      <step>
       <para>Install the <package>pacemaker-remote</package> and
         <package>crmsh</package> packages: </para>
       <screen>&prompt.root;<command>zypper</command> in pacemaker-remote crmsh</screen>
      </step>
      <step>
       <para>Enable and start the &pmremote; service on <systemitem
         class="domainname">&node3;</systemitem>:</para>
       <screen>&prompt.root;<command>systemctl</command> enable pacemaker_remote
&prompt.root;<command>systemctl</command> start pacemaker_remote</screen>
      </step>
      <step>
       <para>Create the <filename>/etc/pacemaker</filename> directory and
        change its group:</para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0750 /etc/pacemaker
&prompt.root;<command>chgrp</command> haclient /etc/pacemaker</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>On <systemitem class="domainname">&node1;</systemitem>,
      run <command>csync2</command> to copy the authentication key from
      your cluster node to your guest node:</para>
     <screen>&prompt.root;<command>csync2</command> -xv</screen>
    </step>
    <step>
     <para>
      To verify the host connection to the remote node, run <command>ssh</command>
      on one of the cluster nodes. This connection will fail, but
      <emphasis>how</emphasis> it fails gives you hints about its
      connectivity:
     </para>
     <screen>&prompt.root;<command>ssh</command> -p 3121 &node3;</screen>
     <para>From the output you can tell if your connection works:</para>
     <variablelist>
      <varlistentry>
       <term>Working connection</term>
       <listitem>
        <screen>ssh_exhange_identification: read: Connection reset by peer.</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Broken connection</term>
       <listitem>
        <screen>ssh: connect to host &node3; port 3121: No route to host
ssh: connect to host &node3; port 3121: Connection refused</screen>
        <para>If you see either of the previous messages, the connection does
        not work. Use the <option>-v</option> option for <command>ssh</command>
         and execute the command again to see debugging messages. This could
         be helpful to find connection, authentication, or configuration
         problems. Multiple <option>-v</option> options increase the verbosity.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>If needed, add more remote nodes and configure them as described above.</para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.ha.pmremote.config.uc1.integrate">
   <title>Integrating Remote Node into Cluster</title>
   <para>To integrate the remote nodes into the cluster, perform the following
    steps on the cluster node <systemitem class="domainname">&node1;</systemitem>:</para>
   <procedure>
    <step>
     <para>Start &pace;:</para>
     <screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
    </step>
    <step>
     <para>
      Check if your remote node and IP address is listed
     in the file <filename>/etc/hosts</filename>.</para>
    </step>
    <step>
     <para>
      Create a <systemitem>ocf:pacemaker:remote</systemitem> primitive:
     </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> &node3; ocf:pacemaker:remote \
     params server=&node3; reconnect_interval=15m \
     op monitor interval=30s
&prompt.crm.conf;<command>commit</command>
&prompt.crm.conf;<command>exit</command></screen>
    </step>
    <step>
     <para>Check the status of the cluster with the command
      <command>crm status</command>. It
      should contain a running cluster with nodes that are all accessible:
     </para>
     <screen>&prompt.root;<command>crm</command> status
[...]
Online: [ alice bob ]
RemoteOnline: [ charlie ]

Full list of resources:
&node3; (ocf:pacemaker:remote): Started &node1;
 [...]</screen>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.ha.pmremote.config.uc1.start.resources">
   <title>Starting Resources on Remote Node</title>
   <para>
    Once the remote node is integrated into the cluster, starting resources
    on a remote node is the exact same as on cluster nodes.
   </para>
   <warning>
    <para>
     Never involve a remote node connection resource in a resource group,
     colocation constraint, or order constraint.
    </para>
   </warning>
   <formalpara>
    <title>Fencing Remote Nodes</title>
    <para>
     Remote nodes are fenced the same way as cluster nodes. No special
     considerations are required. Configure fencing resources for use with
     remote nodes the same as you would with cluster nodes.
    </para>
   </formalpara>
   <para>
    Note, however, remote nodes does not take part in initiating a
    fencing action. Only cluster nodes are capable of actually executing a
    fencing operation against another node.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.pmremote.install.virt-guest-nodes">
  <title>Use Case 2: Setting Up a Cluster with Guest Nodes</title>
  <remark>toms 2017-07-11: Are other virtualization techniques supported, like
  XEN...?</remark>
  <remark>gao-yan 2017-07-24: Yes. As we know libvirt also supports Xen,
   which means we can use the RA ocf:heartbeat:VirtualDomain to manage Xen
   guests as well. But of course we can either use the RA ocf:heartbeat:Xen
   to do that.</remark>
  <para>
   Before you proceed, install and set up a basic two-node cluster. This task
   is described in <citetitle>&instquick;</citetitle>. This will lead to a
   two-node cluster with the two physical hosts <systemitem
    class="domainname">&node1;</systemitem> and <systemitem
     class="domainname">&node2;</systemitem>.
  </para>
  <para>In the following example setup, KVM is used for setting up the virtual
   guest nodes:</para>
  <orderedlist>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.uc2.virt-auth-keys"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.uc2.guest"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.integrate.uc2.guestsintocluster"/></para>
   </listitem>
  </orderedlist>

  <sect2 xml:id="sec.ha.pmremote.config.uc2.virt-auth-keys">
   <title>Configuring Authentication Key</title>
   <para>
    On the cluster node <systemitem class="domainname">&node1;</systemitem>
    proceed as follows:
   </para>
   <procedure>
      <step>
       <para>Create a specific authentication key for the &pmremote;
        service: </para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0755 /etc/pacemaker
&prompt.root;<command>dd</command> if=/dev/urandom of=/etc/pacemaker/authkey bs=4k count=1</screen>
       <para>It needs to be synchronized across all cluster nodes and guest nodes
        with &csync;.
        The key for the &pmremote; service is different from the cluster
        authentication key that you create in the &yast; cluster module.</para>
      </step>
      <step>
       <para>In <filename>/etc/csync2/csync2.cfg</filename>, add the
        following line:</para>
       <screen>include /etc/pacemaker/authkey</screen>
      </step>
      <step>
       <para>List all cluster and guest nodes in the &csync; configuration
        file.</para>
      </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.pmremote.config.uc2.guest">
   <title>Configuring the Guest Node</title>
   <para>
    The following procedure prepares a guest node <systemitem
      class="domainname">&node4;</systemitem> on your cluster node
    <systemitem class="domainname">&node1;</systemitem>:
   </para>
   <procedure>
    <step>
     <para>Create a KVM guest on <systemitem class="domainname"
      >&node1;</systemitem>. For details refer to the
      <citetitle>&virtual;</citetitle>, chapter <citetitle>Guest Installation</citetitle>
      available from <link xlink:href="https://www.suse.com/doc"/>.
     </para>
    </step>
    <step>
     <para>On <systemitem class="domainname"
      >&node4;</systemitem>, proceed as follows:</para>
     <substeps>
      <step>
       <para>In the firewall settings, open the TCP port 3121 for
        &pmremote;.</para>
      </step>
      <step>
       <para>Install the <package>pacemaker-remote</package> and
         <package>crmsh</package> packages: </para>
       <screen>&prompt.root;<command>zypper</command> in pacemaker-remote crmsh</screen>
      </step>
      <step>
       <para>Enable and start the &pmremote; service on <systemitem
         class="domainname">&node1;</systemitem>:</para>
       <screen>&prompt.root;<command>systemctl</command> enable pacemaker_remote
&prompt.root;<command>systemctl</command> start pacemaker_remote</screen>
      </step>
      <step>
       <para>Create the <filename>/etc/pacemaker</filename> directory and
        change its group:</para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0750 /etc/pacemaker
&prompt.root;<command>chgrp</command> haclient /etc/pacemaker</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>On <systemitem class="domainname">&node1;</systemitem>,
      run <command>csync2</command> to copy the authentication key from
      your physical host to your guest node:</para>
     <screen>&prompt.root;<command>csync2</command> -xv</screen>
    </step>
    <step>
     <para>
      To verify the host connection to the guest, run <command>ssh</command>
      on one of the cluster nodes. This connection will fail, but
      <emphasis>how</emphasis> it fails gives you hints about its
      connectivity:
     </para>
     <screen>&prompt.root;<command>ssh</command> -p 3121 &node4;</screen>
     <para>From the output you can tell if your connection works:</para>
     <variablelist>
      <varlistentry>
       <term>Working connection</term>
       <listitem>
        <screen>ssh_exhange_identification: read: Connection reset by peer.</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Broken connection</term>
       <listitem>
        <screen>ssh: connect to host &node4; port 3121: No route to host
ssh: connect to host &node4; port 3121: Connection refused</screen>
        <para>If you see either of the previous messages, the connection does
        not work. Use the <option>-v</option> option for <command>ssh</command>
         and execute the command again to see debugging messages. This could
         be helpful to find connection, authentication, or configuration
         problems. Multiple <option>-v</option> options increase the verbosity.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>If needed, add more guest nodes and configure them as described above.</para>
    </step>
    <step>
     <para>
      Shut down the guest node and proceed with <xref
       linkend="sec.ha.pmremote.integrate.uc2.guestsintocluster"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.pmremote.integrate.uc2.guestsintocluster">
   <title>Integrating a Guest Node into the Cluster</title>
   <para>To integrate the guest node into the cluster, perform the following
    steps on the cluster node <systemitem class="domainname">&node1;</systemitem>:</para>
   <procedure>
    <step>
     <para>Start &pace;:</para>
     <screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
    </step>
    <step>
     <para>Check if your KVM guest node and IP address is listed
     in the file <filename>/etc/hosts</filename>.</para>
    </step>
    <step xml:id="st.ha.pmremote.integrate.guestsintocluster.virsh-dump">
     <para>Dump the XML configuration of the KVM guest(s) that you need
      in the next step:
     </para>
     <screen>&prompt.root;<command>virsh</command> list --all
 Id    Name         State
-----------------------------------
 -     &node4;       shut off
&prompt.root;<command>virsh</command> dumpxml &node4; > /etc/pacemaker/&node4;.xml</screen>
    </step>
    <step>
     <para>
      Create a <systemitem>VirtualDomain</systemitem> resource to launch the
      virtual machine. Use the dumped configuration from <xref
       linkend="st.ha.pmremote.integrate.guestsintocluster.virsh-dump"/>:
     </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> vm-&node4; ocf:heartbeat:VirtualDomain \
  params hypervisor="qemu:///system" \
         config="/etc/pacemaker/&node4;.xml" \
  meta remote-node=&node4;</screen>
     <para>
      &pace; will automatically monitor &pmrm; connections for failure,
      so it is not necessary to create a recurring monitor on the
      <systemitem>VirtualDomain</systemitem> resource.
     </para>
    </step>
    <step>
     <para>Check the status of the cluster with the command
      <command>crm status</command>. It
      should contain a running cluster with nodes that are all accessible.
     </para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.ha.pmremote.testing.uc2">
   <title>Testing the Setup</title>
   <para>To demonstrate how resources are executed, use a few dummy resources.
    These dummy resources serve for testing purposes only.
   </para>
   <procedure>
    <step>
     <para>Create a few dummy resources:</para>
     <screen>&prompt.root;<command>crm</command> configure primitive fake1 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake2 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake3 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake4 ocf:pacemaker:Dummy</screen>
    </step>
    <step>
     <para>Check the status of the <command>crm status</command> command.
      You should see something like the following:
     </para>
     <screen>&prompt.root;<command>crm</command> status
[...]
Online: [ &node1; &node2; ]
GuestOnline: [ &node4;@&node1; ]

Full list of resources:
vm-doro (ocf:heartbeat:VirtualDomain): Started &node1;
fake1           (ocf:pacemaker:Dummy): Started &node1;
fake2           (ocf:pacemaker:Dummy): Started &node1;
fake3           (ocf:pacemaker:Dummy): Started &node2;
fake4           (ocf:pacemaker:Dummy): Started &node2;</screen>
    </step>
    <step>
     <para>To move one of the Dummy primitives to run it on the guest node
      (&node4;), use the following command:</para>
     <screen>&prompt.root;<command>crm</command> resource move fake1 &node4;</screen>
     <para>The status will change to this:</para>
     <screen>&prompt.root;<command>crm</command> status
[...]
Online: [ &node1; &node2; ]
GuestOnline: [ &node4;@&node1; ]

Full list of resources:
vm-doro (ocf:heartbeat:VirtualDomain): Started &node1;
fake1           (ocf:pacemaker:Dummy): Started &node4;
fake2           (ocf:pacemaker:Dummy): Started &node1;
fake3           (ocf:pacemaker:Dummy): Started &node2;
fake4           (ocf:pacemaker:Dummy): Started &node2;</screen>
    </step>
    <step>
     <para> To test whether fencing works, kill the <systemitem class="daemon"
      >pacemaker_remoted</systemitem> daemon: </para>
     <screen>&prompt.root;<command>kill</command> -9 $(pidof pacemaker_remoted)</screen>
    </step>
    <step>
     <para>After a few seconds, check the status of the cluster again. It
      should look like this:</para>
     <screen>&prompt.root;<command>crm</command> status
[...]
Online: [ &node1; &node2; ]

Full list of resources:
vm-&node4; (ocf::heartbeat:VirtualDomain): Started &node1;
fake1           (ocf:pacemaker:Dummy): Stopped
fake2           (ocf:pacemaker:Dummy): Started &node2;
fake3           (ocf:pacemaker:Dummy): Started &node1;
fake4           (ocf:pacemaker:Dummy): Started &node2;

Failed Actions:
* &node4;_monitor_30000 on alice 'unknown error' (1): call=8, status=Error, exitreason='none',
    last-rc-change='Tue Jul 18 13:11:51 2017', queued=0ms, exec=0ms</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.pmremote.upgrade">
  <title>Upgrading Cluster and &pmrm; Nodes</title>
  <!-- toms 2017-04-18: Pacemaker version in different SLEHA:
   GA (1.1.12), SP1(1.1.13), SP2(1.1.15) -->
  <para>
   Rolling upgrade is supported when &pmremote; is being used.
   <!-- toms 2017-07-24: see http://docserv.nue.suse.com/documents/SLE-HA12-SP3/SLE-HA-guide/single-html/#sec.ha.migration.upgrade.rolling
    -->
  </para>
  <!-- Taken from ha_migration.xml -->
  <para>
   In a rolling upgrade one cluster node at a time is upgraded while the
   rest of the cluster is still running: you take the first node offline,
   upgrade it and bring it back online to join the cluster. Then you continue
   one by one until all cluster nodes are upgraded to a major version.
   See the section <citetitle>Rolling Upgrade</citetitle> in the &haguide;
   <!--<xref linkend="sec.ha.migration.upgrade.rolling"/>-->
   for more information.
  </para>
  <para>
   To upgrade your cluster and &pmrm; nodes, do the following:
  </para>
  <procedure>
   <step>
    <para>
     For rolling upgrade from &sle; 12 SP1 to 12 SP2:
     Make sure you upgrade your remote nodes to the latest SP1 maintenance update.
    </para>
   </step>
   <step>
    <para>
     Upgrade all cluster nodes to &sle; &productnumber; one by one.
    </para>
   </step>
   <step>
    <para>
     Upgrade all &pmrm; nodes to &sle; &productnumber; one by one.
    </para>
   </step>
  </procedure>
  <!--
  <para>
   In the future, rolling upgrades from SP2 to SP3 needs only the last two
   steps.
  </para>
  -->
 </sect1>

 <xi:include href="common_legal.xml"/>
</article>
