<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--
FATE#320599: Pacemaker Remote
-->
<?provo dirname="nfs_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art_sle_ha_pmremote"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>&paceremote;</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
   <productnumber>&productnumber;</productnumber>
   <productname>&productname;</productname>
   <date><?dbtimestamp format="m/d/Y X"?></date>
   <abstract>
    <para>
     This document guides you through the setup of a cluster with virtual
     guest nodes, managed by &pace; and &pmremote;.
     <emphasis>Remote</emphasis> in this context does not mean physical
     proximity, but <quote>non-membership</quote> of a cluster.
    </para>
   </abstract>
  </info>

 <sect1 xml:id="sec.ha.pmremote.usage">
  <title>Usage Scenario</title>
  <para>
   The procedures in this guide describe the process of setting up a minimal
   cluster with the following characteristics:
  </para>
  <itemizedlist>
   <listitem>
    <remark>toms 2017-03-23: Is this supported from 12 GA, SP1, or SP2?
    According to aspiers, since &sls; 12 at least</remark>
    <para>A cluster node running &productname; &productnumber;. In this guide, its host name is
     <systemitem class="domainname">&wsI;</systemitem>.</para>
   </listitem>
   <listitem>
    <para>Failover of resources from one node to the other if the active host
     breaks down (active/passive setup).</para>
   </listitem>
   <listitem>
    <para>Virtual guest nodes running &pmremote;. In this guide, one guest
     node is named <systemitem class="domainname">&wsII;</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     &pace; to manage guest nodes and remote nodes.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

  <sect1 xml:id="sec.ha.pmremote.concept">
   <title>Conceptual Overview and Terminology</title>
   <!-- Add a picture? -->
   <para>
    A regular cluster may contain up to 32 nodes. With the &pmremote; service,
    &ha; clusters can be extended to include additional nodes beyond this
    limit. Remote nodes do not need to have the full cluster stack installed,
    they only run the &pmremote; service. The service acts as a proxy,
    allowing the cluster stack on the “regular” cluster nodes to connect to
    the service (see <xref linkend="tab.ha.pmremote.comparison"/>).
    Thus, the node that runs the &pmremote; service is
    effectively integrated into the cluster as a remote node (see <xref
     linkend="vl.ha.pmremote.terminology"/>).
   </para>
   <table xml:id="tab.ha.pmremote.comparison">
    <title>Comparison of Regular HA Stack with and without &pmremote;</title>
    <tgroup cols="2">
     <thead>
      <row>
       <entry>Regular Cluster Stack</entry>
       <entry>Extended Cluster Stack with &pmremote;</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry><informalfigure>
        <mediaobject>
         <imageobject>
          <imagedata fileref="ha_pmremote-without.svg"/>
         </imageobject>
        </mediaobject>
       </informalfigure></entry>
       <entry><informalfigure>
        <mediaobject>
         <imageobject>
          <imagedata fileref="ha_pmremote-with.svg"/>
         </imageobject>
        </mediaobject>
       </informalfigure></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <para>Assuming the physical host contains several virtual guest nodes,
    the standard sequence of &pace; with &pmremote; will be as follows:
   </para>
   <orderedlist>
    <listitem>
     <para>On the cluster node, virtual machines are launced by &pace;.</para>
    </listitem>
    <listitem>
     <para>The cluster connects to the &pmremote; service of the virtual
      machines.</para>
    </listitem>
    <listitem>
     <para>The virtual machines are integrated into the cluster by &pmremote;.
     </para>
    </listitem>
   </orderedlist>

   <para>
    It is important to understand the several distinctive
    roles a virtual machine can serve in clusters:
    <!--
    In &pace; clusters, a virtual machine can serve in several distinctive
    roles:-->
   </para>
   <itemizedlist>
    <listitem>
     <para>
      A virtual machine can run a full cluster stack. In this case, a cluster
      node is not managed by the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster can manage a virtual machine as a resource without being
      aware of the services running inside the virtual machine. In this
      scenario, the virtual machine is opaque to the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      A virtual machine can act as a cluster resource and run the
      &pmremote; service daemon.
      This allows the cluster to manage services running on the virtual
      machine. In this situation, the virtual machine is transparent to
      the cluster.
     </para>
    </listitem>
   </itemizedlist>

   <variablelist xml:id="vl.ha.pmremote.terminology">
    <title>Terminology</title>
    <varlistentry>
     <term>Pacemaker Remote (&pmremote;)</term>
     <listitem>
      <para>
       A service daemon that makes it possible to use a node as a &pace;
       node without deploying the full cluster stack. While nodes that run
       this service may run cluster resources and most
       commandline tools, they do not support other functions of full cluster
       nodes. This includes fencing execution, quorum voting, and DC
       eligibility.
      </para>
      <note>
       <title>Different Naming for Service and Daemon</title>
       <para>The systemd service of Pacemaker Remote is named &pmremote;
        whereas the name or the process/daemon is &pmrm_daemon;
        (a trailing <quote>d</quote> after its name).
       </para>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Cluster Node</term>
     <listitem>
      <para>
       A node that runs all &pace; components along with the complete
       high-availability &corosync; stack. Cluster nodes may perform the
       following tasks:
      </para>
      <itemizedlist>
       <listitem>
        <para>Run cluster resources.</para>
       </listitem>
       <listitem>
        <para>Run all command-line tools, such as <command>crm</command>,
         <command>crm_mon</command>, and others.</para>
       </listitem>
       <listitem>
        <para>Count toward cluster quorum.</para>
       </listitem>
       <listitem>
        <para>Serve as the cluster's designated coordinator (DC).</para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Remote Node</term>
     <listitem>
      <para>
       A special resource (ocf:pacemaker:remote) is required to be running on
       a cluster node for managing communication with the baremetal remote
       node in that case.
       <remark>toms 2017-03-29: According to gao-yan: The scenario of baremetal
        remote node deserves a separate chapter I think.
       </remark>
       A physical machine that runs the &pmremote; daemon. Remote nodes
       includes a special resource (often called <quote>baremetal case</quote>)
       for managing communication with the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Guest Node</term>
     <listitem>
      <para>
       A virtual host that runs &pmremote;. Unlike remote nodes,
       a guest node is a resource managed by the cluster. As such, they are
       not bond to the 32 node limitation of the cluster stack. However, they
       they behave as regular cluster nodes.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>

 <sect1 xml:id="sec.ha.pmremote.install">
  <title>Installation</title>
  <para>The installation consists of these steps:</para>
  <orderedlist>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.phys.host"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.kvm.guest"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.integrate.guestsintocluster"/></para>
   </listitem>
  </orderedlist>

  <sect2 xml:id="sec.ha.pmremote.config.phys.host">
   <title>Setting up a Corosync Cluster with Physical Hosts</title>
   <para>
    On the physical host <systemitem class="domainname">&wsI;</systemitem>
    proceed as follows:
   </para>
   <procedure>
    <!--<title>Configuring the Physical Host</title>-->
    <step>
     <para>Install your cluster as described in the <citetitle>&instquick;</citetitle>.
      <!--<xref linkend="art.ha.install.quick"/>-->
     </para>
    </step>
    <step>
     <para>Make sure you open the TCP port 3121 for &pmremote; in your
      firewall settings.</para>
    </step>
    <step>
     <para>Configure the cluster nodes:</para>
     <substeps>
      <step>
       <para>Add the IP addresses and names of your guest nodes into <filename>/etc/hosts</filename>.</para>
      </step>
      <step>
       <para>Allow cluster-related services in the local firewall on all
        cluster nodes.</para>
      </step>
      <step>
       <para>Create an authentication key to all cluster and remote nodes:</para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0755 /etc/pacemaker
&prompt.root;<command>dd</command> if=/dev/urandom of=/etc/pacemaker/authkey bs=4k count=1</screen>
      </step>
      <step>
       <para>If you want to enable the <systemitem class="daemon"
        >pacemaker_remote</systemitem> service, run the following command:</para>
       <screen>&prompt.root;<command>chkconfig</command> pacemaker_remote on</screen>
      </step>
      <step>
       <para>Restart &pace; using the <command>systemctl restart pacemaker</command> command.</para>
      </step>
      <step>
       <para>Add the following line in the <filename>/etc/csync2/csync2.cfg</filename> file:</para>
       <screen>include /etc/pacemaker/authkey</screen>
      </step>
      <!-- Run csync2? -->
     </substeps>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.pmremote.config.kvm.guest">
   <title>Configuring the KVM Guests</title>
   <para>
    The following procedure creates a KVM guest node <systemitem
      class="domainname">&wsII;</systemitem> on your physical host
    <systemitem class="domainname">&wsI;</systemitem>:
   </para>
   <procedure>
    <step>
     <para>Create a KVM guest on <systemitem class="domainname"
      >&wsI;</systemitem>. For details refer to <citetitle>&deploy;</citetitle>.
      Make sure to configure the guest with a host name and a static IP address. </para>
    </step>
    <step>
     <para>Add the KVM guest host name <systemitem
      class="domainname">&wsII;</systemitem> and its IP address to the hosts
     machine at <filename>/etc/hosts</filename>.</para>
    </step>
    <step>
     <para>Configure Firewall settings on your guest node and open TCP
      port 3121.</para>
    </step>
    <step>
     <para>Verify connectivity between guests and host by using
      <command>ping</command> and <command>ssh</command>.</para>
    </step>
    <step>
     <para>Install the <package>pacemaker-remote</package> package and
     enable it:</para>
     <screen>&prompt.root;<command>zypper</command> in pacemaker-remote
&prompt.root;<command>systemctl</command> enable pacemaker_remote</screen>
    </step>
    <step>
     <para>Create the <filename>/etc/pacemaker</filename> directory and change
      the group:</para>
     <screen>&prompt.root;<command>mkdir</command> -p --mode=0750 /etc/pacemaker
&prompt.root;<command>chgrp</command> haclient /etc/pacemaker</screen>
    </step>
    <step>
     <para>Run <command>csync2</command> to copy the authentication key from
      your physical host to your guest node:</para>
     <screen>&prompt.root;<command>csync</command> -xv</screen>
    </step>
    <step>
     <para>Verify host connection to guest. This can be done by
      making a SSH connection from the host. This connection
      will fail, but <emphasis>how</emphasis> it fails gives you hints
      about its connectivity.
     </para>
     <para>Run <command>ssh</command> on one of the cluster nodes:</para>
     <screen>&prompt.root;<command>ssh</command> -p 3121 &wsII;</screen>
     <para>An error connection message should be as follows:</para>
     <screen>ssh_exhange_identification: read: Connection reset by peer.</screen>
     <para>If you see either of the following messages, the connection does
      not work:</para>
     <screen>ssh: connect to host &wsII; port 3121: No route to host
ssh: connect to host &wsII; port 3121: Connection refused</screen>
    </step>
    <step>
     <para>Shut down the KVM guest as &pace; will manage the guests.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.pmremote.integrate.guestsintocluster">
   <title>Integrating KVM Guests into Cluster</title>
   <para>To integrate the KVM guests into the cluster, perform the following
    steps on the physical host <systemitem class="domainname">&wsI;</systemitem>:</para>
   <procedure>
    <step>
     <para>Start &pace;:</para>
     <screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
    </step>
    <step>
     <para>Check the status of the command <command>crm status</command>. It
     should contain a running cluster with nodes that are all accessible.
     </para>
    </step>
    <step>
     <para>Check if all KVM guests nodes and IP addresses are listed
     in the file <filename>/etc/hosts</filename>.</para>
    </step>
    <step>
     <para>Dump the XML configuration of the KVM guest(s):
     </para>
     <screen>&prompt.root;<command>virsh</command> list --all
 Id    Name         State
-----------------------------------
 -     &wsII;       shut off
&prompt.root;<command>virsh</command> dump &wsII; > /etc/pacemaker/&wsII;.xml</screen>
    </step>
    <step>
     <para>
      Create a new <systemitem>VirtualDomain</systemitem> resource:
      <remark>toms 2017-03-23: Check the command, this is probably not correct:</remark>
     </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> vm.&wsII; ocf:heartbeat:VirtualDomain \
  params hypervisor="qemu:///system" \
         config="/etc/pacemaker/&wsII;.xml \
  meta remote-node=&wsII;</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.pmremote.testing">
  <title>Testing the Setup</title>
  <para>To demonstrate how resources are executed, use a few dummy resources.
   These dummy resources serve for testing purposes only.
  </para>
  <procedure>
   <step>
    <para>Create a few dummy resources:</para>
    <screen>&prompt.root;<command>crm</command> configure primitive fake1 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake2 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake3 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake4 ocf:pacemaker:Dummy</screen>
   </step>
   <step>
    <para>Check the status of the <command>crm status</command> command.
    You should see something like the following:
    </para>
    <screen>TBD</screen>
   </step>
   <step>
    <para>To move one of the Dummy primitives to run it on another node, use
    the following command:</para>
    <screen>TBD</screen>
    <para>The status will change to this:</para>
    <screen>TBD</screen>
   </step>
  </procedure>

  <para> To test whether fencing works, kill the <systemitem class="daemon"
    >pacemaker_remoted</systemitem> daemon, and check how &pace; reacts: </para>
  <screen>&prompt.root;<command>kill</command> -9 $(pidof pacemaker_remoted)</screen>
  <para>After a few seconds, the status of the cluster should look like this:</para>
  <screen>&prompt.root;<command>crm</command> status
TBD</screen>
 </sect1>

<!--
  * Install and setup the "normal" cluster nodes as usual
    (section 5.3)
  * Install pacemaker-remote, resource-agents and crmsh on the remote nodes
    (section 5.1.2)
  * Create cluster directories on all nodes (cluster + remote_nodes)
  * Create a special key for pacemaker remote (that's 4K key and different
    from our cluster secure key.
  * Enable + start the pacemaker_remote service on all remote_nodes
  * verify connection of remote_hosts of the pacemaker nodes
    (section 5.2)
  * Integrate remote nodes into the cluster
    (section 5.4)
 -->
 <xi:include href="common_legal.xml"/>
</article>