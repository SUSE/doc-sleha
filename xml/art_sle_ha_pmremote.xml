<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--

-->
<?provo dirname="nfs_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art_sle_ha_pmremote"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>&paceremote;</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
   <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp?></date>
  <abstract>
   <para>
    TBD
   </para>
  </abstract>
  </info>

 <!-- Do we need a "Usage Scenario" section here? -->

 <sect1 xml:id="sec.ha.pmremote.concept">
  <title>Conceptual Overview</title>
  <para>TBD</para>
  <variablelist>
   <varlistentry>
    <term>Pacemaker Remote (<systemitem class="daemon">pacemaker_remote</systemitem>)</term>
    <listitem>
     <para>The service daemon without the full cluster stack.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster Node</term>
    <listitem>
     <para>A node running the full high availability stack.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Remote Node</term>
    <listitem>
     <para>A physical host running the <systemitem
      class="daemon">pacemaker_remote</systemitem> daemon.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Guest Node</term>
    <listitem>
     <para>A virtual host running the <systemitem
      class="daemon">pacemaker_remote</systemitem> daemon.</para>
    </listitem>
   </varlistentry>
  </variablelist>
  <!-- Add a picture? -->
 </sect1>

 <sect1 xml:id="sec.ha.pmremote.install">
  <title>Installation</title>
  <para></para>
  <procedure>
   <step>
    <para>Install your cluster as described in the <citetitle>&instquick;</citetitle>.
     <!--<xref linkend="art.ha.install.quick"/>-->
     </para>
   </step>
   <step>
    <para>
     Install the packages <package>pacemaker-remote</package>
     and <package>resource-agents</package>.</para>
   </step>
   <!-- Should we add all remote nodes into csync.cfg and /etc/hosts ? -->
   <step>
    <para>Configure the cluster nodes:</para>
    <substeps>
     <step>
      <para>Allow cluster-related services through the local firewall on all
       cluster nodes.</para>
     </step>
     <step>
      <para>Create an authentication key to all cluster and remote
       nodes:</para>
      <screen>&prompt.root;<command>mkdir</command> -p --mode=0755 /etc/pacemaker
&prompt.root;<command>dd</command> if=/dev/urandom of=/etc/pacemaker/authkey bs=4k count=1</screen>
      <remark>toms 2017-03-03: is on SLEHAE the file /etc/corosync/authkey being used?</remark>
     </step>
     <step>
      <para>If you want to enable the <systemitem class="daemon"
       >pacemaker_remote</systemitem> service, run:</para>
      <screen>&prompt.root;<command>chkconfig</command> pacemaker_remote on</screen>
     </step>
     <step>
      <para>Restart &pace; with <command>systemctl restart pacemaker</command>.</para>
     </step>
     <step>
      <para>Insert the following line in the <filename>/etc/csync2/csync2.cfg</filename>:</para>
      <screen>include /etc/pacemaker/authkey</screen>
     </step>
     <!-- Run csync2? -->
    </substeps>
   </step>
   <!--<step>
    <para>Check if all necessary firewall ports for an HA cluster are open
     (for example, 3121, and more)</para>
   </step>-->
   <step>
    <para>Define a primitive resource per remote node:</para>
    <screen>&prompt.root;<command>crm</command> configure primitive suse09</screen>
   </step>
  </procedure>
 </sect1>

<!--
  * Install and setup the "normal" cluster nodes as usual
    (section 5.3)
  * Install pacemaker-remote, resource-agents and crmsh on the remote nodes
    (section 5.1.2)
  * Create cluster directories on all nodes (cluster + remote_nodes)
  * Create a special key for pacemaker remote (that's 4K key and different
    from our cluster secure key.
  * Enable + start the pacemaker_remote service on all remote_nodes
  * verify connection of remote_hosts of the pacemaker nodes
    (section 5.2)
  * Integrate remote nodes into the cluster
    (section 5.4)
 -->

</article>