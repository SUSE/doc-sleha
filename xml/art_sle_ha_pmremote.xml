<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--
FATE#320599: Pacemaker Remote
-->
<?provo dirname="nfs_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art_sle_ha_pmremote"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>&paceremote;</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
   <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp?></date>
  <abstract>
   <para>
    TBD
   </para>
  </abstract>
  </info>

  <para><remark>toms 2017-03-23: Do we need a "Usage Scenario" section here?</remark></para>

 <sect1 xml:id="sec.ha.pmremote.concept">
  <title>Conceptual Overview</title>
  <para>TBD</para>
  <variablelist>
   <varlistentry>
    <term>Pacemaker Remote (<systemitem class="daemon">pacemaker_remote</systemitem>)</term>
    <listitem>
     <para>The service daemon without the full cluster stack.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster Node</term>
    <listitem>
     <para>A node running the full high availability stack.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Remote Node</term>
    <listitem>
     <para>A physical host running the <systemitem
      class="daemon">pacemaker_remote</systemitem> daemon.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Guest Node</term>
    <listitem>
     <para>A virtual host running the <systemitem
      class="daemon">pacemaker_remote</systemitem> daemon.</para>
    </listitem>
   </varlistentry>
  </variablelist>
  <!-- Add a picture? -->
 </sect1>

 <sect1 xml:id="sec.ha.pmremote.install">
  <title>Installation</title>
  <para>Bascically, the installation consists of these steps:</para>
  <orderedlist>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.phys.host"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.kvm.guest"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.integrate.guestsintocluster"/></para>
   </listitem>
  </orderedlist>

  <sect2 xml:id="sec.ha.pmremote.config.phys.host">
   <title>Configuring the Physical Host</title>
   <para>
    In this guide, the hostname <systemitem class="domainname">&wsI;</systemitem>
    is used for the physical host.
   </para>
   <procedure>
    <!--<title>Configuring the Physical Host</title>-->
    <step>
     <para>Install your cluster as described in the <citetitle>&instquick;</citetitle>.
      <!--<xref linkend="art.ha.install.quick"/>-->
     </para>
     <remark>toms 2017-03-23: what about Firewall settings? Should we open TCP
     ports 2224, 3121, and 21064, and UDP port 5405?</remark>
    </step>
    <step>
     <para>
      Install the packages <package>pacemaker-remote</package>
      and <package>resource-agents</package>.</para>
    </step>
    <step>
     <para>Configure the cluster nodes:</para>
     <remark>toms 2017-03-23: Should we add all remote nodes into csync.cfg and /etc/hosts ?</remark>
     <substeps>
      <step>
       <para>Allow cluster-related services through the local firewall on all
        cluster nodes.</para>
      </step>
      <step>
       <para>Create an authentication key to all cluster and remote
        nodes:</para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0755 /etc/pacemaker
&prompt.root;<command>dd</command> if=/dev/urandom of=/etc/pacemaker/authkey bs=4k count=1</screen>
       <remark>toms 2017-03-03: is on SLEHAE the file /etc/corosync/authkey being used?</remark>
      </step>
      <step>
       <para>If you want to enable the <systemitem class="daemon"
        >pacemaker_remote</systemitem> service, run:</para>
       <screen>&prompt.root;<command>chkconfig</command> pacemaker_remote on</screen>
      </step>
      <step>
       <para>Restart &pace; with <command>systemctl restart pacemaker</command>.</para>
      </step>
      <step>
       <para>Insert the following line in the <filename>/etc/csync2/csync2.cfg</filename>:</para>
       <screen>include /etc/pacemaker/authkey</screen>
      </step>
      <!-- Run csync2? -->
     </substeps>
    </step>
    <!--<step>
    <para>Check if all necessary firewall ports for an HA cluster are open
     (for example, 3121, and more)</para>
   </step>-->
    <step>
     <para>Define a primitive resource per remote node:</para>
     <screen>&prompt.root;<command>crm</command> configure primitive suse09</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.pmremote.config.kvm.guest">
   <title>Configuring the KVM Guests</title>
   <para>
    In this guide, the hostname <systemitem class="domainname">&wsII;</systemitem>
    is used for a KVM guest.
   </para>
   <procedure>
    <step>
     <para>Create a KVM guest. Make sure to configure the guest with
     a hostname and a static IP address. </para>
    </step>
    <step>
     <para>Add the KVM guest hostname <systemitem
      class="domainname">&wsII;</systemitem> and its IP address to the hosts
     machine at <filename>/etc/hosts</filename>.</para>
    </step>
    <step>
     <para>Configure Firewall settings on guest.</para>
    </step>
    <step>
     <para>Verify connectivity between guests and host by using
      <command>ping</command> and <command>ssh</command>.</para>
    </step>
    <step>
     <para>Install the <package>pacemake_remote</package> package and
     enable it:</para>
     <screen>&prompt.root;<command>zypper</command> in pacemake_remote
&prompt.root;<command>systemctl</command> enable pacemaker_remote</screen>
    </step>
    <step>
     <para>Copy the authentication key from your host:</para>
     <screen>&prompt.root;<command>mkdir</command> -p --mode=0750 /etc/pacemaker
&prompt.root;<command>chgrp</command> haclient /etc/pacemaker
&prompt.root;<command>scp</command> root@&wsI;:/etc/pacemaker/authkey /etc/pacemaker</screen>
    </step>
    <step>
     <para>Verify host connection to guest. This can be done by
      making a SSH connection from the host. This connection
      will fail, but <emphasis>how</emphasis> it fails gives you hints
      about its connectivity.
     </para>
     <para>Run <command>ssh</command> on one of the cluster nodes:</para>
     <screen>&prompt.root;<command>ssh</command> -p 3121 &wsII;</screen>
     <para>A <quote>successful</quote> connection looks like this:</para>
     <screen>ssh_exhange_identification: read: Connection reset by peer.</screen>
     <para>If you see the following messages, the connection does not work:</para>
     <screen>ssh: connect to host &wsII; port 3121: No route to host</screen>
     <para>Or:</para>
     <screen>ssh: connect to host &wsII; port 3121: Connection refused</screen>
    </step>
    <step>
     <para>Shut down the KVM guest(s) as &pace; will manage the guests itself.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.pmremote.integrate.guestsintocluster">
   <title>Integrating KVM Guests into Cluster</title>
   <para>To integrate the KVM guests into the cluster, do the following on
    the physical host <systemitem class="domainname">&wsI;</systemitem>:</para>
   <procedure>
    <step>
     <para>Start &pace;:</para>
     <screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
    </step>
    <step>
     <para>Check the status of the command <command>crm status</command>. It
     should contain a running cluster with nodes that are all accessible.
     </para>
    </step>
    <step>
     <para>Check if all KVM guests hostnames and IP addresses are listed
     in the file <filename>/etc/hosts</filename>.</para>
    </step>
    <step>
     <para>Dump the XML configuration of the KVM guest(s):
     </para>
     <screen>&prompt.root;<command>virsh</command> list --all
 Id    Name         State
-----------------------------------
 -     &wsII;       shut off
&prompt.root;<command>virsh</command> dump &wsII; > /etc/pacemaker/&wsII;.xml</screen>
    </step>
    <step>
     <para>
      Create a new <systemitem>VirtualDomain</systemitem> resource:
      <remark>toms 2017-03-23: Check the command, this is probably not correct:</remark>
     </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> vm.&wsII; ocf:heartbeat:VirtualDomain \
  params hypervisor="qemu:///system" \
         config="/etc/pacemaker/&wsII;.xml \
  meta remote-node=&wsII;</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

<!--
  * Install and setup the "normal" cluster nodes as usual
    (section 5.3)
  * Install pacemaker-remote, resource-agents and crmsh on the remote nodes
    (section 5.1.2)
  * Create cluster directories on all nodes (cluster + remote_nodes)
  * Create a special key for pacemaker remote (that's 4K key and different
    from our cluster secure key.
  * Enable + start the pacemaker_remote service on all remote_nodes
  * verify connection of remote_hosts of the pacemaker nodes
    (section 5.2)
  * Integrate remote nodes into the cluster
    (section 5.4)
 -->

</article>