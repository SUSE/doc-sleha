<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--
FATE#320599: Pacemaker Remote
-->
<?provo dirname="nfs_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art_sle_ha_pmremote"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>&paceremote;</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
   <productnumber>&productnumber;</productnumber>
   <productname>&productname;</productname>
   <date><?dbtimestamp format="m/d/Y X"?></date>
   <abstract>
    <para>
     This document guides you through the setup of a cluster with virtual
     guest nodes, managed by &pace; and &pmremote;.
     <emphasis>Remote</emphasis> in this context means not physical proximity,
     but <quote>non-membership</quote> of a cluster.
    </para>
   </abstract>
  </info>

 <sect1 xml:id="sec.ha.pmremote.usage">
  <title>Usage Scenario</title>
  <para>
   The procedures in this document will lead to a minimal setup of a cluster
   with the following properties:
  </para>
  <itemizedlist>
   <listitem>
    <remark>toms 2017-03-23: Is this supported from 12 GA, SP1, or SP2?</remark>
    <para>A cluster node running &sls; 12 SP2. In this guide it is named
     <systemitem class="domainname">&wsI;</systemitem> as its hostname.</para>
   </listitem>
   <listitem>
    <para>Failover of resources from one node to the other if the active host
     breaks down (active/passive setup).</para>
   </listitem>
   <listitem>
    <para>Virtual guest nodes running &pmremote;.</para>
   </listitem>
   <listitem>
    <para>
     &pace; to manage guest nodes and remote nodes.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.ha.pmremote.term">
  <title>Terminology</title>
  <variablelist>
   <varlistentry>
    <term>Pacemaker Remote (&pmremote;)</term>
    <listitem>
     <para>
      A service daemon that makes it possible to use a node as a &pace;
      node without deploying the full cluster stack. While nodes that run
      <command>&pmrm;</command> may run cluster resources and most
      commandline tools, they do not support other functions of full cluster
      nodes. This includes fencing executiong, quorum voting, and DC
      eligibility.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster Node</term>
    <listitem>
     <para>
      A node that runs all &pace; components along with the complete
      high-availability &corosync; stack. Cluster nodes may perform the
      following tasks:
     </para>
     <itemizedlist>
      <listitem>
       <para>Run cluster resources.</para>
      </listitem>
      <listitem>
       <para>Run all command-line tools, such as <command>crm</command>,
        <command>crm_mon</command>, and others.</para>
      </listitem>
      <listitem>
       <para>Count toward cluster quorum.</para>
      </listitem>
      <listitem>
       <para>Serve as the cluster's designated controller (DC).</para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Remote Node</term>
    <listitem>
     <para>
      A physical machine that runs the &pmremote; daemon. Remote nodes
      includes a special resource (often called <quote>baremetal case</quote>)
      for managing communication wtih the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Guest Node</term>
    <listitem>
     <para>
      A virtual host that runs &pmremote;. Unlike remote nodes,
      a guest node is a resource managed by the cluster. As such, they are
      not bond to the 32 node limitation of the cluster stack. However, they
      behave like <quote>normal</quote> cluster nodes.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

  <sect1 xml:id="sec.ha.pmremote.concept">
   <title>Conceptual Overview</title>
   <!-- Add a picture? -->
   <para>
    A <quote>normal</quote> cluster can contain up to 32 nodes.
    Using &pmremote;, the cluster can be extended to include additional
    nodes beyond this limit.
   </para>
   <para>Pretending the physical host contains several virtual guest nodes,
    with &pmremote; this will be the usual sequence for &pace;:</para>
   <orderedlist>
    <listitem>
     <para>On the cluster node, virtual machines are launced by &pace;.</para>
    </listitem>
    <listitem>
     <para>The cluster connects to the &pmremote; service of the virtual
      machines.</para>
    </listitem>
    <listitem>
     <para>The virtual machines are integrated into the cluster.
     <remark>toms 2017-03-24: This needs more clarification: how? By whom?</remark>
     </para>
    </listitem>
   </orderedlist>

   <para>
    Apart from the concept, it is important to know the several distinctive
    roles a virtual machine can serve in &pace; clusters:
    <!--
    In &pace; clusters, a virtual machine can serve in several distinctive
    roles:-->
   </para>
   <itemizedlist>
    <listitem>
     <para>
      A virtual machine can run a full cluster stack. In this case, a cluster
      node is not managed by the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster can manage a virtual machine as a resource without being
      aware of the services running inside the virtual machine. In this
      scenario, the virtual machine is opaque to the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      A virtual machine can act as a cluster resource and run the
      &pmremote; service daemon.
      This allows the cluster to manage services running on the virtual
      machine. In this situation, the virtual machine is transparent to
      the cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect1>

 <sect1 xml:id="sec.ha.pmremote.install">
  <title>Installation</title>
  <para>Bascically, the installation consists of these steps:</para>
  <orderedlist>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.phys.host"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.config.kvm.guest"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec.ha.pmremote.integrate.guestsintocluster"/></para>
   </listitem>
  </orderedlist>

  <sect2 xml:id="sec.ha.pmremote.config.phys.host">
   <title>Configuring the Physical Host</title>
   <para>
    In this guide, the hostname <systemitem class="domainname">&wsI;</systemitem>
    is used for the physical host. On <systemitem
     class="domainname">&wsI;</systemitem> do the following:
   </para>
   <procedure>
    <!--<title>Configuring the Physical Host</title>-->
    <step>
     <para>Install your cluster as described in the <citetitle>&instquick;</citetitle>.
      <!--<xref linkend="art.ha.install.quick"/>-->
     </para>
     <remark>toms 2017-03-23: what about firewall settings? Should we open TCP
     ports 2224, 3121, and 21064, and UDP port 5405?</remark>
    </step>
    <step>
     <para>
      Install the packages <package>acemaker-remote</package>
      and <package>resource-agents</package>.</para>
    </step>
    <step>
     <para>Configure the cluster nodes:</para>
     <remark>toms 2017-03-23: Should we add all remote nodes into csync.cfg and /etc/hosts ?</remark>
     <substeps>
      <step>
       <para>Allow cluster-related services through the local firewall on all
        cluster nodes.</para>
      </step>
      <step>
       <para>Create an authentication key to all cluster and remote
        nodes:</para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0755 /etc/pacemaker
&prompt.root;<command>dd</command> if=/dev/urandom of=/etc/pacemaker/authkey bs=4k count=1</screen>
       <remark>toms 2017-03-03: is on SLEHAE the file /etc/corosync/authkey being used?</remark>
      </step>
      <step>
       <para>If you want to enable the <systemitem class="daemon"
        >pacemaker_remote</systemitem> service, run:</para>
       <screen>&prompt.root;<command>chkconfig</command> pacemaker_remote on</screen>
      </step>
      <step>
       <para>Restart &pace; with <command>systemctl restart pacemaker</command>.</para>
      </step>
      <step>
       <para>Insert the following line in the <filename>/etc/csync2/csync2.cfg</filename>:</para>
       <screen>include /etc/pacemaker/authkey</screen>
      </step>
      <!-- Run csync2? -->
     </substeps>
    </step>
    <!--<step>
    <para>Check if all necessary firewall ports for an HA cluster are open
     (for example, 3121, and more)</para>
   </step>-->
    <step>
     <para>Define a primitive resource per remote node:</para>
     <screen>&prompt.root;<command>crm</command> configure primitive suse09</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.pmremote.config.kvm.guest">
   <title>Configuring the KVM Guests</title>
   <para>
    In this guide, the hostname <systemitem class="domainname">&wsII;</systemitem>
    is used for a KVM guest.
   </para>
   <procedure>
    <step>
     <para>Create a KVM guest. Make sure to configure the guest with
     a hostname and a static IP address. </para>
    </step>
    <step>
     <para>Add the KVM guest hostname <systemitem
      class="domainname">&wsII;</systemitem> and its IP address to the hosts
     machine at <filename>/etc/hosts</filename>.</para>
    </step>
    <step>
     <para>Configure Firewall settings on guest.</para>
    </step>
    <step>
     <para>Verify connectivity between guests and host by using
      <command>ping</command> and <command>ssh</command>.</para>
    </step>
    <step>
     <para>Install the <package>pacemake_remote</package> package and
     enable it:</para>
     <screen>&prompt.root;<command>zypper</command> in pacemake_remote
&prompt.root;<command>systemctl</command> enable pacemaker_remote</screen>
    </step>
    <step>
     <para>Copy the authentication key from your host:</para>
     <screen>&prompt.root;<command>mkdir</command> -p --mode=0750 /etc/pacemaker
&prompt.root;<command>chgrp</command> haclient /etc/pacemaker
&prompt.root;<command>scp</command> root@&wsI;:/etc/pacemaker/authkey /etc/pacemaker</screen>
    </step>
    <step>
     <para>Verify host connection to guest. This can be done by
      making a SSH connection from the host. This connection
      will fail, but <emphasis>how</emphasis> it fails gives you hints
      about its connectivity.
     </para>
     <para>Run <command>ssh</command> on one of the cluster nodes:</para>
     <screen>&prompt.root;<command>ssh</command> -p 3121 &wsII;</screen>
     <para>A <quote>successful</quote> connection looks like this:</para>
     <screen>ssh_exhange_identification: read: Connection reset by peer.</screen>
     <para>If you see the following messages, the connection does not work:</para>
     <screen>ssh: connect to host &wsII; port 3121: No route to host</screen>
     <para>Or:</para>
     <screen>ssh: connect to host &wsII; port 3121: Connection refused</screen>
    </step>
    <step>
     <para>Shut down the KVM guest(s) as &pace; will manage the guests itself.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.pmremote.integrate.guestsintocluster">
   <title>Integrating KVM Guests into Cluster</title>
   <para>To integrate the KVM guests into the cluster, do the following on
    the physical host <systemitem class="domainname">&wsI;</systemitem>:</para>
   <procedure>
    <step>
     <para>Start &pace;:</para>
     <screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
    </step>
    <step>
     <para>Check the status of the command <command>crm status</command>. It
     should contain a running cluster with nodes that are all accessible.
     </para>
    </step>
    <step>
     <para>Check if all KVM guests hostnames and IP addresses are listed
     in the file <filename>/etc/hosts</filename>.</para>
    </step>
    <step>
     <para>Dump the XML configuration of the KVM guest(s):
     </para>
     <screen>&prompt.root;<command>virsh</command> list --all
 Id    Name         State
-----------------------------------
 -     &wsII;       shut off
&prompt.root;<command>virsh</command> dump &wsII; > /etc/pacemaker/&wsII;.xml</screen>
    </step>
    <step>
     <para>
      Create a new <systemitem>VirtualDomain</systemitem> resource:
      <remark>toms 2017-03-23: Check the command, this is probably not correct:</remark>
     </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> vm.&wsII; ocf:heartbeat:VirtualDomain \
  params hypervisor="qemu:///system" \
         config="/etc/pacemaker/&wsII;.xml \
  meta remote-node=&wsII;</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.pmremote.testing">
  <title>Testing the Setup</title>
  <para>To demonstrate how resources are executed, use a few dummy resources.
   These dummy resources serve for testing purposes only.
  </para>
  <procedure>
   <step>
    <para>Create a few dummy resources:</para>
    <screen>&prompt.root;<command>crm</command> configure primitive fake1 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake2 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake3 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake4 ocf:pacemaker:Dummy</screen>
   </step>
   <step>
    <para>Check the status of the <command>crm status</command> command.
    You should see something like the following:
    </para>
    <screen>TBD</screen>
   </step>
   <step>
    <para>To move one of the Dummy primitives to run it on another node, use
    the following command:</para>
    <screen>TBD</screen>
    <para>The status will change to this:</para>
    <screen>TBD</screen>
   </step>
  </procedure>

  <para>
   To test whether fencing works, kill the <systemitem
    class="daemon">pacemaker_remote</systemitem> daemon and see how &pace;
   react:
  </para>
  <screen>&prompt.root;<command>kill</command> -9 $(pidof pacemaker_remote)</screen>
  <para>After a few seconds, the status of the cluster should look like this:</para>
  <screen>&prompt.root;<command>crm</command> status
TBD</screen>
 </sect1>

<!--
  * Install and setup the "normal" cluster nodes as usual
    (section 5.3)
  * Install pacemaker-remote, resource-agents and crmsh on the remote nodes
    (section 5.1.2)
  * Create cluster directories on all nodes (cluster + remote_nodes)
  * Create a special key for pacemaker remote (that's 4K key and different
    from our cluster secure key.
  * Enable + start the pacemaker_remote service on all remote_nodes
  * verify connection of remote_hosts of the pacemaker nodes
    (section 5.2)
  * Integrate remote nodes into the cluster
    (section 5.4)
 -->

</article>