<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="cha.ha.requirements"
  xmlns="http://docbook.org/ns/docbook" 
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:dm="urn:x-suse:ns:docmanager" 
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <?dbfo-need height="10em"?>
  <title>System Requirements and Recommendations</title>
 <info>
      <dm:docmanager>
        <dm:bugtracker>
          <dm:assignee>taroth@suse.com</dm:assignee>
        </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
      <abstract>
        <para>
    The following section informs you about system requirements, and some
    prerequisites for &productnamereg;. It also includes recommendations
    for cluster setup.
   </para>
      </abstract>
    </info>
    <sect1 xml:id="sec.ha.requirements.hw">
  <title>Hardware Requirements</title>

  <para>
   The following list specifies hardware requirements for a cluster based on
   &productnamereg;. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </para>

  <variablelist>
   <varlistentry>
    <term>Servers</term>
    <listitem>
     <para> 1 to 32 Linux servers with software as specified in <xref
       linkend="sec.ha.requirements.sw"/>. </para>
     &sys-req-hw-nodes;
     <para> Using <literal>pacemaker_remote</literal>, the cluster can be
      extended to include additional Linux servers beyond the 32-node limit.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Communication Channels</term>
    <listitem>
      &sys-req-hw-comm-channels;
     <para>For details, refer to <xref linkend="cha.ha.netbonding"/> and <xref
       linkend="pro.ha.installation.setup.channel2"/>, respectively.
      <!--bmwiedemann 2015-01-12: are there alternatives to Ethernet? WLAN?
     taroth 2015-01-12: checked this with the developers, the answer was:
     cable connections are more robust-->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Node Fencing/&stonith;</term>
    <listitem>
     &sys-req-hw-stonith;
     <para>Unless SBD is used, each node in the &ha; cluster must have at least
      one &stonith; device. We strongly recommend multiple
      &stonith; devices per node.</para>
     &important-stonith;
    </listitem>
  </varlistentry>
  </variablelist>
 </sect1>
<?dbfo-need height="10em"?>
 <sect1 xml:id="sec.ha.requirements.sw">
  <title>Software Requirements</title>

  <para>
   On all nodes that will be part of the cluster the following software
   must be installed.
  </para>

<!--taroth 2011-11-02: for the records, we do *not* support mixed clusters
       (SP1/SP2), exception: during rolling upgrade (info by lmb@ha-devel)-->

  <itemizedlist>
   <listitem>
    &sys-req-sw-sles;
   </listitem>
   <listitem>
    &sys-req-sw-sleha;
   </listitem>
   <listitem>
    <para>
     (Optional) For &geo; clusters: &hageo; &productnumber;
     (with all available online updates)
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.ha.requirements.disk">
  <title>Storage Requirements</title>

  <para>
   Some services require shared storage. If using an external NFS share, it must
   be reliably accessible from all cluster nodes via redundant communication
   paths.</para>
  <para>To make data highly available, a shared disk system (Storage Area
   Network, or SAN) is recommended for your cluster. If a shared disk
   subsystem is used, ensure the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to
     the manufacturer&rsquo;s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
     Hardware-based RAID is recommended. Host-based software RAID is not
     supported for all configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD* to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the device provided
     by DRBD&mdash;never the backing device. Use the same (bonded) NICs
     that the rest of the cluster uses to leverage the redundancy provided
     there.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   When using SBD as &stonith; mechanism, additional requirements apply
   for the shared storage. For details, see <xref
    linkend="sec.ha.storage.protect.req"/>.
   </para>
 </sect1>
 <sect1 xml:id="sec.ha.requirements.other">
  <title>Other Requirements and Recommendations</title>

  <para>
   For a supported and useful &ha; setup, consider the following
   recommendations:
  </para>

  <variablelist>
   <varlistentry xml:id="vle.ha.req.nodes">
    <term>Number of Cluster Nodes</term>
    <listitem>
     <important>
      <title>Odd Number of Cluster Nodes</title>
      <para>
       For clusters with more than three nodes, it is strongly recommended to use
       an odd number of cluster nodes.
      </para>
      <para>In order to keep services running, a cluster with more than two nodes
       relies on quorum (majority vote) to resolve cluster partitions.
       A two- or three-node cluster can tolerate the failure of one node at a time,
       a five-node cluster can tolerate failures of two nodes, etc.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Time Synchronization</term>
    &sys-req-other-ntp;
   </varlistentry>
   <varlistentry>
    <term>Network Interface Card (NIC) Names/term>
     </term>
    <listitem>
     <para>
      Must be identical on all nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Host Name and IP Address</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        Use static IP addresses.
       </para>
      </listitem>
      <listitem>
       &sys-req-other-etc-hosts;
       <para>
        For details on how Pacemaker gets the node names, see also
        <link xlink:href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-node-name.html"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.ha.req.ssh">
    <term>SSH</term>
    <listitem>
     &sys-req-other-ssh;
     <note>
      <title>Regulatory Requirements</title>
      <para>
       If passwordless SSH access does not comply with regulatory
       requirements, you can use the work-around described in
       <xref linkend="app.crmreport.nonroot"/> for running 
       <command>crm report</command>.
      </para>
      <para>
       For the <guimenu>History Explorer</guimenu> there is currently no
       alternative for passwordless login.
      </para>
     </note>
    </listitem>
   </varlistentry>

  </variablelist>
 </sect1>
</chapter>