<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="cha-ha-requirements"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <?dbfo-need height="10em"?>
  <title>System requirements and recommendations</title>
 <info>
      <abstract>
        <para>
    The following section informs you about system requirements and
    prerequisites for &productnamereg;. It also includes recommendations
    for cluster setup.
   </para>
      </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer/>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release/>
   <dm:repository/>
  </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-requirements-hw">
  <title>Hardware requirements</title>

  <para>
   The following list specifies hardware requirements for a cluster based on
   &productnamereg;. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </para>

  <variablelist>
   <varlistentry>
    <term>Servers</term>
    <listitem>
     <para> 1 to 32 Linux servers with software as specified in <xref
       linkend="sec-ha-requirements-sw"/>. </para>
     &sys-req-hw-nodes;
     <para> Using <literal>pacemaker_remote</literal>, the cluster can be
      extended to include additional Linux servers beyond the 32-node limit.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Communication channels</term>
    <listitem>
      &sys-req-hw-comm-channels;
     <para>For details, refer to <xref linkend="cha-ha-netbonding"/> and <xref
       linkend="pro-ha-installation-setup-channel2"/>, respectively.
      <!--bmwiedemann 2015-01-12: are there alternatives to Ethernet? WLAN?
     taroth 2015-01-12: checked this with the developers, the answer was:
     cable connections are more robust-->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Node fencing/&stonith;</term>
    <listitem>
     &sys-req-hw-stonith;
     <para>Unless SBD is used, each node in the &ha; cluster must have at least
      one &stonith; device. We strongly recommend multiple
      &stonith; devices per node.</para>
     &important-stonith;
    </listitem>
  </varlistentry>
  </variablelist>
 </sect1>
<?dbfo-need height="10em"?>
 <sect1 xml:id="sec-ha-requirements-sw">
  <title>Software requirements</title>

  <para>
   All nodes need at least the following modules and extensions:
  </para>

&sys-req-sw-modules;

  <para>
   Depending on the system role you select during
   installation, the following software patterns are installed by default:
  </para>
  <variablelist>
   <varlistentry>
    <term>HA Node system role</term>
    <listitem>
     <para>
      &ha; (<literal>ha_sles</literal>)
     </para>
     <para>
      Enhanced Base System (<literal>enhanced_base</literal>)
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>HA GEO Node system role</term>
    <listitem>
     <para>
      &geo; Clustering for &ha; (<literal>ha_geo</literal>)
     </para>
     <para>
      Enhanced Base System (<literal>enhanced_base</literal>)
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <title>Minimal installation</title>
   <para>
    An installation via those system roles results in a minimal installation only.
    You might need to add more packages manually, if required.</para>
   <para>
    For machines that originally had another system role assigned, you need to
    manually install the <systemitem>ha_sles</systemitem> or
    <systemitem>ha_geo</systemitem> patterns and any further packages that you
    need.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-requirements-disk">
  <title>Storage requirements</title>

  <para>
   Some services require shared storage. If using an external NFS share, it must
   be reliably accessible from all cluster nodes via redundant communication
   paths.</para>
  <para>To make data highly available, a shared disk system (Storage Area
   Network, or SAN) is recommended for your cluster. If a shared disk
   subsystem is used, ensure the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to
     the manufacturer&rsquo;s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD* to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the device provided
     by DRBD&mdash;never the backing device. To make use of the
     redundancy it is possible to use the same NICs as the rest of the cluster.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   When using disk-based SBD as the &stonith; mechanism, additional requirements apply
   for the shared storage. For details, see <xref
    linkend="sec-ha-storage-protect-req"/>.
   </para>
 </sect1>
 <sect1 xml:id="sec-ha-requirements-other">
  <title>Other requirements and recommendations</title>

  <para>
   For a supported and useful &ha; setup, consider the following
   recommendations:
  </para>

  <variablelist>
   <varlistentry xml:id="vle-ha-req-nodes">
    <term>Number of cluster nodes</term>
    <listitem>
     <para>For clusters with more than two nodes, it is strongly recommended to use
       an odd number of cluster nodes to have quorum. For more information
       about quorum, see <xref linkend="sec-ha-config-basics-global"/>.
       A regular cluster can contain up to 32 nodes. With the &pmremote;
       service, &ha; clusters can be extended to include additional nodes
       beyond this limit. See &pcmkremquick; for more details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Time synchronization</term>
    &sys-req-other-ntp;
   </varlistentry>
   <varlistentry>
    <term>Network Interface Card (NIC) names</term>
    <listitem>
     <para>
      Must be identical on all nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Host name and IP address</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        Use static IP addresses.
       </para>
      </listitem>
      <listitem>
       <para>
        Only the primary IP address is supported.
       </para>
      </listitem>
      <listitem>
       &sys-req-other-etc-hosts;
       <para>
        For details on how Pacemaker gets the node names, see also
        <link xlink:href="https://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-node-name.html"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-req-ssh">
    <term>SSH</term>
    <listitem>
     &sys-req-other-ssh;
     <para>
      If you set up the cluster with the bootstrap scripts, the SSH keys are
      automatically created and copied. If you set up the cluster with the
      &yast; cluster module, you must configure the SSH keys yourself.
     </para>
     <note>
      <title>Regulatory requirements</title>
      <para>
       By default, the cluster performs operations as the &rootuser; user.
       If passwordless &rootuser; SSH access does not comply with regulatory requirements,
       you can set up the cluster as a user with <command>sudo</command> privileges instead.
       This user can run cluster operations, including <command>crm report</command>.
      </para>
      <para>
       Additionally, if you cannot store SSH keys directly on the nodes, you can
       use local SSH keys instead via SSH agent forwarding, as described in
       <xref linkend="sec-ha-manual-config-crm-user-privileges"/>.
      </para>
      <para>
       For even stricter security policies, you can set up a non-root user with only
       limited <command>sudo</command> privileges specifically for <command>crm report</command>,
       as described in <xref linkend="sec-crmreport-nonroot-sudo"/>.
      </para>
      <para>
       For the <guimenu>History Explorer</guimenu> there is currently no
       alternative for passwordless login.
      </para>
     </note>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
