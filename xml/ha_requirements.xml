<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ha.requirements">
 <title>System Requirements and Recommendations</title>
 <abstract>
  <para>
   The following section informs you about system requirements, and some prerequisites
   for &productnamereg;. It also includes recommendations for cluster setup.
  </para>
 </abstract>
 <sect1 id="sec.ha.requirements.hw">
  <title>Hardware Requirements</title>

  <para>
   The following list specifies hardware requirements for a cluster based on
   &productnamereg;. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     1 to 32 Linux servers with software as specified in
     <xref linkend="sec.ha.requirements.sw"/>. The servers do not require
     identical hardware (memory, disk space, etc.), but they must have the
     same architecture. Cross-platform clusters are not supported.
    </para>
   </listitem>
   <listitem>
    <para> At least two TCP/IP communication media per cluster node. Cluster
     nodes use multicast or unicast for communication so the network equipment
     must support the communication means you want to use. The communication
     media should support a data rate of 100 Mbit/s or higher. Preferably, the
     Ethernet channels should be bonded. </para>
   </listitem>
   <listitem>
    <para>
     Optional: A shared disk subsystem connected to all servers in the
     cluster from where it needs to be accessed. See <xref
      linkend="sec.ha.requirements.disk"/>.
    </para>
   </listitem>
   <listitem>
    <para> A <xref linkend="glo.stonith" xrefstyle="select:title nopage"/>
     mechanism. A &stonith; device is a power switch which the cluster uses
     to reset nodes that are thought to be dead or behaving in a strange manner.
     This is the only reliable way to ensure that no data corruption is
     performed by nodes that hang and only appear to be dead. </para>
    <!--<important>
     <title>No Support Without &stonith;</title>
     <para>
      A cluster without &stonith; is not supported.
     </para>
    </important>
    <para>
     For more information, refer to <xref linkend="cha.ha.fencing"/>.
     </para>-->
   </listitem>
  </itemizedlist>
 </sect1>
<?dbfo-need height="10em"?>
 <sect1 id="sec.ha.requirements.sw">
  <title>Software Requirements</title>

  <para>
   Ensure that the following software requirements are met:
  </para>

<!--taroth 2011-11-02: for the records, we do *not* support mixed clusters
       (SP1/SP2), exception: during rolling upgrade (info by lmb@ha-devel)-->

  <itemizedlist>
   <listitem>
    <para>
     &slsreg; &productnumber; (with all available online updates) is installed on
     all nodes that will be part of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &productnumber;  (with all available online updates)
     is installed on all nodes that will be part of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>If you want to use &geo; clusters, make sure that &hageo;
     &productnumber; (with all available online updates) is installed on all
     nodes that will be part of the cluster. </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.requirements.disk">
  <title>Storage Requirements</title>

  <para> To make data highly available, a shared disk system (Storage Area
   Network, or SAN) is recommended for your cluster. If a shared disk subsystem
   is used, ensure the following: </para>

  <itemizedlist>
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to
     the manufacturer&rsquo;s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
     Hardware-based RAID is recommended. Host-based software RAID is not
     supported for all configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD* to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the device provided
     by DRBD&mdash;never the backing device. Use the same (bonded) NICs that
     the rest of the cluster uses to leverage the redundancy provided there.
    </para>
   </listitem>
   
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.requirements.other">
  <title>Other Requirements and Recommendations</title>
  <para>For a supported and useful &ha; setup, consider the following
   recommendations:</para>
  <variablelist>
   <varlistentry id="vle.ha.req.nodes">
    <term>Number of Cluster Nodes</term>
    <listitem>
     <para>
      <remark>taroth 2016-06-26: the following is taken from the Cloud docs - DEVs,
      if this is also true for SLE HA?</remark>
      <remark>krig 2014-08-20: Technically, a cluster can consist of a single node. That seems a
       bit pointless, though. The only reason to have a single-node cluster
       that I can think of would be if the intention is to add more nodes
       later, and you'd want to configure things the "right" way from the
       start. I don't think the recommendation for an odd number of nodes is
       entirely correct since STONITH is required anyway. It's possible to
       run a cluster without quorum as long as STONITH is
       configured. However, this should probably be run by someone with more
       experience than me.</remark>Each
      cluster must consist of at least two cluster nodes.</para>
     <important>
      <title>Odd Number of Cluster Nodes</title>
      <para>It is strongly recommended to use an <emphasis>odd</emphasis> number
       of cluster nodes with a <emphasis>minimum</emphasis> of three nodes. </para>
      <para>A cluster needs <xref linkend="gloss.quorum"
        xrefstyle="select:label nopage"/> to keep services running. Therefore a
       three-node cluster can tolerate only failure of one node at a time,
       whereas a five-node cluster can tolerate failures of two nodes
       etc.</para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.req.stonith">
    <term>&stonith;</term>
    <listitem>
     <important>
      <title>No Support Without &stonith;</title>
      <para> A cluster without &stonith; is not supported. </para>
     </important>
     <para>For a supported &ha; setup, ensure the following:</para>
     <itemizedlist>
      <listitem>
       <para>Each node in the &ha; cluster must have at least one
        &stonith; device (usually a piece of hardware). We strongly
        recommend multiple &stonith; devices per node, unless SBD is used.
        SBD provides a way to enable &stonith; and fencing in clusters
        without external power switches, but it requires shared storage.</para>
      </listitem>
      <listitem>
       <para>The global cluster options <systemitem>stonith-enabled</systemitem>
        and <systemitem>startup-fencing</systemitem> must be set to
         <literal>true</literal>. As soon as you change them, you will lose
        support.</para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.req.communication">
    <term>Redundant Communication Paths</term>
    <listitem>
     <para>For a supported &ha; setup, it is required to set up cluster
      communication via two or more redundant paths. This can be done via: </para>
     <itemizedlist>
      <listitem>
       <para>
        <xref linkend="cha.ha.netbonding" xrefstyle="select:title"/>. </para>
      </listitem>
      <listitem>
       <para> A second communication channel in &corosync;. For details, see
         <xref linkend="pro.ha.installation.setup.channel2"/>. </para>
      </listitem>
     </itemizedlist>
     <para> If possible, choose network device bonding. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Time Synchronization</term>
    <listitem>
     <para> Cluster nodes should synchronize to an NTP server outside the
      cluster. For more information, see the <citetitle>&admin;</citetitle>
      for &sls; &productnumber;, available at <ulink
       url="http://www.suse.com/doc/"/>. Refer to the chapter <citetitle>Time
       Synchronization with NTP</citetitle>. </para>
     <para> If nodes are not synchronized, log files and cluster reports are
      very hard to analyze. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NIC Names</term>
    <listitem>
     <para> Must be identical on all nodes. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Host Name and IP Address</term>
    <listitem>
     <para> Configure host name resolution by editing the
       <filename>/etc/hosts</filename> file on <emphasis>each</emphasis> server
      in the cluster. To ensure that cluster communication is not slowed down or
      tampered with by any DNS: </para>
     <itemizedlist>
      <listitem>
       <para> Use static IP addresses. </para>
      </listitem>
      <listitem>
       <para> List all cluster nodes in this file with their fully qualified
        host name and short host name. It is essential that members of the cluster
        can find each other by name. If the names are not available,
        internal cluster communication will fail. </para>
      </listitem>
     </itemizedlist>
     <para> For more information, see the <citetitle>&admin;</citetitle> for
      &sls; &productnumber;, available at <ulink
       url="http://www.suse.com/doc"/>. Refer to chapter <citetitle>Basic
        Networking</citetitle>, section <citetitle>Configuring a Network Connection with &yast;</citetitle> &gt; 
        <citetitle>Configuring Host Name and DNS</citetitle>. </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.req.storage" >
    <term>Storage Requirements</term>
    <listitem>
     <!-- taroth 2014-04-14: https://bugzilla.novell.com/show_bug.cgi?id=873373 -->
     <para>Some services may require shared storage. For requirements, see <xref
       linkend="sec.ha.requirements.disk"/>. You can also use an external NFS
      share or DRBD. If using an external NFS share, it must be reliably accessible from all cluster nodes
        via redundant communication paths. See <xref
         linkend="vle.ha.req.communication"/>. </para>
     <para>When using SBD as &stonith; device, additional requirements apply for
      the shared storage. For
      details, see <ulink url="http://linux-ha.org/wiki/SBD_Fencing"/>,
      section <citetitle>Requirements</citetitle>.</para>
    </listitem>
   </varlistentry>
   <varlistentry  id="vle.ha.req.ssh">
    <term>SSH</term>
    <listitem>
     <para> All cluster nodes must be able to access each other via SSH. Tools
      like <command>hb_report</command> or <command>crm_report</command> 
      (for troubleshooting) and &hawk;'s <guimenu>History
      Explorer</guimenu> require passwordless SSH access between the nodes, otherwise they
      can only collect data from the current node. In case you use a
      non-standard SSH port, use the <option>-X</option> option (see man
      page). For example, if your SSH port is <literal>3479</literal>, invoke an
      <literal>hb_report</literal> with:</para>
     <!--taroth 2015-08-22: fate#314906: Support for a non-standard SSH port in hb_reports script-->
     <screen>&prompt.root;hb_report -X "-p 3479" [...]</screen>
     <note>
      <title>Regulatory Requirements</title>
      <para> If passwordless SSH access does not comply with regulatory
       requirements, you can use the following work-around for
       <command>crm_report</command>: </para>
      <para> Create a user that can log in without a password (for example,
       using public key authentication). Configure <command>sudo</command> for
       this user so it does not require a &rootuser; password. Start
       <command>crm_report</command> from command line with the
        <option>-u</option> option to specify the user's name. For more
       information, see the <command>crm_report</command> man page. </para>
      <para> For the <guimenu>History Explorer</guimenu> there is currently no alternative for
       passwordless login. </para>
     </note>
    </listitem>
   </varlistentry>
   <!--taroth 2011-11-02:  add link to official web page for support status of
    virtual environments as soon as it is available (see thread on ha-devel), 
    info by lmb@ha-devel:
    We support running inside all virtualization environments where SLES is
    fully supported. We have tests covering KVM/Xen/VMWare; but we basically
    simply inherit the support statement from SLES, since SLE HA has no
    specific hardware dependencies.
    -->
  </variablelist>
 </sect1>
</chapter>
