<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!--taroth 2010-02-10: todo: https://fate.novell.com/307380
  Testcase
setup 
1. start cmirrord 
2. vgconvert -cy $VGNAME 
3. lvcreate -m 1 ++name $LVNAME ++size $LVSIZE $VGNAME 
   A mirrored $LVNAME should be created successfully with userspace log. 
   recover: Following scenarios should be tested under stressed IO load . 
   device failure during writing
   device failure during syncing
   system crash during writing
   system crash during syncing


Testcase Fate#314367:
1. create a cluster, at lease two nodes requires. for example /dev/sda 
   is the shared storage 
2. create a clustered VG which requires pacemaker have started dlmd and clvmd. 
   pvcreate /dev/sda vgcreate -cy vg /dev/sda 
3. create mirrored log lv in cluster lvcreate -nlv -m1 -l10%VG vg -\-mirrorlog mirrored 
4. use lvs to show the sync percent. the sync percent should grow to 100% which means the mirrored disk synced. 
5. now you have clustered /dev/vg/lv which have mirrored-log. 
5.1 read or write /dev/vg/lv to test if the write is OK. 
5.2 use lvchange -an to deactive and lvchange -ay to re-active 
5.3 use lvconvert to convert mirrored-log to disk-log use lvconvert to convert disk-log to mirrored-log. 
6. create a mirrored-log lv in a another cluster VG which is a different vg from previous vg. 
   run the same command in step 5 for both cluster LV(mirrored log) at same time.

-->
<!--taroth 2010-08-19: for next revision, see also Sander's book (chapter about
cLVM) for more information and details to integrate here - really helpful-->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ha.clvm">
 <title>Cluster Logical Volume Manager (cLVM)</title>
 <info>
      <abstract>
        <para>
    When managing shared storage on a cluster, every node must be informed
    about changes that are done to the storage subsystem. The Linux Volume
    Manager&nbsp;2 (LVM2), which is widely used to manage local storage,
    has been extended to support transparent management of volume groups
    across the whole cluster. Clustered volume groups can be managed using
    the same commands as local storage.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec.ha.clvm.overview">
  <title>Conceptual Overview</title>

<!--<para>
   Internally, cLVM uses the Distributed Lock Manager (DLM) component of the
   cluster stack to coordinate access to the LVM2 metadata. As DLM in turn
   integrates with the other components of the stack such as fencing, the
   integrity of the shared storage is protected at all times.
  </para>-->

  <para>
   Clustered LVM is coordinated with different tools:
  </para>

  <variablelist>
   <varlistentry>
    <term>Distributed Lock Manager (DLM)</term>
    <listitem>
     <para>
      Coordinates disk access for cLVM.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Logical Volume Manager2 (LVM2)</term>
    <listitem>
     <para>
      Enables flexible distribution of one file system over several disks.
      LVM provides a virtual pool of disk space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Clustered Logical Volume Manager (cLVM)</term>
    <listitem>
     <para>
      Coordinates access to the LVM2 metadata so every node knows about
      changes. cLVM does not coordinate access to the shared data itself; to
      enable cLVM to do so, you must configure OCFS2 or other cluster-aware
      applications on top of the cLVM-managed storage.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.ha.clvm.config">
  <title>Configuration of cLVM</title>

  <para>
   Depending on your scenario it is possible to create a RAID&nbsp;1
   device with cLVM with the following layers:
  </para>

  <remark>toms 2010-03-12 (DEV): What are the advantages and disadvantages?</remark>

  <itemizedlist>
   <listitem>
    <formalpara>
     <title>LVM</title>
     <para>
      This is a very flexible solution if you want to increase or decrease
      your file system size, add more physical storage, or create snapshots
      of your file systems. This method is described in
      <xref linkend="sec.ha.clvm.scenario.iscsi"/>.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>DRBD</title>
     <para>
      This solution only provides RAID&nbsp;0 (striping) and
      RAID&nbsp;1 (mirroring). The last method is described in
      <xref linkend="sec.ha.clvm.scenario.drbd"/>.
     </para>
    </formalpara>
   </listitem>
   <!--taroth 2015-09-23: willl not be shipped for SP1--> 
   <!--<listitem>
     <formalpara>
     <title>Cluster MD</title>
     <para>Cluster MD provides a way to use MD RAID1 simultaneously across the
      cluster. For details, see <xref linkend="cha.ha.cluster-md"/>. </para>
    </formalpara>
    </listitem>-->
  </itemizedlist>
  <para>
   Make sure you have fulfilled the following prerequisites:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A shared storage device is available, such as provided by a Fibre
     Channel, FCoE, SCSI, iSCSI SAN, or DRBD*.
    </para>
   </listitem>
   <listitem>
    <para>
     In case of DRBD, both nodes must be primary (as described in the
     following procedure).
    </para>
   </listitem>
   <listitem>
    <para>
     Check if the locking type of LVM2 is cluster-aware. The keyword
     <literal>locking_type</literal> in
     <filename>/etc/lvm/lvm.conf</filename> must contain the value
     <literal>3</literal> (the default is <literal>1</literal>). Copy the configuration to all nodes, if necessary.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Create Cluster Resources First</title>
   <para>
    First create your cluster resources as described in
    <xref linkend="sec.ha.clvm.config.resources"/> and then your LVM
    volumes. Otherwise it is impossible to remove the volumes later.
   </para>
  </note>

  <sect2 xml:id="sec.ha.clvm.config.cmirrord">
   <title>Configuring Cmirrord</title>
<!-- FATE#314367,  -->
   <para>
    To track mirror log information in a cluster, the
    <systemitem class="daemon">cmirrord</systemitem> daemon is used. Cluster
    mirrors are not possible without this daemon running.
   </para>
   <para>
    We assume that <filename>/dev/sda</filename> and
    <filename>/dev/sdb</filename> are the shared storage devices. Replace
    these with your own device name(s), if necessary. Proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      Create a cluster with at least two nodes.
     </para>
    </step>
    <step>
     <para>
      Configure your cluster to run <command>dlm</command>,
      <command>clvmd</command>, and &stonith;:
     </para>
<screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> clvmd ocf:lvm2:clvmd \
        op stop interval="0" timeout="100" \
        op start interval="0" timeout="90" \
        op monitor interval="20" timeout="60"
&prompt.crm.conf;<command>primitive</command> dlm ocf:pacemaker:controld \
        op start interval="0" timeout="90" \
        op stop interval="0" timeout="100" \
        op monitor interval="60" timeout="60"
&prompt.crm.conf;<command>primitive</command> sbd_stonith stonith:external/sbd \
        params pcmk_delay_max=30
&prompt.crm.conf;<command>group</command> base-group dlm clvmd
&prompt.crm.conf;<command>clone</command> base-clone base-group \
        meta interleave="true"</screen>
    </step>
    <step>
     <para>
      Leave &crmsh; with <command>exit</command> and commit your changes.
     </para>
    </step>
    <step>
     <para>
      Create a clustered volume group (VG):
<!-- which requires 
          Pacemaker to have started dlmd and clvmd-->
     </para>
<screen>&prompt.root;<command>pvcreate</command> /dev/sda /dev/sdb
&prompt.root;<command>vgcreate</command> -cy vg /dev/sda /dev/sdb</screen>
    </step>
    <step>
     <para>
      Create a mirrored-log logical volume (LV) in your cluster:
     </para>
<screen>&prompt.root;<command>lvcreate</command> -nLv -m1 -l10%VG vg --mirrorlog mirrored</screen>
    </step>
    <step>
     <para>
      Use <command>lvs</command> to show the progress. If the percentage
      number has reached 100%, the mirrored disk is successfully
      synchronized.
     </para>
    </step>
    <step xml:id="st.ha.clvm.config.cmirrord.test">
     <para>
      To test the clustered volume <filename>/dev/vg/lv</filename>, use the
      following steps:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Read or write to <filename>/dev/vg/lv</filename>.
       </para>
      </step>
      <step>
       <para>
        Deactivate your LV with <command>lvchange</command>
        <option>-an</option>.
       </para>
      </step>
      <step>
       <para>
        Activate your LV with <command>lvchange</command>
        <option>-ay</option>.
       </para>
      </step>
      <step>
       <para>
        Use <command>lvconvert</command> to convert a mirrored log to a disk
        log.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Create a mirrored-log LV in another cluster VG. This is a different
      volume group from the previous one.
     </para>
    </step>
   </procedure>
<!-- FATE#312248: -->
   <para>
    The current cLVM can only handle one physical volume (PV) per mirror
    side. If one mirror is actually made up of several PVs that need to be
    concatenated or striped, <command>lvcreate</command> does not understand
    this. For this reason, <command>lvcreate</command> and
    <systemitem>cmirrord</systemitem> metadata needs to understand
    <quote>grouping</quote> of PVs into one side, effectively supporting
    RAID10.
   </para>
   <para>
    To support RAID10 for <systemitem class="daemon">cmirrord</systemitem>,
    use the following procedure (assuming that <filename>/dev/sda</filename>
    and <filename>/dev/sdb</filename> are the shared storage devices):
   </para>
   <procedure>
<!--<step>
        <para>Create a cluster with at least two nodes.</para>
      </step>
      -->
    <step>
     <para>
      Create a volume group (VG):
     </para>
<screen>&prompt.root;<command>pvcreate</command> /dev/sda /dev/sdb
&prompt.root;<command>vgcreate</command> vg /dev/sda /dev/sdb</screen>
    </step>
    <step>
     <para>
      Open the file <filename>/etc/lvm/lvm.conf</filename> and go to the
      section <literal>allocation</literal>. Set the following line and save
      the file:
     </para>
<screen>mirror_legs_require_separate_pvs = 1</screen>
    </step>
    <step>
     <para>
      Add your tags to your PVs:
     </para>
<screen>&prompt.root;<command>pvchange</command> --addtag @a /dev/sda
&prompt.root;<command>pvchange</command> --addtag @b /dev/sdb</screen>
     <para>
      A tag is an unordered keyword or term assigned to the metadata of a
      storage object. Tagging allows you to classify collections of LVM
      storage objects in ways that you find useful by attaching an unordered
      list of tags to their metadata.
     </para>
    </step>
    <step>
     <para>
      List your tags:
     </para>
<screen>&prompt.root;<command>pvs</command> -o pv_name,vg_name,pv_tags /dev/sd{a,b}</screen>
     <para>
      You should receive this output:
     </para>
<screen>  PV         VG     PV Tags
  /dev/sda vgtest a
  /dev/sdb vgtest b</screen>
    </step>
   </procedure>
   <para>
    If you need further information regarding LVM, refer to the &sls;
    &productnumber; <citetitle>&storage;</citetitle>, chapter
    <citetitle>LVM Configuration</citetitle>. It is available from
    <link xlink:href="http://www.suse.com/documentation/"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.clvm.config.resources">
   <title>Creating the Cluster Resources</title>
   <para>
    Preparing the cluster for use of cLVM includes the following basic
    steps:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="pro.ha.clvm.dlmresource" xrefstyle="select:title"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="pro.ha.clvm.lvmresources" xrefstyle="select:title"/>
     </para>
    </listitem>
   </itemizedlist>
   <procedure xml:id="pro.ha.clvm.dlmresource">
    <title>Creating a DLM Resource</title>
<!--bcn#633216 and lmb's setup proposal on [ha-devel]-->
    <note>
     <title>DLM Resource for Both cLVM and OCFS2</title>
     <para>
      Both cLVM and OCFS2 need a DLM resource that runs on all nodes in the
      cluster and therefore is usually configured as a clone. If you have a
      setup that includes both OCFS2 and cLVM, configuring
      <emphasis>one</emphasis> DLM resource for both OCFS2 and cLVM is
      enough.
     </para>
    </note>
    <step>
     <para>
      Start a shell and log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Run <command>crm</command> <option>configure</option>.
     </para>
    </step>
    <step>
     <para>
      Check the current configuration of the cluster resources with
      <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If you have already configured a DLM resource (and a corresponding
      base group and base clone), continue with
      <xref linkend="pro.ha.clvm.lvmresources"/>.
     </para>
     <para>
      Otherwise, configure a DLM resource and a corresponding base group and
      base clone as described in <xref linkend="pro.ocfs2.resources"/>.
     </para>
    </step>
    <step>
     <para>
      Leave the crm live configuration with <command>exit</command>.
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro.ha.clvm.lvmresources">
    <title>Creating LVM and cLVM Resources</title>
    <step>
     <para>
      Start a shell and log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Run <command>crm</command> <option>configure</option>.
     </para>
    </step>
    <step>
     <para>
      Configure a cLVM resource as follows:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> clvm ocf:lvm2:clvmd \
      params daemon_timeout="30"</screen>
<!--taroth 2012-01-12: removing the monitoring op as suggested by xwhu-->
<!--op monitor interval="60" timeout="60"-->
    </step>
    <step>
     <para>
      Configure an LVM resource for the volume group as follows:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> vg1 ocf:heartbeat:LVM \
      params volgrpname="cluster-vg" \
      op monitor interval="60" timeout="60"</screen>
    </step>
    <step>
     <para>
      If you want the volume group to be activated exclusively on
      <emphasis>one</emphasis> node, configure the LVM resource as described
      below and omit <xref linkend="step.ha.clvm.lvmresources.group"/>:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> vg1 ocf:heartbeat:LVM \
      params volgrpname="cluster-vg" exclusive="yes" \
      op monitor interval="60" timeout="60"</screen>
     <para>
      In this case, cLVM will protect all logical volumes within the VG from
      being activated on multiple nodes, as an additional measure of
      protection for non-clustered applications.
     </para>
    </step>
    <step xml:id="step.ha.clvm.lvmresources.group">
     <para>
      To ensure that the cLVM and LVM resources are activated cluster-wide,
      add both primitives to the base group you have created in
      <xref linkend="pro.ocfs2.resources"/>:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Enter
       </para>
<screen>&prompt.crm.conf;<command>edit</command> base-group</screen>
      </step>
      <step>
       <para>
        In the vi editor that opens, modify the group as follows and save
        your changes:
       </para>
<screen>&prompt.crm.conf;<command>group</command> base-group dlm clvm vg1 ocfs2-1</screen>
       <important>
        <title>Setup Without OCFS2</title>
        <para>
         If your setup does not include OCFS2, omit the
         <literal>ocfs2-1</literal> primitive from the base group.
        </para>
       </important>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
<!--taroth 2014-08-28: appendix file commented for now, because it needs
       alignement with changes here:
       To check if you have configured all needed resources, also refer to
      <xref linkend="cha.ha.config.example"/>.-->
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.clvm.scenario.iscsi">
   <title>Scenario: cLVM With iSCSI on SANs</title>
   <para>
    The following scenario uses two SAN boxes which export their iSCSI
    targets to several clients. The general idea is displayed in
    <xref linkend="fig.ha.clvm.scenario.iscsi"/>.
   </para>
   <figure xml:id="fig.ha.clvm.scenario.iscsi">
    <title>Setup of iSCSI with cLVM</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ha_clvm.svg" width="80%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ha_clvm.png" width="45%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <warning>
    <title>Data Loss</title>
    <para>
     The following procedures will destroy any data on your disks!
    </para>
   </warning>
   <para>
    Configure only one SAN box first. Each SAN box needs to export its own
    iSCSI target. Proceed as follows:
   </para>
<!-- 
    Another setup:
    1. Configure iSCSI daemon on file /etc/ietd.conf
    
    Target iqn.2010-03.de.&wsI;disks
    Lun 0 Sectors=x,Type=nullio
    Lun 1 Path=/dev/sda1,Type=blockio,ScsiId=0
    Lun 2 Path=/dev/sda2,Type=blockio,ScsiId=1
    
    Lun 0 are used only for testing and tuning purpose.
    
    2. Start the Daemon:
    # rciscsitarget start
    
    3. Check:
    # netstat -nltp | grep iet
    tcp 0 0 0.0.0.0:3260 0.0.0.0:* LISTEN 12586/ietd
    tcp 0 0 :::3260 :::* LISTEN 12586/ietd
    
    # tail /var/log/messages
    <ADD LOG MESSAGE>
    
    => Disks are shared over Ethernet
    
    
    Configuring Node Machines:
    1. /etc/iscsi/iscsi.conf:
    node.start = automatic
    
    2. rcopen-iscsi start
    
    3. Detect remote disks:
    # iscsi_discovery <SERVER_IP>
    192.168.1.2:3260,1 iqn.2010-03.&wsI;disks
    192.168.1.2:3260,1 iqn.2010-03.&wsI;disks
    
    
   -->
   <procedure xml:id="pro.ha.clvm.scenario.iscsi.targets">
    <title>Configuring iSCSI Targets (SAN)</title>
    <step>
     <para>
      Run &yast; and click <menuchoice> <guimenu>Network
      Services</guimenu> <guimenu>iSCSI LIO Target</guimenu> </menuchoice> to
      start the iSCSI Server module.
     </para>
    </step>
    <step>
     <para>
      If you want to start the iSCSI target whenever your computer is
      booted, choose <guimenu>When Booting</guimenu>, otherwise choose
      <guimenu>Manually</guimenu>.
     </para>
    </step>
    <step>
     <para>
      If you have a firewall running, enable <guimenu>Open Port in
      Firewall</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Switch to the <guimenu>Global</guimenu> tab. If you need
      authentication enable incoming or outgoing authentication or both. In
      this example, we select <guimenu>No Authentication</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Add a new iSCSI target:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Switch to the <guimenu>Targets</guimenu> tab.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Add</guimenu>.
       </para>
      </step>
      <step xml:id="st.ha.clvm.iscsi.iqn">
       <para>
        Enter a target name. The name needs to be formatted like this:
       </para>
<screen>iqn.<replaceable>DATE</replaceable>.<replaceable>DOMAIN</replaceable></screen>
       <para>
        For more information about the format, refer to <citetitle>Section
        3.2.6.3.1. Type "iqn." (iSCSI Qualified Name) </citetitle> at
        <link xlink:href="http://www.ietf.org/rfc/rfc3720.txt"/>.
       </para>
      </step>
      <step>
       <para>
        If you want a more descriptive name, you can change it as long as
        your identifier is unique for your different targets.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Add</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Enter the device name in <guimenu>Path</guimenu> and use a
        <guimenu>Scsiid</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Next</guimenu> twice.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Confirm the warning box with <guimenu>Yes</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Open the configuration file <filename>/etc/iscsi/iscsid.conf</filename>
      and change the parameter <literal>node.startup</literal> to
      <literal>automatic</literal>.
     </para>
    </step>
   </procedure>
   <para>
    Now set up your iSCSI initiators as follows:
   </para>
   <procedure xml:id="pro.ha.clvm.scenarios.iscsi.initiator">
    <title>Configuring iSCSI Initiators</title>
    <step>
     <para>
      Run &yast; and click <menuchoice> <guimenu>Network
      Services</guimenu> <guimenu>iSCSI Initiator</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      If you want to start the iSCSI initiator whenever your computer is
      booted, choose <guimenu>When Booting</guimenu>, otherwise set
      <guimenu>Manually</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Change to the <guimenu>Discovery</guimenu> tab and click the
      <guimenu>Discovery</guimenu> button.
     </para>
    </step>
    <step>
     <para>
      Add your IP address and your port of your iSCSI target (see
      <xref linkend="pro.ha.clvm.scenario.iscsi.targets"/>). Normally, you
      can leave the port as it is and use the default value.
     </para>
    </step>
    <step>
     <para>
      If you use authentication, insert the incoming and outgoing user name
      and password, otherwise activate <guimenu>No Authentication</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Select <guimenu>Next</guimenu>. The found connections are displayed in
      the list.
     </para>
    </step>
    <step>
     <para>
      Proceed with <guimenu>Finish</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Open a shell, log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Test if the iSCSI initiator has been started successfully:
     </para>
<screen>&prompt.root;<command>iscsiadm</command> -m discovery -t st -p &wwwip;
&wwwip;:3260,1 iqn.2010-03.de.&wsI;:san1</screen>
    </step>
    <step>
     <para>
      Establish a session:
     </para>
<screen>&prompt.root;<command>iscsiadm</command> -m node -l -p &wwwip; -T iqn.2010-03.de.&wsI;:san1
Logging in to [iface: default, target: iqn.2010-03.de.&wsI;:san1, portal: &wwwip;,3260]
Login to [iface: default, target: iqn.2010-03.de.&wsI;:san1, portal: &wwwip;,3260]: successful</screen>
     <para>
      See the device names with <command>lsscsi</command>:
     </para>
<screen>...
[4:0:0:2]    disk    IET      ...     0     /dev/sdd
[5:0:0:1]    disk    IET      ...     0     /dev/sde</screen>
     <para>
      Look for entries with <literal>IET</literal> in their third column. In
      this case, the devices are <filename>/dev/sdd</filename> and
      <filename>/dev/sde</filename>.
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro.ha.clvm.scenarios.iscsi.lvm">
    <title>Creating the LVM Volume Groups</title>
    <step>
     <para>
      Open a &rootuser; shell on one of the nodes you have run the iSCSI
      initiator from
      <xref linkend="pro.ha.clvm.scenarios.iscsi.initiator"/>.
     </para>
    </step>
    <step>
     <para>
      Prepare the physical volume for LVM with the command
      <command>pvcreate</command> on the disks <filename>/dev/sdd</filename>
      and <filename>/dev/sde</filename>:
     </para>
<screen>&prompt.root;<command>pvcreate</command> /dev/sdd
&prompt.root;<command>pvcreate</command> /dev/sde</screen>
    </step>
    <step>
     <para>
      Create the cluster-aware volume group on both disks:
     </para>
<screen>&prompt.root;<command>vgcreate</command> --clustered y clustervg /dev/sdd /dev/sde</screen>
    </step>
    <step>
     <para>
      Create logical volumes as needed:
     </para>
<screen>&prompt.root;<command>lvcreate</command> --name clusterlv --size 500M clustervg</screen>
    </step>
    <step>
     <para>
      Check the physical volume with <command>pvdisplay</command>:
     </para>
<screen>  --- Physical volume ---
      PV Name               /dev/sdd
      VG Name               clustervg
      PV Size               509,88 MB / not usable 1,88 MB
      Allocatable           yes
      PE Size (KByte)       4096
      Total PE              127
      Free PE               127
      Allocated PE          0
      PV UUID               52okH4-nv3z-2AUL-GhAN-8DAZ-GMtU-Xrn9Kh
      
      --- Physical volume ---
      PV Name               /dev/sde
      VG Name               clustervg
      PV Size               509,84 MB / not usable 1,84 MB
      Allocatable           yes
      PE Size (KByte)       4096
      Total PE              127
      Free PE               127
      Allocated PE          0
      PV UUID               Ouj3Xm-AI58-lxB1-mWm2-xn51-agM2-0UuHFC</screen>
    </step>
    <step>
     <para>
      Check the volume group with <command>vgdisplay</command>:
     </para>
<screen>  --- Volume group ---
      VG Name               clustervg
      System ID
      Format                lvm2
      Metadata Areas        2
      Metadata Sequence No  1
      VG Access             read/write
      VG Status             resizable
      Clustered             yes
      Shared                no
      MAX LV                0
      Cur LV                0
      Open LV               0
      Max PV                0
      Cur PV                2
      Act PV                2
      VG Size               1016,00 MB
      PE Size               4,00 MB
      Total PE              254
      Alloc PE / Size       0 / 0
      Free  PE / Size       254 / 1016,00 MB
      VG UUID               UCyWw8-2jqV-enuT-KH4d-NXQI-JhH3-J24anD</screen>
    </step>
   </procedure>
   <para>
    After you have created the volumes and started your resources you should
    have a new device named
    <filename>/dev/dm-<replaceable>*</replaceable></filename>.
<!--   <filename>/dev/clustervg/clusterlv</filename>-->
    It is recommended to use a clustered file system on top of your LVM
    resource, for example OCFS. For more information, see
    <xref linkend="cha.ha.ocfs2"/>.
   </para>
<!-- 
     Create the cmirrord daemon:
     This is already done with cLVM (taroth 2010-09-28: according to lmb, Xin
     Wei extended the clvm RA to always just start cmirrord as well,
     so it doesn't need to be manually configured anymore).
     If you want to create one
     manually, to the following:
     
     &prompt.crm.conf;primitive cmirrord ocf:lvm2:cmirrord
     &prompt.crm.conf;commit
    
     Not clear: Does cmirrord needs to be in the group dlm-clvm?
    
    -->
<!--<procedure id="">
    <title>Configure iSCSI Initiator (Clients)</title>
    <step>
     <para>Start &yast; and the iSCSI Client module.</para>
    </step>
    <step>
     <para>If you want to start the iSCSI target whenever your computer
      is booted, choose <guimenu>When Booting</guimenu> otherwise set
       <guimenu>Manually</guimenu>.</para>
    </step>
    <step>
     <para>Enter your <guimenu>Initiator Name</guimenu> which you have
      created in <xref linkend="st.ha.clvm.iscsi.iqn"/>.</para>
    </step>
   </procedure>-->
  </sect2>

<!--<para>
   To set up a cluster-aware volume group, several tasks must be completed
   successfully:
  </para>-->

  <sect2 xml:id="sec.ha.clvm.scenario.drbd">
   <title>Scenario: cLVM With DRBD</title>
   <para>
    The following scenarios can be used if you have data centers located in
    different parts of your city, country, or continent.
   </para>
   <procedure xml:id="pro.ha.clvm.withdrbd">
    <title>Creating a Cluster-Aware Volume Group With DRBD</title>
    <step>
     <para>
      Create a primary/primary DRBD resource:
     </para>
     <substeps performance="required">
      <step>
       <para>
        First, set up a DRBD device as primary/secondary as described in
        <xref linkend="pro.drbd.configure"/>. Make sure the disk state is
        <literal>up-to-date</literal> on both nodes. Check this with
        <command>drbdadm status</command>.
       </para>
      </step>
      <step>
       <para>
        Add the following options to your configuration file (usually
        something like <filename>/etc/drbd.d/r0.res</filename>):
       </para>
<screen>resource r0 {
  startup {
    become-primary-on both;
  }

  net {
     allow-two-primaries;
  }
  ...
}</screen>
      </step>
      <step>
       <para>
        Copy the changed configuration file to the other node, for example:
       </para>
<screen>&prompt.root;<command>scp</command> /etc/drbd.d/r0.res &wsII;:/etc/drbd.d/</screen>
      </step>
      <step>
       <para>
        Run the following commands on <emphasis>both</emphasis> nodes:
       </para>
<screen>&prompt.root;<command>drbdadm</command> disconnect r0
&prompt.root;<command>drbdadm</command> connect r0
&prompt.root;<command>drbdadm</command> primary r0</screen>
      </step>
      <step>
       <para>
        Check the status of your nodes:
    </para>
    <screen>&prompt.root;<command>drbdadm</command> status r0</screen>
<!--
<screen>&prompt.root;<command>cat</command> /proc/drbd
...
0: cs:Connected ro:Primary/Primary ds:UpToDate/UpToDate C r&#45;&#45;&#45;&#45;</screen>
-->
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Include the clvmd resource as a clone in the pacemaker configuration,
      and make it depend on the DLM clone resource. See
      <xref linkend="pro.ha.clvm.dlmresource"/> for detailed instructions.
      Before proceeding, confirm that these resources have started
      successfully on your cluster. You may use <command>crm status</command>
      or the Web interface to check the running services.
     </para>
    </step>
    <step>
     <para>
      Prepare the physical volume for LVM with the command
      <command>pvcreate</command>. For example, on the device
      <filename>/dev/drbd_r0</filename> the command would look like this:
     </para>
<screen>&prompt.root;<command>pvcreate</command> /dev/drbd_r0</screen>
    </step>
    <step>
     <para>
      Create a cluster-aware volume group:
     </para>
<screen>&prompt.root;<command>vgcreate</command> --clustered y myclusterfs /dev/drbd_r0</screen>
    </step>
    <step>
     <para>
      Create logical volumes as needed. You may probably want to change the
      size of the logical volume. For example, create a 4&nbsp;GB logical
      volume with the following command:
     </para>
<screen>&prompt.root;<command>lvcreate</command> --name testlv -L 4G myclusterfs</screen>
    </step>
<!--taroth 2010-08-23: bnc#633225#c2: moved these steps into general section 
     sec.ha.clvm.config.resources-->
<!-- <step>
     <para>
      To ensure that the volume group is activated cluster-wide, configure a
      LVM resource as follows:
        </para>
<screen>&prompt.crm.conf;<command>primitive</command> vg1 ocf:heartbeat:LVM \
        params volgrpname="myclusterfs"
<command>clone</command> vg1-clone vg1 \
        meta interleave="true" ordered="true"
<command>colocation</command> colo-vg1 inf: vg1-clone dlm-clvm-clone
<command>order</command> order-vg1 inf: dlm-clvm-clone vg1-clone</screen>
    </step>
    <step>
     <para>
      If you want the volume group to only be activated exclusively on one
      node, use the following example; in this case, cLVM will protect all
      logical volumes within the VG from being activated on multiple nodes,
      as an additional measure of protection for non-clustered applications:
       </para>
<screen><command>primitive</command> vg1 ocf:heartbeat:LVM \
        params volgrpname="myclusterfs" exclusive="yes"
<command>colocation</command> colo-vg1 inf: vg1 dlm-clvm-clone
<command>order</command> order-vg1 inf: dlm-clvm-clone vg1</screen>
    </step>-->
    <step>
     <para>
      <remark role="grammar">taroth 2011-10-24: comment by bwiedemann: as file system
      mounts or raw usage - *as* raw usage passt nicht - for?</remark>
      The logical volumes within the VG are now available as file system
      mounts or raw usage. Ensure that services using them have proper
      dependencies to collocate them with and order them after the VG has
      been activated.
     </para>
    </step>
   </procedure>
   <para>
    After finishing these configuration steps, the LVM2 configuration can be
    done like on any stand-alone workstation.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ha.clvm.drbd">
  <title>Configuring Eligible LVM2 Devices Explicitly</title>

  <para>
   When several devices seemingly share the same physical volume signature
   (as can be the case for multipath devices or DRBD), it is recommended to
   explicitly configure the devices which LVM2 scans for PVs.
  </para>

  <para>
   For example, if the command <command>vgcreate</command> uses the physical
   device instead of using the mirrored block device, DRBD will be confused
   which may result in a split brain condition for DRBD.
  </para>

  <para>
   To deactivate a single device for LVM2, do the following:
  </para>

  <procedure>
   <step>
    <para>
     Edit the file <filename>/etc/lvm/lvm.conf</filename> and search for the
     line starting with <literal>filter</literal>.
    </para>
   </step>
   <step>
    <para>
     The patterns there are handled as regular expressions. A leading
     <quote>a</quote> means to accept a device pattern to the scan, a
     leading <quote>r</quote> rejects the devices that follow the device
     pattern.
    </para>
   </step>
   <step>
    <para>
     To remove a device named <filename>/dev/sdb1</filename>, add the
     following expression to the filter rule:
    </para>
<screen>"r|^/dev/sdb1$|"</screen>
    <para>
     The complete filter line will look like the following:
    </para>
<screen>filter = [ "r|^/dev/sdb1$|", "r|/dev/.*/by-path/.*|",<!--
 --> "r|/dev/.*/by-id/.*|", "a/.*/" ]</screen>
    <para>
     A filter line, that accepts DRBD and MPIO devices but rejects all other
     devices would look like this:
    </para>
<screen>filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|",<!--
--> "r/.*/" ]</screen>
   </step>
<!--   
   <step>
    <para>
     Explicitly accept all the eligible devices, and reject all others.
     For example, a filter line which accepts all MPIO and drbd devices would
     look like this:
    </para>
    <screen>filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|", "r/.*/"]</screen>
   </step> -->
   <step>
    <para>
     Write the configuration file and copy it to all cluster nodes.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ha.clvm.more">
  <title>For More Information</title>

  <para>
   Thorough information is available from the pacemaker mailing list,
   available at
   <link xlink:href="http://www.clusterlabs.org/wiki/Help:Contents"/>.
  </para>

  <para>
   The official cLVM FAQ can be found at
   <link xlink:href="http://sources.redhat.com/cluster/wiki/FAQ/CLVM"/>.
  </para>
 </sect1>
</chapter>
