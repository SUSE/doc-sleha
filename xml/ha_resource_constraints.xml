<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="sec-ha-config-basics-constraints" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Configuring resource constraints</title>
 <info>
  <abstract>
   <para>
    Having all the resources configured is only part of the job. Even if the
    cluster knows all needed resources, it still might not be able to handle
    them correctly. Resource constraints let you specify which cluster nodes
    resources can run on, what order resources will load in, and what other
    resources a specific resource is dependent on.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-ha-config-basics-constraints-types">
   <title>Types of constraints</title>
   <para>
    There are three different kinds of constraints available:
   </para>
   <variablelist>
    <varlistentry>
     <term>Resource location
    </term>
     <listitem>
      <para>
       Location constraints define on which nodes a resource may be
       run, may not be run or is preferred to be run.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Resource colocation</term>
     <listitem>
      <para>
       Colocation constraints tell the cluster which resources may or
       may not run together on a node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Resource order</term>
     <listitem>
      <para>
       Order constraints define the sequence of actions.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <important>
    <title>Restrictions for constraints and certain types of resources</title>
    <itemizedlist>
     <listitem>
      <para>Do not create colocation constraints for <emphasis>members</emphasis> of a resource
       group. Create a colocation constraint pointing to the resource group as a whole instead. All
       other types of constraints are safe to use for members of a resource group.</para>
     </listitem>
     <listitem>
      <para>Do not use any constraints on a resource that has a clone resource or a promotable clone
       resource applied to it. The constraints must apply to the clone or promotable clone resource, not
       to the child resource.</para>
     </listitem>
    </itemizedlist>
   </important>
  </sect1>

  <sect1 xml:id="sec-ha-config-basics-constraints-scores">
   <title>Scores and infinity</title>
   <para>
    When defining constraints, you also need to deal with scores. Scores of
    all kinds are integral to how the cluster works. Practically everything
    from migrating a resource to deciding which resource to stop in a
    degraded cluster is achieved by manipulating scores in some way. Scores
    are calculated on a per-resource basis and any node with a negative
    score for a resource cannot run that resource. After calculating the
    scores for a resource, the cluster then chooses the node with the
    highest score.
   </para>
   <para>
    <literal>INFINITY</literal> is currently deﬁned as
    <literal>1,000,000</literal>. Additions or subtractions with it stick to
    the following three basic rules:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Any value + INFINITY = INFINITY
     </para>
    </listitem>
    <listitem>
     <para>
      Any value - INFINITY = -INFINITY
     </para>
    </listitem>
    <listitem>
     <para>
      INFINITY - INFINITY = -INFINITY
     </para>
    </listitem>
   </itemizedlist>
   <para>
    When defining resource constraints, you specify a score for each
    constraint. The score indicates the value you are assigning to this
    resource constraint. Constraints with higher scores are applied before
    those with lower scores. By creating additional location constraints
    with different scores for a given resource, you can specify an order for
    the nodes that a resource will fail over to.
   </para>
  </sect1>

  <sect1 xml:id="sec-ha-config-basics-constraints-templates">
   <title>Resource templates and constraints</title>
   <para>
    If you have defined a resource template (see
    <xref linkend="sec-ha-config-basics-resources-templates"/>), it can be
    referenced in the following types of constraints:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      order constraints
     </para>
    </listitem>
    <listitem>
     <para>
      colocation constraints
     </para>
    </listitem>
    <listitem>
     <para>
      rsc_ticket constraints (for &geo; clusters)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    However, colocation constraints must not contain more than one reference
    to a template. Resource sets must not contain a reference to a template.
   </para>
   <para>
    Resource templates referenced in constraints stand for all primitives
    which are derived from that template. This means, the constraint applies
    to all primitive resources referencing the resource template.
    Referencing resource templates in constraints is an alternative to
    resource sets and can simplify the cluster configuration considerably.
    For details about resource sets, refer to
    <xref linkend="sec-ha-config-basics-constraints-rscset"/>.
   </para>
  </sect1>

 <sect1 xml:id="sec-ha-conf-add-location-constraints">
  <title>Adding location constraints</title>
  <para>
   A location constraint determines on which node a resource may be run, is
   preferably run, or may not be run. An example of a location constraint is to
   place all resources related to a certain database on the same node. This type
   of constraint may be added multiple times for each resource. All location
   constraints are evaluated for a given resource.
  </para>
  <para>
   You can add location constraints using either &hawk2; or &crmsh;.
  </para>
  <sect2 xml:id="sec-conf-hawk2-cons-loc">
   <title>Adding location constraints with &hawk2;</title>
   <procedure xml:id="pro-hawk2-constraints-location">
    <title>Adding a location constraint</title>
    <step>
     <para>
      Log in to &hawk2;:
     </para>
 <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
    </step>
    <step>
     <para>
      From the left navigation bar, select <menuchoice>
      <guimenu>Configuration</guimenu>  <guimenu>Add Constraint</guimenu>
      <guimenu>Location</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Enter a unique <guimenu>Constraint ID</guimenu>.
     </para>
    </step>
    <step xml:id="step-hawk2-loc-rsc">
     <para>
      From the list of <guimenu>Resources</guimenu> select the resource or
      resources for which to define the constraint.
     </para>
    </step>
    <step>
     <para>
      Enter a <guimenu>Score</guimenu>. The score indicates the value you are
      assigning to this resource constraint. Positive values indicate the
      resource can run on the <guimenu>Node</guimenu> you specify in the next
      step. Negative values mean it should not run on that node. Constraints
      with higher scores are applied before those with lower scores.
     </para>
     <para>
      Some often-used values can also be set via the drop-down box:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To force the resources to run on the node, click the arrow
        icon and select <literal>Always</literal>. This sets the score to
        <literal>INFINITY</literal>.
       </para>
      </listitem>
      <listitem>
       <para>
        If you never want the resources to run on the node, click the arrow icon
        and select <literal>Never</literal>. This sets the score to
        <literal>-INFINITY</literal>, meaning that the resources must not run on
        the node.
       </para>
      </listitem>
      <listitem>
       <para>
        To set the score to <literal>0</literal>, click the arrow icon and
        select <literal>Advisory</literal>. This disables the constraint. This
        is useful when you want to set resource discovery but do not want to
        constrain the resources.
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      Select a <guimenu>Node</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Create</guimenu> to finish the configuration. A message at
      the top of the screen shows if the action has been successful.
     </para>
    </step>
   </procedure>
   <figure pgwide="0">
    <title>&hawk2;&mdash;location constraint</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="hawk2-loc-constraint.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="hawk2-loc-constraint.png" width="100%"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

   <sect2 xml:id="sec-ha-manual-config-constraints-locational">
    <title>Adding location constraints with &crmsh;</title>
    <para>
     The <command>location</command> command defines on which nodes a resource
     may be run, may not be run or is preferred to be run.
    </para>
    <para>
     A simple example that expresses a preference to run the
     resource <literal>fs1</literal> on the node with the name
     <systemitem class="server">&node1;</systemitem> to 100 would be the
     following:
    </para>
<screen>&prompt.crm.conf;<command>location loc-fs1 fs1 100: &node1;</command></screen>
    <para>
     Another example is a location with ping:
    </para>
<screen>&prompt.crm.conf;<command>primitive ping ping \
    params name=ping dampen=5s multiplier=100 host_list="r1 r2"</command>
&prompt.crm.conf;<command>clone cl-ping ping meta interleave=true</command>
&prompt.crm.conf;<command>location loc-node_pref internal_www \
    rule 50: #uname eq &node1; \
    rule ping: defined ping</command></screen>
    <para>
     The parameter <parameter>host_list</parameter> is a space-separated list
     of hosts to ping and count.
     Another use case for location constraints are grouping primitives as a
     <emphasis>resource set</emphasis>. This can be useful if several
     resources depend on, for example, a ping attribute for network
     connectivity. In former times, the <literal>-inf/ping</literal> rules
     needed to be duplicated several times in the configuration, making it
     unnecessarily complex.
    </para>
    <para>
     The following example creates a resource set
     <varname>loc-&node1;</varname>, referencing the virtual IP addresses
     <varname>vip1</varname> and <varname>vip2</varname>:
    </para>
<screen>&prompt.crm.conf;<command>primitive vip1 IPaddr2 params ip=&subnetI;.5</command>
&prompt.crm.conf;<command>primitive vip2 IPaddr2 params ip=&subnetI;.6</command>
&prompt.crm.conf;<command>location loc-&node1; { vip1 vip2 } inf: &node1;</command></screen>
    <para>
     In some cases it is much more efficient and convenient to use resource
     patterns for your <command>location</command> command. A resource
     pattern is a regular expression between two slashes. For example, the
     above virtual IP addresses can be all matched with the following:
    </para>
<screen>&prompt.crm.conf;<command>location loc-&node1; /vip.*/ inf: &node1;</command></screen>
   </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-conf-add-colocation-constraints">
  <title>Adding colocation constraints</title>
  <para>
   A colocation constraint tells the cluster which resources may or may not
   run together on a node. As a colocation constraint defines a dependency
   between resources, you need at least two resources to create a colocation
   constraint.
  </para>
  <para>
   You can add colocation constraints using either &hawk2; or &crmsh;.
  </para>
  <sect2 xml:id="sec-conf-hawk2-cons-col">
   <title>Adding colocation constraints with &hawk2;</title>
   <procedure xml:id="pro-hawk2-constraints-colocation">
    <title>Adding colocation constraints</title>
    <step>
     <para>
      Log in to &hawk2;:
     </para>
 <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
    </step>
    <step>
     <para>
      From the left navigation bar, select <menuchoice> <guimenu>Configuration</guimenu>
      <guimenu>Add Constraint</guimenu> <guimenu>Colocation</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Enter a unique <guimenu>Constraint ID</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter a <guimenu>Score</guimenu>. The score determines the location
      relationship between the resources. Positive values indicate that the
      resources should run on the same node. Negative values indicate that the
      resources should not run on the same node. The score will be combined with
      other factors to decide where to put the resource.
     </para>
     <para>
      Some often-used values can also be set via the drop-down box:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To force the resources to run on the same node, click the
        arrow icon and select <literal>Always</literal>. This sets the score to
        <literal>INFINITY</literal>.
       </para>
      </listitem>
      <listitem>
       <para>
        If you never want the resources to run on the same node, click the arrow
        icon and select <literal>Never</literal>. This sets the score to
        <literal>-INFINITY</literal>, meaning that the resources must not run on
        the same node.
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      To define the resources for the constraint:
     </para>
     <substeps performance="required">
      <step xml:id="step-hawk2-col-rsc">
       <para>
        From the drop-down box in the <guimenu>Resources</guimenu> category,
        select a resource (or a template).
       </para>
       <para>
        The resource is added and a new empty drop-down box appears beneath.
       </para>
      </step>
      <step>
       <para>
        Repeat this step to add more resources.
       </para>
       <para>
        As the topmost resource depends on the next resource and so on, the
        cluster will first decide where to put the last resource, then place the
        depending ones based on that decision. If the constraint cannot be
        satisﬁed, the cluster may not allow the dependent resource to run.
       </para>
      </step>
      <step>
       <para>
        To swap the order of resources within the colocation constraint, click
        the arrow up icon next to a resource to swap it with the entry above.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      If needed, specify further parameters for each resource (such as
      <literal>Started</literal>, <literal>Stopped</literal>,
      <literal>Promote</literal>, <literal>Demote</literal>): Click the empty
      drop-down box next to the resource and select the desired entry.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Create</guimenu> to finish the configuration. A message at
      the top of the screen shows if the action has been successful.
     </para>
    </step>
   </procedure>
   <figure pgwide="0">
    <title>&hawk2;&mdash;colocation constraint</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="hawk2-col-constraint.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="hawk2-col-constraint.png" width="100%"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

   <sect2 xml:id="sec-ha-manual-config-constraints-collocational">
    <title>Adding colocation constraints with &crmsh;</title>
    <para>
     The <command>colocation</command> command is used to define what
     resources should run on the same or on different hosts.
    </para>
    <para>
     You can set a score of either +inf or -inf, defining
     resources that must always or must never run on the same node.
     You can also use non-infinite scores. In that case the colocation
     is called <emphasis>advisory</emphasis> and the cluster may decide not
     to follow them in favor of not stopping other resources if there is a
     conflict.
    </para>
    <para>
     For example, to always run the resources <literal>resource1</literal>
     and <literal>resource2</literal> on the same host, use the following constraint:
    </para>
<screen>&prompt.crm.conf;<command>colocation coloc-2resource inf: resource1 resource2</command></screen>
    <para>
     For a primary/secondary configuration, it is necessary to know if the
     current node is a primary in addition to running the resource locally.
    </para>
   </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-conf-add-order-constraints">
  <title>Adding order constraints</title>
  <para>
   Order constraints can be used to start or stop a service right before or
   after a different resource meets a special condition, such as being started,
   stopped, or promoted to primary. For example, you cannot mount a file system
   before the device is available to a system. As an order constraint defines
   a dependency between resources, you need at least two resources to create
   an order constraint.
  </para>
  <para>
   You can add order constraints using either &hawk2; or &crmsh;.
  </para>
  <sect2 xml:id="sec-conf-hawk2-cons-order">
   <title>Adding order constraints with &hawk2;</title>
   <procedure xml:id="pro-hawk2-constraints-order">
    <title>Adding an order constraint</title>
    <step>
     <para>
      Log in to &hawk2;:
     </para>
 <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
    </step>
    <step>
     <para>
      From the left navigation bar, select <menuchoice> <guimenu>Configuration</guimenu>
      <guimenu>Add Constraint</guimenu> <guimenu>Order</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Enter a unique <guimenu>Constraint ID</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter a <guimenu>Score</guimenu>. If the score is greater than zero, the
      order constraint is mandatory, otherwise it is optional.
     </para>
     <para>
      Some often-used values can also be set via the drop-down box:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To make the order constraint mandatory, click the arrow icon
        and select <literal>Mandatory</literal>.
       </para>
      </listitem>
      <listitem>
       <para>
        To make the order constraint a suggestion only, click the
        arrow icon and select <literal>Optional</literal>.
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>Serialize</literal>: To ensure that no two stop/start actions
        occur concurrently for the resources, click the arrow icon and select
        <literal>Serialize</literal>. This makes sure that one resource must
        complete starting before the other can be started. A typical use case
        are resources that put a high load on the host during start-up.
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      For order constraints, you can usually keep the option
      <guimenu>Symmetrical</guimenu> enabled. This specifies that resources are
      stopped in reverse order.
     </para>
    </step>
    <step>
     <para>
      To define the resources for the constraint:
     </para>
     <substeps performance="required">
      <step xml:id="step-hawk2-order-rsc">
       <para>
        From the drop-down box in the <guimenu>Resources</guimenu> category,
        select a resource (or a template).
       </para>
       <para>
        The resource is added and a new empty drop-down box appears beneath.
       </para>
      </step>
      <step>
       <para>
        Repeat this step to add more resources.
       </para>
       <para>
        The topmost resource will start first, then the second, etc. Usually the
        resources will be stopped in reverse order.
       </para>
      </step>
      <step>
       <para>
        To swap the order of resources within the order constraint, click the
        arrow up icon next to a resource to swap it with the entry above.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      If needed, specify further parameters for each resource (like
      <literal>Started</literal>, <literal>Stopped</literal>,
      <literal>Promote</literal>, <literal>Demote</literal>): Click the empty
      drop-down box next to the resource and select the desired entry.
     </para>
    </step>
    <step>
     <para>
      Confirm your changes to finish the configuration. A message at the top of
      the screen shows if the action has been successful.
     </para>
    </step>
   </procedure>
   <figure pgwide="0">
    <title>&hawk2;&mdash;order constraint</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="hawk2-order-constraint.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="hawk2-order-constraint.png" width="100%"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

   <sect2 xml:id="sec-ha-manual-config-constraints-ordering">
    <title>Adding order constraints using &crmsh;</title>
    <para>
     The <command>order</command> command defines a sequence of action.
    </para>
    <para>
     For example, to always start <literal>resource1</literal> before
     <literal>resource2</literal>, use the following constraint:
    </para>
<screen>&prompt.crm.conf;<command>order res1_before_res2 Mandatory: resource1 resource2</command></screen>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-config-basics-constraints-rscset">
   <title>Using resource sets to define constraints</title>
   <para>
     As an alternative format for defining location, colocation, or order
     constraints, you can use <emphasis>resource sets</emphasis>, where
     primitives are grouped together in one set. Previously this was
     possible either by defining a resource group (which could not always
     accurately express the design), or by defining each relationship as an
     individual constraint. The latter caused a constraint explosion as the
     number of resources and combinations grew. The configuration via
     resource sets is not necessarily less verbose, but is easier to
     understand and maintain.
   </para>
   <para>
    You can configure resource sets using either &hawk2; or &crmsh;.
   </para>

   <sect2 xml:id="sec-conf-hawk2-cons-set">
    <title>Using resource sets to define constraints with &hawk2;</title>
    <procedure xml:id="pro-hawk2-constraints-sets">
     <title>Using a resource set for constraints</title>
     <step>
      <para>
       To use a resource set within a location constraint:
      </para>
      <substeps performance="required">
       <step>
        <para>
         Proceed as outlined in <xref linkend="pro-hawk2-constraints-location"/>,
         apart from
         <xref linkend="step-hawk2-loc-rsc" xrefstyle="select:label"/>: Instead
         of selecting a single resource, select multiple resources by pressing
         <keycap function="control"/> or <keycap function="shift"/> and mouse
         click. This creates a resource set within the location constraint.
        </para>
       </step>
       <step>
        <para>
         To remove a resource from the location constraint, press
         <keycap function="control"/> and click the resource again to deselect
         it.
        </para>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       To use a resource set within a colocation or order constraint:
      </para>
      <substeps performance="required">
       <step>
        <para>
         Proceed as described in
         <xref linkend="pro-hawk2-constraints-colocation"/> or
         <xref
          linkend="pro-hawk2-constraints-order"/>, apart from the
         step where you define the resources for the constraint
         (<xref linkend="step-hawk2-col-rsc" xrefstyle="select:label"/> or
         <xref
          linkend="step-hawk2-order-rsc"/>):
        </para>
       </step>
       <step>
        <para>
         Add multiple resources.
        </para>
       </step>
       <step>
        <para>
         To create a resource set, click the chain icon next to a resource to
         link it to the resource above. A resource set is visualized by a frame
         around the resources belonging to a set.
        </para>
       </step>
       <step>
        <para>
         You can combine multiple resources in a resource set or create multiple
         resource sets.
        </para>
        <figure>
         <title>&hawk2;&mdash;two resource sets in a colocation constraint</title>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="hawk2-constraint-set.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="hawk2-constraint-set.png" width="100%"/>
          </imageobject>
         </mediaobject>
        </figure>
       </step>
       <step>
        <para>
         To unlink a resource from the resource above, click the scissors icon
         next to the resource.
        </para>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Confirm your changes to finish the constraint configuration.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 xml:id="sec-ha-config-basics-constraints-rscset-constraints">
    <title>Using resource sets to define constraints with &crmsh;</title>
    <example xml:id="ex-config-basic-resourceset-loc">
     <title>A resource set for location constraints</title>
     <para>
      For example, you can use the following configuration of a resource
      set (<varname>loc-&node1;</varname>) in the &crmsh; to place
      two virtual IPs (<varname>vip1</varname> and <varname>vip2</varname>)
      on the same node, <varname>&node1;</varname>:
     </para>
<screen>&prompt.crm.conf;<command>primitive vip1 IPaddr2 params ip=&subnetI;.5</command>
&prompt.crm.conf;<command>primitive vip2 IPaddr2 params ip=&subnetI;.6</command>
&prompt.crm.conf;<command>location loc-&node1; { vip1 vip2 } inf: &node1;</command></screen>
    </example>
    <para>
     If you want to use resource sets to replace a configuration of
     colocation constraints, consider the following two examples:
    </para>
    <example>
     <title>A chain of colocated resources</title>
<screen>&lt;constraints&gt;
    &lt;rsc_colocation id="coloc-1" rsc="B" with-rsc="A" score="INFINITY"/&gt;
    &lt;rsc_colocation id="coloc-2" rsc="C" with-rsc="B" score="INFINITY"/&gt;
    &lt;rsc_colocation id="coloc-3" rsc="D" with-rsc="C" score="INFINITY"/&gt;
&lt;/constraints&gt;</screen>
    </example>
    <para>
     The same configuration expressed by a resource set:
    </para>
<screen>&lt;constraints&gt;
   &lt;rsc_colocation id="coloc-1" score="INFINITY" &gt;
    &lt;resource_set id="colocated-set-example" sequential="true"&gt;
     &lt;resource_ref id="A"/&gt;
     &lt;resource_ref id="B"/&gt;
     &lt;resource_ref id="C"/&gt;
     &lt;resource_ref id="D"/&gt;
    &lt;/resource_set&gt;
   &lt;/rsc_colocation&gt;
&lt;/constraints&gt;</screen>
    <para>
     If you want to use resource sets to replace a configuration of
     order constraints, consider the following two examples:
    </para>
    <example>
     <title>A chain of ordered resources</title>
<screen>&lt;constraints&gt;
    &lt;rsc_order id="order-1" first="A" then="B" /&gt;
    &lt;rsc_order id="order-2" first="B" then="C" /&gt;
    &lt;rsc_order id="order-3" first="C" then="D" /&gt;
&lt;/constraints&gt;</screen>
    </example>
    <para>
     The same purpose can be achieved by using a resource set with ordered
     resources:
    </para>
    <example>
     <title>A chain of ordered resources expressed as resource set</title>
<screen>&lt;constraints&gt;
    &lt;rsc_order id="order-1"&gt;
    &lt;resource_set id="ordered-set-example" sequential="true"&gt;
    &lt;resource_ref id="A"/&gt;
    &lt;resource_ref id="B"/&gt;
    &lt;resource_ref id="C"/&gt;
    &lt;resource_ref id="D"/&gt;
    &lt;/resource_set&gt;
    &lt;/rsc_order&gt;
&lt;/constraints&gt;</screen>
    </example>
    <para>
     Sets can be either ordered (<literal>sequential=true</literal>) or
     unordered (<literal>sequential=false</literal>). Furthermore, the
     <literal>require-all</literal> attribute can be used to switch between
     <literal>AND</literal> and <literal>OR</literal> logic.
    </para>
   </sect2>

   <sect2 xml:id="sec-ha-manual-config-constraints-weak-bond">
    <title>Collocating sets for resources without dependency</title>
    <para>
     Sometimes it is useful to place a group of resources on the same node
     (defining a colocation constraint), but without having hard
     dependencies between the resources. For example, you want two
     resources to be placed on the same node, but you do <emphasis>not</emphasis>
     want the cluster to restart the other one if one of them fails.
    </para>
    <para>
     This can be achieved on the &crmshell; by using the <command>weak-bond</command> command:
    </para>
<screen>&prompt.root;<command>crm configure assist weak-bond <replaceable>resource1</replaceable> <replaceable>resource2</replaceable></command></screen>
    <para>
     The <command>weak-bond</command> command creates a dummy resource and
     a colocation constraint with the given resources automatically.
    </para>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-config-basics-failover">
   <title>Specifying resource failover nodes</title>
   <para>
    A resource will be automatically restarted if it fails. If that cannot
    be achieved on the current node, or it fails <literal>N</literal> times
    on the current node, it tries to fail over to another node. Each time
    the resource fails, its fail count is raised. You can define several
    failures for resources (a <literal>migration-threshold</literal>), after
    which they will migrate to a new node. If you have more than two nodes
    in your cluster, the node a particular resource fails over to is chosen
    by the &ha; software.
   </para>
   <para>
    However, you can specify the node a resource will fail over to by
    configuring one or several location constraints and a
    <literal>migration-threshold</literal> for that resource.
   </para>
   <para>
    You can specify resource failover nodes using either &hawk2; or &crmsh;.
   </para>
   <example xml:id="ex-ha-config-basics-failover">
    <title>Migration threshold&mdash;process flow</title>
    <para>
     For example, let us assume you have configured a location constraint
     for resource <literal>rsc1</literal> to preferably run on
     <literal>&node1;</literal>. If it fails there,
     <literal>migration-threshold</literal> is checked and compared to the
     fail count. If <literal>failcount</literal> &gt;= migration-threshold, then the resource is
     migrated to the node with the next best preference.
    </para>
    <para>
     After the threshold has been reached, the node will no longer be
     allowed to run the failed resource until the resource's fail count is
     reset. This can be done manually by the cluster administrator or by
     setting a <literal>failure-timeout</literal> option for the resource.
    </para>
    <para>
     For example, a setting of <literal>migration-threshold=2</literal> and
     <literal>failure-timeout=60s</literal> would cause the resource to
     migrate to a new node after two failures. It would be allowed to move
     back (depending on the stickiness and constraint scores) after one
     minute.
    </para>
   </example>
   <para>
    There are two exceptions to the migration threshold concept, occurring
    when a resource either fails to start or fails to stop:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Start failures set the fail count to <literal>INFINITY</literal> and
      thus always cause an immediate migration.
     </para>
    </listitem>
    <listitem>
     <para>
      Stop failures cause fencing (when <literal>stonith-enabled</literal>
      is set to <literal>true</literal> which is the default).
     </para>
     <para>
      In case there is no STONITH resource defined (or
      <literal>stonith-enabled</literal> is set to
      <literal>false</literal>), the resource will not migrate.
     </para>
    </listitem>
   </itemizedlist>

   <sect2 xml:id="sec-conf-hawk2-failover">
    <title>Specifying resource failover nodes with &hawk2;</title>
    <procedure xml:id="pro-hawk2-failover">
     <title>Specifying a failover node</title>
     <step>
      <para>
       Log in to &hawk2;:
      </para>
  <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
     </step>
     <step>
      <para>
       Configure a location constraint for the resource as described in
       <xref linkend="pro-hawk2-constraints-location"/>.
      </para>
     </step>
     <step>
      <para>
       Add the <literal>migration-threshold</literal> meta attribute to the
       resource as described in
       <xref linkend="pro-conf-hawk2-rsc-modify" xrefstyle="select:label title nopage"/>,
       <xref linkend="step-hawk2-rsc-modify-params" xrefstyle="select:label"/>
       and enter a value for the migration-threshold. The value should be
       positive and less than INFINITY.
      </para>
     </step>
     <step>
      <para>
       If you want to automatically expire the fail count for a resource, add the
       <literal>failure-timeout</literal> meta attribute to the resource as
       described in
       <xref linkend="pro-conf-hawk2-primitive-add" xrefstyle="select:label title nopage"/>,
       <xref linkend="step-hawk2-rsc-modify-params" xrefstyle="select:label"/>
       and enter a <guimenu>Value</guimenu> for the
       <literal>failure-timeout</literal>.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk2-failover-node.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk2-failover-node.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       If you want to specify additional failover nodes with preferences for a
       resource, create additional location constraints.
      </para>
     </step>
    </procedure>
    <para>
     Instead of letting the fail count for a resource expire automatically, you
     can also clean up fail counts for a resource manually at any time. Refer to
     <xref linkend="sec-conf-hawk2-manage-cleanup"/> for details.
    </para>
   </sect2>

   <sect2 xml:id="sec-ha-manual-config-failover">
    <title>Specifying resource failover nodes with &crmsh;</title>
    <para>
     To determine a resource failover, use the meta attribute
     <literal>migration-threshold</literal>. If the fail count exceeds
     <literal>migration-threshold</literal> on all nodes, the resource
     will remain stopped. For example:
    </para>
 <screen>&prompt.crm.conf;<command>location rsc1-&node1; rsc1 100: &node1;</command></screen>
    <para>
     Normally, <literal>rsc1</literal> prefers to run on <literal>&node1;</literal>.
     If it fails there, <literal>migration-threshold</literal> is checked and compared
     to the fail count. If <literal>failcount</literal> &gt;= <literal>migration-threshold</literal>,
     then the resource is migrated to the node with the next best preference.
    </para>
    <para>
     Start failures set the fail count to inf depend on the
     <option>start-failure-is-fatal</option> option. Stop failures cause
     fencing. If there is no STONITH defined, the resource will not migrate.
    </para>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-config-basics-failback">
   <title>Specifying resource failback nodes (resource stickiness)</title>
 &failback-nodes;
   <para>
    Consider the following implications when specifying resource stickiness
    values:
   </para>
   <variablelist>
    <varlistentry>
     <term>Value is <literal>0</literal>:</term>
     <listitem>
      <para>
       The resource is placed optimally in the
       system. This may mean that it is moved when a <quote>better</quote>
       or less loaded node becomes available. The option is almost
       equivalent to automatic failback, except that the resource may be
       moved to a node that is not the one it was previously active on.
       This is the &pace; default.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Value is greater than <literal>0</literal>:</term>
     <listitem>
      <para>
       The resource will prefer to remain in its current location, but may
       be moved if a more suitable node is available. Higher values indicate
       a stronger preference for a resource to stay where it is.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Value is less than <literal>0</literal>:</term>
     <listitem>
      <para>
       The resource prefers to move away from its current location. Higher
       absolute values indicate a stronger preference for a resource to be
       moved.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Value is <literal>INFINITY</literal>:</term>
     <listitem>
      <para>
       The resource will always remain in its current location unless forced
       off because the node is no longer eligible to run the resource (node
       shutdown, node standby, reaching the
       <literal>migration-threshold</literal>, or configuration change).
       This option is almost equivalent to completely disabling automatic
       failback.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Value is <literal>-INFINITY</literal>:</term>
     <listitem>
      <para>
       The resource will always move away from its current location.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <sect2 xml:id="sec-config-hawk2-failback">
    <title>Specifying resource failback nodes with &hawk2;</title>
    <procedure xml:id="pro-hawk2-stickiness">
     <title>Specifying resource stickiness</title>
     <step>
      <para>
       Log in to &hawk2;:
      </para>
  <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
     </step>
     <step>
      <para>
       Add the <literal>resource-stickiness</literal> meta attribute to the
       resource as described in
       <xref linkend="pro-conf-hawk2-rsc-modify" xrefstyle="select:label title nopage"
        />,
       <xref linkend="step-hawk2-rsc-modify-params" xrefstyle="select:label"/>.
      </para>
     </step>
     <step>
      <para>
       Specify a value between <literal>-INFINITY</literal> and
       <literal>INFINITY</literal> for <literal>resource-stickiness</literal>.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk2-rsc-stickiness.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk2-rsc-stickiness.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
    </procedure>
   </sect2>

   <!--sect2 xml:id="sec-ha-manual-config-failback">
    <title>Specifying resource failback nodes with &crmsh;</title>
     <remark>trichardson 2023-01-18: There were no actual steps here, only a
      duplicated description. Let's add crmsh steps at some point.</remark>
   </sect2-->
  </sect1>

  <sect1 xml:id="sec-ha-config-basics-utilization">
   <title>Placing resources based on their load impact</title>
   <para>
    Not all resources are equal. Some, such as &xen; guests, require that
    the node hosting them meets their capacity requirements. If resources
    are placed such that their combined need exceed the provided capacity,
    the resources diminish in performance (or even fail).
   </para>
   <para>
    To take this into account, the &hasi; allows you to specify the
    following parameters:
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      The capacity a certain node <emphasis>provides</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      The capacity a certain resource <emphasis>requires</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      An overall strategy for placement of resources.
     </para>
    </listitem>
   </orderedlist>
   <para>
    You can configure these setting using either &hawk2; or &crmsh;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &hawk2;: <xref linkend="sec-config-hawk2-utilization"/>
     </para>
    </listitem>
    <listitem>
     <para>
      &crmsh;: <xref linkend="sec-ha-manual-config-utilization"/>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    A node is considered eligible for a resource if it has sufficient free
    capacity to satisfy the resource's requirements. To manually configure the
    resource's requirements and the capacity a
    node provides, use utilization attributes. You can name the utilization
    attributes according to your preferences and define as many name/value
    pairs as your configuration needs. However, the attribute's values must
    be integers.
   </para>
   <para>
    If multiple resources with utilization attributes are grouped or have
    colocation constraints, the &hasi; takes that into account. If
    possible, the resources will be placed on a node that can fulfill
    <emphasis>all</emphasis> capacity requirements.
   </para>
   <note>
    <title>Utilization attributes for groups</title>
    <para>
     It is impossible to set utilization attributes directly for a resource
     group. However, to simplify the configuration for a group, you can add
     a utilization attribute with the total capacity needed to any of the
     resources in the group.
    </para>
   </note>
   <para>
    The &hasi; also provides means to detect and configure both node
    capacity and resource requirements automatically:
   </para>
   <para>
    The <systemitem>NodeUtilization</systemitem> resource agent checks the
    capacity of a node (regarding CPU and RAM).
    To configure automatic detection, create a clone resource of the
    following class, provider, and type:
    <literal>ocf:pacemaker:NodeUtilization</literal>. One instance of the
    clone should be running on each node. After the instance has started, a
    utilization section will be added to the node's configuration in CIB.
    For more information, see <command>crm ra info NodeUtilization</command>.
   </para>
   <para>
    For automatic detection of a resource's minimal requirements (regarding
    RAM and CPU) the <systemitem>Xen</systemitem> resource agent has been
    improved. Upon start of a <systemitem>Xen</systemitem> resource, it will
    reflect the consumption of RAM and CPU. Utilization attributes will
    automatically be added to the resource configuration.
   </para>
   <note>
    <title>Different resource agents for &xen; and libvirt</title>
    <para>
     The <systemitem>ocf:heartbeat:Xen</systemitem> resource agent should not be
     used with <literal>libvirt</literal>, as <literal>libvirt</literal> expects
     to be able to modify the machine description file.
    </para>
    <para>
     For <literal>libvirt</literal>, use the
     <systemitem>ocf:heartbeat:VirtualDomain</systemitem> resource agent.
    </para>
   </note>
   <para>
    Apart from detecting the minimal requirements, you can also
    monitor the current utilization via the
    <systemitem>VirtualDomain</systemitem> resource agent. It detects CPU
    and RAM use of the virtual machine. To use this feature, configure a
    resource of the following class, provider and type:
    <literal>ocf:heartbeat:VirtualDomain</literal>. The following instance
    attributes are available:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <varname>autoset_utilization_cpu</varname>
     </para>
    </listitem>
    <listitem>
     <para>
      <varname>autoset_utilization_hv_memory</varname> (for Xen) or
      <varname>autoset_utilization_host_memory</varname> (for KVM)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    These attributes default to <literal>true</literal>. This updates the utilization
    values in the CIB during each monitoring cycle. For more information, see
    <command>crm ra info VirtualDomain</command>.
   </para>
   <note>
    <title><literal>hv_memory</literal> and <literal>host_memory</literal></title>
    <para>
     In the <systemitem>NodeUtilization</systemitem> and <systemitem>VirtualDomain</systemitem>
     resource agents, <literal>hv_memory</literal> and <literal>host_memory</literal>
     both default to <literal>true</literal>. However, Xen only requires
     <literal>hv_memory</literal>, and KVM only requires <literal>host_memory</literal>.
     To avoid confusion, we recommend disabling the unnecessary attributes. For example:
    </para>
    <example xml:id="ex-kvm-disable-hv-memory">
     <title>Creating resource agents for KVM with <literal>hv_memory</literal> disabled</title>
<screen>&prompt.root;<command>crm configure primitive p_nu NodeUtilization \
      params utilization_hv_memory=false \
      op monitor timeout=20s interval=60</command>
&prompt.root;<command>crm configure primitive p_vm VirtualDomain \
      params autoset_utilization_hv_memory=false \
      op monitor timeout=30s interval=10s</command></screen>
    </example>
    <example xml:id="ex-xen-disable-host-memory">
     <title>Creating resource agents for Xen with <literal>host_memory</literal> disabled</title>
<screen>&prompt.root;<command>crm configure primitive p_nu NodeUtilization \
      params utilization_host_memory=false \
      op monitor timeout=20s interval=60</command>
&prompt.root;<command>crm configure primitive p_vm VirtualDomain \
      params autoset_utilization_host_memory=false \
      op monitor timeout=30s interval=10s</command></screen>
    </example>
   </note>
   <para>
    Independent of manually or automatically configuring capacity and
    requirements, the placement strategy must be specified with the
    <literal>placement-strategy</literal> property (in the global cluster
    options). The following values are available:
   </para>
  &placement-strategy-values;

   <sect2 xml:id="sec-config-hawk2-utilization">
    <title>Placing resources based on their load impact with &hawk2;</title>
    <para>
     Utilization attributes are used to configure both the resource's
     requirements and the capacity a node provides. You first need to configure a
     node's capacity before you can configure the capacity a resource requires.
    </para>
    <procedure xml:id="pro-hawk2-utilization-node">
     <title>Configuring the capacity a node provides</title>
     <step>
      <para>
       Log in to &hawk2;:
      </para>
  <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
     </step>
     <step>
      <para>
       From the left navigation bar, select <menuchoice><guimenu>Monitoring</guimenu>
       <guimenu>Status</guimenu></menuchoice>.
      </para>
     </step>
     <step>
      <para>
       On the <guimenu>Nodes</guimenu> tab, select the node whose capacity you
       want to configure.
      </para>
     </step>
     <step>
      <para>
       In the <guimenu>Operations</guimenu> column, click the arrow down icon and
       select <guimenu>Edit</guimenu>.
      </para>
      <para>
       The <guimenu>Edit Node</guimenu> screen opens.
      </para>
     </step>
     <step>
      <para>
       Below <guimenu>Utilization</guimenu>, enter a name for a utilization
       attribute into the empty drop-down box.
      </para>
      <para>
       The name can be arbitrary (for example, <literal>RAM_in_GB</literal>).
      </para>
     </step>
     <step>
      <para>
       Click the <guimenu>Add</guimenu> icon to add the attribute.
      </para>
     </step>
     <step>
      <para>
       In the empty text box next to the attribute, enter an attribute value. The
       value must be an integer.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk2-utilization-node.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk2-utilization-node.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       Add as many utilization attributes as you need and add values for all of
       them.
      </para>
     </step>
     <step>
      <para>
       Confirm your changes. A message at the top of the screen shows if the
       action has been successful.
      </para>
     </step>
    </procedure>
    <procedure xml:id="pro-hawk2-utilization-rsc">
     <title>Configuring the capacity a resource requires</title>
     <para>
      Configure the capacity a certain resource requires from a node either when
      creating a primitive resource or when editing an existing primitive resource.
     </para>
     <para>
      Before you can add utilization attributes to a resource, you need to have
      set utilization attributes for your cluster nodes as described in
      <xref linkend="pro-hawk2-utilization-node"
      xrefstyle="select:label"/>.
     </para>
     <step>
      <para>
       Log in to &hawk2;:
      </para>
  <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
     </step>
     <step>
      <para>
       To add a utilization attribute to an existing resource: Go to <menuchoice>
       <guimenu>Manage</guimenu><guimenu>Status</guimenu> </menuchoice> and open
       the resource configuration dialog as described in
       <xref linkend="sec-conf-hawk2-manage-edit"/>.
      </para>
      <para>
       If you create a new resource: Go to <menuchoice>
       <guimenu>Configuration</guimenu><guimenu>Add Resource</guimenu>
       </menuchoice> and proceed as described in
       <xref linkend="sec-conf-hawk2-rsc-primitive"/>.
      </para>
     </step>
     <step>
      <para>
       In the resource configuration dialog, go to the
       <guimenu>Utilization</guimenu> category.
      </para>
     </step>
     <step>
      <para>
       From the empty drop-down box, select one of the utilization attributes
       that you have configured for the nodes in
       <xref linkend="pro-hawk2-utilization-node"
       xrefstyle="select:label"/>.
      </para>
     </step>
     <step>
      <para>
       In the empty text box next to the attribute, enter an attribute value. The
       value must be an integer.
      </para>
     </step>
     <step>
      <para>
       Add as many utilization attributes as you need and add values for all of
       them.
      </para>
     </step>
     <step>
      <para>
       Confirm your changes. A message at the top of the screen shows if the
       action has been successful.
      </para>
     </step>
    </procedure>
    <para>
     After you have configured the capacities your nodes provide and the
     capacities your resources require, set the placement strategy in
     the global cluster options. Otherwise the capacity configurations have no
     effect. Several strategies are available to schedule the load: for example,
     you can concentrate it on as few nodes as possible, or balance it evenly
     over all available nodes.
    </para>
    <procedure xml:id="pro-ha-config-hawk2-placement">
     <title>Setting the placement strategy</title>
     <step>
      <para>
       Log in to &hawk2;:
      </para>
  <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
     </step>
     <step>
      <para>
       From the left navigation bar, select <menuchoice>
       <guimenu>Configuration</guimenu><guimenu>Cluster Configuration</guimenu>
       </menuchoice> to open the respective screen. It shows global
       cluster options and resource and operation defaults.
      </para>
     </step>
     <step>
      <para>
       From the empty drop-down box in the upper part of the screen, select
       <literal>placement-strategy</literal>.
      </para>
      <para>
       By default, its value is set to <guimenu>Default</guimenu>, which means
       that utilization attributes and values are not considered.
      </para>
     </step>
     <step>
      <para>
       Depending on your requirements, set <guimenu>Placement Strategy</guimenu>
       to the appropriate value.
      </para>
     </step>
     <step>
      <para>
       Confirm your changes.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 xml:id="sec-ha-manual-config-utilization">
    <title>Placing resources based on their load impact with &crmsh;</title>
    <para>
     To configure the resource's requirements and the capacity a node
     provides, use utilization attributes.
     You can name the utilization attributes according to your preferences
     and define as many name/value pairs as your configuration needs. In
     certain cases, some agents update the utilization themselves, for
     example the <systemitem class="resource">VirtualDomain</systemitem>.
    </para>
    <para>
     In the following example, we assume that you already have a basic
     configuration of cluster nodes and resources. You now additionally want
     to configure the capacities a certain node provides and the capacity a
     certain resource requires.
    </para>
    <procedure>
     <title>Adding or modifying utilization attributes with <command>crm</command></title>
     <step>
      <para>
       Log in as &rootuser; and start the <command>crm</command>
       interactive shell:
      </para>
 <screen>&prompt.root;<command>crm configure</command></screen>
     </step>
     <step>
      <para>
       To specify the capacity a node <emphasis>provides</emphasis>, use the
       following command and replace the placeholder
       <replaceable>NODE_1</replaceable> with the name of your node:
      </para>
 <screen>&prompt.crm.conf;<command>node <replaceable>NODE_1</replaceable> utilization hv_memory=16384 cpu=8</command></screen>
      <para>
       With these values, <replaceable>NODE_1</replaceable> would be assumed
       to provide 16GB of memory and 8 CPU cores to resources.
      </para>
     </step>
     <step>
      <para>
       To specify the capacity a resource <emphasis>requires</emphasis>, use:
      </para>
 <screen>&prompt.crm.conf;<command>primitive xen1 Xen ... \
      utilization hv_memory=4096 cpu=4</command></screen>
      <para>
       This would make the resource consume 4096 of those memory units from
       <replaceable>NODE_1</replaceable>, and 4 of the CPU units.
      </para>
     </step>
     <step>
      <para>
       Configure the placement strategy with the <command>property</command>
       command:
      </para>
 <screen>&prompt.crm.conf;<command>property</command> ...</screen>
      <para>
       The following values are available:
      </para>
      &placement-strategy-values;
     </step>
     <step>
      <para>
       Commit your changes before leaving &crmsh;:
      </para>
 <screen>&prompt.crm.conf;<command>commit</command></screen>
     </step>
    </procedure>
    <para>
     The following example demonstrates a three node cluster of equal nodes,
     with four virtual machines:
    </para>
<screen>&prompt.crm.conf;<command>node &node1; utilization hv_memory="4000"</command>
&prompt.crm.conf;<command>node &node2; utilization hv_memory="4000"</command>
&prompt.crm.conf;<command>node &node3; utilization hv_memory="4000"</command>
&prompt.crm.conf;<command>primitive xenA Xen \
    utilization hv_memory="3500" meta priority="10" \
    params xmfile="/etc/xen/shared-vm/vm1"</command>
&prompt.crm.conf;<command>primitive xenB Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm2"</command>
&prompt.crm.conf;<command>primitive xenC Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm3"</command>
&prompt.crm.conf;<command>primitive xenD Xen \
    utilization hv_memory="1000" meta priority="5" \
    params xmfile="/etc/xen/shared-vm/vm4"</command>
&prompt.crm.conf;<command>property placement-strategy="minimal"</command></screen>
    <para>
     With all three nodes up, xenA will be placed onto a node first, followed
     by xenD. xenB and xenC would either be allocated together or one of them
     with xenD.
    </para>
    <para>
     If one node failed, too little total memory would be available to host
     them all. xenA would be ensured to be allocated, as would xenD. However,
     only one of xenB or xenC could still be placed, and since their priority
     is equal, the result is not defined yet. To resolve this ambiguity as
     well, you would need to set a higher priority for either one.
    </para>
   </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-config-basics-constraints-more">
  <title>For more information</title>
  <para>
   For more information on configuring constraints and detailed background
   information about the basic concepts of ordering and colocation, refer
   to the following documents at <link xlink:href="http://www.clusterlabs.org/pacemaker/doc/"/>:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     &paceex;, chapter <citetitle>Resource Constraints</citetitle>
    </para>
   </listitem>
   <listitem>
    <para>
     <citetitle>Colocation Explained</citetitle>
    </para>
   </listitem>
   <listitem>
    <para>
     <citetitle>Ordering Explained</citetitle>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
