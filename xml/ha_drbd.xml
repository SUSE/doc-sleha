<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!--

For the next release:
* bnc#577381: drbd disks are set to StandAlone after network restart

TEST SCENARIO:

# mkdir /loopdev
# dd if=/dev/zero of=/loopdev/drbd1.img bs=1k count=50000
50000+0 records in
50000+0 records out
51200000 bytes (51 MB) copied, 2.01732 s, 25.4 MB/s
# losetup /dev/loop0 /loopdev/drbd1.img
# losetup 
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop0         0      0         0  0 /loopdev/drbd1.img

-->
<chapter version="5.0" xml:id="cha-ha-drbd"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>DRBD</title>
 <info>
      <abstract>
        <para>
    The <emphasis>distributed replicated block device</emphasis> (DRBD*)
    allows you to create a mirror of two block devices that are located at
    two different sites across an IP network. When used with &corosync;,
    DRBD supports distributed high-availability Linux clusters. This chapter
    shows you how to install and set up DRBD.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-drbd-overview">
  <title>Conceptual overview</title>

  <para>
   DRBD replicates data on the primary device to the secondary device in a
   way that ensures that both copies of the data remain identical. Think of
   it as a networked RAID&nbsp;1. It mirrors data in real-time, so its
   replication occurs continuously. Applications do not need to know that in
   fact their data is stored on different disks.
<!--When using a
   cluster aware file system such as ocfs2, it is also possible to run both
   nodes as primary devices.-->
  </para>

  <para>
   DRBD is a Linux Kernel module and sits between the I/O scheduler at the
   lower end and the file system at the upper end, see
   <xref linkend="fig-ha-drbd-concept"/>. To communicate with DRBD, users
   use the high-level command <command>drbdadm</command>. For maximum
   flexibility DRBD comes with the low-level tool
   <command>drbdsetup</command>.
  </para>

  <figure xml:id="fig-ha-drbd-concept">
   <title>Position of DRBD within Linux</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_drbd.svg" width="80%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_drbd.png" width="80%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <important>
   <title>Unencrypted data</title>
   <para>
    The data traffic between mirrors is not encrypted. For secure data
    exchange, you should deploy a Virtual Private Network (VPN) solution for
    the connection.
   </para>
  </important>


  <para>
   DRBD allows you to use any block device supported by Linux, usually:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     partition or complete hard disk
    </para>
   </listitem>
   <listitem>
    <para>
     software RAID
    </para>
   </listitem>
   <listitem>
    <para>
     Logical Volume Manager (LVM)
    </para>
   </listitem>
   <listitem>
    <para>
     Enterprise Volume Management System (EVMS)
    </para>
   </listitem>
  </itemizedlist>

  <para>
   By default, DRBD uses the TCP ports <literal>7788</literal> and higher
   for communication between DRBD nodes. Make sure that your firewall does
   not prevent communication on the used ports.
  </para>

  <para>
   You must set up the DRBD devices before creating file systems on them.
   Everything pertaining to user data should be done solely via the
   <filename>/dev/drbd<replaceable>N</replaceable></filename> device and
   not on the raw device, as DRBD uses the last part of the raw device for
   metadata. Using the raw device will cause inconsistent data.
  </para>

  <para>
   With udev integration, you will also get symbolic links in the form
   <filename>/dev/drbd/by-res/<replaceable>RESOURCES</replaceable></filename>
   which are easier to use and provide safety against remembering the wrong
   minor number of the device.
  </para>

  <para>
   For example, if the raw device is 1024&nbsp;MB in size, the DRBD
   device has only 1023&nbsp;MB available for data, with about
   70&nbsp;KB hidden and reserved for the metadata. Any attempt to access
   the remaining kilobytes via raw disks fails because
   it is not available for user data.
   <!-- taroth 2015-07-21: from bsc#938291 by nick wang:
    reason: as /dev/drbd<N> or /dev/drbd_<N> will protect the metadata,
    it is forbidden to access the raw device like /dev/sdaX directly-->
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-drbd-install">
  <title>Installing DRBD services</title>
  <para>
   Install the &hasi; on both &sls; machines in your networked
   cluster as described in <xref linkend="part-install"/>. Installing
   &hasi; also installs the DRBD program files.
  </para>

  <para>
   If you do not need the complete cluster stack but only want to use DRBD,
   install the packages <package>drbd</package>,
    <package>drbd-kmp-<replaceable>FLAVOR</replaceable></package>,
    <package>drbd-utils</package>, and <package>yast2-drbd</package>.
  </para>

  <para>
   To simplify the work with <command>drbdadm</command>, use the Bash
   completion support.
   If you want to enable it in your current shell session, insert the
   following command:
  </para>

<screen>&prompt.root;<command>source</command> /etc/bash_completion.d/drbdadm.sh</screen>

  <para>
   To use it permanently for &rootuser;, create, or extend a file
   <filename>/root/.bashrc</filename> and insert the previous line.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-drbd-configure">
  <title>Setting up DRBD service</title>
  <note>
   <title>Adjustments needed</title>
   <para>
    The following procedure uses the server names &node1; and &node2;,
    and the cluster resource name <literal>r0</literal>. It sets up
    &node1; as the primary node and <filename>/dev/sda1</filename> for
    storage. Make sure to modify the instructions to use your own nodes and
    file names.
   </para>
  </note>

  <remark>toms 2014-02-13: phil: perhaps the example should be changed to
    use an LV instead;band eg. "nfs" instead of "r0".</remark>

  <para>
   The following sections assumes you have two nodes, &node1;
   and &node2;, and that they should use the TCP port <literal>7788</literal>.
   Make sure this port is open in your firewall.
  </para>

  <procedure>
    <step>
      <para>Prepare your system:</para>
      <substeps>
        <step>
          <para>Make sure the block devices in your Linux nodes are ready
            and partitioned (if needed).</para>
        </step>
        <step>
          <para>
            If your disk already contains a file system that you do not need
            anymore, destroy the file system structure with the following
            command:
          </para>
          <screen>&prompt.root;<command>dd</command> if=/dev/zero of=<replaceable>YOUR_DEVICE</replaceable> count=16 bs=1M</screen>
          <para>If you have more file systems to destroy, repeat this step on
           all devices you want to include into your DRBD setup.</para>
        </step>
          <step>
            <para>If the cluster is already using DRBD, put your cluster
              in maintenance mode: </para>
            <screen>&prompt.root;<command>crm</command> configure property maintenance-mode=true</screen>
            <para> If you skip this step when your cluster uses already
              DRBD, a syntax error in the live configuration will lead
              to a service shutdown. </para>
            <para>As an alternative, you can also use
                <command>drbdadm</command>
              <option>-c <replaceable>FILE</replaceable></option> to
              test a configuration file.</para>
          </step>
      </substeps>
    </step>
    <step>
      <para>Configure DRBD by choosing your method:</para>
      <itemizedlist>
        <listitem>
          <para><xref linkend="sec-ha-drbd-configure-manually"/></para>
        </listitem>
        <listitem>
          <para><xref linkend="sec-ha-drbd-configure-yast"/></para>
        </listitem>
      </itemizedlist>
    </step>
    <step>
      <para>
        If you have configured &csync; (which should be the default), the
        DRBD configuration files are already included in the list of files
        need to be synchronized. To synchronize them, use:
      </para>
      <screen>&prompt.root;<command>csync2</command> -xv /etc/drbd.d/</screen>
      <para>
        If you do not have &csync; (or do not want to use it), copy the DRBD
        configuration files manually to the other node:
      </para>
      <screen>&prompt.root;<command>scp</command> /etc/drbd.conf &node2;:/etc/
&prompt.root;<command>scp</command> /etc/drbd.d/*  &node2;:/etc/drbd.d/</screen>
    </step>
    <step>
      <para>Perform the initial synchronization (see <xref linkend="sec-ha-drbd-configure-init"/>).</para>
    </step>
    <step>
      <para>
        Reset the cluster's maintenance mode flag:
      </para>
      <screen>&prompt.root;<command>crm</command> configure property maintenance-mode=false</screen>
    </step>
  </procedure>

  <sect2 xml:id="sec-ha-drbd-configure-manually">
    <title>Configuring DRBD manually</title>
    <note>
     <title>Restricted support for <quote>auto promote</quote> feature</title>
     <para>
      The DRBD9 feature <quote>auto promote</quote> can use a clone
      and file system resource instead of a master/slave connection. When using
      this feature while a file system is being mounted, DRBD will change to
      primary mode automatically.
     </para>
     <para>
      The auto promote feature has currently restricted support.
      &drbd-restricted-support;
     </para>
    </note>
    <para>
     To set up DRBD manually, proceed as follows:
    </para>

  <procedure xml:id="pro-drbd-configure">
   <title>Manually configuring DRBD</title>
    <para>Beginning with DRBD version 8.3, the former configuration file is
      split into separate files, located under the directory
      <filename>/etc/drbd.d/</filename>.</para>
   <step>
      <para>
       Open the file <filename>/etc/drbd.d/global_common.conf</filename>. It
       contains already some global, pre-defined values. Go to the
       <literal>startup</literal> section and insert these lines:
      </para>
<screen>startup {
    # wfc-timeout degr-wfc-timeout outdated-wfc-timeout
    # wait-after-sb;
    wfc-timeout 100;
    degr-wfc-timeout 120;
}</screen>
      <para>
       These options are used to reduce the timeouts when booting, see
       <link xlink:href="https://docs.linbit.com/docs/users-guide-9.0/#ch-configure"/>
       for more details.
      </para>
     </step>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/r0.res</filename>. Change the
       lines according to your situation and save it:
      </para>
<screen>resource r0 { <co xml:id="co-drbd-config-r0"/>
  device /dev/drbd0; <co xml:id="co-drbd-config-device"/>
  disk /dev/sda1; <co xml:id="co-drbd-config-disk"/>
  meta-disk internal; <co xml:id="co-drbd-config-meta-disk"/>
  on &node1; { <co xml:id="co-drbd-config-resname"/>
    address  &subnetI;.10:7788; <co xml:id="co-drbd-config-address"/>
    node-id 0; <co xml:id="co-drbd-config-node-id"/>
  }
  on &node2; { <xref linkend="co-drbd-config-resname" xrefstyle="selec:nopage"/>
    address &subnetI;.11:7788; <xref linkend="co-drbd-config-address" xrefstyle="selec:nopage"/>
    node-id 1; <xref linkend="co-drbd-config-node-id"/>
  }
  disk {
    resync-rate 10M; <co xml:id="co-drbd-config-syncer-rate"/>
  }
  connection-mesh { <co xml:id="co-drbd-config-connection-mesh"/>
    hosts &node1; &node2;;
  }
}</screen>
      <calloutlist>
       <callout arearefs="co-drbd-config-r0">
        <para>
         DRBD resource name that allows some association to the service that needs them.
         For example, <systemitem>nfs</systemitem>,
         <systemitem>http</systemitem>, <systemitem>mysql_0</systemitem>,
         <systemitem>postgres_wal</systemitem>, etc.
         Here a more general name <literal>r0</literal> is used.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-device">
        <!--taroth 2010-08-16: fix for bnc#623524-->
        <para>
         The device name for DRBD and its minor number.
        </para>
        <para>
         In the example above, the minor number 0 is used for DRBD. The udev
         integration scripts will give you a symbolic link
         <filename>/dev/drbd/by-res/nfs/0</filename>. Alternatively, omit
         the device node name in the configuration and use the following
         line instead:
        </para>
        <para>
         <literal>drbd0 minor 0</literal> (<literal>/dev/</literal> is
         optional) or <literal>/dev/drbd0</literal>
        </para>
       </callout>
       <callout arearefs="co-drbd-config-disk">
        <para>
         The raw device that is replicated between nodes. Note, in this
         example the devices are the <emphasis>same</emphasis> on both nodes.
         If you need different devices, move the <literal>disk</literal>
          parameter into the <literal>on</literal> host.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-meta-disk">
        <para>
         The meta-disk parameter usually contains the value
         <literal>internal</literal>, but it is possible to specify an
         explicit device to hold the meta data. See
         <link xlink:href="https://docs.linbit.com/docs/users-guide-9.0/#s-metadata"/>
         for more information.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-resname">
        <para>
         The <literal>on</literal> section states which host this
         configuration statement applies to.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-address">
        <para>
         The IP address and port number of the respective node. Each
         resource needs an individual port, usually starting with
         <literal>7788</literal>. Both ports must be the same for a
          DRBD resource.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-node-id">
        &drbd-node-id;
       </callout>
       <callout arearefs="co-drbd-config-syncer-rate">
        <para>
         The synchronization rate. Set it to one third of the lower of the
         disk- and network bandwidth. It only limits the resynchronization,
         not the replication.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-connection-mesh">
        &drbd-connection-mesh;
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
      Check the syntax of your configuration file(s). If the following
      command returns an error, verify your files:
      </para>
      <screen>&prompt.root;<command>drbdadm</command> dump all</screen>
     </step>
    <step>
      <para>Continue with <xref linkend="sec-ha-drbd-configure-init"/>.</para>
    </step>
  </procedure>
  </sect2>
  <sect2 xml:id="sec-ha-drbd-configure-yast">
    <title>Configuring DRBD with &yast;</title>
    <para>&yast; can be used to start with an initial setup of DRBD.
        After you have created your DRBD setup, you can fine-tune the
        generated files manually.
    </para>
    <para>
        However, when you have changed the configuration files,
        do not use the &yast; DRBD module anymore. The DRBD module supports
        only a limited set of basic configuration. If you use it again,
        it is very likely that the module will not show your changes.
    </para>
    <para>
      To set up DRBD with &yast;, proceed as follows:</para>
  <procedure xml:id="pro-drbd-configure-yast">
   <title>Using &yast; to configure DRBD</title>
   <step>
    <para>
     Start &yast; and select the configuration module <menuchoice>
     <guimenu>High Availability</guimenu> <guimenu>DRBD</guimenu>
     </menuchoice>. If you already have a DRBD configuration, &yast;
     warns you. &yast; will change your configuration and will save your
     old DRBD configuration files as <filename>*.YaSTsave</filename>.
    </para>
   </step>
   <step>
    <para>
     Leave the booting flag in <menuchoice><guimenu>Start-up
     Configuration</guimenu><guimenu>Booting</guimenu></menuchoice> as it is
     (by default it is <literal>off</literal>); do not change that as
     &pace; manages this service.
    </para>
   </step>
   <step>
     <remark>toms 2015-09-29: FATE#318391</remark>
     <para>If you have a firewall running, enable <guimenu>Open Port in
      Firewall</guimenu>.</para>
   </step>
   <step>
    <para>Go to the <guimenu>Resource Configuration</guimenu> entry.
      Press <guimenu>Add</guimenu> to create a new resource (see
      <xref linkend="fig-ha-drbd-yast-resconfig"/>).
    </para>
    <figure xml:id="fig-ha-drbd-yast-resconfig">
     <title>Resource configuration</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast_drbd-resconfig.png" width="90%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast_drbd-resconfig.png" width="90%"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
      The following parameters need to be set:
    </para>

    <variablelist>
      <varlistentry>
        <term><guimenu>Resource Name</guimenu></term>
        <listitem>
          <para>The name of the DRBD resource (mandatory)</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Name</guimenu></term>
        <listitem>
          <para>The host name of the relevant node</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Address:Port</guimenu></term>
        <listitem>
          <para>
          The IP address and port number (default
           <systemitem>7788</systemitem>) for the respective
          node
         </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Device</guimenu></term>
        <listitem>
          <para>
          The block device path that is used to access the replicated data.
          If the device contains a minor number, the associated block device
          is usually named <filename>/dev/drbdX</filename>, where
          <replaceable>X</replaceable> is the device minor number. If the
          device does not contain a minor number, make sure to add
          <literal>minor 0</literal> after the device name.
         </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Disk</guimenu></term>
        <listitem>
          <para>
          The raw device that is replicated between both nodes. If you use
          LVM, insert your LVM device name.
         </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Meta-disk</guimenu></term>
        <listitem>
          <para>
          The <guimenu>Meta-disk</guimenu> is either set to the value
          <literal>internal</literal> or specifies an explicit device
          extended by an index to hold the meta data needed by DRBD.
         </para>
         <para>
          A real device may also be used for multiple DRBD resources. For
          example, if your <guimenu>Meta-Disk</guimenu> is
          <filename>/dev/sda6[0]</filename> for the first resource, you may
          use <filename>/dev/sda6[1]</filename> for the second resource.
          However, there must be at least 128 MB space for each resource
          available on this disk. The fixed metadata size limits the maximum
          data size that you can replicate.
         </para>
        </listitem>
      </varlistentry>
    </variablelist>

    <!--
    <informaltable>
     <tgroup cols="2">
      <colspec colwidth="1*"/>
      <colspec colwidth="3*"/>
      <tbody>
       <row>
        <entry>
         <para>
          <guimenu>Resource Name</guimenu>
         </para>
        </entry>
        <entry>
         <para>
          The name of the DRBD resource (mandatory)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          <guimenu>Name</guimenu>
         </para>
        </entry>
        <entry>
         <para>
          The host name of the relevant node
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          <guimenu>Address:Port</guimenu>
         </para>
        </entry>
        <entry>
         <para>
          The IP address and port number (default 7788) for the respective
          node
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          <guimenu>Device</guimenu>
         </para>
        </entry>
        <entry>
         <para>
          The block device path that is used to access the replicated data.
          If the device contains a minor number, the associated block device
          is usually named <filename>/dev/drbdX</filename>, where
          <replaceable>X</replaceable> is the device minor number. If the
          device does not contain a minor number, make sure to add
          <literal>minor 0</literal> after the device name.
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          <guimenu>Disk</guimenu>
         </para>
        </entry>
        <entry>
         <para>
          The raw device that is replicated between both nodes. If you use
          LVM, insert your LVM device name.
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          <guimenu>Meta-disk</guimenu>
         </para>
        </entry>
        <entry>
         <para>
          The <guimenu>Meta-disk</guimenu> is either set to the value
          <literal>internal</literal> or specifies an explicit device
          extended by an index to hold the meta data needed by DRBD.
         </para>
         <para>
          A real device may also be used for multiple drbd resources. For
          example, if your <guimenu>Meta-Disk</guimenu> is
          <filename>/dev/sda6[0]</filename> for the first resource, you may
          use <filename>/dev/sda6[1]</filename> for the second resource.
          However, there must be at least 128 MB space for each resource
          available on this disk. The fixed metadata size limits the maximum
          data size that you can replicate.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
     -->
    <para>
     All of these options are explained in the examples in the
     <filename>/usr/share/doc/packages/drbd/drbd.conf</filename> file and in
     the man page of <command>drbd.conf(5)</command>.
    </para>
   </step>
   <step>
    <para>Click <guimenu>Save</guimenu>.</para>
   </step>
   <step>
    <para>Click <guimenu>Add</guimenu> to enter the second DRBD resource
     and finish with <guimenu>Save</guimenu>.
    </para>
   </step>
   <step>
     <para>Close the resource configuration with <guimenu>Ok</guimenu>
      and <guimenu>Finish</guimenu>.
     </para>
   </step>
   <step>
     <remark>toms 2015-09-30: FATE#317957</remark>
     <para>If you use LVM with DRBD, it is necessary to change some options
       in the LVM configuration file (see the <guimenu>LVM
       Configuration</guimenu> entry). This change can be done by the
       &yast; DRBD module automatically.
     </para>
     <para>
       The disk name of localhost for the DRBD resource and the default filter
       will be rejected in the LVM filter. Only <filename>/dev/drbd</filename>
       can be scanned for an LVM device.</para>
     <para>
       For example, if <filename>/dev/sda1</filename> is used as a DRBD disk,
       the device name will be inserted as the first entry in the LVM filter.
       To change the filter manually, click the
       <guimenu>Modify LVM Device Filter Automatically</guimenu> check box.
     </para>
   </step>
   <step>
     <para>Save your changes with <guimenu>Finish</guimenu>.</para>
   </step>
    <step>
      <para>Continue with <xref linkend="sec-ha-drbd-configure-init"/>.</para>
    </step>
  </procedure>
  </sect2>
  <sect2 xml:id="sec-ha-drbd-configure-init">
    <title>Initializing and formatting DRBD resource</title>
    <para>After you have prepared your system and configured DRBD,
      initialize your disk for the first time:
    </para>
    <procedure>
        <step>
          <para>
           On <emphasis>both</emphasis> nodes (&node1; and &node2;),
           initialize the meta data storage:
          </para>
          <screen>&prompt.root;<command>drbdadm</command> create-md r0
&prompt.root;<command>drbdadm</command> up r0<!--
&prompt.root;<command>systemctl</command> start drbd--></screen>
        </step>
        <step>
         <para>
          To shorten the initial resynchronization of your DRBD resource
          check the following:
         </para>
         <itemizedlist>
          <listitem>
           <para>
            If the DRBD devices on all nodes have the same data (for example,
            by destroying the file system structure with the
            <command>dd</command> command as shown in
            <xref linkend="sec-ha-drbd-configure"/>), then skip the initial
            resynchronization with the following command (on both nodes):
           </para>
           <screen>&prompt.root;<command>drbdadm</command
              > new-current-uuid --clear-bitmap r0/0</screen>
           <para>The state will be <literal>Secondary/Secondary UpToDate/UpToDate</literal></para>
          </listitem>
          <listitem>
           <para>
            Otherwise, proceed with the next step.
           </para>
          </listitem>
         </itemizedlist>
        </step>
        <step>
         <para>
          On the primary node &node1;, start the resynchronization process:
         </para>
         <screen>&prompt.root;<command>drbdadm</command> primary --force r0</screen>
        </step>
        <step>
         <para> Check the status with: </para>
         <screen>&prompt.root;<command>drbdadm</command> status r0
r0 role:Primary
  disk:UpToDate
  &node2; role:Secondary
  peer-disk:UpToDate</screen>
        </step>
        <step>
          <para> Create your file system on top of your DRBD device, for
            example: </para>
          <screen>&prompt.root;<command>mkfs.ext3</command> /dev/drbd0</screen>
        </step>
        <step>
          <para> Mount the file system and use it: </para>
          <screen>&prompt.root;<command>mount</command> /dev/drbd0 /mnt/</screen>
        </step>
    </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-migrate">
  <title>Migrating from DRBD&nbsp;8 to DRBD&nbsp;9</title>
  <para>
   Between DRBD&nbsp;8 (shipped with &sle; &hasi;&nbsp;12 SP1) and
   DRBD&nbsp;9 (shipped with &sle; &hasi;&nbsp;12 SP2), the metadata format
   has changed. DRBD&nbsp;9 does not automatically convert previous metadata
   files to the new format.
  </para>
  <para>
   After migrating to 12&nbsp;SP2 and before starting DRBD, convert the DRBD
   metadata to the version 9 format manually. To do so, use
   <command>drbdadm</command> <option>create-md</option>. No configuration
   needs to be changed.
  </para>
  <!-- toms 2016-08-03: According to Nick, we shouldn't tell the customer
      to mention how to create a three-node DRBD resource (if we don't
      support it anyway).
  -->
  <!--<para>
   For three nodes and more, you need change the configuration and add the
   <parameter>connection-mesh</parameter> section and the
   <parameter>node-id</parameter> parameter for every node.
  </para>-->
  <note>
   <title>Restricted support</title>
   <para>&drbd-restricted-support;</para>
  </note>
  <para>
   DRBD&nbsp;9 will fall back to be compatible with version 8.
   For three nodes and more, you need to re-create the metadata
   to use DRBD version 9 specific options.
  </para>
  <para>
   If you have a stacked DRBD resource, refer also to <xref
    linkend="sec-ha-drbd-resource-stacking"/> for more information.
  </para>
  <para>
   To keep your data and allow to add new nodes without re-creating new
   resources, do the following:
  </para>
  <procedure>
   <step>
    <para>
     Set one node in standby mode.
    </para>
   </step>
   <step>
    <para>
     Update all the DRBD packages on all of your nodes, see <xref
      linkend="sec-ha-drbd-install"/>.
    </para>
   </step>
   <step>
    <para>Add the new node information to your resource configuration<!--
     (compare with <xref linkend="..."/>)-->:</para>
    <itemizedlist>
     <listitem>
      <para><parameter>node-id</parameter> on every <literal>on</literal>
       section.
      </para>
     </listitem>
     <listitem>
      <para>
       <parameter>connection-mesh</parameter> section contains all host names
       in the <parameter>hosts</parameter> parameter.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     See the example configuration in <xref linkend="pro-drbd-configure"/>.
    </para>
   </step>
   <step>
    <para>
     Enlarge the space of your DRBD disks when using <literal>internal</literal>
     as <literal>meta-disk</literal> key. Use a device that supports enlarging
     the space like LVM.
     As an alternative, change to an external disk for metadata
     and use <literal>meta-disk <replaceable>DEVICE</replaceable>;</literal>.
    </para>
   </step>
   <step>
    <para>
     Re-create the metadata based on the new configuration:
    </para>
    <screen>&prompt.root;<command>drbdadm</command> create-md <replaceable
     >RESOURCE</replaceable></screen>
   </step>
   <step>
    <para>
     Cancel the standby mode.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-resource-stacking">
  <title>Creating a stacked DRBD device</title>
  <para>
   A stacked DRBD device contains two other devices of which at least one
   device is also a DRBD resource. In other words, DRBD adds an additional
   node on top of an already existing DRBD resource (see <xref
    linkend="fig-ha-drbd-resource-stacking"/>). Such a replication setup
   can be used for backup and disaster recovery purposes.
  </para>
 <!--taroth 2017-09-21: Please remove gradient blend in SVG version
  of this graphic as it breaks the graphic when using FOP. For now,
  used PNG instead-->
  <figure xml:id="fig-ha-drbd-resource-stacking">
   <title>Resource stacking</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata width="80%" fileref="ha_stacked_drbd.png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata width="50%" fileref="ha_stacked_drbd.png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Three-way replication uses asynchronous (DRBD protocol A) and
   synchronous replication (DRBD protocol C). The asynchronous part is used for
   the stacked resource whereas the synchronous part is used for the backup.
  </para>

  <para>
   Your production environment uses the stacked device. For example,
   if you have a DRBD device <filename>/dev/drbd0</filename> and a stacked
   device <filename>/dev/drbd10</filename> on top, the file system will
   be created on <filename>/dev/drbd10</filename>, see <xref
    linkend="exa-ha-drbd-stacked-drbd"/> for more details.
  </para>

  <example xml:id="exa-ha-drbd-stacked-drbd">
   <title>Configuration of a three-node stacked DRBD resource</title>
   <screen># /etc/drbd.d/r0.res
resource r0 {
  protocol C;
  device    /dev/drbd0;
  disk      /dev/sda1;
  meta-disk internal;

  on &cluster1;-&node1; {
    address    &subnetI;.1:7900;<!-- 192.168.122.57 -->
  }

  on &cluster1;-&node2; {
    address    &subnetI;.2:7900;<!-- 192.168.122.29 -->
  }
}

resource r0-U {
  protocol A;
  device     /dev/drbd10;

  stacked-on-top-of r0 {
    address    &subnetII;.1:7910;<!-- 10.0.0.1: -->
  }

  on &cluster2;-&node3; {
    disk       /dev/sda10;
    address    &subnetII;.2:7910; # Public IP of the backup node<!-- 10.0.0.2 -->
    meta-disk  internal;
  }
}</screen>
  </example>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-fencing">
  <title>Using resource-level fencing with &stonith;</title>
  <para>
   When a DRBD replication link becomes interrupted, &pace; tries to promote
   the DRBD resource to another node. To prevent &pace; from starting a service
   with outdated data, enable resource-level fencing in the DRBD configuration
   file.
  </para>
   <para>
    The fencing policy can have different values (see man page <command>drbdsetup</command> and the
     <option>--fencing</option> option).
    As a &productname; cluster is normally used with a &stonith; device, the value
    <constant>resource-and-stonith</constant> is used in
    <xref linkend="ex-ha-drbd-fencing"/>.
  </para>

  <example xml:id="ex-ha-drbd-fencing">
   <title>Configuration of DRBD with resource-level fencing using the Cluster
    Information Base (CIB)</title>
   <screen>resource <replaceable>RESOURCE</replaceable> {
  net {
    fencing resource-and-stonith;
    # ...
  }
  handlers {
    fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
    after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
    # ...
  }
  ...
}</screen>
  </example>
  <para>If the DRBD replication link becomes disconnected, DRBD does the
  following:</para>
  <orderedlist>
   <listitem>
    <para>DRBD calls the <command>crm-fence-peer.9.sh</command> script.</para>
   </listitem>
   <listitem>
    <para>The script contacts the cluster manager.
    </para>
   </listitem>
   <listitem>
    <para>The script determines the &pace; resource associated with this
     DRBD resource.</para>
   </listitem>
   <listitem>
    <para>The script ensures that the DRBD resource no longer gets
    promoted to any other node. It stays on the currently active one.</para>
   </listitem>
   <listitem>
    <para>If the replication link becomes connected again and DRBD
    completes its synchronization process, then the constraint is removed.
    The cluster manager is now free to promote the resource.
    </para>
   </listitem>
  </orderedlist>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-test">
  <title>Testing the DRBD service</title>

  <para>
   If the install and configuration procedures worked as expected, you are
   ready to run a basic test of the DRBD functionality. This test also helps
   with understanding how the software works.
  </para>

  <procedure xml:id="pro-drbd-test">
   <step>
    <para>
     Test the DRBD service on &node1;.
    </para>
    <substeps>
     <step>
      <para>
       Open a terminal console, then log in as
       <systemitem>root</systemitem>.
      </para>
     </step>
     <step>
      <para>
       Create a mount point on &node1;, such as
       <filename>/srv/r0</filename>:
      </para>
<screen>&prompt.root;<command>mkdir</command> -p /srv/r0</screen>
     </step>
     <step>
      <para>
       Mount the <command>drbd</command> device:
      </para>
<screen>&prompt.root;<command>mount</command> -o rw /dev/drbd0 /srv/r0</screen>
     </step>
     <step>
      <para>
       Create a file from the primary node:
      </para>
<screen>&prompt.root;<command>touch</command> /srv/r0/from_&node1;</screen>
     </step>
     <step>
      <para>
       Unmount the disk on &node1;:
      </para>
<screen>&prompt.root;<command>umount</command> /srv/r0</screen>
     </step>
     <step>
      <para>
       Downgrade the DRBD service on &node1; by typing the following
       command on &node1;:
      </para>
<screen>&prompt.root;<command>drbdadm</command> secondary r0</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Test the DRBD service on &node2;.
    </para>
    <substeps>
     <step>
      <para>
       Open a terminal console, then log in as <systemitem>root</systemitem>
       on &node2;.
      </para>
     </step>
     <step>
      <para>
       On &node2;, promote the DRBD service to primary:
      </para>
<screen>&prompt.root;<command>drbdadm</command> primary r0</screen>
     </step>
     <step>
      <para>
       On &node2;, check to see if &node2; is primary:
      </para>
<screen>&prompt.root;<command>drbdadm</command> status r0</screen>
     </step>
     <step>
      <para> On &node2;, create a mount point such as
        <filename>/srv/r0</filename>: </para>
<screen>&prompt.root;<command>mkdir</command> /srv/r0</screen>
     </step>
     <step>
      <para>
       On &node2;, mount the DRBD device:
      </para>
<screen>&prompt.root;<command>mount</command> -o rw /dev/drbd0 /srv/r0</screen>
     </step>
     <step>
      <para>
       Verify that the file you created on &node1; exists:
      </para>
<screen>&prompt.root;<command>ls</command> /srv/r0/from_&node1;</screen>
      <para> The <filename>/srv/r0/from_&node1;</filename> file should be
       listed. </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     If the service is working on both nodes, the DRBD setup is complete.
    </para>
   </step>
   <step>
    <para>
     Set up &node1; as the primary again.
    </para>
    <substeps performance="required">
     <step>
      <para>
       Dismount the disk on &node2; by typing the following command on
       &node2;:
      </para>
<screen>&prompt.root;<command>umount</command> /srv/r0</screen>
     </step>
     <step>
      <para>
       Downgrade the DRBD service on &node2; by typing the following
       command on &node2;:
      </para>
<screen>&prompt.root;<command>drbdadm</command> secondary r0</screen>
     </step>
     <step>
      <para>
       On &node1;, promote the DRBD service to primary:
      </para>
<screen>&prompt.root;<command>drbdadm</command> primary r0</screen>
     </step>
     <step>
      <para>
       On &node1;, check to see if &node1; is primary:
      </para>
<screen>&prompt.root;<command>drbdadm</command> status r0</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To get the service to automatically start and fail over if the server
     has a problem, you can set up DRBD as a high availability service with
     Pacemaker/&corosync;. For information about installing and
     configuring for &sle; &productnumber; see
     <xref linkend="part-config"/>.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-monitor">
  <title>Monitoring DRBD devices</title>
  <para>DRBD comes with the utility <command>drbdmon</command> which offers
   real time monitoring. It shows all the configured resources and their
   problems.
  </para>
  <figure>
   <title>Showing a good connection by <command>drbdmon</command></title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="drbd-drbdmon-ok.png" width="70%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>In case of problems, <command>drbdadm</command> shows an error message:
  </para>
  <figure>
   <title>Showing a bad connection by <command>drbdmon</command></title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="drbd-drbdmon-bad.png" width="70%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-tuning">
  <title>Tuning DRBD</title>

  <para>
   There are several ways to tune DRBD:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Use an external disk for your metadata. This might help, at the cost of
     maintenance ease.
    </para>
   </listitem>
   <listitem>
    <para>
     Tune your network connection, by changing the receive and send buffer
     settings via <command>sysctl</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Change the <systemitem>max-buffers</systemitem>,
     <systemitem>max-epoch-size</systemitem> or both in the DRBD
     configuration.
    </para>
   </listitem>
   <listitem>
    <para>
     Increase the <systemitem>al-extents</systemitem> value, depending on
     your IO patterns.
    </para>
   </listitem>
   <listitem>
    <para>
     If you have a hardware RAID controller with a BBU (<emphasis>Battery
     Backup Unit</emphasis>), you might benefit from setting
     <systemitem>no-disk-flushes</systemitem>,
     <systemitem>no-disk-barrier</systemitem> and/or
     <systemitem>no-md-flushes</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     Enable read-balancing depending on your workload. See
     <link xlink:href="https://www.linbit.com/en/read-balancing/"/> for
     more details.
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="sec-ha-drbd-trouble">
  <title>Troubleshooting DRBD</title>

  <para>
   The DRBD setup involves many components and problems may arise
   from different sources. The following sections cover several common
   scenarios and recommend various solutions.
  </para>

  <sect2 xml:id="sec-ha-drbd-trouble-config">
   <title>Configuration</title>
   <para>
    If the initial DRBD setup does not work as expected, there is probably
    something wrong with your configuration.
   </para>
   <para>
    To get information about the configuration:
   </para>
   <procedure>
    <step>
     <para>
      Open a terminal console, then log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Test the configuration file by running <command>drbdadm</command> with
      the <command>-d</command> option. Enter the following command:
     </para>
<screen>&prompt.root;<command>drbdadm</command> -d adjust r0</screen>
     <para>
      In a dry run of the <command>adjust</command> option,
      <command>drbdadm</command> compares the actual configuration of the
      DRBD resource with your DRBD configuration file, but it does not
      execute the calls. Review the output to make sure you know the source
      and cause of any errors.
     </para>
    </step>
    <step>
     <para>
      If there are errors in the <filename>/etc/drbd.d/*</filename> and
      <filename>drbd.conf</filename> files, correct them before continuing.
     </para>
    </step>
    <step>
     <para>
      If the partitions and settings are correct, run
      <command>drbdadm</command> again without the <command>-d</command>
      option.
     </para>
<screen>&prompt.root;<command>drbdadm</command> adjust r0</screen>
     <para>
      This applies the configuration file to the DRBD resource.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-drbd-hostnames">
   <title>Host names</title>
   <para>
    For DRBD, host names are case-sensitive (<systemitem>Node0</systemitem>
    would be a different host than <systemitem>node0</systemitem>), and
    compared to the host name as stored in the Kernel (see the
    <command>uname -n</command> output).
   </para>
   <para>
    If you have several network devices and want to use a dedicated network
    device, the host name will likely not resolve to the used IP address. In
    this case, use the parameter <literal>disable-ip-verification</literal>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-drbd-port">
   <title>TCP port 7788</title>
   <para>
    If your system cannot connect to the peer, this might be a problem with
    your local firewall. By default, DRBD uses the TCP port
    <literal>7788</literal> to access the other node. Make sure that this
    port is accessible on both nodes.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-drbd-trouble-broken">
   <title>DRBD devices broken after reboot</title>
   <para>
    In cases when DRBD does not know which of the real devices holds the
    latest data, it changes to a split brain condition. In this case, the
    respective DRBD subsystems come up as secondary and do not connect to
    each other. In this case, the following message can be found in the
    logging data:
   </para>
<screen>Split-Brain detected, dropping connection!</screen>
   <para>
    To resolve this situation, enter the following commands on the node which has
    data to be discarded:
   </para>
<screen>&prompt.root;<command>drbdadm</command> secondary r0</screen>
   <para>
    If the state is in <literal>WFconnection</literal>, disconnect first:
   </para>
   <screen>&prompt.root;<command>drbdadm</command> disconnect r0</screen>
   <para>
    On the node which has the latest data enter the following:
   </para>
<screen>&prompt.root;<command>drbdadm</command> connect  --discard-my-data r0</screen>
   <para>
    That resolves the issue by overwriting one node's data with the peer's
    data, therefore getting a consistent view on both nodes.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ha-drbd-more">
  <title>For more information</title>

  <para>
   The following open source resources are available for DRBD:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The project home page <link xlink:href="http://www.drbd.org"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     See <xref linkend="article-nfs-storage"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://clusterlabs.org/wiki/DRBD_HowTo_1.0"/> by the
     Linux Pacemaker Cluster Stack Project.
    </para>
   </listitem>
   <listitem>
    <para>
     The following man pages for DRBD are available in the distribution:
     <command>drbd(8)</command>, <command>drbdmeta(8)</command>,
     <command>drbdsetup(8)</command>, <command>drbdadm(8)</command>,
     <command>drbd.conf(5)</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Find a commented example configuration for DRBD at
     <filename>/usr/share/doc/packages/drbd-utils/drbd.conf.example</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Furthermore, for easier storage administration across your cluster, see
     the recent announcement about the <citetitle>DRBD-Manager</citetitle>
     at <link xlink:href="https://www.linbit.com/en/drbd-manager/"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
