<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!--

For the next release:
* bnc#577381: drbd disks are set to StandAlone after network restart

TEST SCENARIO:

# mkdir /loopdev
# dd if=/dev/zero of=/loopdev/drbd1.img bs=1k count=50000
50000+0 records in
50000+0 records out
51200000 bytes (51 MB) copied, 2.01732 s, 25.4 MB/s
# losetup /dev/loop0 /loopdev/drbd1.img
# losetup
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop0         0      0         0  0 /loopdev/drbd1.img

-->
<chapter version="5.0" xml:id="cha-ha-drbd"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>DRBD</title>
 <info>
      <abstract>
        <para>
    The <emphasis>distributed replicated block device</emphasis> (DRBD*)
    allows you to create a mirror of two block devices that are located at
    two different sites across an IP network. When used with &corosync;,
    DRBD supports distributed high-availability Linux clusters. This chapter
    shows you how to install and set up DRBD.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-drbd-overview">
  <title>Conceptual overview</title>

  <para>
   DRBD replicates data on the primary device to the secondary device in a
   way that ensures that both copies of the data remain identical. Think of
   it as a networked RAID&nbsp;1. It mirrors data in real-time, so its
   replication occurs continuously. Applications do not need to know that in
   fact their data is stored on different disks.
<!--When using a
   cluster aware file system such as ocfs2, it is also possible to run both
   nodes as primary devices.-->
  </para>

  <para>
   DRBD is a Linux Kernel module and sits between the I/O scheduler at the
   lower end and the file system at the upper end, see
   <xref linkend="fig-ha-drbd-concept"/>. To communicate with DRBD, users
   use the high-level command <command>drbdadm</command>. For maximum
   flexibility DRBD comes with the low-level tool
   <command>drbdsetup</command>.
  </para>

  <figure xml:id="fig-ha-drbd-concept">
   <title>Position of DRBD within Linux</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_drbd.svg" width="80%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_drbd.png" width="80%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <important>
   <title>Unencrypted data</title>
   <para>
    The data traffic between mirrors is not encrypted. For secure data
    exchange, you should deploy a Virtual Private Network (VPN) solution for
    the connection.
   </para>
  </important>


  <para>
   DRBD allows you to use any block device supported by Linux, usually:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     partition or complete hard disk
    </para>
   </listitem>
   <listitem>
    <para>
     software RAID
    </para>
   </listitem>
   <listitem>
    <para>
     Logical Volume Manager (LVM)
    </para>
   </listitem>
   <listitem>
    <para>
     Enterprise Volume Management System (EVMS)
    </para>
   </listitem>
  </itemizedlist>

  <para>
   By default, DRBD uses the TCP ports <literal>7788</literal> and higher
   for communication between DRBD nodes. Make sure that your firewall does
   not prevent communication on the used ports.
  </para>

  <para>
   You must set up the DRBD devices before creating file systems on them.
   Everything pertaining to user data should be done solely via the
   <filename>/dev/drbd<replaceable>N</replaceable></filename> device and
   not on the raw device, as DRBD uses the last part of the raw device for
   metadata. Using the raw device causes inconsistent data.
  </para>

  <para>
   With udev integration, you can also get symbolic links in the form
   <filename>/dev/drbd/by-res/<replaceable>RESOURCES</replaceable></filename>
   which are easier to use and provide safety against remembering the wrong
   minor number of the device.
  </para>

  <para>
   For example, if the raw device is 1024&nbsp;MB in size, the DRBD
   device has only 1023&nbsp;MB available for data, with about
   70&nbsp;KB hidden and reserved for the metadata. Any attempt to access
   the remaining kilobytes via raw disks fails because
   it is not available for user data.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-drbd-install">
  <title>Installing DRBD services</title>
  <para>
   Install the &ha; pattern on both &sls; machines in your networked
   cluster as described in <xref linkend="part-install"/>. Installing
   the pattern also installs the DRBD program files.
  </para>

  <para>
   If you do not need the complete cluster stack but only want to use DRBD,
   install the packages <package>drbd</package>,
    <package>drbd-kmp-<replaceable>FLAVOR</replaceable></package>,
    <package>drbd-utils</package>, and <package>yast2-drbd</package>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-drbd-configure">
  <title>Setting up DRBD service</title>
  <note>
   <title>Adjustments needed</title>
   <para>
    The following procedure uses the server names &node1; and &node2;,
    and the DRBD resource name <literal>r0</literal>. It sets up
    &node1; as the primary node and <filename>/dev/disk/by-id/example-disk1</filename> for
    storage. Make sure to modify the instructions to use your own nodes and
    file names.
   </para>
  </note>

  <para>
   The following sections assumes you have two nodes, &node1;
   and &node2;, and that they should use the TCP port <literal>7788</literal>.
   Make sure this port is open in your firewall.
  </para>

  <procedure>
    <step>
      <para>Prepare your system:</para>
      <substeps>
        <step>
          <para>Make sure the block devices in your Linux nodes are ready
            and partitioned (if needed).</para>
        </step>
        <step>
          <para>
            If your disk already contains a file system that you do not need
            anymore, destroy the file system structure with the following
            command:
          </para>
          <screen>&prompt.root;<command>dd if=/dev/zero of=<replaceable>YOUR_DEVICE</replaceable> count=16 bs=1M</command></screen>
          <para>If you have more file systems to destroy, repeat this step on
           all devices you want to include into your DRBD setup.</para>
        </step>
          <step>
            <para>If the cluster is already using DRBD, put your cluster
              in maintenance mode: </para>
            <screen>&prompt.root;<command>crm maintenance on</command></screen>
            <para> If you skip this step when your cluster already uses
              DRBD, a syntax error in the live configuration leads
              to a service shutdown. </para>
            <para>As an alternative, you can also use
                <command>drbdadm</command>
              <option>-c <replaceable>FILE</replaceable></option> to
              test a configuration file.</para>
          </step>
      </substeps>
    </step>
    <step>
      <para>Configure DRBD by choosing your method:</para>
      <itemizedlist>
        <listitem>
          <para><xref linkend="sec-ha-drbd-configure-manually"/></para>
        </listitem>
        <listitem>
          <para><xref linkend="sec-ha-drbd-configure-yast"/></para>
        </listitem>
      </itemizedlist>
    </step>
    <step>
      <para>
        If you have configured &csync; (which should be the default), the
        DRBD configuration files are already included in the list of files that
        need to be synchronized. To synchronize them, run the following command:
      </para>
      <screen>&prompt.root;<command>csync2 -xv</command></screen>
      <para>
        If you do not have &csync; (or do not want to use it), copy the DRBD
        configuration files manually to the other node:
      </para>
      <screen>&prompt.root;<command>scp /etc/drbd.conf &node2;:/etc/</command>
&prompt.root;<command>scp /etc/drbd.d/* &node2;:/etc/drbd.d/</command></screen>
    </step>
    <step>
      <para>Perform the initial synchronization (see <xref linkend="sec-ha-drbd-configure-init"/>).</para>
    </step>
    <step>
      <para>
        Reset the cluster's maintenance mode flag:
      </para>
      <screen>&prompt.root;<command>crm maintenance off</command></screen>
    </step>
  </procedure>

  <sect2 xml:id="sec-ha-drbd-configure-manually">
    <title>Configuring DRBD manually</title>
    <note>
     <title>Restricted support for <quote>auto promote</quote> feature</title>
     <para>
      The DRBD9 <quote>auto-promote</quote> feature can automatically promote a
      resource to the primary role when one of its devices is mounted or opened
      for writing.
     </para>
     <para>
      The auto promote feature has currently restricted support.
      &drbd-restricted-support;
     </para>
    </note>
    <para>
     To set up DRBD manually, proceed as follows:
    </para>

  <procedure xml:id="pro-drbd-configure">
   <title>Manually configuring DRBD</title>
    <para>Beginning with DRBD version 8.3, the former configuration file is
      split into separate files, located under the directory
      <filename>/etc/drbd.d/</filename>.</para>
   <step>
      <para>
       Open the file <filename>/etc/drbd.d/global_common.conf</filename>. It
       already contains global, pre-defined values. Go to the
       <literal>startup</literal> section and insert these lines:
      </para>
<screen>startup {
    # wfc-timeout degr-wfc-timeout outdated-wfc-timeout
    # wait-after-sb;
    wfc-timeout 100;
    degr-wfc-timeout 120;
}</screen>
      <para>
       These options are used to reduce the timeouts when booting, see
       <link xlink:href="https://docs.linbit.com/docs/users-guide-9.0/#ch-configure"/>
       for more details.
      </para>
     </step>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/r0.res</filename>. Change the
       lines according to your situation and save it:
      </para>
<screen>resource r0 { <co xml:id="co-drbd-config-r0"/>
  device /dev/drbd0; <co xml:id="co-drbd-config-device"/>
  disk /dev/disk/by-id/example-disk1; <co xml:id="co-drbd-config-disk"/>
  meta-disk internal; <co xml:id="co-drbd-config-meta-disk"/>
  on &node1; { <co xml:id="co-drbd-config-resname"/>
    address  &subnetI;.10:7788; <co xml:id="co-drbd-config-address"/>
    node-id 0; <co xml:id="co-drbd-config-node-id"/>
  }
  on &node2; { <xref linkend="co-drbd-config-resname" xrefstyle="selec:nopage"/>
    address &subnetI;.11:7788; <xref linkend="co-drbd-config-address" xrefstyle="selec:nopage"/>
    node-id 1; <xref linkend="co-drbd-config-node-id"/>
  }
  disk {
    resync-rate 10M; <co xml:id="co-drbd-config-syncer-rate"/>
  }
  connection-mesh { <co xml:id="co-drbd-config-connection-mesh"/>
    hosts &node1; &node2;;
  }
  net {
    protocol  C; <co xml:id="co-drbd-config-protocol"/>
    fencing resource-and-stonith; <co xml:id="co-drbd-config-fencing-policy"/>
   }
  handlers { <co xml:id="co-drbd-config-fencing-handlers"/>
    fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
    after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
   }
}</screen>
      <calloutlist>
       <callout arearefs="co-drbd-config-r0">
        <para>
         A DRBD resource name that decribes the associated service.
         For example, <systemitem>nfs</systemitem>,
         <systemitem>http</systemitem>, <systemitem>mysql_0</systemitem>,
         <systemitem>postgres_wal</systemitem>, etc.
         In this example, a more general name <literal>r0</literal> is used.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-device">
        <para>
         The device name for DRBD and its minor number.
        </para>
        <para>
         In the example above, the minor number 0 is used for DRBD. The udev
         integration scripts give you a symbolic link
         <filename>/dev/drbd/by-res/nfs/0</filename>. Alternatively, omit
         the device node name in the configuration and use the following
         line instead:
        </para>
        <para>
         <literal>drbd0 minor 0</literal> (<literal>/dev/</literal> is
         optional) or <literal>/dev/drbd0</literal>
        </para>
       </callout>
       <callout arearefs="co-drbd-config-disk">
        <para>
         The raw device that is replicated between nodes. Note, in this
         example the devices are the <emphasis>same</emphasis> on both nodes.
         If you need different devices, move the <literal>disk</literal>
          parameter into the <literal>on</literal> host.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-meta-disk">
        <para>
         The meta-disk parameter usually contains the value
         <literal>internal</literal>, but it is possible to specify an
         explicit device to hold the metadata. See
         <link xlink:href="https://docs.linbit.com/docs/users-guide-9.0/#s-metadata"/>
         for more information.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-resname">
        <para>
         The <literal>on</literal> section states which host this
         configuration statement applies to.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-address">
        <para>
         The IP address and port number of the respective node. Each
         resource needs an individual port, usually starting with
         <literal>7788</literal>. Both ports must be the same for a
          DRBD resource.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-node-id">
        &drbd-node-id;
       </callout>
       <callout arearefs="co-drbd-config-syncer-rate">
        <para>
         The synchronization rate. Set it to one third of the lower of the
         disk- and network bandwidth. It only limits the resynchronization,
         not the replication.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-connection-mesh">
        &drbd-connection-mesh;
       </callout>
       <callout arearefs="co-drbd-config-protocol">
        <para>The protocol to use for this connection. Protocol <literal>C</literal>
         provides better data availability and does not consider a write to be
         complete until it has reached all local and remote disks.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-fencing-policy">
        <para>
         Specifies the fencing policy. For clusters with a &stonith; device
         configured, use <literal>resource-and-stonith</literal>.
        </para>
       </callout>
       <callout arearefs="co-drbd-config-fencing-handlers">
        <para>
         Enables resource-level fencing. If the DRBD replication link
         becomes disconnected, &pace; tries to promote the DRBD resource
         to another node. For more information, see <xref linkend="sec-ha-drbd-fencing"/>.
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
      Check the syntax of your configuration files. If the following
      command returns an error, verify your files:
      </para>
      <screen>&prompt.root;<command>drbdadm dump all</command></screen>
     </step>
    <step>
      <para>Continue with <xref linkend="sec-ha-drbd-configure-init"/>.</para>
    </step>
  </procedure>
  </sect2>
  <sect2 xml:id="sec-ha-drbd-configure-yast">
    <title>Configuring DRBD with &yast;</title>
    <para>&yast; can be used to start with an initial setup of DRBD.
        After you have created your DRBD setup, you can fine-tune the
        generated files manually.
    </para>
    <para>
        However, when you have changed the configuration files,
        do not use the &yast; DRBD module anymore. The DRBD module supports
        only a limited set of basic configuration. If you use it again,
        the module might not show your changes.
    </para>
    <para>
      To set up DRBD with &yast;, proceed as follows:</para>
  <procedure xml:id="pro-drbd-configure-yast">
   <title>Using &yast; to configure DRBD</title>
   <step>
    <para>
     Start &yast; and select the configuration module <menuchoice>
     <guimenu>High Availability</guimenu> <guimenu>DRBD</guimenu>
     </menuchoice>. If you already have a DRBD configuration, &yast;
     warns you. &yast; changes your configuration and saves your
     old DRBD configuration files as <filename>*.YaSTsave</filename>.
    </para>
   </step>
   <step>
    <para>
     Leave the booting flag in <menuchoice><guimenu>Start-up
     Configuration</guimenu><guimenu>Booting</guimenu></menuchoice> as it is
     (by default it is <literal>off</literal>); do not change that as
     &pace; manages this service.
    </para>
   </step>
   <step>
     <para>If you have a firewall running, enable <guimenu>Open Port in
      Firewall</guimenu>.</para>
   </step>
   <step>
    <para>Go to the <guimenu>Resource Configuration</guimenu> entry.
      Select <guimenu>Add</guimenu> to create a new resource (see
      <xref linkend="fig-ha-drbd-yast-resconfig"/>).
    </para>
    <figure xml:id="fig-ha-drbd-yast-resconfig">
     <title>Resource configuration</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast_drbd-resconfig.png" width="90%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast_drbd-resconfig.png" width="90%"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
      The following parameters need to be set:
    </para>

    <variablelist>
      <varlistentry>
        <term><guimenu>Resource Name</guimenu></term>
        <listitem>
          <para>The name of the DRBD resource (mandatory)</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Name</guimenu></term>
        <listitem>
          <para>The host name of the relevant node</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Address:Port</guimenu></term>
        <listitem>
          <para>
          The IP address and port number (default
           <systemitem>7788</systemitem>) for the respective
          node
         </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Device</guimenu></term>
        <listitem>
          <para>
          The block device path that is used to access the replicated data.
          If the device contains a minor number, the associated block device
          is usually named <filename>/dev/drbdX</filename>, where
          <replaceable>X</replaceable> is the device minor number. If the
          device does not contain a minor number, make sure to add
          <literal>minor 0</literal> after the device name.
         </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Disk</guimenu></term>
        <listitem>
          <para>
          The raw device that is replicated between both nodes. If you use
          LVM, insert your LVM device name.
         </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><guimenu>Meta-disk</guimenu></term>
        <listitem>
          <para>
          The <guimenu>Meta-disk</guimenu> is either set to the value
          <literal>internal</literal> or specifies an explicit device
          extended by an index to hold the metadata needed by DRBD.
         </para>
         <para>
          A real device may also be used for multiple DRBD resources. For
          example, if your <guimenu>Meta-Disk</guimenu> is
          <filename>/dev/disk/by-id/example-disk6[0]</filename> for the first resource, you may
          use <filename>/dev/disk/by-id/example-disk6[1]</filename> for the second resource.
          However, there must be at least 128&nbsp;MB space for each resource
          available on this disk. The fixed metadata size limits the maximum
          data size that you can replicate.
         </para>
        </listitem>
      </varlistentry>
    </variablelist>

    <para>
     All these options are explained in the examples in the
     <filename>/usr/share/doc/packages/drbd/drbd.conf</filename> file and in
     the man page of <command>drbd.conf(5)</command>.
    </para>
   </step>
   <step>
    <para>Click <guimenu>Save</guimenu>.</para>
   </step>
   <step>
    <para>Click <guimenu>Add</guimenu> to enter the second DRBD resource
     and finish with <guimenu>Save</guimenu>.
    </para>
   </step>
   <step>
     <para>Close the resource configuration with <guimenu>Ok</guimenu>
      and <guimenu>Finish</guimenu>.
     </para>
   </step>
   <step>
     <para>If you use LVM with DRBD, it is necessary to change certain options
       in the LVM configuration file (see the <guimenu>LVM
       Configuration</guimenu> entry). This change can be done by the
       &yast; DRBD module automatically.
     </para>
     <para>
       The disk name of localhost for the DRBD resource and the default filter
       will be rejected in the LVM filter. Only <filename>/dev/drbd</filename>
       can be scanned for an LVM device.</para>
     <para>
       For example, if <filename>/dev/disk/by-id/example-disk1</filename> is used as a DRBD disk,
       the device name will be inserted as the first entry in the LVM filter.
       To change the filter manually, click the
       <guimenu>Modify LVM Device Filter Automatically</guimenu> check box.
     </para>
   </step>
   <step>
     <para>Save your changes with <guimenu>Finish</guimenu>.</para>
   </step>
    <step>
      <para>Continue with <xref linkend="sec-ha-drbd-configure-init"/>.</para>
    </step>
  </procedure>
  </sect2>
  <sect2 xml:id="sec-ha-drbd-configure-init">
    <title>Initializing and formatting DRBD resource</title>
    <para>After you have prepared your system and configured DRBD,
      initialize your disk for the first time:
    </para>
    <procedure>
        <step>
          <para>
           On <emphasis>both</emphasis> nodes (&node1; and &node2;),
           initialize the metadata storage:
          </para>
          <screen>&prompt.root;<command>drbdadm create-md r0</command>
&prompt.root;<command>drbdadm up r0</command><!--
&prompt.root;<command>systemctl start drbd</command>--></screen>
        </step>
        <step>
         <para>
          To shorten the initial resynchronization of your DRBD resource
          check the following:
         </para>
         <itemizedlist>
          <listitem>
           <para>
            If the DRBD devices on all nodes have the same data (for example,
            by destroying the file system structure with the
            <command>dd</command> command as shown in
            <xref linkend="sec-ha-drbd-configure"/>), then skip the initial
            resynchronization with the following command (on both nodes):
           </para>
           <screen>&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap r0/0</command></screen>
           <para>The state should be <literal>Secondary/Secondary UpToDate/UpToDate</literal></para>
          </listitem>
          <listitem>
           <para>
            Otherwise, proceed with the next step.
           </para>
          </listitem>
         </itemizedlist>
        </step>
        <step>
         <para>
          On the primary node &node1;, start the resynchronization process:
         </para>
         <screen>&prompt.root;<command>drbdadm primary --force r0</command></screen>
        </step>
        <step>
         <para> Check the status with: </para>
         <screen>&prompt.root;<command>drbdadm status r0</command>
r0 role:Primary
  disk:UpToDate
  &node2; role:Secondary
  peer-disk:UpToDate</screen>
        </step>
        <step>
          <para> Create your file system on top of your DRBD device, for
            example: </para>
          <screen>&prompt.root;<command>mkfs.ext3 /dev/drbd0</command></screen>
        </step>
        <step>
          <para> Mount the file system and use it: </para>
          <screen>&prompt.root;<command>mount /dev/drbd0 /mnt/</command></screen>
        </step>
    </procedure>
  </sect2>
  <sect2 xml:id="sec-ha-drbd-configure-cluster-resource">
   <title>Creating cluster resources for DRBD</title>
   <para>
    After you have initialized your DRBD device, create a cluster resource to manage
    the DRBD device, and a promotable clone to allow this resource to run on both nodes:
   </para>
   <procedure xml:id="pro-ha-drbd-configure-cluster-resource">
    <title>Creating cluster resources for DRBD</title>
    <step>
     <para>
      Start the <command>crm</command> interactive shell:
     </para>
<screen>&prompt.root;<command>crm configure</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the DRBD resource <literal>r0</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive drbd-r0 ocf:linbit:drbd \
  params drbd_resource="r0" \
  op monitor interval=15 role=Promoted \
  op monitor interval=30 role=Unpromoted</command></screen>
    </step>
    <step>
     <para>
      Create a promotable clone for the <literal>drbd-r0</literal> primitive:
     </para>
<screen>&prompt.crm.conf;<command>clone cl-drbd-r0 drbd-r0 \
  meta promotable="true" promoted-max="1" promoted-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true" interleave=true</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-resource-stacking">
  <title>Creating a stacked DRBD device</title>
  <para>
   A stacked DRBD device contains two other devices of which at least one
   device is also a DRBD resource. In other words, DRBD adds an additional
   node on top of an existing DRBD resource (see <xref
    linkend="fig-ha-drbd-resource-stacking"/>). Such a replication setup
   can be used for backup and disaster recovery purposes.
  </para>
 <!--taroth 2017-09-21: Please remove gradient blend in SVG version
  of this graphic as it breaks the graphic when using FOP. For now,
  used PNG instead-->
  <figure xml:id="fig-ha-drbd-resource-stacking">
   <title>Resource stacking</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata width="80%" fileref="ha_stacked_drbd.png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata width="50%" fileref="ha_stacked_drbd.png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Three-way replication uses asynchronous (DRBD protocol A) and
   synchronous replication (DRBD protocol C). The asynchronous part is used for
   the stacked resource whereas the synchronous part is used for the backup.
  </para>

  <para>
   Your production environment uses the stacked device. For example,
   if you have a DRBD device <filename>/dev/drbd0</filename> and a stacked
   device <filename>/dev/drbd10</filename> on top, the file system
   is created on <filename>/dev/drbd10</filename>. See <xref
    linkend="exa-ha-drbd-stacked-drbd"/> for more details.
  </para>

  <example xml:id="exa-ha-drbd-stacked-drbd">
   <title>Configuration of a three-node stacked DRBD resource</title>
   <screen># /etc/drbd.d/r0.res
resource r0 {
  protocol C;
  device    /dev/drbd0;
  disk      /dev/disk/by-id/example-disk1;
  meta-disk internal;

  on &cluster1;-&node1; {
    address    &subnetI;.1:7900;<!-- 192.168.122.57 -->
  }

  on &cluster1;-&node2; {
    address    &subnetI;.2:7900;<!-- 192.168.122.29 -->
  }
}

resource r0-U {
  protocol A;
  device     /dev/drbd10;

  stacked-on-top-of r0 {
    address    &subnetII;.1:7910;<!-- 10.0.0.1: -->
  }

  on &cluster2;-&node3; {
    disk       /dev/disk/by-id/example-disk10;
    address    &subnetII;.2:7910; # Public IP of the backup node<!-- 10.0.0.2 -->
    meta-disk  internal;
  }
}</screen>
  </example>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-fencing">
  <title>Using resource-level fencing with &stonith;</title>
  <para>
   When a DRBD replication link becomes interrupted, &pace; tries to promote
   the DRBD resource to another node. To prevent &pace; from starting a service
   with outdated data, enable resource-level fencing in the DRBD configuration
   file.
  </para>
   <para>
    The fencing policy can have different values (see man page <command>drbdsetup</command> and the
     <option>--fencing</option> option).
    As a &productname; cluster is normally used with a &stonith; device, the value
    <constant>resource-and-stonith</constant> is used in
    <xref linkend="ex-ha-drbd-fencing"/>.
  </para>

  <example xml:id="ex-ha-drbd-fencing">
   <title>Configuration of DRBD with resource-level fencing using the Cluster
    Information Base (CIB)</title>
   <screen>resource <replaceable>RESOURCE</replaceable> {
  net {
    fencing resource-and-stonith;
    # ...
  }
  handlers {
    fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
    after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
    # ...
  }
  ...
}</screen>
  </example>
  <para>If the DRBD replication link becomes disconnected, DRBD does the
  following:</para>
  <orderedlist>
   <listitem>
    <para>DRBD calls the <command>crm-fence-peer.9.sh</command> script.</para>
   </listitem>
   <listitem>
    <para>The script contacts the cluster manager.
    </para>
   </listitem>
   <listitem>
    <para>The script determines the &pace; resource associated with this
     DRBD resource.</para>
   </listitem>
   <listitem>
    <para>The script ensures that the DRBD resource no longer gets
    promoted to any other node. It stays on the currently active one.</para>
   </listitem>
   <listitem>
    <para>If the replication link becomes connected again and DRBD
    completes its synchronization process, then the constraint is removed.
    The cluster manager is now free to promote the resource.
    </para>
   </listitem>
  </orderedlist>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-test">
  <title>Testing the DRBD service</title>

  <para>
   If the install and configuration procedures worked as expected, you are
   ready to run a basic test of the DRBD functionality. This test also helps
   with understanding how the software works.
  </para>

  <procedure xml:id="pro-drbd-test">
   <step>
    <para>
     Test the DRBD service on &node1;.
    </para>
    <substeps>
     <step>
      <para>
       Open a terminal console, then log in as
       <systemitem>root</systemitem>.
      </para>
     </step>
     <step>
      <para>
       Create a mount point on &node1;, such as
       <filename>/srv/r0</filename>:
      </para>
<screen>&prompt.root;<command>mkdir -p /srv/r0</command></screen>
     </step>
     <step>
      <para>
       Mount the <command>drbd</command> device:
      </para>
<screen>&prompt.root;<command>mount -o rw /dev/drbd0 /srv/r0</command></screen>
     </step>
     <step>
      <para>
       Create a file from the primary node:
      </para>
<screen>&prompt.root;<command>touch /srv/r0/from_&node1;</command></screen>
     </step>
     <step>
      <para>
       Unmount the disk on &node1;:
      </para>
<screen>&prompt.root;<command>umount /srv/r0</command></screen>
     </step>
     <step>
      <para>
       Downgrade the DRBD service on &node1; by typing the following
       command on &node1;:
      </para>
<screen>&prompt.root;<command>drbdadm secondary r0</command></screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Test the DRBD service on &node2;.
    </para>
    <substeps>
     <step>
      <para>
       Open a terminal console, then log in as <systemitem>root</systemitem>
       on &node2;.
      </para>
     </step>
     <step>
      <para>
       On &node2;, promote the DRBD service to primary:
      </para>
<screen>&prompt.root;<command>drbdadm primary r0</command></screen>
     </step>
     <step>
      <para>
       On &node2;, check to see if &node2; is primary:
      </para>
<screen>&prompt.root;<command>drbdadm status r0</command></screen>
     </step>
     <step>
      <para> On &node2;, create a mount point such as
        <filename>/srv/r0</filename>: </para>
<screen>&prompt.root;<command>mkdir /srv/r0</command></screen>
     </step>
     <step>
      <para>
       On &node2;, mount the DRBD device:
      </para>
<screen>&prompt.root;<command>mount -o rw /dev/drbd0 /srv/r0</command></screen>
     </step>
     <step>
      <para>
       Verify that the file you created on &node1; exists:
      </para>
<screen>&prompt.root;<command>ls /srv/r0/from_&node1;</command></screen>
      <para> The <filename>/srv/r0/from_&node1;</filename> file should be
       listed. </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     If the service is working on both nodes, the DRBD setup is complete.
    </para>
   </step>
   <step>
    <para>
     Set up &node1; as the primary again.
    </para>
    <substeps performance="required">
     <step>
      <para>
       Dismount the disk on &node2; by typing the following command on
       &node2;:
      </para>
<screen>&prompt.root;<command>umount /srv/r0</command></screen>
     </step>
     <step>
      <para>
       Downgrade the DRBD service on &node2; by typing the following
       command on &node2;:
      </para>
<screen>&prompt.root;<command>drbdadm secondary r0</command></screen>
     </step>
     <step>
      <para>
       On &node1;, promote the DRBD service to primary:
      </para>
<screen>&prompt.root;<command>drbdadm primary r0</command></screen>
     </step>
     <step>
      <para>
       On &node1;, check to see if &node1; is primary:
      </para>
<screen>&prompt.root;<command>drbdadm status r0</command></screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To get the service to automatically start and fail over if the server
     has a problem, you can set up DRBD as a high availability service with
     Pacemaker/&corosync;. For information about installing and
     configuring for &sle; &productnumber; see
     <xref linkend="part-config"/>.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-monitor">
  <title>Monitoring DRBD devices</title>
  <para>DRBD comes with the utility <command>drbdmon</command> which offers
   real time monitoring. It shows all the configured resources and their
   problems.
  </para>
  <figure>
   <title>Showing a good connection by <command>drbdmon</command></title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="drbd-drbdmon-ok.png" width="70%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>In case of problems, <command>drbdadm</command> shows an error message:
  </para>
  <figure>
   <title>Showing a bad connection by <command>drbdmon</command></title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="drbd-drbdmon-bad.png" width="70%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

 <sect1 xml:id="sec-ha-drbd-tuning">
  <title>Tuning DRBD</title>

  <para>
   There are several ways to tune DRBD:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Use an external disk for your metadata. This might help, at the cost of
     maintenance ease.
    </para>
   </listitem>
   <listitem>
    <para>
     Tune your network connection, by changing the receive and send buffer
     settings via <command>sysctl</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Change the <systemitem>max-buffers</systemitem>,
     <systemitem>max-epoch-size</systemitem> or both in the DRBD
     configuration.
    </para>
   </listitem>
   <listitem>
    <para>
     Increase the <systemitem>al-extents</systemitem> value, depending on
     your IO patterns.
    </para>
   </listitem>
   <listitem>
    <para>
     If you have a hardware RAID controller with a BBU (<emphasis>Battery
     Backup Unit</emphasis>), you might benefit from setting
     <systemitem>no-disk-flushes</systemitem>,
     <systemitem>no-disk-barrier</systemitem> and/or
     <systemitem>no-md-flushes</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     Enable read-balancing depending on your workload. See
     <link xlink:href="https://www.linbit.com/en/read-balancing/"/> for
     more details.
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="sec-ha-drbd-trouble">
  <title>Troubleshooting DRBD</title>

  <para>
   The DRBD setup involves many components and problems may arise
   from different sources. The following sections cover several common
   scenarios and recommend solutions.
  </para>

  <sect2 xml:id="sec-ha-drbd-trouble-config">
   <title>Configuration</title>
   <para>
    If the initial DRBD setup does not work as expected, there may be
    something wrong with your configuration.
   </para>
   <para>
    To get information about the configuration:
   </para>
   <procedure>
    <step>
     <para>
      Open a terminal console, then log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Test the configuration file by running <command>drbdadm</command> with
      the <command>-d</command> option. Enter the following command:
     </para>
<screen>&prompt.root;<command>drbdadm -d adjust r0</command></screen>
     <para>
      In a dry run of the <command>adjust</command> option,
      <command>drbdadm</command> compares the actual configuration of the
      DRBD resource with your DRBD configuration file, but it does not
      execute the calls. Review the output to make sure you know the source
      and cause of any errors.
     </para>
    </step>
    <step>
     <para>
      If there are errors in the <filename>/etc/drbd.d/*</filename> and
      <filename>drbd.conf</filename> files, correct them before continuing.
     </para>
    </step>
    <step>
     <para>
      If the partitions and settings are correct, run
      <command>drbdadm</command> again without the <command>-d</command>
      option.
     </para>
<screen>&prompt.root;<command>drbdadm adjust r0</command></screen>
     <para>
      This applies the configuration file to the DRBD resource.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-drbd-hostnames">
   <title>Host names</title>
   <para>
    For DRBD, host names are case-sensitive (<systemitem>Node0</systemitem>
    would be a different host than <systemitem>node0</systemitem>), and
    compared to the host name as stored in the Kernel (see the
    <command>uname -n</command> output).
   </para>
   <para>
    If you have several network devices and want to use a dedicated network
    device, the host name might not resolve to the used IP address. In
    this case, use the parameter <literal>disable-ip-verification</literal>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-drbd-port">
   <title>TCP port 7788</title>
   <para>
    If your system cannot connect to the peer, this might be a problem with
    your local firewall. By default, DRBD uses the TCP port
    <literal>7788</literal> to access the other node. Make sure that this
    port is accessible on both nodes.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-drbd-trouble-broken">
   <title>DRBD devices broken after reboot</title>
   <para>
    In cases when DRBD does not know which of the real devices holds the
    latest data, it changes to a split-brain condition. In this case, the
    respective DRBD subsystems come up as secondary and do not connect to
    each other. In this case, the following message can be found in the
    logging data:
   </para>
<screen>Split-Brain detected, dropping connection!</screen>
   <para>
    To resolve this situation, enter the following commands on the node which has
    data to be discarded:
   </para>
<screen>&prompt.root;<command>drbdadm secondary r0</command></screen>
   <para>
    If the state is in <literal>WFconnection</literal>, disconnect first:
   </para>
   <screen>&prompt.root;<command>drbdadm disconnect r0</command></screen>
   <para>
    On the node which has the latest data enter the following:
   </para>
<screen>&prompt.root;<command>drbdadm connect  --discard-my-data r0</command></screen>
   <para>
    That resolves the issue by overwriting one node's data with the peer's
    data, therefore getting a consistent view on both nodes.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ha-drbd-more">
  <title>For more information</title>

  <para>
   The following open source resources are available for DRBD:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The project home page <link xlink:href="https://www.drbd.org"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     See <xref linkend="article-nfs-storage"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The following man pages for DRBD are available in the distribution:
     <command>drbd(8)</command>, <command>drbdmeta(8)</command>,
     <command>drbdsetup(8)</command>, <command>drbdadm(8)</command>,
     <command>drbd.conf(5)</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Find a commented example configuration for DRBD at
     <filename>/usr/share/doc/packages/drbd-utils/drbd.conf.example</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Furthermore, for easier storage administration across your cluster, see
     the recent announcement about the <citetitle>DRBD-Manager</citetitle>
     at <link xlink:href="https://www.linbit.com/en/drbd-manager/"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
