<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-config-basics">
 <title>Configuration and administration basics</title>
 <info>
      <abstract>
        <para>
    The main purpose of an HA cluster is to manage user services. Typical
    examples of user services are an Apache Web server or a database. From
    the user's point of view, the services do something specific when
    ordered to do so. To the cluster, however, they are only resources which
    may be started or stopped&mdash;the nature of the service is
    irrelevant to the cluster.
   </para>
        <para>
    This chapter introduces some basic concepts you need to know
    when administering your cluster. The following
    chapters show you how to execute the main configuration and
    administration tasks with each of the management tools &productname;
    provides.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>

  <sect1 xml:id="sec-ha-config-basics-scenarios">
   <title>Use case scenarios</title>
   <para>Clusters usually fall into one of two categories:</para>
   <itemizedlist>
    <listitem>
     <para>Two-node clusters</para>
    </listitem>
    <listitem>
     <para>Clusters with more than two nodes. This usually means an odd number of nodes.</para>
    </listitem>
   </itemizedlist>
   <para>
    Adding also different topologies, different use cases can be derived.
    The following use cases are the most common:
   </para>

   <variablelist>
    <title></title>
    <varlistentry><!-- 1.1 -->
     <term>Two-node cluster in one location</term>
     <listitem>
      <formalpara>
       <title>Configuration:</title>
       <para>FC SAN or similar shared storage, layer 2 network.</para>
      </formalpara>
      <formalpara>
       <title>Usage scenario:</title>
       <para>Embedded clusters that focus on service high
       availability and not data redundancy for data replication.
       Such a setup is used for radio stations or assembly line controllers,
       for example.
       </para>
      </formalpara>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="vl-2x2node-2locs"><!-- 2.1 -->
     <term>Two-node clusters in two locations (most widely used)</term>
     <listitem>
      <formalpara>
       <title>Configuration:</title>
       <para>Symmetrical stretched cluster, FC SAN, and layer 2 network
        all across two locations.</para>
      </formalpara>
      <formalpara>
       <title>Usage scenario:</title>
       <para>Classic stretched clusters, focus on high availability of services
        and local data redundancy. For databases and enterprise
        resource planning. One of the most popular setups.
       </para>
      </formalpara>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="vl-n-nodes-3locs"><!-- 3.1 -->
     <term>Odd number of nodes in three locations</term>
     <listitem>
      <formalpara>
       <title>Configuration:</title>
       <para>2&times;N+1 nodes, FC SAN across two main locations. Auxiliary
        third site with no FC SAN, but acts as a majority maker.
        Layer 2 network at least across two main locations.
       </para>
      </formalpara>
      <formalpara>
       <title>Usage scenario:</title>
       <para>Classic stretched cluster, focus on high availability of services
        and data redundancy. For example, databases, enterprise resource planning.
       </para>
      </formalpara>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>

  <sect1 xml:id="sec-ha-config-basics-global">
  <title>Quorum determination</title>
  <para>
   Whenever communication fails between one or more nodes and the rest of the
   cluster, a cluster partition occurs. The nodes can only communicate with
   other nodes in the same partition and are unaware of the separated nodes.
   A cluster partition is defined as having quorum (being <quote>quorate</quote>)
   if it has the majority of nodes (or votes).
   How this is achieved is done by <emphasis>quorum calculation</emphasis>.
   Quorum is a requirement for fencing.
   </para>
   <para>
   Quorum is not calculated or determined by &pace;. &corosync; can handle quorum for
   two-node clusters directly without changing the &pace; configuration.
  </para>

  <para>How quorum is calculated is influenced by the following factors:</para>
   <variablelist>
    <varlistentry xml:id="vl-ha-config-basics-global-number-of-cluster-nodes">
     <term>Number of cluster nodes</term>
     <listitem>
         <para>To keep services running, a cluster with more than two nodes
       relies on quorum (majority vote) to resolve cluster partitions.
       Based on the following formula, you can calculate the minimum
       number of operational nodes required for the cluster to function:</para>
       <screen>N â‰¥ C/2 + 1

N = minimum number of operational nodes
C = number of cluster nodes</screen>
      <para>For example, a five-node cluster needs a minimum of three operational
       nodes (or two nodes which can fail). </para>
      <para>
       We strongly recommend to use either a two-node cluster or an odd number
       of cluster nodes.
       Two-node clusters make sense for stretched setups across two sites.
       Clusters with an odd number of nodes can either be built on one single
       site or might be spread across three sites.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Corosync configuration</term>
     <listitem>
      <para>&corosync; is a messaging and membership layer, see
      <xref linkend="sec-ha-config-basics-corosync-2-node"/> and
       <xref linkend="sec-ha-config-basics-corosync-n-node"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

  <sect2 xml:id="sec-ha-config-basics-corosync-2-node">
   <title>&corosync; configuration for two-node clusters</title>
   <para>
    When using the bootstrap scripts, the &corosync; configuration contains
    a <literal>quorum</literal> section with the following options:
   </para>
   <example xml:id="ex-ha-config-basics-corosync-quorum">
    <title>Excerpt of &corosync; configuration for a two-node cluster</title>
    <screen>quorum {
   # Enable and configure quorum subsystem (default: off)
   # see also corosync.conf.5 and votequorum.5
   provider: corosync_votequorum
   expected_votes: 2
   two_node: 1
}</screen>
   </example>
   <para>
    By default, when <literal>two_node: 1</literal> is set, the
    <literal>wait_for_all</literal> option is automatically enabled.
    If <literal>wait_for_all</literal> is not enabled, the cluster should be
    started on both nodes in parallel. Otherwise, the first node performs
    a startup-fencing on the missing second node.
   </para>
  </sect2>
  <sect2 xml:id="sec-ha-config-basics-corosync-n-node">
   <title>&corosync; configuration for n-node clusters</title>
   <para> When not using a two-node cluster, we strongly recommend an odd
    number of nodes for your N-node cluster. For quorum
    configuration, you have the following options: </para>
   <itemizedlist>
    <listitem>
     <para>Adding additional nodes with the <command>crm cluster join</command>
     command, or</para>
    </listitem>
    <listitem>
     <para>Adapting the &corosync; configuration manually.</para>
    </listitem>
   </itemizedlist>
   <para>
    If you adjust <filename>/etc/corosync/corosync.conf</filename> manually,
    use the following settings:
   </para>
   <example>
    <title>Excerpt of &corosync; configuration for an n-node cluster</title>
    <screen>quorum {
   provider: corosync_votequorum <co xml:id="co-corosync-quorum-n-node-corosync-votequorum"/>
   expected_votes: <replaceable>N</replaceable> <co xml:id="co-corosync-quorum-n-node-expected-votes"/>
   wait_for_all: 1 <co xml:id="co-corosync-quorum-n-node-wait-for-all"/>
}</screen>
    <calloutlist>
     <callout arearefs="co-corosync-quorum-n-node-corosync-votequorum">
      <para>Use the quorum service from &corosync;</para>
     </callout>
     <callout arearefs="co-corosync-quorum-n-node-expected-votes">
      <para>The number of votes to expect. This parameter can either be
       provided inside the <literal>quorum</literal> section, or is
       automatically calculated when the <literal>nodelist</literal>
       section is available.</para>
     </callout>
     <callout arearefs="co-corosync-quorum-n-node-wait-for-all">
      <para>
       Enables the wait for all (WFA) feature.
       When WFA is enabled, the cluster will be quorate for the first time
       only after all nodes have become visible.
       To avoid some startup race conditions, setting <option>wait_for_all</option>
       to <literal>1</literal> may help.
       For example, in a five-node cluster every node has one vote and thus,
       <option>expected_votes</option> is set to <literal>5</literal>.
       When three or more nodes are visible to each other, the cluster
       partition becomes quorate and can start operating.
      </para>
     </callout>
    </calloutlist>
   </example>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-config-basics-global-options">
  <title>Global cluster options</title>
  <para> Global cluster options control how the cluster behaves when
   confronted with certain situations. They are grouped into sets and can be
   viewed and modified with the cluster management tools like &hawk2; and
   the <command>crm</command> shell. </para>
  <para> The predefined values can usually be kept. However, to make key
   functions of your cluster work as expected, you might need to adjust the
   following parameters after basic cluster setup:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <xref linkend="sec-ha-config-basics-global-quorum" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec-ha-config-basics-global-stonith" xrefstyle="select:title"/>
    </para>
   </listitem>
  </itemizedlist>

 <sect2 xml:id="sec-ha-config-basics-global-quorum">
  <title>Global option <literal>no-quorum-policy</literal></title>
  <para>
   This global option defines what to do when a cluster partition does not
   have quorum (no majority of nodes is part of the partition).
  </para>
  <para>
   The following values are available:
  </para>
  <variablelist>
   <varlistentry>
    <term><literal>ignore</literal>
    </term>
    <listitem>
     <para></para>
     <para>
      Setting <literal>no-quorum-policy</literal> to <literal>ignore</literal> makes
      a cluster partition behave like it has quorum, even if it does not. The cluster
      partition is allowed to issue fencing and continue resource management.
     </para>
     <para>
      On &slsa;&nbsp;11 this was the recommended setting for a two-node cluster.
      Starting with &slsa;&nbsp;12, the value <literal>ignore</literal> is obsolete
      and must not be used.
      Based on configuration and conditions, &corosync; gives cluster nodes
      or a single node <quote>quorum</quote>&mdash;or not.
     </para>
     <para>
     For two-node clusters the only meaningful behavior is to always
     react in case of node loss. The first step should always be
     to try to fence the lost node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>freeze</literal>
    </term>
    <listitem>
     <para>
      If quorum is lost, the cluster partition freezes. Resource management
      is continued: running resources are not stopped (but might be
      restarted in response to monitor events), but no further resources
      are started within the affected partition.
     </para>
     <para>
      This setting is recommended for clusters where certain resources
      depend on communication with other nodes (for example, OCFS2 mounts).
      In this case, the default setting
      <literal>no-quorum-policy=stop</literal> is not useful, as it would
      lead to the following scenario: stopping those resources would not be
      possible while the peer nodes are unreachable. Instead, an attempt to
      stop them would eventually time out and cause a <literal>stop
      failure</literal>, triggering escalated recovery and fencing.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>stop</literal> (default value)</term>
    <listitem>
     <para>
      If quorum is lost, all resources in the affected cluster partition
      are stopped in an orderly fashion.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>suicide</literal>
    </term>
    <listitem>
     <para>
      If quorum is lost, all nodes in the affected cluster partition are
      fenced. This option works only in combination with SBD, see
      <xref linkend="cha-ha-storage-protect"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect2>

 <sect2 xml:id="sec-ha-config-basics-global-stonith">
  <title>Global option <literal>stonith-enabled</literal></title>
  <para>
   This global option defines whether to apply fencing, allowing &stonith;
   devices to shoot failed nodes and nodes with resources that cannot be
   stopped. By default, this global option is set to
   <literal>true</literal>, because for normal cluster operation it is
   necessary to use &stonith; devices. According to the default value,
   the cluster refuses to start any resources if no &stonith;
   resources have been defined.
  </para>
  <para>
   If you need to disable fencing for any reasons, set
   <literal>stonith-enabled</literal> to <literal>false</literal>, but be
   aware that this has impact on the support status for your product.
   Furthermore, with <literal>stonith-enabled="false"</literal>, resources
   like the Distributed Lock Manager (DLM) and all services depending on
   DLM (such as lvmlockd, GFS2, and OCFS2) will fail to start.
  </para>
  <important>
   <title>No support without &stonith;</title>
   <para>
    A cluster without &stonith; is not supported.
   </para>
  </important>
  </sect2>
 </sect1>


 <xi:include href="ha_config_hawk2.xml"/>
 <xi:include href="ha_config_cli.xml"/>


 <sect1 xml:id="sec-ha-config-basics-more">
  <title>For more information</title>

  <variablelist>
   <varlistentry>
    <term><link xlink:href="https://crmsh.github.io/"/>
    </term>
    <listitem>
     <para>
      Home page of the &crmshell; (&crmsh;), the advanced command line
      interface for &ha; cluster management.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="https://crmsh.github.io/documentation"/>
    </term>
    <listitem>
     <para>
      Holds several documents about the &crmshell;, including a
      <citetitle>Getting Started</citetitle> tutorial for basic cluster
      setup with &crmsh; and the comprehensive
      <citetitle>Manual</citetitle> for the &crmshell;. The latter is
      available at <link xlink:href="https://crmsh.github.io/man-2.0/"/>.
      Find the tutorial at
      <link xlink:href="https://crmsh.github.io/start-guide/"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="https://clusterlabs.org/"/>
    </term>
    <listitem>
     <para>
      Home page of Pacemaker, the cluster resource manager shipped with
      &productname;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="https://www.clusterlabs.org/pacemaker/doc/"/>
    </term>
    <listitem>
     <para>
      Holds several comprehensive manuals and some shorter documents
      explaining general concepts. For example:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        &paceex;: Contains comprehensive and detailed information
        for reference.
       </para>
      </listitem>
      <listitem>
       <para>
        <citetitle>Colocation Explained</citetitle>
       </para>
      </listitem>
      <listitem>
       <para>
        <citetitle>Ordering Explained</citetitle>
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
