<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-ha-bootstrap-install" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>Using the bootstrap script</title>
  <info>
    <abstract>
      <para>

      </para>
    </abstract>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker></dm:bugtracker>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  </info>

  <sect1 xml:id="sec-ha-bootstrap-install-overview">
    <!-- Duplicated from xml/article_installation.xml -->
    <title>Overview of the <command>crm cluster init</command> script</title>
    <para>
      The <command>crm cluster init</command> command executes a bootstrap script that defines the
      basic parameters needed for cluster communication, resulting in a running one-node cluster.
      The script checks and configures the following components:
    </para>
    <variablelist>
    <varlistentry>
      <term>NTP</term>
      <listitem>
        <para>
          Checks if NTP is configured to start at boot time. If not, a message appears.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>SSH</term>
      <listitem>
        <para>
          Detects or generates SSH keys for passwordless login between cluster nodes.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&csync;</term>
      <listitem>
        <para>
          Configures &csync; to replicate configuration files across all nodes in a cluster.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&corosync;</term>
      <listitem>
        <para>
          Configures the cluster communication system.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>SBD/watchdog</term>
      <listitem>
        <para>
          Checks if a watchdog exists and asks you whether to configure SBD as the node fencing mechanism.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Virtual floating IP</term>
      <listitem>
        <para>
          Asks you whether to configure a virtual IP address for cluster administration with &hawk2;.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Firewall</term>
      <listitem>
        <para>
          Opens the ports in the firewall that are needed for cluster communication.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Cluster name</term>
      <listitem>
        <para>
          Defines a name for the cluster, by default <systemitem>hacluster</systemitem>. This is
          optional and mostly useful for &geo; clusters. Usually, the cluster name reflects the
          geographical location and makes it easier to distinguish a site inside a &geo; cluster.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&qdevice;/&qnet;</term>
      <listitem>
        <para>
          Asks you whether to configure &qdevice;/&qnet; to participate in quorum decisions.
          We recommend using &qdevice; and &qnet; for clusters with an even number of nodes,
          and especially for two-node clusters.
        </para>
      </listitem>
    </varlistentry>
    </variablelist>
    <note>
      <title>&pace; default settings</title>
      <para>
        The options set by the bootstrap script might not be the same as the &pace;
        default settings. You can check which settings the bootstrap script changed in
        <filename>/var/log/crmsh/crmsh.log</filename>. Any options set during the bootstrap
        process can be modified later with the &yast; cluster module.
      </para>
    </note>
    <note>
      <title>Cluster configuration for different platforms</title>
      <para>
        The <command>crm cluster init</command> script detects the system environment (for example,
        &ms; Azure) and adjusts certain cluster settings based on the profile for that environment.
        For more information, see the file <filename>/etc/crm/profiles.yml</filename>.
      </para>
    </note>
  </sect1>

  <sect1 xml:id="sec-ha-bootstrap-install-first-node">
  <!-- Initially duplicated from xml/article_installation.xml, expansion is WIP -->
    <title>Setting up the first node with <command>crm cluster init</command></title>
    <para>
      Setting up the first node with the <command>crm cluster init</command> script
      requires only a minimum of time and manual intervention.
    </para>
    <para>
      The steps in this procedure show the default option followed by alternative or additional
      options. For a minimal setup with only the default options,
      see <xref linkend="article-installation"/>.
    </para>
    <procedure xml:id="pro-ha-bootstrap-install-first-node">
    <title>Setting up the first node with <command>crm cluster init</command></title>
    <step>
      <para>
        Log in to the first cluster node as &rootuser;, or as a user with <command>sudo</command>
        privileges.
      </para>
    </step>
    <step>
      <para>
      Start the bootstrap script:
      </para>
      <para>
        You can start the script without specifying any options. This prompts you for input for
        some settings, as described in the next steps, and uses &crmsh;'s default values for
        other settings.
      </para>
      <itemizedlist>
        <listitem>
          <para>
            If you logged in as &rootuser;, you can run this command with no additional parameters:
          </para>
<screen>&prompt.root;<command>crm cluster init</command></screen>
        </listitem>
        <listitem>
          <para>
            If you logged in as a <command>sudo</command> user without SSH agent forwarding,
            run this command with <command>sudo</command>:
          </para>
<screen>&prompt.user;<command>sudo crm cluster init</command></screen>
        </listitem>
        <listitem>
          <para>
            If you logged in as a <command>sudo</command> user with SSH agent forwarding enabled,
            you must preserve the environment variable <literal>SSH_AUTH_SOCK</literal>
            and tell the script to use your local SSH keys instead of generating keys on the node:
          </para>
<screen>&prompt.user;<command>sudo --preserve-env=SSH_AUTH_SOCK crm cluster init --use-ssh-agent</command></screen>
        </listitem>
      </itemizedlist>
      <para>
        Alternatively, you can specify additional options as part of the initialization command.
        You can include multiple options in the same command. Some examples are shown below.
        For more options, run <command>crm cluster help init</command>.
      </para>
      <variablelist>
      <varlistentry>
        <term>Cluster name</term>
        <listitem>
          <para>
            The default cluster name is <literal>hacluster</literal>. To choose a different name,
            use the option <option>--name</option> (or <option>-n</option>). For example:
          </para>
<screen>&prompt.root;<command>crm cluster init --name <replaceable>CLUSTERNAME</replaceable></command></screen>
          <para>
            Choose a meaningful name, like the geographical location of the cluster. This is
            especially helpful if you create a &geo; cluster later, as it simplifies the
            identification of a site.
          </para>
        </listitem>
      </varlistentry>
        <varlistentry>
          <term>Multicast</term>
          <listitem>
            <para>
              Unicast is the default transport type for cluster communication. To use multicast
              instead, use the option <option>--multicast</option> (or <option>-U</option>).
              For example:
            </para>
<screen>&prompt.root;<command>crm cluster init --multicast</command></screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>SBD disks</term>
          <listitem>
            <para>
              In a later step, the script asks if you want to set up SBD and prompts you for a disk
              to use. To configure the cluster with multiple SBD disks, use the option
              <option>--sbd-device</option> (or <option>-s</option>) multiple times. For example:
            </para>
<screen>&prompt.root;<command>crm cluster init --sbd-device /dev/disk/by-id/<replaceable>ID1</replaceable> --sbd-device /dev/disk/by-id/<replaceable>ID2</replaceable></command></screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Redundant communication channel</term>
          <listitem>
            <para>
              Supported clusters must have two communication channels. The preferred method is to
              use network device bonding. If you cannot use bonding, you can set up a redundant
              communication channel in &corosync; (also known as a second ring or heartbeat line).
              By default, the script prompts you for a network address for a single ring.
              To configure the cluster with two rings, use the option <option>--interface</option>
              (or <option>-i</option>) twice. For example:
            </para>
<screen>&prompt.root;<command>crm cluster init --interface eth0 --interface eth1</command></screen>
            <note role="compact">
              <para>
                You can also use <option>--multi-heartbeats</option> (or <option>-M</option>) to set
                up a second &corosync; ring . This option uses the first two network interfaces by
                default, whereas <option>-i</option> allows you to specify any two network interfaces.
              </para>
            </note>
          </listitem>
        </varlistentry>
      </variablelist>
    </step>
    <step>
      <para>
      Configure the cluster communication layer (&corosync;):
      </para>
      <substeps>
      <step>
        <para>
        Enter a network address to bind to. By default, the script
        proposes the network address of <systemitem>eth0</systemitem>.
        Alternatively, enter a different network address, for example, the
        address of <literal>bond0</literal>.
        </para>
      </step>
      <step>
        <para>
        Accept the proposed port (<literal>5405</literal>) or enter a different one.
        </para>
      </step>
      <step>
        <para>
          If you started the script with an option that configures a redundant communication channel,
          enter <literal>y</literal> to accept a second heartbeat line, then either accept the
          proposed network address and port or enter different ones.
        </para>
      </step>
      </substeps>
    </step>
    <step>
      <para>
        Choose whether to set up SBD as the node fencing mechanism. If you are using a different
        fencing mechanism or want to set up SBD later, enter <literal>n</literal> to skip this step.
      </para>
      <para>
        If you chose <literal>y</literal>, select the type of SBD to use:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            To use diskless SBD, enter <literal>none</literal>.
          </para>
        </listitem>
        <listitem>
          <para>
            To use disk-based SBD, enter a persistent path to the partition of the block device you
            want to use. The path must be consistent across all nodes in the cluster, for example,
            <filename>/dev/disk/by-id/<replaceable>ID</replaceable></filename>.
          </para>
          <para>
            The script creates a small partition on the device to be used for SBD.
          </para>
        </listitem>
      </itemizedlist>
    </step>
    <step>
      <para>
        Choose whether to configure a virtual IP address for cluster administration with &hawk2;.
        Instead of logging in to an individual cluster node with &hawk2;, you can connect
        to the virtual IP address.
      </para>
      <para>
        If you chose <literal>y</literal>, enter an unused IP address to use for &hawk2;.
      </para>
    </step>
    <step>
      <para>
        Choose whether to configure &qdevice; and &qnet;. If you do not need to use &qdevice; or
        have not set up the &qnet; server yet, enter <literal>n</literal> to skip this step.
        You can set up &qdevice; and &qnet; later if required.
      </para>
      <para>
        If you chose <literal>y</literal>, provide the following information:
      </para>
      <substeps>
        <step>
          <para>
            Enter the host name or IP address of the &qnet; server. The cluster node must have
            SSH access to this server to complete the configuration.
          </para>
          <para>
            For the remaining fields, you can accept the default values or change them as required:
          </para>
        </step>
        <step>
          <para>
            Accept the proposed port (<literal>5403</literal>) or enter a different one.
          </para>
        </step>
        <step>
          <para>
            Choose the algorithm that determines how votes are assigned.
          </para>
        </step>
        <step>
          <para>
            Choose the method to use when a tie-breaker is required.
          </para>
        </step>
        <step>
          <para>
            Choose whether to enable TLS.
          </para>
        </step>
        <step performance="optional">
          <para>
            Enter heuristics commands to affect how votes are determined. To skip this step, leave
            the field blank.
          </para>
        </step>
      </substeps>
    </step>
    </procedure>
    <para>
    The script checks for NTP configuration and a hardware watchdog service. If required,
    it generates the public and private SSH keys used for passwordless SSH access and
    &csync; synchronization and starts the respective services. Finally, the script
    starts the cluster services to bring the cluster online and enables &hawk2;.
    The URL to use for &hawk2; is displayed on the screen.
    </para>
  </sect1>

  <sect1 xml:id="sec-ha-bootstrap-install-hawk2-login">
  <!-- Duplicated from xml/article_installation.xml -->
    <title>Logging in to the &hawk2; web interface</title>
    <para>
      You now have a running one-node cluster. To view its status, proceed as follows:
    </para>
    <procedure xml:id="pro-ha-bootstrap-install-hawk2-login">
      <title>Logging in to the &hawk2; Web interface</title>
      <step>
        <para>
          On any machine, start a Web browser and make sure that JavaScript and cookies are enabled.
        </para>
      </step>
      <step>
        <para>
          As URL, enter the virtual IP address that you configured with the bootstrap script:
        </para>
<screen>https://<replaceable>VIRTUAL_IP</replaceable>:7630/</screen>
        <note>
          <title>Certificate warning</title>
          <para>
            If a certificate warning appears when you try to access the URL for the first time,
            a self-signed certificate is in use. Self-signed certificates are not considered
            trustworthy by default.
          </para>
          <para>
            Ask your cluster operator for the certificate details to verify the certificate.
          </para>
          <para>
            To proceed anyway, you can add an exception in the browser to bypass the warning.
          </para>
        </note>
      </step>
      <step>
        <para>
          On the &hawk2; login screen, enter the <guimenu>Username</guimenu> and
          <guimenu>Password</guimenu> of the user that was created by the bootstrap script
          (user <systemitem class="username">hacluster</systemitem>, password <literal>linux</literal>).
        </para>
        <important>
          <title>Secure password</title>
          <para>
            Replace the default password with a secure one as soon as possible:
          </para>
<screen>&prompt.root;<command>passwd hacluster</command></screen>
        </important>
      </step>
      <step>
        <para>
          Click <guimenu>Log In</guimenu>. The &hawk2; Web interface shows the
          <guimenu>Status</guimenu> screen by default:
        </para>
        <figure xml:id="fig-ha-bootstrap-install-one-node-status">
          <title>Status of the one-node cluster in &hawk2;</title>
          <mediaobject>
            <imageobject>
            <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </sect1>

  <sect1 xml:id="sec-ha-install-node-join-bootstrap">
  <!-- Duplicated from xml/article_installation.xml -->
    <title>Adding nodes with <command>crm cluster join</command></title>
    <para>
      You can add more nodes to the cluster with the <command>crm cluster join</command> bootstrap script.
      The script only needs access to an existing cluster node, and completes the basic setup
      on the current machine automatically.
    </para>
    <para>
      For more information, run the <command>crm cluster join --help</command> command.
    </para>
    <procedure xml:id="pro-ha-install-node-join-bootstrap">
    <title>Adding nodes with <command>crm cluster join</command></title>
    <step>
      <para>
        Log in to this node as the same user you set up the first node with.
      </para>
    </step>
    <step>
      <para>
        Start the bootstrap script:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            If you set up the first node as &rootuser;, you can run this command with
            no additional parameters:
          </para>
<screen>&prompt.root;<command>crm cluster join</command></screen>
        </listitem>
        <listitem>
          <para>
            If you set up the first node as a <command>sudo</command> user, you must
            specify the user and node with the <option>-c</option> option:
          </para>
<screen>&prompt.user;<command>sudo crm cluster join -c <replaceable>USER</replaceable>@<replaceable>NODE1</replaceable></command></screen>
        </listitem>
        <listitem>
          <para>
            If you set up the first node as a <command>sudo</command> user with SSH agent forwarding,
            use the following command:
          </para>
<screen>&prompt.user;<command>sudo --preserve-env=SSH_AUTH_SOCK \
crm cluster join --use-ssh-agent -c <replaceable>USER</replaceable>@<replaceable>NODE1</replaceable></command></screen>
        </listitem>
      </itemizedlist>
      <para>
        If NTP is not configured to start at boot time, a message
        appears. The script also checks for a hardware watchdog device.
        You are warned if none is present.
      </para>
    </step>
    <step>
      <para>
      If you did not already specify the first cluster node
      with <option>-c</option>, you will be prompted for its IP address.
      </para>
    </step>
    <step>
      <para>
        If you did not already configure passwordless SSH access between the cluster nodes,
        you will be prompted for the password of the first node.
      </para>
      <para>
        After logging in to the specified node, the script copies the
        &corosync; configuration, configures SSH and &csync;,
        brings the current machine online as a new cluster node, and
        starts the service needed for &hawk2;.
      </para>
    </step>
    </procedure>
    <para>
      Repeat this procedure for each node. You can check the status of the cluster at any time
      with the <command>crm status</command> command, or by logging in to &hawk2; and navigating to
      <menuchoice><guimenu>Status</guimenu><guimenu>Nodes</guimenu></menuchoice>.
    </para>
  </sect1>
</chapter>
