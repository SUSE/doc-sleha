<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-ha-bootstrap-install" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>Using the bootstrap script</title>
  <info>
    <abstract>
      <para>

      </para>
    </abstract>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker></dm:bugtracker>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  </info>

  <sect1 xml:id="sec-ha-bootstrap-install-overview">
    <!-- Duplicated from xml/article_installation.xml -->
    <title>Overview of the <command>crm cluster init</command> script</title>
    <para>
      The <command>crm cluster init</command> command executes a bootstrap script that defines the
      basic parameters needed for cluster communication, resulting in a running one-node cluster.
      The script checks and configures the following components:
    </para>
    <variablelist>
    <varlistentry>
      <term>NTP</term>
      <listitem>
        <para>
          Checks if NTP is configured to start at boot time. If not, a message appears.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>SSH</term>
      <listitem>
        <para>
          Detects or generates SSH keys for passwordless login between cluster nodes.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&csync;</term>
      <listitem>
        <para>
          Configures &csync; to replicate configuration files across all nodes in a cluster.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&corosync;</term>
      <listitem>
        <para>
          Configures the cluster communication system.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>SBD/watchdog</term>
      <listitem>
        <para>
          Checks if a watchdog exists and asks you whether to configure SBD as the node fencing mechanism.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Virtual floating IP</term>
      <listitem>
        <para>
          Asks you whether to configure a virtual IP address for cluster administration with &hawk2;.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Firewall</term>
      <listitem>
        <para>
          Opens the ports in the firewall that are needed for cluster communication.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Cluster name</term>
      <listitem>
        <para>
          Defines a name for the cluster, by default <systemitem>hacluster</systemitem>. This is
          optional and mostly useful for &geo; clusters. Usually, the cluster name reflects the
          geographical location and makes it easier to distinguish a site inside a &geo; cluster.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&qdevice;/&qnet;</term>
      <listitem>
        <para>
          Asks you whether to configure &qdevice;/&qnet; to participate in quorum decisions.
          We recommend using &qdevice; and &qnet; for clusters with an even number of nodes,
          and especially for two-node clusters.
        </para>
      </listitem>
    </varlistentry>
    </variablelist>
    <note>
      <title>&pace; default settings</title>
      <para>
        The options set by the bootstrap script might not be the same as the &pace;
        default settings. You can check which settings the bootstrap script changed in
        <filename>/var/log/crmsh/crmsh.log</filename>. Any options set during the bootstrap
        process can be modified later with the &yast; cluster module.
      </para>
    </note>
    <note>
      <title>Cluster configuration for different platforms</title>
      <para>
        The <command>crm cluster init</command> script detects the system environment (for example,
        &ms; Azure) and adjusts certain cluster settings based on the profile for that environment.
        For more information, see the file <filename>/etc/crm/profiles.yml</filename>.
      </para>
    </note>
  </sect1>

  <sect1 xml:id="sec-ha-bootstrap-install-first-node">
  <!-- Initially duplicated from xml/article_installation.xml, expansion is WIP -->
    <title>Setting up the first node with <command>crm cluster init</command></title>
    <para>
      Setting up the first node with the <command>crm cluster init</command> script
      requires only a minimum of time and manual intervention.
    </para>
    <para>
      This steps in this procedure show the default option followed by alternative or additional
      options. For a minimal setup with only the default options, see <xref linkend="article-installation"/>.
    </para>
    <procedure xml:id="pro-ha-bootstrap-install-first-node">
    <title>Setting up the first node with <command>crm cluster init</command></title>
    <step>
      <para>
      Start the bootstrap script:
      </para>
      <screen>&prompt.root;<command>crm cluster init --name <replaceable>CLUSTERNAME</replaceable></command></screen>
      <para>Replace the <replaceable>CLUSTERNAME</replaceable>
      placeholder with a meaningful name, like the geographical location of your
      cluster (for example, <literal>&cluster1;</literal>).
      This is especially helpful to create a &geo; cluster later on,
      as it simplifies the identification of a site.
      </para>
      <para>
      If you need to use multicast instead of unicast (the default) for your cluster
      communication, use the option <option>--multicast</option> (or <option>-U</option>).
      </para>
      <para>
      The script checks for NTP configuration and a hardware watchdog service. If required,
      it generates the public and private SSH keys used for passwordless SSH access and
      &csync; synchronization and starts the respective services.
      </para>
    </step>
    <step>
      <para>
      Configure the cluster communication layer (&corosync;):
      </para>
      <substeps>
      <step>
        <para>
        Enter a network address to bind to. By default, the script
        proposes the network address of <systemitem>eth0</systemitem>.
        Alternatively, enter a different network address, for example, the
        address of <literal>bond0</literal>.
        </para>
      </step>
      <step>
        <para>
        Accept the proposed port (<literal>5405</literal>) or enter a different one.
        </para>
      </step>
      </substeps>
    </step>
    <step>
      <para>
      Set up SBD as the node fencing mechanism:</para>
      <substeps>
      <step>
        <para>Confirm with <literal>y</literal> that you want to use SBD.</para>
      </step>
      <step>
        <para>Enter a persistent path to the partition of your block device that
        you want to use for SBD.
        The path must be consistent across all nodes in the cluster.</para>
        <para>The script creates a small partition on the device to be used for SBD.</para>
      </step>
      </substeps>
    </step>
    <step>
      <para>Configure a virtual IP address for cluster administration with &hawk2;:</para>
      <substeps>
      <step>
        <para>Confirm with <literal>y</literal> that you want to configure a
        virtual IP address.</para></step>
      <step>
        <para>Enter an unused IP address that you want to use as administration IP
        for &hawk2;: <literal>&subnetI;.10</literal>
        </para>
        <para>Instead of logging in to an individual cluster node with &hawk2;,
        you can connect to the virtual IP address.</para>
      </step>
      </substeps>
    </step>
    <step>
      <para>
      Choose whether to configure &qdevice; and &qnet;. For the minimal setup
      described in this document, decline with <literal>n</literal> for now.
      </para>
    </step>
    </procedure>
    <para>
    Finally, the script will start the cluster services to bring the
    cluster online and enable &hawk2;. The URL to use for &hawk2; is
    displayed on the screen.
    </para>
  </sect1>

  <sect1 xml:id="sec-ha-bootstrap-install-hawk2-login">
  <!-- Duplicated from xml/article_installation.xml -->
    <title>Logging in to the &hawk2; web interface</title>
    <para>
      You now have a running one-node cluster. To view its status, proceed as follows:
    </para>
    <procedure xml:id="pro-ha-bootstrap-install-hawk2-login">
      <title>Logging in to the &hawk2; Web interface</title>
      <step>
        <para>
          On any machine, start a Web browser and make sure that JavaScript and cookies are enabled.
        </para>
      </step>
      <step>
        <para>
          As URL, enter the virtual IP address that you configured with the bootstrap script:
        </para>
<screen>https://<replaceable>VIRTUAL_IP</replaceable>:7630/</screen>
        <note>
          <title>Certificate warning</title>
          <para>
            If a certificate warning appears when you try to access the URL for the first time,
            a self-signed certificate is in use. Self-signed certificates are not considered
            trustworthy by default.
          </para>
          <para>
            Ask your cluster operator for the certificate details to verify the certificate.
          </para>
          <para>
            To proceed anyway, you can add an exception in the browser to bypass the warning.
          </para>
        </note>
      </step>
      <step>
        <para>
          On the &hawk2; login screen, enter the <guimenu>Username</guimenu> and
          <guimenu>Password</guimenu> of the user that was created by the bootstrap script
          (user <systemitem class="username">hacluster</systemitem>, password <literal>linux</literal>).
        </para>
        <important>
          <title>Secure password</title>
          <para>
            Replace the default password with a secure one as soon as possible:
          </para>
<screen>&prompt.root;<command>passwd hacluster</command></screen>
        </important>
      </step>
      <step>
        <para>
          Click <guimenu>Log In</guimenu>. The &hawk2; Web interface shows the
          <guimenu>Status</guimenu> screen by default:
        </para>
        <figure xml:id="fig-ha-bootstrap-install-one-node-status">
          <title>Status of the one-node cluster in &hawk2;</title>
          <mediaobject>
            <imageobject>
            <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </sect1>
</chapter>
