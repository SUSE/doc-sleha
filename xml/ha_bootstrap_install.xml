<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="cha-ha-bootstrap-install" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>Using the bootstrap scripts</title>
  <info>
    <abstract>
      <para>
        &productname; includes bootstrap scripts to simplify the installation of a cluster.
        You can use these scripts to set up the cluster on the first node, add more nodes to the
        cluster, remove nodes from the cluster, and adjust certain settings in an existing cluster.
      </para>
    </abstract>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker></dm:bugtracker>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  </info>

  <sect1 xml:id="sec-ha-bootstrap-install-overview">
    <!-- Duplicated from xml/article_installation.xml -->
    <title>Overview of the <command>crm cluster init</command> script</title>
    <para>
      The <command>crm cluster init</command> command executes a bootstrap script that defines the
      basic parameters needed for cluster communication, resulting in a running one-node cluster.
      The script checks and configures the following components:
    </para>
    <variablelist>
    <varlistentry>
      <term>NTP</term>
      <listitem>
        <para>
          Checks if NTP is configured to start at boot time. If not, a message appears.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>SSH</term>
      <listitem>
        <para>
          Detects or generates SSH keys for passwordless login between cluster nodes.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&csync;</term>
      <listitem>
        <para>
          Configures &csync; to replicate configuration files across all nodes in a cluster.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&corosync;</term>
      <listitem>
        <para>
          Configures the cluster communication system.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>SBD/watchdog</term>
      <listitem>
        <para>
          Checks if a watchdog exists and asks you whether to configure SBD as the node fencing mechanism.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Virtual floating IP</term>
      <listitem>
        <para>
          Asks you whether to configure a virtual IP address for cluster administration with &hawk2;.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Firewall</term>
      <listitem>
        <para>
          Opens the ports in the firewall that are needed for cluster communication.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Cluster name</term>
      <listitem>
        <para>
          Defines a name for the cluster, by default <systemitem>hacluster</systemitem>. This is
          optional and mostly useful for &geo; clusters. Usually, the cluster name reflects the
          geographical location and makes it easier to distinguish a site inside a &geo; cluster.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>&qdevice;/&qnet;</term>
      <listitem>
        <para>
          Asks you whether to configure &qdevice;/&qnet; to participate in quorum decisions.
          We recommend using &qdevice; and &qnet; for clusters with an even number of nodes,
          and especially for two-node clusters.
        </para>
      </listitem>
    </varlistentry>
    </variablelist>
    <note>
      <title>&pace; default settings</title>
      <para>
        The options set by the bootstrap script might not be the same as the &pace;
        default settings. You can check which settings the bootstrap script changed in
        <filename>/var/log/crmsh/crmsh.log</filename>. Any options set during the bootstrap
        process can be modified later with the &yast; cluster module.
      </para>
    </note>
    <note>
      <title>Cluster configuration for different platforms</title>
      <para>
        The <command>crm cluster init</command> script detects the system environment (for example,
        &ms; Azure) and adjusts certain cluster settings based on the profile for that environment.
        For more information, see the file <filename>/etc/crm/profiles.yml</filename>.
      </para>
    </note>
  </sect1>

  <sect1 xml:id="sec-ha-bootstrap-install-first-node">
  <!-- Initially duplicated from xml/article_installation.xml, expansion is WIP -->
    <title>Setting up the first node with <command>crm cluster init</command></title>
    <para>
      Setting up the first node with the <command>crm cluster init</command> script
      requires only a minimum of time and manual intervention.
    </para>
    <para>
      The steps in this procedure show the default option followed by alternative or additional
      options. For a minimal setup with only the default options,
      see <xref linkend="article-installation"/>.
    </para>
    <procedure xml:id="pro-ha-bootstrap-install-first-node">
    <title>Setting up the first node with <command>crm cluster init</command></title>
    <step>
      <para>
        Log in to the first cluster node as &rootuser;, or as a user with <command>sudo</command>
        privileges.
      </para>
    </step>
    <step>
      <para>
      Start the bootstrap script:
      </para>
      <para>
        You can start the script without specifying any options. This prompts you for input for
        certain settings as described in later steps, and uses &crmsh;'s default values for
        other settings.
      </para>
      <itemizedlist>
        <listitem>
          <para>
            If you logged in as &rootuser;, you can run this command with no additional parameters:
          </para>
<screen>&prompt.root;<command>crm cluster init</command></screen>
        </listitem>
        <listitem>
          <para>
            If you logged in as a <command>sudo</command> user without SSH agent forwarding,
            run this command with <command>sudo</command>:
          </para>
<screen>&prompt.user;<command>sudo crm cluster init</command></screen>
        </listitem>
        <listitem>
          <para>
            If you logged in as a <command>sudo</command> user with SSH agent forwarding enabled,
            you must preserve the environment variable <literal>SSH_AUTH_SOCK</literal>
            and tell the script to use your local SSH keys instead of generating keys on the node:
          </para>
<screen>&prompt.user;<command>sudo --preserve-env=SSH_AUTH_SOCK crm cluster init --use-ssh-agent</command></screen>
        </listitem>
      </itemizedlist>
      <para>
        Alternatively, you can specify additional options as part of the initialization command.
        You can include multiple options in the same command. Some examples are shown below.
        For more options, run <command>crm cluster init --help</command>.
      </para>
      <variablelist>
      <varlistentry>
        <term>Cluster name</term>
        <listitem>
          <para>
            The default cluster name is <literal>hacluster</literal>. To choose a different name,
            use the option <option>--name</option> (or <option>-n</option>). For example:
          </para>
<screen>&prompt.root;<command>crm cluster init --name <replaceable>CLUSTERNAME</replaceable></command></screen>
          <para>
            Choose a meaningful name, like the geographical location of the cluster. This is
            especially helpful if you create a &geo; cluster later, as it simplifies the
            identification of a site.
          </para>
        </listitem>
      </varlistentry>
        <varlistentry>
          <term>Multicast</term>
          <listitem>
            <para>
              Unicast is the default transport type for cluster communication. To use multicast
              instead, use the option <option>--multicast</option> (or <option>-U</option>).
              For example:
            </para>
<screen>&prompt.root;<command>crm cluster init --multicast</command></screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>SBD disks</term>
          <listitem>
            <para>
              In a later step, the script asks if you want to set up SBD and prompts you for a disk
              to use. To configure the cluster with multiple SBD disks, use the option
              <option>--sbd-device</option> (or <option>-s</option>) multiple times. For example:
            </para>
<screen>&prompt.root;<command>crm cluster init --sbd-device /dev/disk/by-id/<replaceable>ID1</replaceable> --sbd-device /dev/disk/by-id/<replaceable>ID2</replaceable></command></screen>
            <para>
              This option is also useful because you can use tab completion for the device ID,
              which is not available later when the script prompts you for the path.
            </para>
            <!-- Add more info about using multiple SBD disks, like for SAP -->
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Redundant communication channel</term>
          <listitem>
            <para>
              Supported clusters must have two communication channels. The preferred method is to
              use network device bonding. If you cannot use bonding, you can set up a redundant
              communication channel in &corosync; (also known as a second ring or heartbeat line).
              By default, the script prompts you for a network address for a single ring.
              To configure the cluster with two rings, use the option <option>--interface</option>
              (or <option>-i</option>) twice. For example:
            </para>
<screen>&prompt.root;<command>crm cluster init --interface eth0 --interface eth1</command></screen>
            <note role="compact">
              <para>
                You can also use <option>--multi-heartbeats</option> (or <option>-M</option>) to set
                up a second &corosync; ring . This option uses the first two network interfaces by
                default, whereas <option>-i</option> allows you to specify any two network interfaces.
              </para>
            </note>
          </listitem>
        </varlistentry>
      </variablelist>
    </step>
    <step>
      <para>
      Configure the cluster communication layer (&corosync;):
      </para>
      <substeps>
      <step>
        <para>
        Enter a network address to bind to. By default, the script
        proposes the network address of <systemitem>eth0</systemitem>.
        Alternatively, enter a different network address, for example, the
        address of <literal>bond0</literal>.
        </para>
      </step>
      <step>
        <para>
        Accept the proposed port (<literal>5405</literal>) or enter a different one.
        </para>
      </step>
      <step>
        <para>
          If you started the script with an option that configures a redundant communication channel,
          enter <literal>y</literal> to accept a second heartbeat line, then either accept the
          proposed network address and port or enter different ones.
        </para>
      </step>
      </substeps>
    </step>
    <step>
      <para>
        Choose whether to set up SBD as the node fencing mechanism. If you are using a different
        fencing mechanism or want to set up SBD later, enter <literal>n</literal> to skip this step.
      </para>
      <para>
        If you chose <literal>y</literal>, select the type of SBD to use:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            To use diskless SBD, enter <literal>none</literal>.
          </para>
        </listitem>
        <listitem>
          <para>
            To use disk-based SBD, enter a persistent path to the partition of the block device you
            want to use. The path must be consistent across all nodes in the cluster, for example,
            <filename>/dev/disk/by-id/<replaceable>ID</replaceable></filename>.
          </para>
          <para>
            The script creates a small partition on the device to be used for SBD.
          </para>
        </listitem>
      </itemizedlist>
    </step>
    <step>
      <para>
        Choose whether to configure a virtual IP address for cluster administration with &hawk2;.
        Instead of logging in to an individual cluster node with &hawk2;, you can connect
        to the virtual IP address.
      </para>
      <para>
        If you chose <literal>y</literal>, enter an unused IP address to use for &hawk2;.
      </para>
    </step>
    <step>
      <para>
        Choose whether to configure &qdevice; and &qnet;. If you do not need to use &qdevice; or
        have not set up the &qnet; server yet, enter <literal>n</literal> to skip this step.
        You can set up &qdevice; and &qnet; later if required.
      </para>
      <para>
        If you chose <literal>y</literal>, provide the following information:
      </para>
      <substeps>
        <step>
          <para>
            Enter the host name or IP address of the &qnet; server. The cluster node must have
            SSH access to this server to complete the configuration.
          </para>
          <para>
            For the remaining fields, you can accept the default values or change them as required:
          </para>
        </step>
        <step>
          <para>
            Accept the proposed port (<literal>5403</literal>) or enter a different one.
          </para>
        </step>
        <step>
          <para>
            Choose the algorithm that determines how votes are assigned.
          </para>
        </step>
        <step>
          <para>
            Choose the method to use when a tie-breaker is required.
          </para>
        </step>
        <step>
          <para>
            Choose whether to enable TLS.
          </para>
        </step>
        <step>
          <para>
            Enter heuristics commands to affect how votes are determined. To skip this step, leave
            the field blank.
          </para>
        </step>
      </substeps>
    </step>
    </procedure>
    <para>
    The script checks for NTP configuration and a hardware watchdog service. If required,
    it generates the public and private SSH keys used for passwordless SSH access and
    &csync; synchronization and starts the respective services. Finally, the script
    starts the cluster services to bring the cluster online and enables &hawk2;.
    The URL to use for &hawk2; is displayed on the screen.
    </para>
    <para>
      To log in to &hawk2;, see <xref linkend="sec-conf-hawk2-login"/>.
    </para>
    <important>
      <title>Secure password for <systemitem class="username">hacluster</systemitem></title>
      <para>
        The <command>crm cluster init</command> script creates a default user
        (<systemitem class="username">hacluster</systemitem>) and password
        (<literal>linux</literal>). Replace the default password with a secure one
        as soon as possible:
      </para>
<screen>&prompt.root;<command>passwd hacluster</command></screen>
    </important>
  </sect1>

  <sect1 xml:id="sec-ha-install-node-join-bootstrap">
  <!-- Duplicated from xml/article_installation.xml -->
    <title>Adding nodes with <command>crm cluster join</command></title>
    <para>
      You can add more nodes to the cluster with the <command>crm cluster join</command> bootstrap script.
      The script only needs access to an existing cluster node, and completes the basic setup
      on the current machine automatically.
    </para>
    <para>
      For more information, run the <command>crm cluster join --help</command> command.
    </para>
    <procedure xml:id="pro-ha-install-node-join-bootstrap">
    <title>Adding nodes with <command>crm cluster join</command></title>
    <step>
      <para>
        Log in to this node as the same user you set up the first node with.
      </para>
    </step>
    <step>
      <para>
        Start the bootstrap script:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            If you set up the first node as &rootuser;, you can run this command with
            no additional parameters:
          </para>
<screen>&prompt.root;<command>crm cluster join</command></screen>
        </listitem>
        <listitem>
          <para>
            If you set up the first node as a <command>sudo</command> user, you must
            specify the user and node with the <option>-c</option> option:
          </para>
<screen>&prompt.user;<command>sudo crm cluster join -c <replaceable>USER</replaceable>@<replaceable>NODE1</replaceable></command></screen>
        </listitem>
        <listitem>
          <para>
            If you set up the first node as a <command>sudo</command> user with SSH agent forwarding,
            use the following command:
          </para>
<screen>&prompt.user;<command>sudo --preserve-env=SSH_AUTH_SOCK \
crm cluster join --use-ssh-agent -c <replaceable>USER</replaceable>@<replaceable>NODE1</replaceable></command></screen>
        </listitem>
      </itemizedlist>
      <para>
        If NTP is not configured to start at boot time, a message appears. The script also checks
        for a hardware watchdog device. You are warned if none is present.
      </para>
    </step>
    <step>
      <para>
        If you did not already specify the first cluster node with <option>-c</option>,
        you are prompted for its IP address.
      </para>
    </step>
    <step>
      <para>
        If you did not already configure passwordless SSH access between the cluster nodes,
        you are prompted for the password of the first node.
      </para>
      <para>
        After logging in to the specified node, the script copies the &corosync; configuration,
        configures SSH and &csync;, brings the current machine online as a new cluster node,
        and starts the service needed for &hawk2;.
      </para>
    </step>
    </procedure>
    <para>
      Repeat this procedure for each node. You can check the status of the cluster at any time
      with the <command>crm status</command> command, or by logging in to &hawk2; and navigating to
      <menuchoice><guimenu>Status</guimenu><guimenu>Nodes</guimenu></menuchoice>.
    </para>
  </sect1>

  <sect1 xml:id="sec-ha-install-cluster-remove">
    <title>Removing nodes with <command>crm cluster remove</command></title>
    <para>
      You can remove nodes from the cluster with the <command>crm cluster remove</command>
      bootstrap script.
    </para>
    <para>
      If you run <command>crm cluster remove</command> with no additional parameters, you are
      prompted for the IP address or host name of the node to remove. Alternatively, you can
      specify the node when you run the command:
    </para>
<screen>&prompt.root;<command>crm cluster remove <replaceable>NODE</replaceable></command></screen>
    <para>
       On the specified node, this stops all cluster services and removes the local cluster
       configuration files. On the rest of the cluster nodes, the specified node is removed
       from the cluster configuration.
    </para>
    <para>
      In most cases, you must run <command>crm cluster remove</command> from a different node,
      <emphasis>not</emphasis> from the node you want to remove. However, to remove the last node
      and delete the cluster, you can use <option>--force</option> (or <option>-F</option>):
    </para>
<screen>&prompt.root;<command>crm cluster remove --force <replaceable>LASTNODE</replaceable></command></screen>
    <para>
      For more information, run <command>crm cluster remove --help</command>.
    </para>
  </sect1>
</chapter>
