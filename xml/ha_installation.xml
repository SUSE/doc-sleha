<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<chapter id="cha.ha.installation.yast">
 <title>Installation and Basic Setup with &yast;</title>
 <abstract>
  <para>
   There are two ways to install the software needed for &ha; clusters:
   either from a command line, using <command>zypper</command>, or with
   &yast; which provides a graphical user interface. After installing the
   software on all nodes that will be part of your cluster, the next step is
   to initially configure the cluster so that the nodes can communicate with
   each other and to start the services needed to bring the cluster online.
   The initial cluster setup can either be done manually (be editing and
   copying the configuration files) or with the &yast; cluster module.
  </para>
<!--taroth 2010-03-09: adding the following sentence with regard to
   https://bugzilla.novell.com/show_bug.cgi?id=573817#c6-->
  <para>
   This chapter describes how to do a new installation and setup with
   &productname; &productnumber; from scratch. Refer to chapter
   <xref linkend="cha.ha.migration"/> if you want to migrate an existing
   cluster that runs an older version of &productname; or if you want to
   update any software packages on nodes that are part of a running cluster.
  </para>
 </abstract>
 <sect1 id="sec.ha.installation.inst">
  <title>Installing the &hasi;</title>

  <para>
   The packages needed for configuring and managing a cluster with the
   &hasi; are included in the <literal>&ha;</literal> installation pattern.
   This pattern is only available after &productnamereg; has been installed
   as add-on.
   <remark>taroth: need to use hard-coded
    link here as the target is not included in the same set</remark>
   For information on how to install add-on products, see the &sle;
   &productnumber; &deploy;, available at
   <ulink
    url="http://www.novell.com/documentation"/>. Refer to chapter
   <citetitle>Installing Add-On Products</citetitle>.
  </para>

  <note>
   <title>Installing the Software Packages</title>
   <para>
    The software packages needed for &ha; clusters are not automatically
    copied to the cluster nodes.
   </para>
   <para>
    If you do not want to install &slsreg; &productnumber; and
    &productnamereg; &productnumber; manually on all nodes that will be part
    of your cluster, use &ay; to clone existing nodes. For more information,
    refer to <xref
         linkend="sec.ha.installation.autoyast"/>.
   </para>
  </note>

  <procedure>
   <title>Installing the &ha; Pattern</title>
   <step>
    <para>
     Start &yast; as &rootuser; user and select <menuchoice>
     <guimenu>Software</guimenu> <guimenu>Software Management</guimenu>
     </menuchoice>.
    </para>
    <para>
     Alternatively, start the &yast; package manager as &rootuser; on a
     command line with <command>yast2&nbsp;sw_single</command>.
    </para>
   </step>
   <step>
    <para>
     From the <guimenu>Filter</guimenu> list, select
     <guimenu>Patterns</guimenu> and activate the <guimenu>High
     Availability</guimenu> pattern in the pattern list.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Accept</guimenu> to start the installation of the
     packages.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.installation.setup">
  <title>Initial Cluster Setup</title>

  <para>
   After having installed the HA packages, proceed with the initial cluster
   setup.
<!--taroth 2010-02-02: if there is time left, add
    procedure for manual config: <xref  linkend="pro.ha.installatiom.setup.manual"/>-->
   This includes the following basic steps:
  </para>

  <procedure>
   <step>
    <para>
     <xref linkend="sec.ha.installation.setup.channels" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="sec.ha.installation.setup.security" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="sec.ha.installation.setup.csync2" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>

  <para>
   The following procedures guide you through each of the steps, using the
   &yast; cluster module. To access the cluster configuration dialog, start
   &yast; as &rootuser; and select <menuchoice> <guimenu>&ha;</guimenu>
   <guimenu>Cluster</guimenu> </menuchoice>. Alternatively, start the &yast;
   cluster module as &rootuser; on a command line with
   <command>yast2&nbsp;cluster</command>.
  </para>

  <para>
   If you start the cluster module for the first time, it appears as wizard,
   guiding you through all the steps necessary for basic setup. Otherwise,
   click the categories on the left panel to access the configuration
   options for each step.
  </para>

  <sect2 id="sec.ha.installation.setup.channels">
   <title>Defining the Communication Channels</title>
   <para>
    For successful communication between the cluster nodes, define at least
    one communication channel. However, it is recommended to set up the
    communication via two or more redundant paths (either by using network
    device bonding or by adding a second communication channel with
    &corosync;). For each communication channel, you need to define the
    following parameters:
   </para>
   <variablelist>
    <varlistentry>
     <term>Bind Network Address (<literal>bindnetaddr</literal>)
    </term>
     <listitem>
      <para>
       The network address to bind to. To ease sharing configuration files
       across the cluster, &ais; uses network interface netmask to mask only
       the address bits that are used for routing the network. Set the value
       to the subnet you will use for cluster multicast.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Multicast Address (<literal>mcastaddr</literal>)
    </term>
     <listitem>
      <para>
       Can be an IPv4 or IPv6 address.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Multicast Port (<literal>mcastport</literal>)
    </term>
     <listitem>
      <para>
       The UDP port specified for <literal>mcastaddr</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
<!--(Setting up two or more
    clusters that use the same multicast address, but a different port, also works, 
    but is less efficient).-->
   <para>
    All nodes in the cluster will know each other from using the same
    multicast address and the same port number. For different clusters, use
    a different multicast address.
   </para>
   <para>
    To configure redundant communication with &corosync;, you need to define
    multiple interface sections in
    <filename>/etc/corosync/corosync.conf</filename>, each with a different
    ringnumber. Use the Redundant Ring Protocol (RRP) to tell the cluster
    how to use these interfaces. RRP can have three modes
    (<literal>rrp_mode</literal>): if set to <literal>active</literal>,
    &corosync; uses all interfaces actively. If set to
    <literal>passive</literal>, &corosync; uses the second interface only if
    the first ring fails. If rrp_mode is set to <literal>none</literal>, RRP
    is disabled. With RRP, two physically separate networks are used for
    communication. In case one network fails, the cluster nodes can still
    communicate via the other network.
   </para>
   <para>
    If several rings are configured, each node can have multiple IP
    addresses. As soon as rrp_mode is enabled, the Stream Control
    Transmission Protocol (SCTP) is used for communication between the nodes
    by default (instead of TCP).
   </para>
   <procedure id="pro.ha.installation.setup.channels">
    <title>Defining the Communication Channels</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Communication
      Channels</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      Define the <guimenu>Bind Network Address</guimenu>, the
      <guimenu>Multicast Address</guimenu> and the <guimenu>Multicast
      Port</guimenu> to use for all cluster nodes.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_communication.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_communication.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
<!--taroth 2010-02-05: https://fate.novell.com/307371-->
    <step>
     <para>
      If you want to define a second channel:
     </para>
     <substeps>
      <step>
       <para>
        Activate <guimenu>Redundant Channel</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Bind Network Address</guimenu>, the
        <guimenu>Multicast Address</guimenu> and the <guimenu>Multicast
        Port</guimenu> for the redundant channel.
       </para>
      </step>
      <step>
       <para>
        Select the <guimenu>rrp_mode</guimenu> you want to use. To disable
        the RRP, select <guimenu>None</guimenu>. For more information about
        the modes, click <guimenu>Help</guimenu>.
       </para>
       <para>
        When using RRP, the primary ring (the first channel you have
        configured) gets the ringnumber <literal>0</literal>, the second
        ring (redundant channel) the ringnumber <literal>1</literal> in
        <filename>/etc/corosync/corosync.conf</filename>.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Activate <guimenu>Auto Generate Node ID</guimenu> to automatically
      generate a unique ID for every cluster node.
     </para>
    </step>
    <step>
     <para>
      If you only wanted to modify the communication channels for an
      existing cluster, click <guimenu>Finish</guimenu> to write the
      configuration to <filename>/etc/corosync/corosync.conf</filename> and
      to close the &yast; cluster module. &yast; then automatically also
      adjusts the firewall settings and opens the UDP port used for
      multicast.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click proceed with
      <xref
       linkend="pro.ha.installation.setup.security"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.installation.setup.security">
   <title>Defining Authentication Settings</title>
   <para>
    As next step, define the authentication settings for the cluster. You
    can use HMAC/SHA1 authentication which requires a shared secret, used to
    protect and authenticate messages. The authentication key (password) you
    specify will be used on all nodes in the cluster.
   </para>
   <procedure id="pro.ha.installation.setup.security">
    <title>Enabling Secure Authentication</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the
      <guimenu>Security</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      Activate <guimenu>Enabling Security Auth</guimenu>.
     </para>
    </step>
    <step>
     <para>
      For a newly created cluster, click <guimenu>Generate Auth Key
      File</guimenu>. This creates an authentication key that is written to
      <filename>/etc/corosync/authkey</filename>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      If you only want to modify the authentication settings, click
      <guimenu>Finish</guimenu> to write the configuration to
      <filename>/etc/corosync/corosync.conf</filename> and to close the
      &yast; cluster module.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, proceed with
      <xref
      linkend="sec.ha.installation.setup.csync2"/>.
     </para>
    </step>
   </procedure>
  </sect2>

<!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->

  <sect2 id="sec.ha.installation.setup.csync2">
   <title>Transferring the Configuration to All Nodes</title>
   <para>
    Instead of copying the resulting configuration files to all nodes
    manually, use the <command>csync2</command> tool for replication across
    all nodes in the cluster. &csync; can handle any number of hosts, sorted
    into synchronization groups. Each synchronization group has its own list
    of member hosts and its include/exlude patterns that define which Ô¨Åles
    should be synchronized in the synchronization group. The groups, the
    hostnames belonging to each group, and the include/exclude rules for
    each group are specified in the &csync; configuration file,
    <filename>/etc/csync2/csync2.cfg</filename>.
   </para>
   <para>
    For authentication, &csync; uses the IP addresses and pre-shared-keys
    within a synchronization group. You need to generate one key file for
    each synchronization group and copy it to all group members.
   </para>
   <para>
    For more information about &csync;, refer to
    <ulink
     url="http://oss.linbit.com/csync2/paper.pdf"/>
   </para>
   <procedure id="pro.ha.installation.setup.csync2.yast">
    <title>Configuring &csync; with &yast;</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>&csync;</guimenu>
      category.
     </para>
    </step>
<!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->
    <step>
     <para>
      To specify the synchronization group, click <guimenu>Add</guimenu> in
      the <guimenu>Sync Host</guimenu> group and enter the local hostnames
      of all nodes in your cluster. For each node, you must use exactly the
      strings that are returned by the <command>hostname</command> command.
     </para>
    </step>
    <step id="step.csync2.generate.key">
     <para>
      Click <guimenu>Generate Pre-Shared-Keys</guimenu> to create a key file
      for the synchronization group. The key file is written to
      <filename>/etc/csync2/key_hagroup</filename>. After it has been
      created, it must be copied manually to all members of the cluster.
     </para>
    </step>
    <step>
     <para>
      To populate the <guimenu>Sync File</guimenu> list with the files that
      usually need to be synchronized among all nodes, click <guimenu>Add
      Suggested Files</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_sync.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_sync.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      If you want to <guimenu>Edit</guimenu>, <guimenu>Add</guimenu> or
      <guimenu>Remove</guimenu> files from the list of files to be
      synchronized use the respective buttons. You must enter the absolute
      pathname for each file.
     </para>
    </step>
    <step>
     <para>
      Activate &csync; by clicking <guimenu>Turn &csync; On</guimenu>. This
      will start &csync; automatically at boot time.
<!--taroth 2010-03-02: chkconfig csync2-->
     </para>
    </step>
    <step>
     <para>
      If all options are set according to your wishes, click
      <guimenu>Finish</guimenu> to close the &yast; cluster module. &yast;
      then writes the &csync; configuration to
      <filename>/etc/csync2/csync2.cfg</filename>.
     </para>
    </step>
   </procedure>
   <para>
    After you have configured &csync;, start the synchronization process
    from command line as described below.
   </para>
   <procedure id="pro.ha.installation.setup.csync2.start">
    <title>Synchronizing the Configuration Files with &csync;</title>
    <para>
     To successfully synchronize the files with &csync;, make sure that the
     following prerequisites are met:
    </para>
    <remark>taroth 2010-07-21: todo - http://doccomments.provo.novell.com/admin/viewcomment/14587#: "The
     same Csync2 configuration is available on all nodes. Either include /etc/csync2/csync2.cfg in
     the list of files to be synchronized with Csync2 or copy the file manually to all nodes after
     you have configured it as described in Configuring Csync2 with YaST. " This statement is
     incorrect. You have to have the csync2.cfg on all nodes before you do your first sync. So the
     statement should read like so.. "The same Csync2 configuration is available on all nodes. Copy
     the file /etc/csync2/csync2.cfg manually to all nodes after you have configured it as described
     in Configuring Csync2 with YaST. Also, it's recommend to include /etc/csync2/csync2.cfg in the
     list of files to be synchronized with Csync2.</remark>
    <itemizedlist>
     <listitem>
      <para>
       The same &csync; configuration is available on all nodes. Either
       include <filename>/etc/csync2/csync2.cfg</filename> in the list of
       files to be synchronized with &csync; or copy the file manually to
       all nodes after you have configured it as described in
       <xref linkend="pro.ha.installation.setup.csync2.yast"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       Copy the <filename>/etc/csync2/key_hagroup</filename> file you have
       generated on one node in <xref linkend="step.csync2.generate.key"/>
       to <emphasis>all</emphasis> nodes in the cluster as it is needed for
       authentication by &csync;. However, do not try to regenerate the file
       on the other nodes as it needs to be the same file on all nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       Make sure that <systemitem class="daemon">xinetd</systemitem> is
       running on <emphasis>all</emphasis> nodes, as &csync; depends on that
       daemon. Start <systemitem
        class="daemon">xinetd</systemitem>
       as &rootuser; with the following command:
      </para>
<screen>rcxinetd start</screen>
      <note>
       <title>Starting Services at Boot Time</title>
       <para>
        If you want &csync; and
        <systemitem class="daemon">xinetd</systemitem> to start
        automatically at boot time, execute the following command on all
        nodes:
       </para>
<screen>chkconfig csync2 on
chkconfig xinetd on</screen>
      </note>
     </listitem>
    </itemizedlist>
    <step>
     <para>
      Start the file synchronization by executing the following command on
      <emphasis>one</emphasis> of the nodes:
     </para>
<screen>csync2 <option>-xv</option></screen>
     <para>
      This will synchronize all the files once. If all files can be
      synchronized successfully, &csync; will finish with no errors.
     </para>
     <para>
      If one or several files that are to be synchronized have been modified
      also on other nodes (not only on the current one), &csync; will report
      a conflict. You will get an output similar to the one below:
     </para>
<screen>While syncing file /etc/corosync/corosync.conf:
ERROR from peer hex-14: File is also marked dirty here!
Finished with 1 errors.</screen>
    </step>
    <step>
     <para>
      If you are sure that the file version on the current node is the
      <quote>best</quote> one, you can resolve the conflict by forcing this
      file and resynchronizing:
     </para>
<screen>csync2 -f /etc/corosync/corosync.conf
csync2 -x</screen>
    </step>
   </procedure>
   <para>
    For more information on the &csync; options run
    <command>csync2&nbsp;<option>-help</option></command>.
   </para>
   <note>
    <title>Triggering Synchronization</title>
    <para>
     &csync; does not continuously synchronize files between the nodes. Each
     time you have updated any of the files that need to be synchronized,
     you need to resynchronize the files manually.
    </para>
   </note>
   <para>
    After you have synchronized the key files to all nodes in the cluster,
    start the basic services to bring the cluster online as described in
    <xref linkend="sec.ha.installation.start"
    />.
   </para>
  </sect2>

  <sect2 id="sec.ha.installation.setup.services">
   <title>Starting Services</title>
   <para>
    Optionally, the &yast; cluster module lets you define if to start
    certain services on a node at boot time. You can also use the module to
    start and stop the services manually (in case you do not want to use the
    command line for that). In order to bring the cluster nodes online and
    to start the cluster resource manager, &ais; must be started as a
    service.
   </para>
   <procedure id="pro.ha.installation.setup.services">
    <title>Starting or Stopping Services</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Service</guimenu>
      category.
     </para>
    </step>
    <step>
     <para>
      To start &ais; each time this cluster node is booted, select the
      respective option in the <guimenu>Booting</guimenu> group.
     </para>
    </step>
    <step>
     <para>
      If you want to use the &hbgui; for configuring, managing and
      monitoring cluster resources, activate <guimenu>Start mgmtd as
      Well</guimenu>. This daemon is needed for the GUI.
     </para>
    </step>
    <step>
     <para>
      To start or stop &ais; immediately, click the respective button.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Finish</guimenu> to close the &yast; cluster module.
     </para>
    </step>
   </procedure>
   <para>
    If you selected <guimenu>Off</guimenu> in the <guimenu>Booting</guimenu>
    group, you must start &ais; manually each time this node is booted. To
    start &ais; manually, use the <command>rcopenais&nbsp; start</command>
    command.
   </para>
  </sect2>

<!-- <procedure id="pro.ha.installatiom.setup.manual"><title>Configuring the Cluster Manually</title>
   <step>
   <para>FIXME: possible contents: /etc/corosync/corosync.conf.example - The easiest way to do so
   is to copy the <filename>/etc/corosync/corosync.conf</filename> file to the other nodes in the
   cluster. As each node needs to have a unique node ID, make sure to adjust the node ID
   accordingly after copying the file. - what about synchronization with csnyc2? -  To enable 
   RRP make the following changes to corosync.conf:
   1.
   In the totem section, add rrp_mode=active or rrp_mode=passive
   2.
   Add a second interface section with a different bindnetaddr for your second network. </para>
   </step>
   </procedure>-->
 </sect1>
 <sect1 id="sec.ha.installation.start">
  <title>Bringing the Cluster Online</title>

  <para>
   After the initial cluster configuration is done, you can now start the
   service needed to bring the stack online.
  </para>

  <procedure>
   <title>Starting &ais;/&corosync; and Checking the Status</title>
   <step>
    <para>
     Run the following command on each of the cluster nodes to start
     &ais;/&corosync;:
    </para>
<screen>rcopenais start</screen>
   </step>
   <step>
    <para>
     On one of the nodes, check the cluster status with the following
     command:
    </para>
<screen>crm_mon</screen>
    <para>
     If all nodes are online, the output should be similar to the following:
    </para>
<screen>============
Last updated: Tue Mar  2 18:35:34 2010
Stack: openais
Current DC: e229 - partition with quorum
Version: 1.1.1-530add2a3721a0ecccb24660a97dbfdaa3e68f51
2 Nodes configured, 2 expected votes
0 Resources configured.
============
     
 Online: [ e231 e229 ]</screen>
    <para>
     This output indicates that the cluster resource manager is started and
     is ready to manage resources.
    </para>
   </step>
  </procedure>

  <para>
   After the basic configuration is done and the nodes are online, you can
   now start to configure cluster resources. Use either the
   <command>crm</command> command line tool or the graphical user interface.
   For more information, refer to
   <xref linkend="cha.ha.configuration.gui"
   /> or
   <xref linkend="cha.ha.manual_config"/>.
  </para>
 </sect1>
 <sect1 id="sec.ha.installation.autoyast">
  <title>Mass Deployment with &ay;</title>

<!--from lmb on [ha-devel] 2010-02-19: 
    That wouldn't be included - that doesn't seem to be something that
    autoyast2 can do, since it installs only a node image, replicated
    configuration files are difficult.

    However, if you clone a base install using autoyast2, I'd expect (I've
    not tested this myself!) that with SP1, it comes up to the state where
    it can receive files via csync2 from the already configured nodes, which
   should get it to the state where it can automatically join the cluster.-->

  <para>
   &ay; is a system for installing one or more &sle; systems automatically
   and without user intervention. &sle; lets you create a &ay; profile that
   contains installation and configuration data. The profile tells &ay; what
   to install and how to configure the installed system to get a completely
   ready-to-use system in the end. This profile can then be used for mass
   deployment in different ways.
  </para>

  <para>
   For detailed instructions of how to make use of &ay; in various
   scenarios, see the &sle; &productnumber; &deploy;, available at
   <ulink
    url="http://www.novell.com/documentation"/>. Refer to chapter
   <citetitle>Automated Installation</citetitle>.
  </para>

  <procedure id="pro.ha.installation.clone.node">
   <title>Cloning a Cluster Node with &ay;</title>
   <para>
    The following procedure is suitable for deploying cluster nodes which
    are clones of an already existing node. The cloned nodes will have the
    same packages installed and the same system configuration.
   </para>
   <para>
    If you need to deploy cluster nodes on non-identical hardware, refer to
    the <citetitle>Rule-Based Autoinstallation</citetitle> section in the
    &sle; &productnumber; &deploy;, available at
    <ulink url="http://www.novell.com/documentation"/>.
   </para>
   <important>
    <title>Identical Hardware</title>
    <para>
     This scenario assumes you are rolling out &productname; &productnumber;
     to a set of machines with exactly the same hardware configuration.
    </para>
   </important>
   <step>
    <para>
     Make sure the node you want to clone is correctly installed and
     configured as described in <xref linkend="sec.ha.installation.inst"/>
     and <xref linkend="sec.ha.installation.setup"/>.
    </para>
   </step>
   <step>
    <para>
     Follow the description outlined in the &sle; &productnumber; &deploy;
     for simple mass installation. This includes the following basic steps:
    </para>
    <substeps>
     <step>
      <para>
       Creating an &ay; profile. Use the &ay; GUI to create and modify a
       profile from the existing system configuration. In &ay;, choose the
       <guimenu>&ha;</guimenu> module and click the <guimenu>Clone</guimenu>
       button. If needed, adjust the configuration in the other modules and
       save the resulting control file as XML.
      </para>
     </step>
     <step>
      <para>
       Determining the source of the &ay; profile and the parameter to pass
       to the installation routines for the other nodes.
      </para>
     </step>
     <step>
      <para>
       Determining the source of the &sls; and &productname; installation
       data.
      </para>
     </step>
     <step>
      <para>
       Determining and setting up the boot scenario for autoinstallation.
      </para>
     </step>
     <step>
      <para>
       Passing the command line to the installation routines, either by
       adding the parameters manually or by creating an
       <filename>info</filename> file.
      </para>
     </step>
     <step>
      <para>
       Starting and monitoring the autoinstallation process.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <para>
   After the clone has been successfully installed, execute the following
   steps to make the cloned node join the cluster:
  </para>

  <procedure id="pro.ha.installation.clone.start">
   <title>Bringing the Cloned Node Online</title>
   <step>
    <para>
     Transfer the key configuration files from the already configured nodes
     to the cloned node with &csync; as described in
     <xref linkend="sec.ha.installation.setup.csync2"/>.
    </para>
   </step>
   <step>
    <para>
     Start the &ais; service on the cloned node as described in
     <xref
      linkend="sec.ha.installation.start"/> to bring the node
     online.
    </para>
   </step>
  </procedure>

  <para>
   The cloned node will now join the cluster because the
   <filename>/etc/corosync/corosync.conf</filename> file has been applied
   to the cloned node via &csync;. The CIB is automatically synchronized
   among the cluster nodes.
  </para>
 </sect1>
</chapter>
