<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<chapter id="cha.ha.installation">
 <title>Installation and Basic Setup</title>
 <abstract>
  <para>
   This chapter describes how to install and set up &productnamereg;
   &productnumber; from scratch. Choose between an automatic setup which
   allows you to have a cluster up and running within a few minutes (with
   the choice to adjust any options later on) or decide for a manual setup,
   allowing you to set your individual options right at the beginning.
  </para>
<!--https://bugzilla.novell.com/show_bug.cgi?id=573817#c6-->
  <para>
   Refer to chapter <xref linkend="app.ha.migration"/> if you want to
   migrate an existing cluster that runs an older version of &productname;
   or if you want to update any software packages on nodes that are part of
   a running cluster.
  </para>
 </abstract>
 <sect1 id="sec.ha.installation.terms">
  <title>Definition of Terms</title>

  <variablelist>
   <varlistentry>
    <term>Existing Cluster</term>
    <listitem>
     <para>
      &def-existing-cluster;
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multicast</term>
    <listitem>
     <para>
      &def-multicast; If multicast does not comply with your corporate IT
      policy, use unicast instead.
     </para>
     <note>
      <title>Switches and Multicast</title>
      <para>
       If you want to use multicast for cluster communication, make sure
       your switches support multicast.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.mcastaddr">
    <term>Multicast Address (<systemitem>mcastaddr</systemitem>)
   </term>
    <listitem>
     <para>
      &def-mcastaddr; If IPv6 networking is used, node IDs must be
      specified. You can use any multicast address in your private network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multicast Port (<systemitem>mcastport</systemitem>)</term>
    <listitem>
     <para>
      &def-mcastport; &corosync; uses two ports: the specified
      <literal>mcastport</literal> for receiving multicast, and
      <literal>mcastport -1</literal> for sending multicast.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Unicast</term>
    <listitem>
     <para>
      &def-unicast;
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Bind Network Address (<systemitem>bindnetaddr</systemitem>)
  </term>
    <listitem>
     <para>
      &def-bindnetaddr; To ease sharing configuration files across the
      cluster, &corosync; uses network interface netmask to mask only the address
      bits that are used for routing the network. For example, if the local
      interface is <literal>192.168.5.92</literal> with netmask
      <literal>255.255.255.0</literal>, set
      <systemitem>bindnetaddr</systemitem> to
      <literal>192.168.5.0</literal>. If the local interface is
      <literal>192.168.5.92</literal> with netmask
      <literal>255.255.255.192</literal>, set
      <systemitem>bindnetaddr</systemitem> to
      <literal>192.168.5.64</literal>.
     </para>
     <note>
      <title>Network Address for All Nodes</title>
      <para>
       As the same &corosync; configuration will be used on all nodes, make
       sure to use a network address as
       <systemitem>bindnetaddr</systemitem>, not the address of a specific
       network interface.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.rrp">
    <term>Redundant Ring Protocol (RRP)</term>
    <listitem>
     <para>
      &def-rrp; A logical token-passing ring is imposed on all participating
      nodes to deliver messages in a reliable and sorted manner. A node is
      allowed to broadcast a message only if it holds the token. For more
      information, refer to
      <ulink url="http://www.rcsc.de/pdf/icdcs02.pdf"/>.
     </para>
     <para>
      When having defined redundant communication channels in &corosync;,
      use RRP to tell the cluster how to use these interfaces. RRP can have
      three modes (<literal>rrp_mode</literal>):
     </para>
     <itemizedlist>
      <listitem>
       <para>
        If set to <literal>active</literal>, &corosync; uses both interfaces
        actively.
       </para>
      </listitem>
      <listitem>
       <para>
        If set to <literal>passive</literal>, &corosync; sends messages
        alternatively over the available networks.
       </para>
      </listitem>
      <listitem>
       <para>
        If set to <literal>none</literal>, RRP is disabled.
       </para>
      </listitem>
     </itemizedlist>
<!--With RRP, two physically separate networks are used for communication. In case one
      network fails, the cluster nodes can still communicate via the other network.-->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&csync;</term>
    <listitem>
     <para>
      &def-csync2; &csync; can handle any number of hosts, sorted into
      synchronization groups. Each synchronization group has its own list of
      member hosts and its include/exclude patterns that define which Ô¨Åles
      should be synchronized in the synchronization group. The groups, the
      hostnames belonging to each group, and the include/exclude rules for
      each group are specified in the &csync; configuration file,
      <filename>/etc/csync2/csync2.cfg</filename>.
     </para>
     <para>
      For authentication, &csync; uses the IP addresses and pre-shared keys
      within a synchronization group. You need to generate one key file for
      each synchronization group and copy it to all group members.
     </para>
     <para>
      For more information about &csync;, refer to
      <ulink
       url="http://oss.linbit.com/csync2/paper.pdf"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><systemitem class="resource">conntrack</systemitem> Tools</term>
    <listitem>
     <para>
      &def-conntrack; For detailed information, refer to
      <ulink
       url="http://conntrack-tools.netfilter.org/"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&ay;</term>
    <listitem>
     <para>
      &def-ay; On &sle; you can create an &ay; profile that contains
      installation and configuration data. The profile tells &ay; what to
      install and how to configure the installed system to get a
      ready-to-use system in the end. This profile can then be used for mass
      deployment in different ways (for example, to clone existing cluster
      nodes).
     </para>
     <para>
      For detailed instructions on how to use &ay; in various scenarios, see
      the &sle; &productnumber; &deploy;, available at
      <ulink url="http://www.suse.com/doc"/>. Refer to chapter
      <citetitle>Automated Installation</citetitle>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
<?dbfo-need height="20em"?>
 <sect1 id="sec.ha.installation.overview">
  <title>Overview</title>

  <para>
   The following basic steps are needed for installation and initial cluster
   setup.
<!--They can
   either be executed manually or automatically, using one the methods mentioned in <xref
    linkend="sec.ha.installation.methods"/>.-->
  </para>

  <procedure>
   <step>
    <para>
     <xref linkend="sec.ha.installation.add-on" xrefstyle="select:title"/>:
    </para>
    <para>
     Install the software packages with &yast;. Alternatively, you can
     install them from the command line with <command>zypper</command>:
    </para>
<screen>zypper in -t pattern ha_sles</screen>
   </step>
   <step>
    <para>
     Initial Cluster Setup:
    </para>
    <para>
     After installing the software on all nodes that will be part of your
     cluster, the following steps are needed to initially configure the
     cluster.
    </para>
    <substeps>
     <step>
      <para>
       <xref linkend="sec.ha.installation.setup.channels" xrefstyle="select:title"/>
      </para>
     </step>
     <step>
      <para>
       Optional:
       <xref linkend="sec.ha.installation.setup.security" xrefstyle="select:title"/>
      </para>
     </step>
     <step>
      <para>
       <xref linkend="sec.ha.installation.setup.csync2" xrefstyle="select:title"/>.
       Whereas the configuration of &csync; is done on one node only, the
       services &csync; and
       <systemitem
        class="daemon">xinetd</systemitem> need to be
       started on all nodes.
      </para>
     </step>
     <step>
      <para>
       Optional:
       <xref linkend="sec.ha.installation.setup.conntrackd"  xrefstyle="select:title"/>
      </para>
     </step>
     <step>
      <para>
       <xref linkend="sec.ha.installation.setup.services" xrefstyle="select:title"/>
      </para>
     </step>
     <step>
      <para>
       <xref linkend="sec.ha.installation.start" xrefstyle="select:title"/>.
       The &corosync; service needs to be started on all nodes.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <para>
   The cluster setup steps can either be executed automatically (with
   bootstrap scripts) or manually (with the &yast; cluster module or from
   command line).
  </para>

  <itemizedlist>
   <listitem>
    <para>
     If you decide for an automatic cluster setup, refer to
     <xref
      linkend="sec.ha.installation.setup.auto"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For a manual setup (or for adjusting any options after the automatic
     setup), refer to <xref linkend="sec.ha.installation.setup.manual"/>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   You can also use a combination of both setup methods, for example: set up
   one node with &yast; cluster and then use <command>ha-cluster-join</command>
   to integrate more nodes.
  </para>

  <para>
   Existing nodes can also be cloned for mass deployment with &ay;. The
   cloned nodes will have the same packages installed and the same system
   configuration. For details, refer to
   <xref linkend="sec.ha.installation.autoyast"/>.
  </para>
 </sect1>

 <sect1 id="sec.ha.installation.add-on">
  <title>Installation as Add-on</title>

  <para>
   The packages needed for configuring and managing a cluster with the
   &hasi; are included in the <literal>&ha;</literal> installation pattern.
   This pattern is only available after &productname; has been installed as
   add-on to &sls;. For information on how to install add-on products, see
   the &sle; &productnumber; &deploy;, available at
   <ulink
    url="http://www.suse.com/doc/sles11"/>. Refer to chapter
   <citetitle>Installing Add-On Products</citetitle>.
<!--taroth: need to use hard-coded link here as the target is not included in the same set-->
  </para>

  <procedure id="pro.ha.install.pattern">
   <title>Installing the &ha; Pattern</title>
   <step>
    <para>
     Start &yast; as &rootuser; user and select <menuchoice>
     <guimenu>Software</guimenu> <guimenu>Software Management</guimenu>
     </menuchoice>.
    </para>
    <para>
     Alternatively, start the &yast; package manager as &rootuser; on a
     command line with <command>yast2&nbsp;sw_single</command>.
    </para>
   </step>
   <step>
    <para>
     From the <guimenu>Filter</guimenu> list, select
     <guimenu>Patterns</guimenu> and activate the <guimenu>High
     Availability</guimenu> pattern in the pattern list.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Accept</guimenu> to start installing the packages.
    </para>
    <note>
     <para>
      The software packages needed for &ha; clusters are
      <emphasis>not</emphasis> automatically copied to the cluster nodes.
     </para>
    </note>
   </step>
   <step>
    <para>
     Install the &ha; pattern on <emphasis>all</emphasis> machines that will
     be part of your cluster.
    </para>
    <para>
     If you do not want to install &slsreg; &productnumber; and
     &productname; &productnumber; manually on all nodes that will be part
     of your cluster, use &ay; to clone existing nodes. For more
     information, refer to
     <xref
      linkend="sec.ha.installation.autoyast"/>.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 id="sec.ha.installation.setup.auto">
  <title>Automatic Cluster Setup (ha-cluster-bootstrap)</title>

  <para> The <systemitem class="resource">ha-cluster-bootstrap</systemitem> package provides
      everything you need to get a one-node cluster up and running, to make other nodes join, and to
      remove nodes from an existing cluster: </para>

  <variablelist>
   <varlistentry>
    <term><xref linkend="pro.ha.installation.setup.ha-cluster-init" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      With <command>ha-cluster-init</command>, define the basic parameters needed
      for cluster communication and (optionally) set up a &stonith;
      mechanism to protect your shared storage. This leaves you with a
      running one-node cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="pro.ha.installation.setup.ha-cluster-join" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      With <command>ha-cluster-join</command>, add more nodes to your cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="pro.ha.installation.setup.ha-cluster-remove" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      With <command>ha-cluster-remove</command>, remove nodes from your cluster.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   All commands execute bootstrap scripts that require only a minimum of
   time and manual intervention. Any options set during the bootstrap
   process can be modified later with the &yast; cluster module.
  </para>

  <para>
   Before starting the automatic setup, make sure that the following
   prerequisites are fulfilled on all nodes that will participate in the
   cluster:
  </para>

  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     The requirements listed in <xref linkend="sec.ha.requirements.sw"/> and
     <xref
      linkend="sec.ha.requirements.other"/> are fulfilled.
    </para>
   </listitem>
   <listitem>
    <para> The <systemitem class="resource">ha-cluster-bootstrap</systemitem> package is installed. </para>
   </listitem>
   <listitem>
    <para>
     The network is configured according to your needs. For example, a
     private network is available for cluster communication and network
     device bonding is configured. For information on bonding, refer to
     <xref linkend="cha.ha.netbonding"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     If you want to use SBD for your shared storage, you need one shared
     block device for SBD. The block device need not be formatted. For more
     information, refer to <xref
      linkend="cha.ha.storage.protect"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes must be able to see the shared storage via the same paths
     (<filename>/dev/disk/by-path/...</filename> or
     <filename>/dev/disk/by-id/...</filename>).
    </para>
   </listitem>
  </itemizedlist>

  <procedure id="pro.ha.installation.setup.ha-cluster-init">
   <title>Automatically Setting Up the First Node</title>
   <remark>taroth 2014-06-27: todo: https://bugzilla.novell.com/show_bug.cgi?id=821123:
    Documentation should mention "sleha-init -t ocfs2"</remark>
   <para>
    The <command>ha-cluster-init</command> command checks for configuration of
    NTP and guides you through configuration of the cluster communication
    layer (&corosync;), and (optionally) through the configuration of SBD to
    protect your shared storage. Follow the steps below. For details, refer
    to the <command>ha-cluster-init</command> man page.
   </para>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine you want to use
     as cluster node.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing
    </para>
<screen>ha-cluster-init</screen>
    <para>
     If NTP has not been configured to start at boot time, a message
     appears.
    </para>
    <para>
     If you decide to continue anyway, the script will automatically
     generate keys for SSH access and for the &csync; synchronization tool
     and start the services needed for both.
    </para>
   </step>
   <step>
    <para>
     To configure the cluster communication layer (&corosync;):
    </para>
    <substeps>
     <step>
      <para>
       Enter a network address to bind to. By default, the script will
       propose the network address of <systemitem>eth0</systemitem>.
       Alternatively, enter a different network address, for example the
       address of <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast address. The script proposes a random address that
       you can use as default.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast port. The script proposes <literal>5405</literal>
       as default.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To configure SBD (optional), enter a persistent path to the partition
     of your block device that you want to use for SBD. The path must be
     consistent across all nodes in the cluster.
    </para>
    <para>
<!--FIXME: what happens in the background? any resources added?-->
    </para>
    <para>
     Finally, the script will start the &pace; service to bring the one-node
     cluster online and enable the Web management interface &hawk;. The URL
     to use for &hawk; is displayed on the screen.
    </para>
   </step>
   <step>
    <para> For any details of the setup process, check
            <filename>/var/log/ha-cluster-bootstrap.log</filename>. </para>
   </step>
  </procedure>

  <para>
   You now have a running one-node cluster. If you want to check the cluster
   status or start managing resources, proceed by logging in to one of the
   user interfaces or &hawk;. For more information, refer to
   <xref linkend="cha.ha.configuration.hawk"/>.
  </para>

  <important>
   <title>Secure Password</title>
   <para> The bootstrap procedure creates a linux user named <systemitem
     class="username">hacluster</systemitem> with the password
     <literal>linux</literal>. You need it for logging in to &hawk;. Replace
    the default password with a secure one as soon as possible: </para>
<screen>passwd hacluster</screen>
  </important>

  <procedure id="pro.ha.installation.setup.ha-cluster-join">
   <title>Adding Nodes to an Existing Cluster</title>
   <para>
    If you have a cluster up and running (with one or more nodes), add more
    cluster nodes with the <command>ha-cluster-join</command> bootstrap script.
    The script only needs access to an existing cluster node and will
    complete the basic setup on the current machine automatically. Follow
    the steps below. For details, refer to the <command>ha-cluster-join</command>
    man page.
   </para>
   <para>
    If you have configured the existing cluster nodes with the &yast;
    cluster module, make sure the following prerequisites are fulfilled
    before you run <command>ha-cluster-join</command>:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The &rootuser; user on the existing nodes has SSH keys in place for
      passwordless login.
     </para>
    </listitem>
    <listitem>
     <para>
      &csync; is configured on the existing nodes. For details, refer to
      <xref
       linkend="pro.ha.installation.setup.csync2.yast"/>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If you are logged in to the first node via &hawk;, you can follow the
    changes in cluster status and view the resources being activated in the
    Web interface.
   </para>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine supposed to
     join the cluster.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen>ha-cluster-join</screen>
    <para>
     If NTP has not been configured to start at boot time, a message
     appears.
    </para>
   </step>
   <step>
    <para>
     If you decide to continue anyway, you will be prompted for the IP
     address of an existing node. Enter the IP address.
    </para>
   </step>
   <step>
    <para>
     If you have not already configured a passwordless SSH access between
     both machines, you will also be prompted for the &rootuser; password of
     the existing node.
    </para>
    <para>
     After logging in to the specified node, the script will copy the
     &corosync; configuration, configure SSH and &csync;, and will bring the
     current machine online as new cluster node. Apart from that, it will
     start the service needed for &hawk;.
<!--taroth 2011-11-08: maybe 
      remove the following sentence? asked tserong-->
     If you have configured shared storage with OCFS2, it will also
     automatically create the mountpoint directory for the OCFS2 file
     system.
    </para>
   </step>
   <step>
    <para>
     Repeat the steps above for all machines you want to add to the cluster.
    </para>
   </step>
   <step>
    <para> For details of the process, check <filename>/var/log/ha-cluster-bootstrap.log</filename>. </para>
   </step>
  </procedure>

  <important>
   <title>Check <systemitem>no-quorum-policy</systemitem></title>
   <para>
    After adding all nodes, check if you need to adjust the
    <systemitem>no-quorum-policy</systemitem> in the global cluster options.
    This is especially important for two-node clusters. For more
    information, refer to
    <xref
     linkend="sec.ha.configuration.basics.global.quorum"/>.
   </para>
  </important>

  <procedure id="pro.ha.installation.setup.ha-cluster-remove">
   <title>Removing Nodes From An Existing Cluster</title>
   <para>
    If you have a cluster up and running (with at least two nodes), you can
    remove single nodes from the cluster with the
    <command>ha-cluster-remove</command> bootstrap script. You need to know the
    IP address or hostname of the node you want to remove from the cluster.
    Follow the steps below. For details, refer to the
    <command>ha-cluster-remove</command> man page.
   </para>
<!--taroth 2013-03-12: is it required that rcopenais is running one the node I 
    want to take down? taroth 2013-03-12: according to tserong: no, does not
    matter-->
   <step>
    <para>
     Log in as &rootuser; to one of the cluster nodes.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen>ha-cluster-remove -c <replaceable>IP_ADDR_OR_HOSTNAME</replaceable></screen>
    <para>
     The script enables the <systemitem class="daemon">sshd</systemitem>,
     stops the &pace; service on the specified node, and propagates the files
     to synchronize with &csync; across the remaining nodes.
    </para>
<!--information taken from bnc#810292, c#5-->
    <para>
     If you specified a hostname and the node to remove cannot be contacted
     (or the hostname cannot be resolved), the script will inform you and
     ask if the node should be removed anyway. If you specified an IP
     address and the node cannot be contacted, you will be asked to enter
     the hostname and to confirm whether to remove the node anyway.
    </para>
   </step>
   <step>
    <para>
     To remove more nodes, repeat the step above.
    </para>
   </step>
   <step>
    <para> For details of the process, check <filename>/var/log/ha-cluster-bootstrap.log</filename>. </para>
   </step>
  </procedure>

  <para>
   If you need to re-add the removed node at a later point in time, add it
   with <command>ha-cluster-join</command>. For details, refer to
   <xref 
  linkend="pro.ha.installation.setup.ha-cluster-join"/>.
  </para>
 </sect1>
 <sect1 id="sec.ha.installation.setup.manual">
  <title>Manual Cluster Setup (&yast;)</title>

  <para>
   See <xref linkend="sec.ha.installation.overview"/> for an overview of all
   steps for initial setup.
  </para>

  <remark>taroth 2012-02-29: todo for future revisions - check procedures with
 yast ncurses (not possible to jump around between yast sections/screens with
 the text interface)</remark>

  <sect2 id="sec.ha.installation.setup.yast2cluster">
   <title>&yast; Cluster Module</title>
   <remark>taroth 2014-06-27: todo: https://bugzilla.novell.com/show_bug.cgi?id=879837:
    Documentation about yast2 cluster need to be updated</remark>
   <para>
    The following sections guide you through each of the setup steps, using
    the &yast; cluster module. To access it, start &yast; as &rootuser; and
    select <menuchoice> <guimenu>&ha;</guimenu> <guimenu>Cluster</guimenu>
    </menuchoice>. Alternatively, start the module from command line with
    <command>yast2&nbsp;cluster</command>.
   </para>
   <para>
    If you start the cluster module for the first time, it appears as
    wizard, guiding you through all the steps necessary for basic setup.
    Otherwise, click the categories on the left panel to access the
    configuration options for each step.
   </para>
   <figure>
    <title>&yast; Cluster Module&mdash;Overview</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_cluster_main.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_cluster_main.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    The &yast; cluster module automatically opens the ports in the firewall
    that are needed for cluster communication on the current machine. The
    configuration is written to
    <filename>/etc/sysconfig/SuSEfirewall2.d/services/cluster</filename>.
   </para>
   <para>
    Note that some options in the &yast; cluster module apply only to the
    current node, whereas others may automatically be transferred to all
    nodes. Find detailed information about this in the following sections.
   </para>
  </sect2>

  <sect2 id="sec.ha.installation.setup.channels">
   <title>Defining the Communication Channels</title>
   <para>
    For successful communication between the cluster nodes, define at least
    one communication channel.
   </para>
   <important>
    <title>Redundant Communication Paths</title>
    <para>
     However, it is highly recommended to set up cluster communication via
     two or more redundant paths. This can be done via:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <xref linkend="cha.ha.netbonding" xrefstyle="select:title"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       A second communication channel in &corosync;. For details, see
       <xref
        linkend="pro.ha.installation.setup.channel2"/>.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If possible, choose network device bonding.
    </para>
   </important>
   <procedure id="pro.ha.installation.setup.channel1">
    <title>Defining the First Communication Channel</title>
    <para>
     For communication between the cluster nodes, use either multicast (UDP)
     or unicast (UDPU).
    </para>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Communication
      Channels</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      To use multicast:
     </para>
     <substeps>
      <step>
       <para>
        Set the <guimenu>Transport</guimenu> protocol to
        <literal>UDP</literal>.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Bind Network Address</guimenu>. Set the value to
        the subnet you will use for cluster multicast.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Multicast Address</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Multicast Port</guimenu>.
       </para>
       <para>
        Wit the values entered above, you have now defined
        <emphasis>one</emphasis> communication channel for the cluster. In
        multicast mode, the same <systemitem>bindnetaddr</systemitem>,
        <systemitem>mcastaddr</systemitem>, and
        <systemitem>mcastport</systemitem> will be used for all cluster
        nodes. All nodes in the cluster will know each other by using the
        same multicast address. For different clusters, use different
        multicast addresses.
<!--taroth 2011-10-26: for the records a statement by lmb: Setting up two or more
         clusters that use the same multicast address, but a different port, also works, 
         but is less efficient)-->
       </para>
       <figure>
        <title>&yast; Cluster&mdash;Multicast Configuration</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="yast2_cluster_comm_multicast.png" width="100%"
           format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="yast2_cluster_comm_multicast.png" width="75%"
           format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      To use unicast:
     </para>
     <substeps>
      <step>
       <para>
        Set the <guimenu>Transport</guimenu> protocol to
        <literal>UDPU</literal>.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Bind Network Address</guimenu>. Set the value to
        the subnet you will use for cluster unicast.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Multicast Port</guimenu>.
       </para>
      </step>
      <step>
       <para>
        For unicast communication, &corosync; needs to know the IP addresses
        of all nodes in the cluster. For each node that will be part of the
        cluster, click <guimenu>Add</guimenu> and enter its IP address. To
        modify or remove any addresses of cluster members, use the
        <guimenu>Edit</guimenu> or <guimenu>Del</guimenu> buttons.
       </para>
       <figure>
        <title>&yast; Cluster&mdash;Unicast Configuration</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="yast2_cluster_comm_unicast.png" width="100%"
          format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="yast2_cluster_comm_unicast.png" width="75%"
          format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Activate <guimenu>Auto Generate Node ID</guimenu> to automatically
      generate a unique ID for every cluster node.
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module. &yast; writes the configuration
      to <filename>/etc/corosync/corosync.conf</filename>.
     </para>
    </step>
    <step>
     <para>
      If needed, define a second communication channel as described below.
      Or click <guimenu>Next</guimenu> and proceed with
      <xref linkend="pro.ha.installation.setup.security"
      />.
     </para>
    </step>
   </procedure>

   <procedure id="pro.ha.installation.setup.channel2">
    <title>Defining a Redundant Communication Channel</title>
    <para>
     If network device bonding cannot be used for any reason, the second
     best choice is to define a redundant communication channel (a second
     ring) in &corosync;. That way, two physically separate networks can be
     used for communication. In case one network fails, the cluster nodes
     can still communicate via the other network.
    </para>
    <important>
     <title>Redundant Rings and <filename>/etc/hosts</filename></title>
     <para>
      If multiple rings are configured, each node can have multiple IP
      addresses. This needs to be reflected in the
      <filename>/etc/hosts</filename> file of all nodes.
     </para>
    </important>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Communication
      Channels</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      Activate <guimenu>Redundant Channel</guimenu>. The redundant channel
      must use the same protocol as the first communication channel you
      defined.
     </para>
    </step>
    <step>
     <para>
      If you use multicast, define the <guimenu>Bind Network
      Address</guimenu>, the <guimenu>Multicast Address</guimenu> and the
      <guimenu>Multicast Port</guimenu> for the redundant channel.
     </para>
     <para>
      If you use unicast, define the <guimenu>Bind Network
      Address</guimenu>, the <guimenu>Multicast Port</guimenu> and enter the
      IP addresses of all nodes that will be part of the cluster.
     </para>
     <para>
      Now you have defined an additional communication channel in &corosync;
      that will form a second token-passing ring. In
      <filename>/etc/corosync/corosync.conf</filename>, the primary ring
      (the first channel you have configured) gets the ringnumber
      <literal>0</literal>, the second ring (redundant channel) the
      ringnumber <literal>1</literal>.
     </para>
    </step>
    <step>
     <para>
      To tell &corosync; how and when to use the different channels, select
      the <guimenu>rrp_mode</guimenu> you want to use
      (<literal>active</literal> or <literal>passive</literal>). For more
      information about the modes, refer to
      <xref
       linkend="vle.ha.rrp"/> or click <guimenu>Help</guimenu>.
      As soon as RRP is used, the Stream Control Transmission Protocol
      (SCTP) is used for communication between the nodes (instead of TCP).
      The &hasi; monitors the status of the current rings and automatically
      re-enables redundant rings after faults. Alternatively, you can also
      check the ring status manually with
      <command>corosync-cfgtool</command>. View the available options with
      <option>-h</option>.
     </para>
     <para>
      If only one communication channel is defined,
      <guimenu>rrp_mode</guimenu> is automatically disabled (value
      <literal>none</literal>).
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module. &yast; writes the configuration
      to <filename>/etc/corosync/corosync.conf</filename>.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click <guimenu>Next</guimenu> and
      proceed with
      <xref
      linkend="sec.ha.installation.setup.security"/>.
     </para>
    </step>
   </procedure>
   <para>
    Find an example file for a UDP setup in
    <filename>/etc/corosync/corosync.conf.example</filename>. An example for
    UDPU setup is available in
    <filename>/etc/corosync/corosync.conf.example.udpu</filename>.
   </para>
  </sect2>

  <sect2 id="sec.ha.installation.setup.security">
   <title>Defining Authentication Settings</title>
   <para>
    The next step is to define the authentication settings for the cluster.
    You can use HMAC/SHA1 authentication that requires a shared secret used
    to protect and authenticate messages. The authentication key (password)
    you specify will be used on all nodes in the cluster.
   </para>
   <procedure id="pro.ha.installation.setup.security">
    <title>Enabling Secure Authentication</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the
      <guimenu>Security</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      Activate <guimenu>Enable Security Auth</guimenu>.
     </para>
    </step>
    <step>
     <para>
      For a newly created cluster, click <guimenu>Generate Auth Key
      File</guimenu>. An authentication key is created and written to
      <filename>/etc/corosync/authkey</filename>.
     </para>
     <figure>
      <title>&yast; Cluster&mdash;Security</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_security.png" width="100%" format="PNG"
        />
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="PNG"
        />
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      If you want the current machine to join an existing cluster, do not
      generate a new key file. Instead, copy the
      <filename>/etc/corosync/authkey</filename> from one of the nodes to
      the current machine (either manually or with &csync;).
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module. &yast; writes the configuration
      to <filename>/etc/corosync/corosync.conf</filename>.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click <guimenu>Next</guimenu> and
      proceed with <xref linkend="sec.ha.installation.setup.csync2"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.installation.setup.csync2">
   <title>Transferring the Configuration to All Nodes</title>
   <para>
    Instead of copying the resulting configuration files to all nodes
    manually, use the <command>csync2</command> tool for replication across
    all nodes in the cluster.
   </para>
   <para>
    This requires the following basic steps:
   </para>
   <procedure>
    <step>
     <para>
      <xref linkend="pro.ha.installation.setup.csync2.yast" xrefstyle="select:title"/>.
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro.ha.installation.setup.csync2.start" xrefstyle="select:title"/>.
     </para>
    </step>
   </procedure>
   <procedure id="pro.ha.installation.setup.csync2.yast">
    <title>Configuring &csync; with &yast;</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>&csync;</guimenu>
      category.
     </para>
    </step>
    <step>
     <para>
      To specify the synchronization group, click <guimenu>Add</guimenu> in
      the <guimenu>Sync Host</guimenu> group and enter the local hostnames
      of all nodes in your cluster. For each node, you must use exactly the
      strings that are returned by the <command>hostname</command> command.
     </para>
    </step>
    <step id="step.csync2.generate.key">
     <para>
      Click <guimenu>Generate Pre-Shared-Keys</guimenu> to create a key file
      for the synchronization group. The key file is written to
      <filename>/etc/csync2/key_hagroup</filename>. After it has been
      created, it must be copied manually to all members of the cluster.
     </para>
    </step>
    <step>
     <para>
      To populate the <guimenu>Sync File</guimenu> list with the files that
      usually need to be synchronized among all nodes, click <guimenu>Add
      Suggested Files</guimenu>.
     </para>
     <figure>
      <title>&yast; Cluster&mdash;&csync;</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_sync.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_sync.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      If you want to <guimenu>Edit</guimenu>, <guimenu>Add</guimenu> or
      <guimenu>Remove</guimenu> files from the list of files to be
      synchronized use the respective buttons. You must enter the absolute
      pathname for each file.
     </para>
    </step>
    <step>
     <para>
      Activate &csync; by clicking <guimenu>Turn &csync; ON</guimenu>. This
      will execute the following command to start &csync; automatically at 
      boot time:
     </para>
     <screen>systemctl enable csync2.socket</screen>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module. &yast; then writes the &csync;
      configuration to <filename>/etc/csync2/csync2.cfg</filename>. To start
      the synchronization process now, proceed with
      <xref linkend="pro.ha.installation.setup.csync2.start"/>.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click <guimenu>Next</guimenu> and
      proceed with
      <xref
       linkend="sec.ha.installation.setup.conntrackd"/>.
     </para>
    </step>
   </procedure>
   <procedure id="pro.ha.installation.setup.csync2.start">
    <title>Synchronizing the Configuration Files with &csync;</title>
    <para>
     To successfully synchronize the files with &csync;, make sure that the
     following prerequisites are met:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The same &csync; configuration is available on all nodes. Copy the
       file <filename>/etc/csync2/csync2.cfg</filename> manually to all
       nodes after you have configured it as described in
       <xref linkend="pro.ha.installation.setup.csync2.yast"/>. It is
       recommended to include this file in the list of files to be
       synchronized with &csync;.
      </para>
     </listitem>
     <listitem>
      <para>
       Copy the <filename>/etc/csync2/key_hagroup</filename> file you have
       generated on one node in <xref linkend="step.csync2.generate.key"/>
       to <emphasis>all</emphasis> nodes in the cluster as it is needed for
       authentication by &csync;. However, do not regenerate the file on the
       other nodes as it needs to be the same file on all nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       Both &csync; and <systemitem class="daemon">xinetd</systemitem> must
       be running on <emphasis>all</emphasis> nodes.
      </para>
      <note>
       <title>Starting Services at Boot Time</title>
       <para>
        Execute the following commands on all nodes to make both services
        start automatically at boot time and to start
        <systemitem class="daemon">xinetd</systemitem> now:
       </para>
<screen>&prompt.root;<command>systemctl</command> enable csync2.socket
&prompt.root;<command>systemctl</command> enable xinetd.service
&prompt.root;<command>systemctl</command> start xinetd.service</screen>
      </note>
     </listitem>
    </itemizedlist>
    <step>
     <para>
      On the node that you want to copy the configuration
      <emphasis>from</emphasis>, execute the following command:
     </para>
<screen>csync2 <option>-xv</option></screen>
     <para>
      This will synchronize all the files once by pushing them to the other
      nodes. If all files are synchronized successfully, &csync; will finish
      with no errors.
     </para>
     <para>
      If one or several files that are to be synchronized have been modified
      on other nodes (not only on the current one), &csync; will report a
      conflict. You will get an output similar to the one below:
     </para>
<screen>While syncing file /etc/corosync/corosync.conf:
ERROR from peer hex-14: File is also marked dirty here!
Finished with 1 errors.</screen>
    </step>
    <step>
     <para>
      If you are sure that the file version on the current node is the
      <quote>best</quote> one, you can resolve the conflict by forcing this
      file and resynchronizing:
     </para>
<screen>csync2 -f /etc/corosync/corosync.conf
csync2 -x</screen>
    </step>
   </procedure>
   <para>
    For more information on the &csync; options, run
    <command>csync2&nbsp;<option>-help</option></command>.
   </para>
   <note>
    <title>Pushing Synchronization After Any Changes</title>
    <para>
     &csync; only pushes changes. It does <emphasis>not</emphasis>
     continuously synchronize files between the nodes.
    </para>
    <para>
     Each time you update files that need to be synchronized, you have to
     push the changes to the other nodes: Run
     <command>csync2&nbsp;<option>-xv</option></command> on the node where
     you did the changes. If you run the command on any of the other nodes
     with unchanged files, nothing will happen.
    </para>
   </note>
  </sect2>

  <sect2 id="sec.ha.installation.setup.conntrackd">
   <title>Synchronizing Connection Status Between Cluster Nodes</title>
   <para>
    To enable <emphasis>stateful</emphasis> packet inspection for iptables,
    configure and use the conntrack tools with the following basic steps:
   </para>
   <procedure>
    <step>
     <para>
      <xref linkend="pro.ha.installation.setup.conntrackd" xrefstyle="select:title"/>.
     </para>
    </step>
    <step>
     <para>
      Configuring a resource for
      <systemitem class="daemon"
       >conntrackd</systemitem> (class:
      <literal>ocf</literal>, provider: <literal>heartbeat</literal>). If
      you use &hawk; to add the resource, use the default values proposed by
      &hawk;.
     </para>
    </step>
   </procedure>
   <para>
    After configuring the conntrack tools, you can use them for
    <xref linkend="cha.ha.lvs" xrefstyle="select:title"/>.
   </para>
<!--from fate#311872: It supports the only FTFW syncing mode now.-->
   <procedure id="pro.ha.installation.setup.conntrackd">
    <title>Configuring the <systemitem class="resource">conntrackd</systemitem> with &yast;</title>
    <para>
     Use the &yast; cluster module to configure the user-space
     <systemitem class="daemon">conntrackd</systemitem>. It needs a
     dedicated network interface that is not used for other communication
     channels. The daemon can be started via a resource agent afterward.
    </para>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Configure
      conntrackd</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      Select a <guimenu>Dedicated Interface</guimenu> for synchronizing the
      connection status. The IPv4 address of the selected interface is
      automatically detected and shown in &yast;. It must already be
      configured and it must support multicast.
<!--taroth 2011-11-09: for the records, this has nothing to do with the
       corosync conf-->
     </para>
    </step>
    <step>
     <para>
      Define the <guimenu>Multicast Address</guimenu> to be used for
      synchronizing the connection status.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Group Number</guimenu>, define a numeric ID for the group
      to synchronize the connection status to.
      <remark>emap 2011-11-10: To where?
       The other nodes? - taroth: good question :), will investigate</remark>
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Generate /etc/conntrackd/conntrackd.conf</guimenu> to
      create the configuration file for
      <systemitem class="daemon">conntrackd</systemitem>.
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click <guimenu>Next</guimenu> and
      proceed with
      <xref
       linkend="sec.ha.installation.setup.services"/>.
     </para>
    </step>
   </procedure>
   <figure>
    <title>&yast; Cluster&mdash;<systemitem class="resource">conntrackd</systemitem></title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_cluster_conntrackd.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_cluster_conntrackd.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 id="sec.ha.installation.setup.services">
   <title>Configuring Services</title>
   <para>
    In the &yast; cluster module define whether to start certain services on
    a node at boot time. You can also use the module to start and stop the
    services manually. To bring the cluster nodes online and start the
    cluster resource manager, &pace; must be running as a service.
   </para>
   <procedure id="pro.ha.installation.setup.services">
    <title>Enabling &pace; and <systemitem class="daemon">mgmtd</systemitem></title>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Service</guimenu>
      category.
     </para>
    </step>
    <step>
     <para>
      To start &pace; each time this cluster node is booted, select the
      respective option in the <guimenu>Booting</guimenu> group. If you
      select <guimenu>Off</guimenu> in the <guimenu>Booting</guimenu> group,
      you must start &pace; manually each time this node is booted. To start
      &pace; manually, use the command:
     </para>
      <screen><command>systemctl</command> start pacemaker.service</screen>
    </step>
   <!-- tarothg 2014-05-13: this should not be needed any more, check!-->
    <!--<step>
     <para>
      If you want to use the &hbgui; for configuring, managing and
      monitoring cluster resources, activate <guimenu>Enable
      mgmtd</guimenu>. This daemon is needed for the GUI.
     </para>
    </step>-->
    <step>
     <para>
      To start or stop &pace; immediately, click the respective button.
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster node, confirm your
      changes and close the cluster module. Note that the configuration only
      applies to the current machine, not to all cluster nodes.
     </para>
     <para>
      If you have done the initial cluster setup exclusively with the &yast;
      cluster module, you have now completed the basic configuration steps.
      Proceed with <xref linkend="sec.ha.installation.start"/>.
     </para>
     <figure>
      <title>&yast; Cluster&mdash;Services</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_services.png" width="100%"
         format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_services.png" width="75%"
         format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.installation.start">
   <title>Bringing the Cluster Online</title>
   <para>
    After the initial cluster configuration is done, start the
    &pace; service on <emphasis>each</emphasis> cluster node to
    bring the stack online:
   </para>
   <procedure>
    <title>Starting &pace; and Checking the Status</title>
    <step>
     <para>
      Log in to an existing node.
     </para>
    </step>
    <step>
     <para>
      Check if the service is already running:
     </para>
<screen>&prompt.root;<command>systemctl</command> status pacemaker.service</screen>
     <para>
      If not, start &pace; now:
     </para>
<screen>&prompt.root;<command>systemctl</command> start pacemaker.service</screen>
    </step>
    <step>
     <para>
      Repeat the steps above for each of the cluster nodes.
     </para>
    </step>
    <step>
     <para>
      On one of the nodes, check the cluster status with the following
      command:
     </para>
<screen>crm_mon</screen>
     <para>
      If all nodes are online, the output should be similar to the
      following:
     </para>
<screen>Last updated: Thu Feb 27 10:45:31 2014
Last change: Tue Feb 25 17:01:46 2014 by hacluster via crmd on barett-1
Stack: corosync
Current DC: barett-1 (175704363) - partition with quorum
Version: 1.1.10+git20140207.6290953-1.10-1.1.10+git20140207.6290953
2 Nodes configured
0 Resources configured


Online: [ barett-1 barett-2 ]</screen>
     <para>
      This output indicates that the cluster resource manager is started and
      is ready to manage resources.
     </para>
    </step>
   </procedure>
   <para>
    After the basic configuration is done and the nodes are online, you can
    start to configure cluster resources, using one of the cluster
    management tools like the &crmshell; (&crmsh;) or the &haweb;. For
    more information, refer to the following chapters.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.installation.autoyast">
  <title>Mass Deployment with &ay;</title>

  <para>
   The following procedure is suitable for deploying cluster nodes which are
   clones of an already existing node. The cloned nodes will have the same
   packages installed and the same system configuration.
  </para>

<!--from lmb on [ha-devel] 2010-02-19: 
    That wouldn't be included - that doesn't seem to be something that
    autoyast2 can do, since it installs only a node image, replicated
    configuration files are difficult.

    However, if you clone a base install using autoyast2, I'd expect (I've
    not tested this myself!) that with SP1, it comes up to the state where
    it can receive files via csync2 from the already configured nodes, which
   should get it to the state where it can automatically join the cluster.-->

  <procedure id="pro.ha.installation.clone.node">
   <title>Cloning a Cluster Node with &ay;</title>
   <important>
    <title>Identical Hardware</title>
    <para>
     This scenario assumes you are rolling out &productname; &productnumber;
     to a set of machines with exactly the same hardware configuration.
    </para>
   </important>
   <para>
    If you need to deploy cluster nodes on non-identical hardware, refer to
    the <citetitle>Rule-Based Autoinstallation</citetitle> section in the
    &sle; &productnumber; &deploy;, available at
    <ulink
     url="http://www.suse.com/doc"/>.
   </para>
   <step>
    <para>
     Make sure the node you want to clone is correctly installed and
     configured. For details, refer to
     <xref
      linkend="sec.ha.installation.add-on"/>, and
     <xref
      linkend="sec.ha.installation.setup.auto"/> or
     <xref
      linkend="sec.ha.installation.setup.manual"/>, respectively.
    </para>
   </step>
   <step>
    <para>
     Follow the description outlined in the &sle; &productnumber; &deploy;
     for simple mass installation. This includes the following basic steps:
    </para>
    <substeps>
     <step>
      <para>
       Creating an &ay; profile. Use the &ay; GUI to create and modify a
       profile based on the existing system configuration. In &ay;, choose
       the <guimenu>&ha;</guimenu> module and click the
       <guimenu>Clone</guimenu> button. If needed, adjust the configuration
       in the other modules and save the resulting control file as XML.
      </para>
     </step>
     <step>
      <para>
       Determining the source of the &ay; profile and the parameter to pass
       to the installation routines for the other nodes.
      </para>
     </step>
     <step>
      <para>
       Determining the source of the &sls; and &productname; installation
       data.
      </para>
     </step>
     <step>
      <para>
       Determining and setting up the boot scenario for autoinstallation.
      </para>
     </step>
     <step>
      <para>
       Passing the command line to the installation routines, either by
       adding the parameters manually or by creating an
       <filename>info</filename> file.
      </para>
     </step>
     <step>
      <para>
       Starting and monitoring the autoinstallation process.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <para>
   After the clone has been successfully installed, execute the following
   steps to make the cloned node join the cluster:
  </para>

  <procedure id="pro.ha.installation.clone.start">
   <title>Bringing the Cloned Node Online</title>
   <step>
    <para>
     Transfer the key configuration files from the already configured nodes
     to the cloned node with &csync; as described in
     <xref linkend="sec.ha.installation.setup.csync2"/>.
    </para>
   </step>
   <step>
    <para>
     To bring the node online, start the &pace; service on the cloned node as
     described in <xref
      linkend="sec.ha.installation.start"/>.
    </para>
   </step>
  </procedure>

  <para>
   The cloned node will now join the cluster because the
   <filename>/etc/corosync/corosync.conf</filename> file has been applied to
   the cloned node via &csync;. The CIB is automatically synchronized among
   the cluster nodes.
  </para>
 </sect1>
</chapter>
