<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!--taroth 2010-08-19: for next revision, see also Sander's book 
(chapter about cluster LVM) for more information and details to integrate here-->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ha.ocfs2">
 <title>&ocfs;</title>
 <info>
      <abstract>
        <para>
    Oracle Cluster File System 2 (&ocfs;) is a general-purpose journaling
    file system that has been fully integrated since the Linux 2.6 Kernel.
    &ocfs; allows you to store application binary files, data files, and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user-space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with &corosync; and the Distributed
    Lock Manager (DLM).
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
    </info>
    <indexterm class="startofrange" xml:id="idx.filesystems.ocfs2">
 <primary>file systems</primary>
 <secondary>&ocfs;</secondary></indexterm>
 <sect1 xml:id="sec.ha.ocfs2.features">
  <title>Features and Benefits</title>

  <para>
   &ocfs; can be used for the following storage solutions for example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     General applications and workloads.
    </para>
   </listitem>
   <listitem>
    <para>
     &xen; image store in a cluster. &xen; virtual machines and
     virtual servers can be stored on &ocfs; volumes that are mounted by
     cluster servers. This provides quick and easy portability of &xen;
     virtual machines between servers.
    </para>
   </listitem>
   <listitem>
    <para>
     LAMP (Linux, Apache, MySQL, and PHP &verbar; Perl &verbar;
     Python) stacks.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   As a high-performance, symmetric and parallel cluster file system,
   &ocfs; supports the following functions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an &ocfs; volume in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access.
    </para>
   </listitem>
   <listitem>
    <para>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   &ocfs; also provides the following capabilities:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Metadata caching.
    </para>
   </listitem>
   <listitem>
    <para>
     Metadata journaling.
    </para>
   </listitem>
   <listitem>
    <para>
     Cross-node file data consistency.
    </para>
   </listitem>
   <listitem>
    <para>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB,
     for a maximum volume size of 4 PB (Petabyte).
    </para>
   </listitem>
   <listitem>
    <para>
     Support for up to 32 cluster nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Support for &ocfs;</title>
   <para>
    &ocfs; is only supported by &suse; when used with the pcmk (Pacemaker) stack,
    as provided by &productname;. &suse; does not provide support for &ocfs; in
    combination with the o2cb stack.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec.ha.ocfs2.utils">
  <title>&ocfs; Packages and Management Utilities</title>

  <para>
   The &ocfs; Kernel module (<literal>ocfs2</literal>) is installed
   automatically in the &hasi; on &slsreg; &productnumber;. To use
   &ocfs;, make sure the following packages are installed on each node in
   the cluster: <package>ocfs2-tools</package> and
   the matching <package>ocfs2-kmp-*</package>
   packages for your Kernel.
  </para>

  <para>
   The <package>ocfs2-tools</package> package
   provides the following utilities for management of OFS2 volumes. For
   syntax information, see their man pages.
  </para>

  <table>
   <title>&ocfs; Utilities</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        &ocfs; Utility
       </para>
      </entry>
      <entry>
       <para>
        Description
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        debugfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Examines the state of the OCFS file system for the purpose of
        debugging.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        fsck.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Checks the file system for errors and optionally repairs errors.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mkfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Creates an &ocfs; file system on a device, usually a partition on
        a shared physical or logical disk.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mounted.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Detects and lists all &ocfs; volumes on a clustered system.
        Detects and lists all nodes on the system that have mounted an
        &ocfs; device or lists all &ocfs; devices.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        tunefs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Changes &ocfs; file system parameters, including the volume
        label, number of node slots, journal size for all node slots, and
        volume size.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>
 <sect1 xml:id="sec.ha.ocfs2.create.service">
  <title>Configuring &ocfs; Services and a &stonith; Resource</title>

<!--https://bugzilla.novell.com/show_bug.cgi?id=520714-->

  <para>
   Before you can create &ocfs; volumes, you must configure the following
   resources as services in the cluster: DLM, and a &stonith; resource.
   &ocfs; uses the cluster membership services from Pacemaker which run
   in user space. Therefore, DLM needs to be configured as clone resource
   that is present on each node in the cluster.
  </para>

  <para>
   The following procedure uses the <command>crm</command> shell to
   configure the cluster resources. Alternatively, you can also use
   &hawk2; to configure the resources as described in
   <xref linkend="sec.ha.ocfs2.rsc.hawk2"/>.
  </para>

  <note>
   <title>DLM Resource for Both LVM2 and &ocfs;</title>
   <para>
    Both LVM2 and &ocfs; need a DLM resource that runs on all nodes in
    the cluster and therefore usually is configured as a clone. If you have
    a setup that includes both &ocfs; and LVM2, configuring
    <emphasis>one</emphasis> DLM resource for both &ocfs; and LVM2 is
    enough.
   </para>
  </note>

  <procedure xml:id="pro.ocfs2.stonith">
   <title>Configuring a &stonith; Resource</title>
   <note>
    <title>&stonith; Device Needed</title>
    <para>
     You need to configure a fencing device. Without a &stonith;
     mechanism (like <literal>external/sbd</literal>) in place the
     configuration will fail.
    </para>
   </note>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Create an SBD partition as described in
     <xref linkend="pro.ha.storage.protect.sbd.create"/>.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure <literal>external/sbd</literal> as fencing device with
     <literal>/dev/sdb2</literal> being a dedicated partition on the shared
     storage for heartbeating and fencing:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> sbd_stonith stonith:external/sbd \
  params pcmk_delay_max=30 meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>

  <procedure xml:id="pro.ocfs2.resources">
   <title>Configuring a DLM Resource</title>
   <para>
    The configuration consists of a base group that includes several
    primitives and a base clone. Both base group and base clone can be used
    in various scenarios afterwards (for both &ocfs; and LVM2, for
    example). You only need to extended the base group with the respective
    primitives as needed. As the base group has internal colocation and
    ordering, this simplifies the overall setup as you do not need to
    specify several individual groups, clones and their dependencies.
   </para>
   <para>
    Follow the steps below for one node in the cluster:
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create the primitive resource for DLM:
    </para>
<!--Brendo, 17.11.2010: for dlm use the following params:
      params args="-q 0 -f 0 -t 1000 -a 1000"
      (see man dlm_controld) -> this keeps ocfs2 from blocking if a node is
      missing-->
<screen>&prompt.crm.conf;<command>primitive</command> dlm ocf:pacemaker:controld \
  op monitor interval="60" timeout="60"</screen>
   </step>
   <step>
    <para>
     Create a base-group for the DLM resource. As further cloned primitives
     are created, it will be added to this group.
    </para>
<screen>&prompt.crm.conf;<command>group</command> base-group dlm</screen>
   </step>
   <step>
    <para>
     Clone the base-group so that it runs on all nodes.
    </para>
<screen>&prompt.crm.conf; <command>clone</command> base-clone base-group \
  meta interleave=true target-role=Started</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ha.ocfs2.create">
  <title>Creating &ocfs; Volumes</title>

  <para>
   After you have configured a DLM cluster resource as described in
   <xref linkend="sec.ha.ocfs2.create.service"/>, configure your system to
   use &ocfs; and create OCFs2 volumes.
  </para>

  <note>
   <title>&ocfs; Volumes for Application and Data Files</title>
   <para>
    We recommend that you generally store application files and data files
    on different &ocfs; volumes. If your application volumes and data
    volumes have different requirements for mounting, it is mandatory to
    store them on different volumes.
   </para>
  </note>

  <para>
   Before you begin, prepare the block devices you plan to use for your
   &ocfs; volumes. Leave the devices as free space.
  </para>

  <para>
   Then create and format the &ocfs; volume with the
   <command>mkfs.ocfs2</command> as described in
   <xref linkend="pro.ocfs2.volume"/>. The most important parameters for the
   command are listed in <xref linkend="tab.ha.ofcs2.mkfs.ocfs2.params"/>.
   For more information and the command syntax, refer to the
   <command>mkfs.ocfs2</command> man page.
  </para>

  <table xml:id="tab.ha.ofcs2.mkfs.ocfs2.params">
   <title>Important &ocfs; Parameters</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        &ocfs; Parameter
       </para>
      </entry>
      <entry>
       <para>
        Description and Recommendation
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Volume Label (<option>-L</option>)
       </para>
      </entry>
      <entry>
       <para>
        A descriptive name for the volume to make it uniquely identifiable
        when it is mounted on different nodes. Use the
        <command>tunefs.ocfs2</command> utility to modify the label as
        needed.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Cluster Size (<option>-C</option>)
       </para>
      </entry>
      <entry>
       <para>
        Cluster size is the smallest unit of space allocated to a file to
        hold the data. For the available options and recommendations, refer
        to the <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Number of Node Slots (<option>-N</option>)
       </para>
      </entry>
      <entry>
       <para>
        The maximum number of nodes that can concurrently mount a volume.
        For each of the nodes, &ocfs; creates separate system files, such
        as the journals. Nodes that access the volume
        can be a combination of little-endian architectures (such as x86_64)
        and big-endian architectures (such as s390x).
       </para>
       <para>
        Node-specific files are referred to as local files. A node slot
        number is appended to the local file. For example:
        <literal>journal:0000</literal> belongs to whatever node is assigned
        to slot number <literal>0</literal>.
       </para>
       <para>
        Set each volume's maximum number of node slots when you create it,
        according to how many nodes that you expect to concurrently mount
        the volume. Use the <command>tunefs.ocfs2</command> utility to
        increase the number of node slots as needed. Note that the value
        cannot be decreased.
       </para>
       <para>
        In case the <option>-N</option> parameter is not specified, the
        number of slots is decided based on the size of the file system.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Block Size (<option>-b</option>)
       </para>
      </entry>
      <entry>
       <para>
        The smallest unit of space addressable by the file system. Specify
        the block size when you create the volume. For the available options
        and recommendations, refer to the <command>mkfs.ocfs2</command> man
        page.
<!-- Brendo, 17.11.2010: to detect the blocksize of the device, use: 
             blockdev -\-getbsz &lt;block-device&gt;. -->
       </para>
      </entry>
     </row>
<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c11-->
     <row>
      <entry>
       <para>
        Specific Features On/Off (<option>--fs-features</option>)
       </para>
      </entry>
      <entry>
       <para>
        A comma separated list of feature flags can be provided, and
        <systemitem>mkfs.ocfs2</systemitem> will try to create the file
        system with those features set according to the list. To turn a
        feature on, include it in the list. To turn a feature off, prepend
        <literal>no</literal> to the name.
       </para>
       <para>
        For an overview of all available flags, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Pre-Defined Features (<option>--fs-feature-level</option>)
       </para>
      </entry>
      <entry>
       <para>
        Allows you to choose from a set of pre-determined file system
        features. For the available options, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c15-->

  <para>
   If you do not specify any features when creating and formatting
   the volume with <command>mkfs.ocfs2</command>, the following features are
   enabled by default: <option>backup-super</option>,
   <option>sparse</option>, <option>inline-data</option>,
   <option>unwritten</option>, <option>metaecc</option>,
   <option>indexed-dirs</option>, and <option>xattr</option>.
  </para>

  <procedure xml:id="pro.ocfs2.volume">
   <title>Creating and Formatting an &ocfs; Volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.ocfs2</command>
     utility. For information about the syntax for this command, refer to
     the <command>mkfs.ocfs2</command> man page.
    </para>
    <para>
     For example, to create a new &ocfs; file system on
     <filename>/dev/sdb1</filename> that supports up to 32 cluster nodes,
     enter the following commands:
    </para>
<screen>&prompt.root; mkfs.ocfs2 -N 32 /dev/sdb1</screen>
<!-- Brendo, 17.11.2010: maybe add -b 4k in the example (this is common
        for software raids) -->
<!--taroth 2014-08-14: additional info from bnc#853631:
       The complete command is:    
       mkfs.ocfs2 -/-cluster-stack=pcmk -/-cluster-name=mycluster -N 32 /dev/sdb1
       Note, cluster-name should be same as defined in corosync.conf
   
       Alternatively, you can load the ocfs2 module, set the cluster_stack to pcmk and
       issue mkfs.ocfs2:
       # modprobe ocfs2_stack_user
       [...]
      This is more reliable because it picks up the cluster name from the clustering
      information.-->
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ha.ocfs2.mount">
  <title>Mounting &ocfs; Volumes</title>

  <para>
   You can either mount an &ocfs; volume manually or with the cluster
   manager, as described in <xref linkend="pro.ocfs2.mount.cluster"/>.
  </para>

  <procedure xml:id="pro.ocfs2.mount.manual">
   <title>Manually Mounting an &ocfs; Volume</title>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the
     <command>mount</command> command.
    </para>
   </step>
  </procedure>

  <warning>
   <title>Manually Mounted &ocfs; Devices</title>
   <para>
    If you mount the &ocfs; file system manually for testing purposes,
    make sure to unmount it again before starting to use it by means of
    cluster resources.
   </para>
  </warning>

  <procedure xml:id="pro.ocfs2.mount.cluster">
   <title>Mounting an &ocfs; Volume with the Cluster Resource Manager</title>
   <para>
    To mount an &ocfs; volume with the &ha; software, configure an
    ocfs2 file system resource in the cluster. The following procedure uses
    the <command>crm</command> shell to configure the cluster resources.
    Alternatively, you can also use &hawk2; to configure the resources as
    described in <xref linkend="sec.ha.ocfs2.rsc.hawk2"/>.
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the &ocfs; file system on every node in
     the cluster:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ocfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/sdb1" directory="/mnt/shared" \
  fstype="ocfs2" options="acl" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Add the <literal>ocfs2-1</literal> primitive
     to the <literal>base-group</literal> you created in
     <xref linkend="pro.ocfs2.resources"/>.
    </para>
<screen>&prompt.crm.conf;<command>modgroup</command> base-group add ocfs2-1</screen>
    <para>The <command>add</command> subcommand appends the new group
     member by default. Because of the base group's internal colocation and ordering, Pacemaker
     will only start the <systemitem class="resource">ocfs2-1</systemitem>
     resource on nodes that also have a <literal>dlm</literal> resource
     already running.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
<!--taroth 2014-08-28: appendix file commented for now, because it needs
      alignement with changes here:
      To check if you have configured all needed resources, also refer to
      <xref linkend="cha.ha.config.example"/>.-->
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ha.ocfs2.rsc.hawk2">
  <title>Configuring &ocfs; Resources With &hawk2;</title>

  <para>
   Instead of configuring the DLM and the file system resource for &ocfs;
   manually with the &crmshell;, you can also use the &ocfs; template
   in &hawk2;'s <guimenu>Setup Wizard</guimenu>.
  </para>

  <important>
   <title>Differences Between Manual Configuration and &hawk2;</title>
   <para>
    The &ocfs; template in the <guimenu>Setup Wizard</guimenu> does
    <emphasis>not</emphasis> include the configuration of a &stonith;
    resource. If you use the wizard, you still need to create an SBD
    partition on the shared storage and configure a &stonith; resource as
    described in <xref linkend="pro.ocfs2.stonith"/>.
   </para>
   <remark>taroth 2014-08-15: todo for next release: unify manual configuration
    and configuration via Hawk (use same resources, same options
    etc.), filed https://bugzilla.novell.com/show_bug.cgi?id=892116 for this</remark>
   <para>
    Using the &ocfs; template in the &hawk2; <guimenu>Setup
    Wizard</guimenu> also leads to a slightly different resource
    configuration than the manual configuration described in
    <xref linkend="pro.ocfs2.resources"/> and
    <xref linkend="pro.ocfs2.mount.cluster"/>.
   </para>
  </important>

  <procedure xml:id="pro.ha.ocfs2.rsc.hawk">
   <title>Configuring &ocfs; Resources with &hawk2;'s <guimenu>Wizard</guimenu></title>
   <step>
    <para>
     Log in to &hawk2;:
    </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Wizard</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Expand the <guimenu>File System</guimenu> category and select
     <literal>OCFS2 File System</literal>.
    </para>
   </step>
   <step>
    <para>
     Follow the instructions on the screen. If you need information about an
     option, click it to display a short help text in &hawk2;. After the last
     configuration step, <guimenu>Verify</guimenu> the values you have entered.
    </para>
    <para>
     The wizard displays the configuration snippet that will be applied to
     the CIB and any additional changes, if required.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk2-wizard-ocfs2-verify.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk2-wizard-ocfs2-verify.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Check the proposed changes. If everything is according to your wishes,
     apply the changes.
    </para>
    <para>
     A message on the screen shows if the action has been successful.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ha.ocfs2.quota">
  <title>Using Quotas on &ocfs; File Systems</title>

<!--taroth 2011-11-24: fate#310056: the following section is a stub, based on an old RN
   entry,  todo: extend for next revision-->

  <para>
   To use quotas on an &ocfs; file system, create and mount the files
   system with the appropriate quota features or mount options,
   respectively: <literal>ursquota</literal> (quota for individual users) or
   <literal>grpquota</literal> (quota for groups). These features can also
   be enabled later on an unmounted file system using
   <command>tunefs.ocfs2</command>.
  </para>

  <para>
   When a file system has the appropriate quota feature enabled, it tracks
   in its metadata how much space and files each user (or group) uses. Since
   &ocfs; treats quota information as file system-internal metadata, you
   do not need to run the <command>quotacheck</command>(8) program. All
   functionality is built into fsck.ocfs2 and the file system driver itself.
  </para>

  <para>
   To enable enforcement of limits imposed on each user or group, run
   <command>quotaon</command>(8) like you would do for any other file
   system.
  </para>

<!--taroth 2011-11-26: according to jan kara (jack), the limitations for
   repquota and warnquota are now obsolete with solving fate#310056, thus
   removing the following completely-->

<!--<para>
   With &ocfs; file systems, the following commands work as usual:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     quota(1)
    </para>
   </listitem>
   <listitem>
    <para>
     setquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     edquota(8)
    </para>
   </listitem>
  </itemizedlist>

  <para>
     Because of a limitation in the current Kernel interface, the following
   commands do not work with &ocfs;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     repquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     warnquota(8)
    </para>
   </listitem>
  </itemizedlist>-->

  <para>
   For performance reasons each cluster node performs quota accounting
   locally and synchronizes this information with a common central storage
   once per 10 seconds. This interval is tunable with
   <command>tunefs.ocfs2</command>, options
   <option>usrquota-sync-interval</option> and
   <option>grpquota-sync-interval</option>. Therefore quota information may
   not be exact at all times and as a consequence users or groups can
   slightly exceed their quota limit when operating on several cluster nodes
   in parallel.
  </para>
 </sect1>
 <sect1 xml:id="sec.ha.ocfs2.more">
  <title>For More Information</title>

  <para>
   For more information about &ocfs;, see the following links:
  </para>

  <variablelist>
   <varlistentry>
    <term><link xlink:href="https://ocfs2.wiki.kernel.org/"/>
    </term>
    <listitem>
     <para>
      The &ocfs; project home page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/"/>
    </term>
    <listitem>
     <para>
      The former &ocfs; project home page at Oracle.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/documentation"/>
    </term>
    <listitem>
     <para>
      The project's former documentation home page.
     </para>
    </listitem>
   </varlistentry>
  </variablelist><indexterm class="endofrange" startref="idx.filesystems.ocfs2"/>
 </sect1>
</chapter>
