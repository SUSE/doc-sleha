<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-ocfs2">
 <title>&ocfs;</title>
 <info>
      <abstract>
        <para>
    Oracle Cluster File System 2 (&ocfs;) is a general-purpose journaling
    file system that has been fully integrated since the Linux 2.6 Kernel.
    &ocfs; allows you to store application binary files, data files, and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with &corosync; and the Distributed
    Lock Manager (DLM).
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
    </info>
 <sect1 xml:id="sec-ha-ocfs2-features">
  <title>Features and benefits</title>

  <para>
   &ocfs; can be used for the following storage solutions for example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     General applications and workloads.
    </para>
   </listitem>
   <listitem>
    <para>
     &xen; image store in a cluster. &xen; virtual machines and
     virtual servers can be stored on &ocfs; volumes that are mounted by
     cluster servers. This provides quick and easy portability of &xen;
     virtual machines between servers.
    </para>
   </listitem>
   <listitem>
    <para>
     LAMP (Linux, Apache, MySQL, and PHP &verbar; Perl &verbar;
     Python) stacks.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   As a high-performance, symmetric and parallel cluster file system,
   &ocfs; supports the following functions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an &ocfs; volume in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access.
    </para>
   </listitem>
   <listitem>
    <para>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   &ocfs; also provides the following capabilities:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Metadata caching.
    </para>
   </listitem>
   <listitem>
    <para>
     Metadata journaling.
    </para>
   </listitem>
   <listitem>
    <para>
     Cross-node file data consistency.
    </para>
   </listitem>
   <listitem>
    <para>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB,
     for a maximum volume size of 4 PB (Petabyte).
    </para>
   </listitem>
   <listitem>
    <para>
     Support for up to 32 cluster nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Support for &ocfs;</title>
   <para>
    &ocfs; is only supported by &suse; when used with the pcmk (Pacemaker) stack,
    as provided by &productname;. &suse; does not provide support for &ocfs; in
    combination with the o2cb stack.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-utils">
  <title>&ocfs; packages and management utilities</title>

  <para>
   The &ocfs; Kernel module (<literal>ocfs2</literal>) is installed
   automatically in the &hasi; on &slsreg; &productnumber;. To use
   &ocfs;, make sure the following packages are installed on each node in
   the cluster: <package>ocfs2-tools</package> and
   the matching <package>ocfs2-kmp-*</package>
   packages for your Kernel.
  </para>

  <para>
   The <package>ocfs2-tools</package> package
   provides the following utilities for management of &ocfs; volumes. For
   syntax information, see their man pages.
  </para>
  <variablelist>
   <varlistentry>
    <term>debugfs.ocfs2</term>
    <listitem>
     <para>
      Examines the state of the OCFS file system for debugging.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>fsck.ocfs2</term>
    <listitem>
     <para>
      Checks the file system for errors and optionally repairs errors.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mkfs.ocfs2</term>
    <listitem>
     <para>
      Creates an &ocfs; file system on a device, usually a partition on
        a shared physical or logical disk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mounted.ocfs2</term>
    <listitem>
     <para>
      Detects and lists all &ocfs; volumes on a clustered system.
      Detects and lists all nodes on the system that have mounted an
      &ocfs; device or lists all &ocfs; devices.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>tunefs.ocfs2</term>
    <listitem>
     <para>
      Changes &ocfs; file system parameters, including the volume
      label, number of node slots, journal size for all node slots, and
      volume size.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-create-service">
  <title>Configuring &ocfs; services and a &stonith; resource</title>

<!--https://bugzilla.novell.com/show_bug.cgi?id=520714-->

  <para>
   Before you can create &ocfs; volumes, you must configure the following
   resources as services in the cluster: DLM and a &stonith; resource.
  </para>

  <para>
   The following procedure uses the <command>crm</command> shell to
   configure the cluster resources. Alternatively, you can also use
   &hawk2; to configure the resources as described in
   <xref linkend="sec-ha-ocfs2-rsc-hawk2"/>.
  </para>

  <procedure xml:id="pro-ocfs2-stonith">
   <title>Configuring a &stonith; resource</title>
   <note>
    <title>&stonith; device needed</title>
    <para>
     You need to configure a fencing device. Without a &stonith;
     mechanism (like <literal>external/sbd</literal>) in place the
     configuration will fail.
    </para>
   </note>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Create an SBD partition as described in
     <xref linkend="pro-ha-storage-protect-sbd-create"/>.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure <literal>external/sbd</literal> as fencing device with
     <literal>/dev/sdb2</literal> being a dedicated partition on the shared
     storage for heartbeating and fencing:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> sbd_stonith stonith:external/sbd \
  params pcmk_delay_max=30 meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
   </step>
  </procedure>

  <para>
    For details on configuring the resource for DLM, see <xref
     linkend="sec-ha-storage-generic-dlm-config"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-create">
  <title>Creating &ocfs; volumes</title>

  <para>
   After you have configured a DLM cluster resource as described in
   <xref linkend="sec-ha-ocfs2-create-service"/>, configure your system to
   use &ocfs; and create OCFs2 volumes.
  </para>

  <note>
   <title>&ocfs; volumes for application and data files</title>
   <para>
    We recommend that you generally store application files and data files
    on different &ocfs; volumes. If your application volumes and data
    volumes have different requirements for mounting, it is mandatory to
    store them on different volumes.
   </para>
  </note>

  <para>
   Before you begin, prepare the block devices you plan to use for your
   &ocfs; volumes. Leave the devices as free space.
  </para>

  <para>
   Then create and format the &ocfs; volume with the
   <command>mkfs.ocfs2</command> as described in
   <xref linkend="pro-ocfs2-volume"/>. The most important parameters for the
   command are listed below.
   For more information and the command syntax, refer to the
   <command>mkfs.ocfs2</command> man page.
  </para>

  <variablelist>
   <varlistentry>
    <term>Volume Label (<option>-L</option>)</term>
    <listitem>
     <para>
      A descriptive name for the volume to make it uniquely identifiable
      when it is mounted on different nodes. Use the
      <command>tunefs.ocfs2</command> utility to modify the label as needed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster Size (<option>-C</option>)</term>
    <listitem>
     <para>
      The smallest unit of space allocated to a file to
      hold the data. For the available options and recommendations, refer
      to the <command>mkfs.ocfs2</command> man page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Number of Node Slots (<option>-N</option>)</term>
    <listitem>
     <para>
      The maximum number of nodes that can concurrently mount a volume.
      For each of the nodes, &ocfs; creates separate system files, such
      as the journals. Nodes that access the volume
      can be a combination of little-endian architectures (such as &amd64;/&intel64;)
      and big-endian architectures (such as &s390;x).
     </para>
     <para>
      Node-specific files are called local files. A node slot
      number is appended to the local file. For example:
      <literal>journal:0000</literal> belongs to whatever node is assigned
      to slot number <literal>0</literal>.
     </para>
     <para>
      Set each volume's maximum number of node slots when you create it,
      according to how many nodes that you expect to concurrently mount
      the volume. Use the <command>tunefs.ocfs2</command> utility to
      increase the number of node slots as needed. Note that the value
      cannot be decreased, and one node slot will consume about 100&nbsp;MiB
      of disk space.
     </para>
     <para>
      If the <option>-N</option> parameter is not specified, the
      number of node slots is decided based on the size of the file system.
      For the default value, refer to the <command>mkfs.ocfs2</command> man page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Block Size (<option>-b</option>)</term>
    <listitem>
     <para>
      The smallest unit of space addressable by the file system. Specify
      the block size when you create the volume. For the available options
      and recommendations, refer to the <command>mkfs.ocfs2</command> man page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Specific Features On/Off (<option>--fs-features</option>)</term>
    <listitem>
     <para>
      A comma-separated list of feature flags can be provided, and
      <systemitem>mkfs.ocfs2</systemitem> will try to create the file
      system with those features set according to the list. To turn a
      feature on, include it in the list. To turn a feature off, prepend
      <literal>no</literal> to the name.
     </para>
     <para>
      For an overview of all available flags, refer to the
      <command>mkfs.ocfs2</command> man page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Predefined Features (<option>--fs-feature-level</option>)</term>
    <listitem>
     <para>
      Allows you to choose from a set of predetermined file system
      features. For the available options, refer to the
      <command>mkfs.ocfs2</command> man page.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   If you do not specify any features when creating and formatting
   the volume with <command>mkfs.ocfs2</command>, the following features are
   enabled by default: <option>backup-super</option>,
   <option>sparse</option>, <option>inline-data</option>,
   <option>unwritten</option>, <option>metaecc</option>,
   <option>indexed-dirs</option>, and <option>xattr</option>.
  </para>

  <procedure xml:id="pro-ocfs2-volume">
   <title>Creating and formatting an &ocfs; volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.ocfs2</command>
     utility. For information about the syntax for this command, refer to
     the <command>mkfs.ocfs2</command> man page.
    </para>
    <para>
     For example, to create a new &ocfs; file system on
     <filename>/dev/sdb1</filename> that supports up to 32 cluster nodes,
     enter the following commands:
    </para>
<screen>&prompt.root; mkfs.ocfs2 -N 32 /dev/sdb1</screen>
<!-- Brendo, 17.11.2010: maybe add -b 4k in the example (this is common
        for software raids) -->
<!--taroth 2014-08-14: additional info from bnc#853631:
       The complete command is:
       mkfs.ocfs2 -/-cluster-stack=pcmk -/-cluster-name=mycluster -N 32 /dev/sdb1
       Note, cluster-name should be same as defined in corosync.conf

       Alternatively, you can load the ocfs2 module, set the cluster_stack to pcmk and
       issue mkfs.ocfs2:
       # modprobe ocfs2_stack_user
       [...]
      This is more reliable because it picks up the cluster name from the clustering
      information.-->
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-mount">
  <title>Mounting &ocfs; volumes</title>

  <para>
   You can either mount an &ocfs; volume manually or with the cluster
   manager, as described in <xref linkend="pro-ocfs2-mount-cluster"/>.
  </para>
  <para>
   To mount multiple &ocfs; volumes, see <xref linkend="pro-ocfs2-mount-multiple"/>.
  </para>

  <procedure xml:id="pro-ocfs2-mount-manual">
   <title>Manually mounting an &ocfs; volume</title>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the
     <command>mount</command> command.
    </para>
   </step>
  </procedure>
   <tip>
    <title>Mounting an existing &ocfs; volume on a single node</title>
    <para>
     You can mount an &ocfs; volume on a single node without a fully functional
     cluster stack: for example, to quickly access the data from a backup.
     To do so, use the <command>mount</command> command with the
     <literal>-o nocluster</literal> option.
    </para>
    <warning role="compact">
     <para>
      This mount method lacks cluster-wide protection. To avoid damaging the file
      system, you must ensure that it is only mounted on one node.
     </para>
    </warning>
   </tip>

  <procedure xml:id="pro-ocfs2-mount-cluster">
   <title>Mounting an &ocfs; volume with the cluster resource manager</title>
   <para>
    To mount an &ocfs; volume with the &ha; software, configure an
    &ocfs; file system resource in the cluster. The following procedure uses
    the <command>crm</command> shell to configure the cluster resources.
    Alternatively, you can also use &hawk2; to configure the resources as
    described in <xref linkend="sec-ha-ocfs2-rsc-hawk2"/>.
   </para>
   <step>
    <para>
     Log in to a node as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the &ocfs; file system on every node in
     the cluster:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ocfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/sdb1" directory="/mnt/shared" fstype="ocfs2" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Add the <literal>ocfs2-1</literal> primitive
     to the <literal>g-storage</literal> group you created in
     <xref linkend="pro-dlm-resources"/>.
    </para>
<screen>&prompt.crm.conf;<command>modgroup</command> g-storage add ocfs2-1</screen>
    <para>
     Because of the base group's internal colocation and ordering,
     the <systemitem class="resource">ocfs2-1</systemitem> resource will only start
     on nodes that also have a <literal>dlm</literal> resource already running.
    </para>
    <important>
     <title>Do not use a group for multiple &ocfs; resources</title>
     <para>
      Adding multiple &ocfs; resources to a group creates a dependency between the
      &ocfs; volumes; for example, stopping <literal>ocfs2-1</literal> will also
      stop <literal>ocfs2-2</literal>, and starting <literal>ocfs2-2</literal>
      will also start <literal>ocfs2-1</literal>.
     </para>
     <para>
      To use multiple &ocfs; resources in the cluster, use colocation and order
      constraints as described in <xref linkend="pro-ocfs2-mount-multiple"/>.
     </para>
    </important>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
   </step>
  </procedure>

  <procedure xml:id="pro-ocfs2-mount-multiple">
   <title>Mounting multiple &ocfs; volumes with the cluster resource manager</title>
   <para>
    To mount multiple &ocfs; volumes in the cluster, configure an &ocfs; file system
    resource for each volume, and colocate them with the <literal>dlm</literal>
    resource you created in <xref linkend="pro-dlm-multiple-resources"/>.
   </para>
   <para>
    Do <emphasis>not</emphasis> add multiple &ocfs; resources to a group with DLM.
    This introduces a dependency between the &ocfs; volumes; for example, stopping
    <literal>ocfs2-1</literal> will also stop <literal>ocfs2-2</literal>.
   </para>
   <step>
    <para>
     Log in to a node as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Create the primitive for the first &ocfs; volume:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ocfs2-1 Filesystem \
  params directory="/srv/ocfs2-1" fstype=ocfs2 device="/dev/disk/by-partlabel/ocfs2-1" \
  op monitor interval=20 timeout=40 \
  op start timeout=60 interval=0 \
  op stop timeout=60 interval=0</screen>
   </step>
   <step>
    <para>
     Create the primitive for the second &ocfs; volume:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ocfs2-2 Filesystem \
  params directory="/srv/ocfs2-2" fstype=ocfs2 device="/dev/disk/by-partlabel/ocfs2-2" \
  op monitor interval=20 timeout=40 \
  op start timeout=60 interval=0 \
  op stop timeout=60 interval=0</screen>
   </step>
   <step>
    <para>
     Clone the &ocfs; resources so that they can run on all nodes:
    </para>
<screen>&prompt.crm.conf;<command>clone</command> cl-ocfs2-1 ocfs2-1 meta interleave=true
&prompt.crm.conf;<command>clone</command> cl-ocfs2-2 ocfs2-2 meta interleave=true</screen>
   </step>
   <step>
    <para>
     Add a colocation constraint for both &ocfs; resources so that they can only
     run on nodes where DLM is also running:
    </para>
<screen>&prompt.crm.conf;<command>colocation</command> co-ocfs2-with-dlm inf: ( cl-ocfs2-1 cl-ocfs2-2 ) cl-dlm</screen>
   </step>
   <step>
    <para>
     Add an order constraint for both &ocfs; resources so that they can only
     start after DLM is already running:
    </para>
<screen>&prompt.crm.conf;<command>order</command> o-dlm-before-ocfs2 Mandatory: cl-dlm ( cl-ocfs2-1 cl-ocfs2-2 )</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with <command>commit</command>
     and leave the crm live configuration with <command>quit</command>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-rsc-hawk2">
  <title>Configuring &ocfs; resources with &hawk2;</title>

  <para>
   Instead of configuring the DLM and the file system resource for &ocfs;
   manually with the &crmshell;, you can also use the &ocfs; template
   in &hawk2;'s <guimenu>Setup Wizard</guimenu>.
  </para>

  <important>
   <title>Differences between manual configuration and &hawk2;</title>
   <para>
    The &ocfs; template in the <guimenu>Setup Wizard</guimenu> does
    <emphasis>not</emphasis> include the configuration of a &stonith;
    resource. If you use the wizard, you still need to create an SBD
    partition on the shared storage and configure a &stonith; resource as
    described in <xref linkend="pro-ocfs2-stonith"/>.
   </para>
   <remark>taroth 2014-08-15: todo for next release: unify manual configuration
    and configuration via Hawk (use same resources, same options
    etc.), filed https://bugzilla.novell.com/show_bug.cgi?id=892116 for this</remark>
   <para>
    Using the &ocfs; template in the &hawk2; <guimenu>Setup
    Wizard</guimenu> also leads to a slightly different resource
    configuration than the manual configuration described in
    <xref linkend="pro-dlm-resources"/> and
    <xref linkend="pro-ocfs2-mount-cluster"/>.
   </para>
  </important>

  <procedure xml:id="pro-ha-ocfs2-rsc-hawk">
   <title>Configuring &ocfs; resources with &hawk2;'s <guimenu>Wizard</guimenu></title>
   <step>
    <para>
     Log in to &hawk2;:
    </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Wizard</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Expand the <guimenu>File System</guimenu> category and select
     <literal>OCFS2 File System</literal>.
    </para>
   </step>
   <step>
    <para>
     Follow the instructions on the screen. If you need information about an
     option, click it to display a short help text in &hawk2;. After the last
     configuration step, <guimenu>Verify</guimenu> the values you have entered.
    </para>
    <para>
     The wizard displays the configuration snippet that will be applied to
     the CIB and any additional changes, if required.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk2-wizard-ocfs2-verify.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk2-wizard-ocfs2-verify.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Check the proposed changes. If everything is according to your wishes,
     apply the changes.
    </para>
    <para>
     A message on the screen shows if the action has been successful.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-quota">
  <title>Using quotas on &ocfs; file systems</title>

<!--taroth 2011-11-24: fate#310056: the following section is a stub, based on an old RN
   entry,  todo: extend for next revision-->

  <para>
   To use quotas on an &ocfs; file system, create and mount the files
   system with the appropriate quota features or mount options,
   respectively: <literal>ursquota</literal> (quota for individual users) or
   <literal>grpquota</literal> (quota for groups). These features can also
   be enabled later on an unmounted file system using
   <command>tunefs.ocfs2</command>.
  </para>

  <para>
   When a file system has the appropriate quota feature enabled, it tracks
   in its metadata how much space and files each user (or group) uses. Since
   &ocfs; treats quota information as file system-internal metadata, you
   do not need to run the <command>quotacheck</command>(8) program. All
   functionality is built into fsck.ocfs2 and the file system driver itself.
  </para>

  <para>
   To enable enforcement of limits imposed on each user or group, run
   <command>quotaon</command>(8) like you would do for any other file
   system.
  </para>

<!--taroth 2011-11-26: according to jan kara (jack), the limitations for
   repquota and warnquota are now obsolete with solving fate#310056, thus
   removing the following completely-->

<!--<para>
   With &ocfs; file systems, the following commands work as usual:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     quota(1)
    </para>
   </listitem>
   <listitem>
    <para>
     setquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     edquota(8)
    </para>
   </listitem>
  </itemizedlist>

  <para>
     Because of a limitation in the current Kernel interface, the following
   commands do not work with &ocfs;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     repquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     warnquota(8)
    </para>
   </listitem>
  </itemizedlist>-->

  <para>
   For performance reasons each cluster node performs quota accounting
   locally and synchronizes this information with a common central storage
   once per 10 seconds. This interval is tuneable with
   <command>tunefs.ocfs2</command>, options
   <option>usrquota-sync-interval</option> and
   <option>grpquota-sync-interval</option>. Therefore quota information may
   not be exact at all times and as a consequence users or groups can
   slightly exceed their quota limit when operating on several cluster nodes
   in parallel.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-more">
  <title>For more information</title>

  <para>
   For more information about &ocfs;, see the following links:
  </para>

  <variablelist>
   <varlistentry>
    <term><link xlink:href="https://ocfs2.wiki.kernel.org/"/>
    </term>
    <listitem>
     <para>
      The &ocfs; project home page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/"/>
    </term>
    <listitem>
     <para>
      The former &ocfs; project home page at Oracle.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/documentation"/>
    </term>
    <listitem>
     <para>
      The project's former documentation home page.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
