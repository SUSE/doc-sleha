<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-ocfs2">
 <title>&ocfs;</title>
 <info>
      <abstract>
        <para>
    Oracle Cluster File System 2 (&ocfs;) is a general-purpose journaling
    file system that has been fully integrated since the Linux 2.6 Kernel.
    &ocfs; allows you to store application binary files, data files, and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with &corosync; and the Distributed
    Lock Manager (DLM).
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
    </info>
 <sect1 xml:id="sec-ha-ocfs2-features">
  <title>Features and benefits</title>

  <para>
   &ocfs; can be used for the following storage solutions for example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     General applications and workloads.
    </para>
   </listitem>
   <listitem>
    <para>
     &xen; image store in a cluster. &xen; virtual machines and
     virtual servers can be stored on &ocfs; volumes that are mounted by
     cluster servers. This provides quick and easy portability of &xen;
     virtual machines between servers.
    </para>
   </listitem>
   <listitem>
    <para>
     LAMP (Linux, Apache, MySQL, and PHP &verbar; Perl &verbar;
     Python) stacks.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   As a high-performance, symmetric and parallel cluster file system,
   &ocfs; supports the following functions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an &ocfs; volume in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access.
    </para>
   </listitem>
   <listitem>
    <para>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   &ocfs; also provides the following capabilities:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Metadata caching.
    </para>
   </listitem>
   <listitem>
    <para>
     Metadata journaling.
    </para>
   </listitem>
   <listitem>
    <para>
     Cross-node file data consistency.
    </para>
   </listitem>
   <listitem>
    <para>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB,
     for a maximum volume size of 4 PB (Petabyte).
    </para>
   </listitem>
   <listitem>
    <para>
     Support for up to 32 cluster nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Support for &ocfs;</title>
   <para>
    &ocfs; is only supported by &suse; when used with the pcmk (Pacemaker) stack,
    as provided by &productname;. &suse; does not provide support for &ocfs; in
    combination with the o2cb stack.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-utils">
  <title>&ocfs; packages and management utilities</title>

  <para>
   The &ocfs; Kernel module (<literal>ocfs2</literal>) is installed
   automatically in the &hasi; on &slsreg; &productnumber;. To use
   &ocfs;, make sure the following packages are installed on each node in
   the cluster: <package>ocfs2-tools</package> and
   the matching <package>ocfs2-kmp-*</package>
   packages for your Kernel.
  </para>

  <para>
   The <package>ocfs2-tools</package> package
   provides the following utilities for management of OFS2 volumes. For
   syntax information, see their man pages.
  </para>

  <table>
   <title>&ocfs; utilities</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        &ocfs; Utility
       </para>
      </entry>
      <entry>
       <para>
        Description
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        debugfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Examines the state of the OCFS file system for debugging.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        fsck.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Checks the file system for errors and optionally repairs errors.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mkfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Creates an &ocfs; file system on a device, usually a partition on
        a shared physical or logical disk.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mounted.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Detects and lists all &ocfs; volumes on a clustered system.
        Detects and lists all nodes on the system that have mounted an
        &ocfs; device or lists all &ocfs; devices.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        tunefs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Changes &ocfs; file system parameters, including the volume
        label, number of node slots, journal size for all node slots, and
        volume size.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-create-service">
  <title>Configuring &ocfs; services and a &stonith; resource</title>

<!--https://bugzilla.novell.com/show_bug.cgi?id=520714-->

  <para>
   Before you can create &ocfs; volumes, you must configure the following
   resources as services in the cluster: DLM and a &stonith; resource.
  </para>

  <para>
   The following procedure uses the <command>crm</command> shell to
   configure the cluster resources. Alternatively, you can also use
   &hawk2; to configure the resources as described in
   <xref linkend="sec-ha-ocfs2-rsc-hawk2"/>.
  </para>

  <procedure xml:id="pro-ocfs2-stonith">
   <title>Configuring a &stonith; resource</title>
   <note>
    <title>&stonith; device needed</title>
    <para>
     You need to configure a fencing device. Without a &stonith;
     mechanism (like <literal>external/sbd</literal>) in place the
     configuration will fail.
    </para>
   </note>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Create an SBD partition as described in
     <xref linkend="pro-ha-storage-protect-sbd-create"/>.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure <literal>external/sbd</literal> as fencing device with
     <literal>/dev/sdb2</literal> being a dedicated partition on the shared
     storage for heartbeating and fencing:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> sbd_stonith stonith:external/sbd \
  params pcmk_delay_max=30 meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
   </step>
  </procedure>

  <para>
    For details on configuring the resource group for DLM, see <xref
     linkend="pro-dlm-resources"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-create">
  <title>Creating &ocfs; volumes</title>

  <para>
   After you have configured a DLM cluster resource as described in
   <xref linkend="sec-ha-ocfs2-create-service"/>, configure your system to
   use &ocfs; and create OCFs2 volumes.
  </para>

  <note>
   <title>&ocfs; volumes for application and data files</title>
   <para>
    We recommend that you generally store application files and data files
    on different &ocfs; volumes. If your application volumes and data
    volumes have different requirements for mounting, it is mandatory to
    store them on different volumes.
   </para>
  </note>

  <para>
   Before you begin, prepare the block devices you plan to use for your
   &ocfs; volumes. Leave the devices as free space.
  </para>

  <para>
   Then create and format the &ocfs; volume with the
   <command>mkfs.ocfs2</command> as described in
   <xref linkend="pro-ocfs2-volume"/>. The most important parameters for the
   command are listed in <xref linkend="tab-ha-ofcs2-mkfs-ocfs2-params"/>.
   For more information and the command syntax, refer to the
   <command>mkfs.ocfs2</command> man page.
  </para>

  <table xml:id="tab-ha-ofcs2-mkfs-ocfs2-params">
   <title>Important &ocfs; parameters</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        &ocfs; Parameter
       </para>
      </entry>
      <entry>
       <para>
        Description and Recommendation
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Volume Label (<option>-L</option>)
       </para>
      </entry>
      <entry>
       <para>
        A descriptive name for the volume to make it uniquely identifiable
        when it is mounted on different nodes. Use the
        <command>tunefs.ocfs2</command> utility to modify the label as
        needed.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Cluster Size (<option>-C</option>)
       </para>
      </entry>
      <entry>
       <para>
        Cluster size is the smallest unit of space allocated to a file to
        hold the data. For the available options and recommendations, refer
        to the <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Number of Node Slots (<option>-N</option>)
       </para>
      </entry>
      <entry>
       <para>
        The maximum number of nodes that can concurrently mount a volume.
        For each of the nodes, &ocfs; creates separate system files, such
        as the journals. Nodes that access the volume
        can be a combination of little-endian architectures (such as &amd64;/&intel64;)
        and big-endian architectures (such as &s390;x).
       </para>
       <para>
        Node-specific files are called local files. A node slot
        number is appended to the local file. For example:
        <literal>journal:0000</literal> belongs to whatever node is assigned
        to slot number <literal>0</literal>.
       </para>
       <para>
        Set each volume's maximum number of node slots when you create it,
        according to how many nodes that you expect to concurrently mount
        the volume. Use the <command>tunefs.ocfs2</command> utility to
        increase the number of node slots as needed. Note that the value
        cannot be decreased, and one node slot will consume about 100&nbsp;MiB
        of disk space.
       </para>
       <para>
        In case the <option>-N</option> parameter is not specified, the
        number of node slots is decided based on the size of the file system.
        For the default value, refer to the <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Block Size (<option>-b</option>)
       </para>
      </entry>
      <entry>
       <para>
        The smallest unit of space addressable by the file system. Specify
        the block size when you create the volume. For the available options
        and recommendations, refer to the <command>mkfs.ocfs2</command> man
        page.
<!-- Brendo, 17.11.2010: to detect the blocksize of the device, use: 
             blockdev -\-getbsz &lt;block-device&gt;. -->
       </para>
      </entry>
     </row>
<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c11-->
     <row>
      <entry>
       <para>
        Specific Features On/Off (<option>--fs-features</option>)
       </para>
      </entry>
      <entry>
       <para>
        A comma separated list of feature flags can be provided, and
        <systemitem>mkfs.ocfs2</systemitem> will try to create the file
        system with those features set according to the list. To turn a
        feature on, include it in the list. To turn a feature off, prepend
        <literal>no</literal> to the name.
       </para>
       <para>
        For an overview of all available flags, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Pre-Defined Features (<option>--fs-feature-level</option>)
       </para>
      </entry>
      <entry>
       <para>
        Allows you to choose from a set of pre-determined file system
        features. For the available options, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c15-->

  <para>
   If you do not specify any features when creating and formatting
   the volume with <command>mkfs.ocfs2</command>, the following features are
   enabled by default: <option>backup-super</option>,
   <option>sparse</option>, <option>inline-data</option>,
   <option>unwritten</option>, <option>metaecc</option>,
   <option>indexed-dirs</option>, and <option>xattr</option>.
  </para>

  <procedure xml:id="pro-ocfs2-volume">
   <title>Creating and formatting an &ocfs; volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.ocfs2</command>
     utility. For information about the syntax for this command, refer to
     the <command>mkfs.ocfs2</command> man page.
    </para>
    <para>
     For example, to create a new &ocfs; file system on
     <filename>/dev/sdb1</filename> that supports up to 32 cluster nodes,
     enter the following commands:
    </para>
<screen>&prompt.root; mkfs.ocfs2 -N 32 /dev/sdb1</screen>
<!-- Brendo, 17.11.2010: maybe add -b 4k in the example (this is common
        for software raids) -->
<!--taroth 2014-08-14: additional info from bnc#853631:
       The complete command is:    
       mkfs.ocfs2 -/-cluster-stack=pcmk -/-cluster-name=mycluster -N 32 /dev/sdb1
       Note, cluster-name should be same as defined in corosync.conf
   
       Alternatively, you can load the ocfs2 module, set the cluster_stack to pcmk and
       issue mkfs.ocfs2:
       # modprobe ocfs2_stack_user
       [...]
      This is more reliable because it picks up the cluster name from the clustering
      information.-->
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-mount">
  <title>Mounting &ocfs; volumes</title>

  <para>
   You can either mount an &ocfs; volume manually or with the cluster
   manager, as described in <xref linkend="pro-ocfs2-mount-cluster"/>.
  </para>

  <procedure xml:id="pro-ocfs2-mount-manual">
   <title>Manually mounting an &ocfs; volume</title>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the
     <command>mount</command> command.
    </para>
   </step>
  </procedure>

  <warning>
   <title>Manually mounted &ocfs; devices</title>
   <para>
    If you mount the &ocfs; file system manually for testing purposes,
    make sure to unmount it again before starting to use it by means of
    cluster resources.
   </para>
  </warning>

  <procedure xml:id="pro-ocfs2-mount-cluster">
   <title>Mounting an &ocfs; volume with the cluster resource manager</title>
   <para>
    To mount an &ocfs; volume with the &ha; software, configure an
    ocfs2 file system resource in the cluster. The following procedure uses
    the <command>crm</command> shell to configure the cluster resources.
    Alternatively, you can also use &hawk2; to configure the resources as
    described in <xref linkend="sec-ha-ocfs2-rsc-hawk2"/>.
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the &ocfs; file system on every node in
     the cluster:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ocfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/sdb1" directory="/mnt/shared" fstype="ocfs2" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Add the <literal>ocfs2-1</literal> primitive
     to the <literal>g-storage</literal> group you created in
     <xref linkend="pro-dlm-resources"/>.
    </para>
<screen>&prompt.crm.conf;<command>modgroup</command> g-storage add ocfs2-1</screen>
    <para>The <command>add</command> subcommand appends the new group
     member by default. Because of the base group's internal colocation and ordering, Pacemaker
     will only start the <systemitem class="resource">ocfs2-1</systemitem>
     resource on nodes that also have a <literal>dlm</literal> resource
     already running.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
<!--taroth 2014-08-28: appendix file commented for now, because it needs
      alignement with changes here:
      To check if you have configured all needed resources, also refer to
      <xref linkend="cha-ha-config-example"/>.-->
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-rsc-hawk2">
  <title>Configuring &ocfs; resources with &hawk2;</title>

  <para>
   Instead of configuring the DLM and the file system resource for &ocfs;
   manually with the &crmshell;, you can also use the &ocfs; template
   in &hawk2;'s <guimenu>Setup Wizard</guimenu>.
  </para>

  <important>
   <title>Differences between manual configuration and &hawk2;</title>
   <para>
    The &ocfs; template in the <guimenu>Setup Wizard</guimenu> does
    <emphasis>not</emphasis> include the configuration of a &stonith;
    resource. If you use the wizard, you still need to create an SBD
    partition on the shared storage and configure a &stonith; resource as
    described in <xref linkend="pro-ocfs2-stonith"/>.
   </para>
   <remark>taroth 2014-08-15: todo for next release: unify manual configuration
    and configuration via Hawk (use same resources, same options
    etc.), filed https://bugzilla.novell.com/show_bug.cgi?id=892116 for this</remark>
   <para>
    Using the &ocfs; template in the &hawk2; <guimenu>Setup
    Wizard</guimenu> also leads to a slightly different resource
    configuration than the manual configuration described in
    <xref linkend="pro-dlm-resources"/> and
    <xref linkend="pro-ocfs2-mount-cluster"/>.
   </para>
  </important>

  <procedure xml:id="pro-ha-ocfs2-rsc-hawk">
   <title>Configuring &ocfs; resources with &hawk2;'s <guimenu>Wizard</guimenu></title>
   <step>
    <para>
     Log in to &hawk2;:
    </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Wizard</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Expand the <guimenu>File System</guimenu> category and select
     <literal>OCFS2 File System</literal>.
    </para>
   </step>
   <step>
    <para>
     Follow the instructions on the screen. If you need information about an
     option, click it to display a short help text in &hawk2;. After the last
     configuration step, <guimenu>Verify</guimenu> the values you have entered.
    </para>
    <para>
     The wizard displays the configuration snippet that will be applied to
     the CIB and any additional changes, if required.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk2-wizard-ocfs2-verify.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk2-wizard-ocfs2-verify.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Check the proposed changes. If everything is according to your wishes,
     apply the changes.
    </para>
    <para>
     A message on the screen shows if the action has been successful.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-quota">
  <title>Using quotas on &ocfs; file systems</title>

<!--taroth 2011-11-24: fate#310056: the following section is a stub, based on an old RN
   entry,  todo: extend for next revision-->

  <para>
   To use quotas on an &ocfs; file system, create and mount the files
   system with the appropriate quota features or mount options,
   respectively: <literal>ursquota</literal> (quota for individual users) or
   <literal>grpquota</literal> (quota for groups). These features can also
   be enabled later on an unmounted file system using
   <command>tunefs.ocfs2</command>.
  </para>

  <para>
   When a file system has the appropriate quota feature enabled, it tracks
   in its metadata how much space and files each user (or group) uses. Since
   &ocfs; treats quota information as file system-internal metadata, you
   do not need to run the <command>quotacheck</command>(8) program. All
   functionality is built into fsck.ocfs2 and the file system driver itself.
  </para>

  <para>
   To enable enforcement of limits imposed on each user or group, run
   <command>quotaon</command>(8) like you would do for any other file
   system.
  </para>

<!--taroth 2011-11-26: according to jan kara (jack), the limitations for
   repquota and warnquota are now obsolete with solving fate#310056, thus
   removing the following completely-->

<!--<para>
   With &ocfs; file systems, the following commands work as usual:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     quota(1)
    </para>
   </listitem>
   <listitem>
    <para>
     setquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     edquota(8)
    </para>
   </listitem>
  </itemizedlist>

  <para>
     Because of a limitation in the current Kernel interface, the following
   commands do not work with &ocfs;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     repquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     warnquota(8)
    </para>
   </listitem>
  </itemizedlist>-->

  <para>
   For performance reasons each cluster node performs quota accounting
   locally and synchronizes this information with a common central storage
   once per 10 seconds. This interval is tuneable with
   <command>tunefs.ocfs2</command>, options
   <option>usrquota-sync-interval</option> and
   <option>grpquota-sync-interval</option>. Therefore quota information may
   not be exact at all times and as a consequence users or groups can
   slightly exceed their quota limit when operating on several cluster nodes
   in parallel.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-more">
  <title>For more information</title>

  <para>
   For more information about &ocfs;, see the following links:
  </para>

  <variablelist>
   <varlistentry>
    <term><link xlink:href="https://ocfs2.wiki.kernel.org/"/>
    </term>
    <listitem>
     <para>
      The &ocfs; project home page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/"/>
    </term>
    <listitem>
     <para>
      The former &ocfs; project home page at Oracle.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/documentation"/>
    </term>
    <listitem>
     <para>
      The project's former documentation home page.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
