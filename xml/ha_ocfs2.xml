<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!--taroth 2010-08-19: for next revision, see also Sander's book 
(chapter about cLVM) for more information and details to integrate here-->
<chapter id="cha.ha.ocfs2">
 <title>OCFS2</title><indexterm class="startofrange" id="idx.filesystems.ocfs2">
 <primary>file systems</primary>
 <secondary>OCFS2</secondary></indexterm>
 <abstract>
  <para>
   Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling file
   system that has been fully integrated since the Linux 2.6 Kernel. OCFS2
   allows you to store application binary files, data files, and databases
   on devices on shared storage. All nodes in a cluster have concurrent read
   and write access to the file system. A user-space control daemon, managed
   via a clone resource, provides the integration with the HA stack, in
   particular with &corosync; and the Distributed Lock Manager (DLM).
  </para>
 </abstract>
 <sect1 id="sec.ha.ocfs2.features">
  <title>Features and Benefits</title>

  <para>
   OCFS2 can be used for the following storage solutions for example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     General applications and workloads.
    </para>
   </listitem>
   <listitem>
    <para>
     &xen; image store in a cluster. &xen; virtual machines and virtual
     servers can be stored on OCFS2 volumes that are mounted by cluster
     servers. This provides quick and easy portability of &xen; virtual
     machines between servers.
    </para>
   </listitem>
   <listitem>
    <para>
     LAMP (Linux, Apache, MySQL, and PHP &verbar; Perl &verbar; Python)
     stacks.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   As a high-performance, symmetric and parallel cluster file system, OCFS2
   supports the following functions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an OCFS2 volume in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access.
    </para>
   </listitem>
   <listitem>
    <para>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   OCFS2 also provides the following capabilities:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Metadata caching.
    </para>
   </listitem>
   <listitem>
    <para>
     Metadata journaling.
    </para>
   </listitem>
   <listitem>
    <para>
     Cross-node file data consistency.
    </para>
   </listitem>
   <listitem>
    <para>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB,
     for a maximum volume size of 4 PB (Petabyte).
    </para>
   </listitem>
   <listitem>
    <para>
     Support for up to 32 cluster nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.ocfs2.utils">
  <title>OCFS2 Packages and Management Utilities</title>

  <para>
   The OCFS2 Kernel module (<literal>ocfs2</literal>) is installed
   automatically in the &hasi; on &slsreg; &productnumber;. To use OCFS2,
   make sure the following packages are installed on each node in the
   cluster: <systemitem class="resource">ocfs2-tools</systemitem> and the
   matching <systemitem class="resource">ocfs2-kmp-*</systemitem> packages
   for your Kernel.
  </para>

  <para>
   The <systemitem class="resource">ocfs2-tools</systemitem> package
   provides the following utilities for management of OFS2 volumes. For
   syntax information, see their man pages.
  </para>

  <table>
   <title>OCFS2 Utilities</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Utility
       </para>
      </entry>
      <entry>
       <para>
        Description
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        debugfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Examines the state of the OCFS file system for the purpose of
        debugging.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        fsck.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Checks the file system for errors and optionally repairs errors.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mkfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Creates an OCFS2 file system on a device, usually a partition on a
        shared physical or logical disk.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mounted.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Detects and lists all OCFS2 volumes on a clustered system. Detects
        and lists all nodes on the system that have mounted an OCFS2 device
        or lists all OCFS2 devices.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        tunefs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Changes OCFS2 file system parameters, including the volume label,
        number of node slots, journal size for all node slots, and volume
        size.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>
 <sect1 id="sec.ha.ocfs2.create.service">
  <title>Configuring OCFS2 Services and a &stonith; Resource</title>
  <remark>taroth 2014-08-14: todo - https://fate.suse.com/316322: Update
   cLVM2/OCFS2 template in crmsh/hawk: check if manual configuration of resources can be
   replaced by using the Hawk wizard for OCFS2 (=does it result in same config?) 
   and if yes, mention it as alternative</remark>
<!--https://bugzilla.novell.com/show_bug.cgi?id=520714-->

  <para>
   Before you can create OCFS2 volumes, you must configure the following
   resources as services in the cluster: DLM, and a &stonith; resource.
   OCFS2 uses the cluster membership services from Pacemaker which run in
   user space. Therefore, DLM needs to be configured as clone
   resource that is present on each node in the cluster.
  </para>

  <para>
   The following procedure uses the <command>crm</command> shell to
   configure the cluster resources. Alternatively, you can also use &hawk; 
   to configure the resources.
  </para>

  <note>
   <title>DLM Resource for Both cLVM and OCFS2</title>
   <para>
    Both cLVM and OCFS2 need a DLM resource that runs on all nodes in the
    cluster and therefore usually is configured as a clone. If you have a
    setup that includes both OCFS2 and cLVM, configuring
    <emphasis>one</emphasis> DLM resource for both OCFS2 and cLVM is enough.
   </para>
  </note>

  <procedure id="pro.ocfs2.resources">
   <title>Configuring a DLM Resource</title>
   <para>
    The configuration consists of a base group that includes several
    primitives and a base clone. Both base group and base clone can be used
    in various scenarios afterwards (for both OCFS2 and cLVM, for example).
    You only need to extended the base group with the respective primitives
    as needed. As the base group has internal colocation and ordering, this
    facilitates the overall setup as you do not have to specify several
    individual groups, clones and their dependencies.
   </para>
   <para>
    Follow the steps below for one node in the cluster:
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm <option>configure</option></command>.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create the primitive resource for DLM:
    </para>
<!--Brendo, 17.11.2010: for dlm use the following params:
      params args="-q 0 -f 0 -t 1000 -a 1000"
      (see man dlm_controld) -> this keeps ocfs2 from blocking if a node is
      missing-->
    <screen>&prompt.crm.conf;<command>primitive</command> dlm ocf:pacemaker:controld \
      op monitor interval="60" timeout="60"</screen>
    <para>
     The <literal>dlm</literal> clone resource controls the distributed lock
     manager service and makes sure this service is started on all nodes in
     the cluster. 
    </para>
   </step>
    <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>

  <procedure id="pro.ocfs2.stonith">
   <title>Configuring a &stonith; Resource</title>
   <note>
    <title>&stonith; Device Needed</title>
    <para>
     You need to configure a fencing device. Without a &stonith; mechanism
     (like <literal>external/sbd</literal>) in place the configuration will
     fail.
    </para>
   </note>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Create an SBD partition as described in
     <xref
      linkend="pro.ha.storage.protect.sbd.create"/>.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm <option>configure</option></command>.
    </para>
   </step>
   <step>
    <para>
     Configure <literal>external/sbd</literal> as fencing device with
     <literal>/dev/sdb2</literal> being a dedicated partition on the shared
     storage for heartbeating and fencing:
    </para>
    <screen>&prompt.crm.conf;<command>primitive</command> sbd_stonith stonith:external/sbd \
      meta target-role="Started" \
      op monitor interval="15" timeout="15" start-delay="15"</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.ocfs2.create">
  <title>Creating OCFS2 Volumes</title>

  <para>
   After you have configured a DLM cluster resource as described
   in <xref linkend="sec.ha.ocfs2.create.service"/>, configure your system
   to use OCFS2 and create OCFs2 volumes.
  </para>

  <note>
   <title>OCFS2 Volumes for Application and Data Files</title>
   <para>
    We recommend that you generally store application files and data files
    on different OCFS2 volumes. If your application volumes and data volumes
    have different requirements for mounting, it is mandatory to store them
    on different volumes.
   </para>
  </note>

  <para>
   Before you begin, prepare the block devices you plan to use for your
   OCFS2 volumes. Leave the devices as free space.
  </para>

  <para>
   Then create and format the OCFS2 volume with the
   <command>mkfs.ocfs2</command> as described in
   <xref linkend="pro.ocfs2.volume"/>. The most important parameters for
   the command are listed in
   <xref linkend="tab.ha.ofcs2.mkfs.ocfs2.params"/>. For more information
   and the command syntax, refer to the <command>mkfs.ocfs2</command> man
   page.
  </para>

  <table id="tab.ha.ofcs2.mkfs.ocfs2.params">
   <title>Important OCFS2 Parameters</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Parameter
       </para>
      </entry>
      <entry>
       <para>
        Description and Recommendation
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Volume Label (<option>-L</option>)
       </para>
      </entry>
      <entry>
       <para>
        A descriptive name for the volume to make it uniquely identifiable
        when it is mounted on different nodes. Use the
        <command>tunefs.ocfs2</command> utility to modify the label as
        needed.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Cluster Size (<option>-C</option>)
       </para>
      </entry>
      <entry>
       <para>
        Cluster size is the smallest unit of space allocated to a file to
        hold the data. For the available options and recommendations, refer
        to the <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Number of Node Slots (<option>-N</option>)
       </para>
      </entry>
      <entry>
       <para>
        The maximum number of nodes that can concurrently mount a volume.
        For each of the nodes, OCFS2 creates separate system files, such as
        the journals, for each of the nodes. Nodes that access the volume
        can be a combination of little-endian architectures (such as x86_64) 
        and big-endian architectures (such as s390x).
       </para>
       <para>
        Node-specific files are referred to as local files. A node slot
        number is appended to the local file. For example:
        <literal>journal:0000</literal> belongs to whatever node is assigned
        to slot number <literal>0</literal>.
       </para>
       <para>
        Set each volume's maximum number of node slots when you create it,
        according to how many nodes that you expect to concurrently mount
        the volume. Use the <command>tunefs.ocfs2</command> utility to
        increase the number of node slots as needed. Note that the value
        cannot be decreased.
       </para>
       <para>In case the <option>-N</option> parameter is not specified, the
        number of slots is decided based on the size of the file system.</para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Block Size (<option>-b</option>)
       </para>
      </entry>
      <entry>
       <para>
        The smallest unit of space addressable by the file system. Specify
        the block size when you create the volume. For the available options
        and recommendations, refer to the <command>mkfs.ocfs2</command> man
        page.
<!-- Brendo, 17.11.2010: to detect the blocksize of the device, use: 
             blockdev -\-getbsz &lt;block-device&gt;. -->
       </para>
      </entry>
     </row>
<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c11-->
     <row>
      <entry>
       <para>
        Specific Features On/Off (<option>--fs-features</option>)
       </para>
      </entry>
      <entry>
       <para>
        A comma separated list of feature flags can be provided, and
        <systemitem>mkfs.ocfs2</systemitem> will try to create the file
        system with those features set according to the list. To turn a
        feature on, include it in the list. To turn a feature off, prepend
        <literal>no</literal> to the name.
       </para>
       <para>
        For on overview of all available flags, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Pre-Defined Features (<option>--fs-feature-level</option>)
       </para>
      </entry>
      <entry>
       <para>
        Allows you to choose from a set of pre-determined file system
        features. For the available options, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c15-->

  <para>
   If you do not specify any specific features when creating and formatting
   the volume with <command>mkfs.ocfs2</command>, the following features are
   enabled by default: <option>backup-super</option>,
   <option>sparse</option>, <option>inline-data</option>,
   <option>unwritten</option>, <option>metaecc</option>,
   <option>indexed-dirs</option>, and <option>xattr</option>.
  </para>

  <procedure id="pro.ocfs2.volume">
   <title>Creating and Formatting an OCFS2 Volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command
     <command>crm status</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.ocfs2</command>
     utility. For information about the syntax for this command, refer to
     the <command>mkfs.ocfs2</command> man page.
    </para>
    <para>
     For example, to create a new OCFS2 file system on
     <filename>/dev/sdb1</filename> that supports up to 32 cluster nodes,
     enter the following commands:</para>
    <screen>&prompt.root; modprobe ocfs2_stack_user
&prompt.root; modprobe ocfs2
&prompt.root; echo pcmk > /sys/fs/ocfs2/cluster_stack
&prompt.root; mkfs.ocfs2 -N 32 /dev/sdb1</screen>
<!-- Brendo, 17.11.2010: maybe add -b 4k in the example (this is common
        for software raids) -->
<!--taroth 2014-08-14: additional info from bnc#853631:
       The complete command is:    
       mkfs.ocfs2 -/-cluster-stack=pcmk -/-cluster-name=mycluster -N 32 /dev/sdb1
       Note, cluster-name should be same as defined in corosync.conf
   
       Alternatively, you can load the ocfs2 module, set the cluster_stack to pcmk and
       issue mkfs.ocfs2:
       # modprobe ocfs2_stack_user
       [...]
      This is more reliable because it picks up the cluster name from the clustering
      information.-->
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.ocfs2.mount">
  <title>Mounting OCFS2 Volumes</title>

  <para>
   You can either mount an OCFS2 volume manually or with the cluster
   manager, as described in <xref linkend="pro.ocfs2.mount.cluster"/>.
  </para>

  <procedure id="pro.ocfs2.mount.manual">
   <title>Manually Mounting an OCFS2 Volume</title>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command
     <command>crm status</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the
     <command>mount</command> command.
    </para>
   </step>
  </procedure>

  <warning>
   <title>Manually Mounted OCFS2 Devices</title>
   <para>
    If you mount the OCFS2 file system manually for testing purposes, make
    sure to unmount it again before starting to use it by means of &corosync;.
   </para>
  </warning>

  <procedure id="pro.ocfs2.mount.cluster">
   <title>Mounting an OCFS2 Volume with the Cluster Manager</title>
   <para>
    To mount an OCFS2 volume with the &ha; software, configure an ocf file
    system resource in the cluster. The following procedure uses the
    <command>crm</command> shell to configure the cluster resources.
    Alternatively, you can also use &hawk; to configure the resources.
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm <option>configure</option></command>.
    </para>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the OCFS2 file system on every node in the
     cluster:
    </para>
    <screen>&prompt.crm.conf;<command>primitive</command> ocfs2-1 ocf:heartbeat:Filesystem \
      params device="/dev/sdb1" directory="/mnt/shared" fstype="ocfs2" options="acl" \
      op monitor interval="20" timeout="40"</screen>
   </step>
   <step>
    <para> Create a base group that consists of the <literal>dlm</literal>
     primitive you created in <xref linkend="pro.ocfs2.resources"/> and the
      <literal>ocfs2-1</literal> primitive. Clone the group: </para>
    <screen>&prompt.crm.conf;<command>group</command> base-group dlm  ocfs2-1
     <command>clone</command> base-clone base-group \
     meta interleave="true"</screen>
    <para>Due to the base group's internal colocation and ordering, Pacemaker
     will only start the <systemitem class="resource">ocfs2-1</systemitem>
     resource on nodes that also have an <literal>dlm</literal> resource already running.</para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>. To check if you have
     configured all needed resources, also refer to
     <xref linkend="cha.ha.config.example"/>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.ocfs2.quota">
  <title>Using Quotas on OCFS2 File Systems</title>

<!--taroth 2011-11-24: fate#310056: the following section is a stub, based on an old RN
   entry,  todo: extend for next revision-->

  <para>
   To use quotas on an OCFS2 file system, create and mount the files system
   with the appropriate quota features or mount options, respectively:
   <literal>ursquota</literal> (quota for individual users) or
   <literal>grpquota</literal> (quota for groups). These features can also
   be enabled later on an unmounted file system using
   <command>tunefs.ocfs2</command>.
  </para>

  <para>
   When a file system has the appropriate quota feature enabled, it tracks
   in its metadata how much space and files each user (or group) uses. Since
   OCFS2 treats quota information as file system-internal metadata, you do
   not need to run the <command>quotacheck</command>(8) program. All
   functionality is built into fsck.ocfs2 and the file system driver itself.
  </para>

  <para>
   To enable enforcement of limits imposed on each user or group, run
   <command>quotaon</command>(8) like you would do for any other file
   system.
  </para>

<!--taroth 2011-11-26: according to jan kara (jack), the limitations for
   repquota and warnquota are now obsolete with solving fate#310056, thus
   removing the following completely-->

<!--<para>
   With OCFS2 file systems, the following commands work as usual:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     quota(1)
    </para>
   </listitem>
   <listitem>
    <para>
     setquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     edquota(8)
    </para>
   </listitem>
  </itemizedlist>

  <para>
     Because of a limitation in the current Kernel interface, the following
   commands do not work with OCFS2:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     repquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     warnquota(8)
    </para>
   </listitem>
  </itemizedlist>-->

  <para>
   For performance reasons each cluster node performs quota accounting
   locally and synchronizes this information with a common central storage
   once per 10 seconds. This interval is tunable with
   <command>tunefs.ocfs2</command>, options
   <option>usrquota-sync-interval</option> and
   <option>grpquota-sync-interval</option>. Therefore quota information may
   not be exact at all times and as a consequence users or groups can
   slightly exceed their quota limit when operating on several cluster nodes
   in parallel.
  </para>
 </sect1>
 <sect1 id="sec.ha.ocfs2.more">
  <title>For More Information</title>

  <para>
   For more information about OCFS2, see the following links:
  </para>

  <variablelist>
   <varlistentry>
    <term><ulink url="http://oss.oracle.com/projects/ocfs2/"/>
    </term>
    <listitem>
     <para>
      OCFS2 project home page at Oracle.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><ulink url="http://oss.oracle.com/projects/ocfs2/documentation"/>
    </term>
    <listitem>
     <para>
      The  project's documentation home page.
     </para>
    </listitem>
   </varlistentry>
  </variablelist><indexterm class="endofrange" startref="idx.filesystems.ocfs2"/>
 </sect1>
</chapter>
