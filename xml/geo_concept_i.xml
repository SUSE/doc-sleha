<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-geo-concept">
 <title>Conceptual Overview</title>
 <info>
  <abstract>
   <para>
    &geo; clusters based on &productnamereg; can be considered
    <quote>overlay</quote> clusters where each cluster site corresponds to a
    cluster node in a traditional cluster. The overlay cluster is managed by
    the booth cluster ticket manager (in the following called booth).
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer/>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release/>
   <dm:repository/>
  </dm:docmanager>
 </info>
 <para>
  Each of the parties involved in a &geo; cluster runs a service, the &boothd;.
  It connects to the booth daemons running at the other sites and exchanges
  connectivity details. For making cluster resources highly available across
  sites, booth relies on cluster objects called tickets. A ticket grants the
  right to run certain resources on a specific cluster site. Booth guarantees
  that every ticket is granted to no more than one site at a time.
 </para>
 <para>
  If the communication between two booth instances breaks down, it might be
  because of a network breakdown between the cluster sites
  <emphasis
   xmlns='http://docbook.org/ns/docbook'>or</emphasis> because of
  an outage of one cluster site. In this case, you need an additional instance
  (a third cluster site or an
  <literal
   xmlns='http://docbook.org/ns/docbook'>arbitrator</literal>) to
  reach consensus about decisions (such as failover of resources across sites).
  Arbitrators are single machines (outside of the clusters) that run a booth
  instance in a special mode. Each &geo; cluster can have one or multiple
  arbitrators.
 </para>
 <para>
  The most common scenario probably is a &geo; cluster with two sites and a
  single arbitrator on a third site. This requires three booth instances.
 </para>
  <para>
   It is also possible to run a two-site &geo; cluster <emphasis>without</emphasis>
   an arbitrator. In this case, a &geo; cluster administrator needs to manually manage
   the tickets. If a ticket should be granted to more than one site at the same time,
   booth displays a warning.
  </para>
 <figure xml:id="fig-ha-geo-example-setup">
  <title>Two-Site Cluster&mdash;2x2 Nodes + Arbitrator (Optional)</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ha_geocluster.svg" width="100%" format="SVG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ha_geocluster.png" width="85%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  The following list explains the components and mechanisms for &geo; clusters
  in more detail.
 </para>
 <variablelist>
  <varlistentry xml:id="vle-ha-geo-components-arbitrator">
   <term>Arbitrator</term>
   <listitem>
    <para>
     Each site runs one booth instance that is responsible for communicating
     with the other sites. If you have a setup with an even number of sites,
     it is useful to have an additional instance to reach consensus about decisions such as
     failover of resources across sites. In this case, add one or more
     arbitrators running at additional sites. Arbitrators are single machines
     that run a booth instance in a special mode. As all booth instances
     communicate with each other, arbitrators help to make more reliable
     decisions about granting or revoking tickets. Arbitrators cannot hold any
     tickets.
    </para>
    <para>
     An arbitrator is especially important for a two-site scenario: For
     example, if site <literal>A</literal> can no longer communicate with site
     <literal>B</literal>, there are two possible causes for that:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       A network failure between <literal>A</literal> and <literal>B</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Site <literal>B</literal> is down.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     However, if site <literal>C</literal> (the arbitrator) can still
     communicate with site <literal>B</literal>, site <literal>B</literal> must
     still be up and running.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vle-ha-geo-components-booth">
   <term>Booth Cluster Ticket Manager</term>
   <listitem>
    <para>
     Booth is the instance managing the ticket distribution, and thus, the
     failover process between the sites of a &geo; cluster. Each of the
     participating clusters and arbitrators runs a service, the &boothd;. It
     connects to the booth daemons running at the other sites and exchanges
     connectivity details. After a ticket has been granted to a site, the booth
     mechanism can manage the ticket automatically: If the site that holds the
     ticket is out of service, the booth daemons will vote which of the other
     sites will get the ticket. To protect against brief connection failures,
     sites that lose the vote (either explicitly or implicitly by being
     disconnected from the voting body) need to relinquish the ticket after a
     time-out. Thus, it is made sure that a ticket will only be redistributed
     after it has been relinquished by the previous site. See also
     <xref linkend="vle-ha-geo-components-deadman"/>.
    </para>
    <para>
     For a &geo; cluster with two sites and arbitrator, you need 3 booth
     instances: one instance per site plus the instance running on the
     arbitrator.
    </para>
    <note>
     <title>Limited Number of Booth Instances</title>
     <para>
      The upper limit is (currently) 16 booth instances.
     </para>
    </note>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vle-ha-geo-components-deadman">
   <term>Dead Man Dependency (<literal>loss-policy="fence"</literal>)</term>
   <listitem>
    <para>
     After a ticket is revoked, it can take a long time until all resources
     depending on that ticket are stopped, especially in case of cascaded
     resources. To cut that process short, the cluster administrator can
     configure a <literal>loss-policy</literal> (together with the ticket
     dependencies) for the case that a ticket gets revoked from a site. If the
     loss-policy is set to <literal>fence</literal>, the nodes that are hosting
     dependent resources are fenced.
    </para>
    <warning>
     <title>Potential Loss of Data</title>
     <para>
      On the one hand, <literal>loss-policy="fence"</literal> considerably
      speeds up the recovery process of the cluster and makes sure that
      resources can be migrated more quickly.
     </para>
     <para>
      On the other hand, it can lead to loss of all unwritten data, such as:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Data lying on shared storage.
       </para>
      </listitem>
      <listitem>
       <para>
        Data in a replicating database (for example, MariaDB or PostgreSQL)
        or on a replicating device (DRBD), where the data has not yet reached
        the other site because of a slow network link.
       </para>
      </listitem>
     </itemizedlist>
    </warning>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vle-ha-geo-components-ticket">
   <term>Ticket</term>
   <listitem>
    <para>
     A ticket grants the right to run certain resources on a specific cluster
     site. A ticket can only be owned by one site at a time. Initially, none of
     the sites has a ticket&mdash;each ticket must be granted once by the
     cluster administrator. After that, tickets are managed by the booth for
     automatic failover of resources. But administrators may also intervene and
     grant or revoke tickets manually.
    </para>
    <para>
     After a ticket is administratively revoked, it is not managed by booth
     anymore. For booth to start managing the ticket again, the ticket must be
     again granted to a site.
    </para>
    <para>
     Resources can be bound to a certain ticket by dependencies. Only if the
     defined ticket is available at a site, the respective resources are
     started. Vice versa, if the ticket is removed, the resources depending on
     that ticket are automatically stopped.
    </para>
    <para>
     The presence or absence of tickets for a site is stored in the CIB as a
     cluster status. With regard to a certain ticket, there are only two states
     for a site: <literal>true</literal> (the site has the ticket) or
     <literal>false</literal> (the site does not have the ticket). The absence
     of a certain ticket (during the initial state of the &geo; cluster) is not
     treated differently from the situation after the ticket has been revoked.
     Both are reflected by the value <literal>false</literal>.
    </para>
    <para>
     A ticket within an overlay cluster is similar to a resource in a
     traditional cluster. But in contrast to traditional clusters, tickets are
     the only type of resource in an overlay cluster. They are primitive
     resources that do not need to be configured or cloned.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>Ticket Failover</term>
   <listitem>
    <para>
     After you have initially granted an automatic ticket to a site, booth
     will manage this ticket automatically.
     If the site holding a ticket should be out of service, the ticket is
     automatically revoked after the expiry time.
     If the remaining sites have quorum, the ticket will be granted to another
     site (fail over). The resources that depend on that ticket fail over to
     the new site that holds the ticket. The <literal>loss-policy</literal>
     (which is defined within the constraint) specifies what happens to the nodes
     that have run the resources before.
    </para>
    <para>
      If automatic failover in case of a split brain scenario is
      <emphasis>not</emphasis> required, administrators can also grant manual
      tickets to the healthy site. How to manage tickets from command line is
      described in <xref linkend="sec-ha-geo-manage-cli" xrefstyle="select:label"/>.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>
 </chapter>
