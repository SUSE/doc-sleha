<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ha.concepts">
 <title>Product Overview</title>
 <info>
      <abstract>
        <para>
    &productnamereg; is an integrated suite of open source clustering
    technologies. It enables you to implement highly available physical and
    virtual Linux clusters, and to eliminate single points of failure. It
    ensures the high availability and manageability of critical network
    resources including data, applications, and services. Thus, it helps you
    maintain business continuity, protect data integrity, and reduce
    unplanned downtime for your mission-critical Linux workloads.
   </para>
        <para>
    It ships with essential monitoring, messaging, and cluster resource
    management functionality (supporting failover, failback, and migration
    (load balancing) of individually managed cluster resources).
   </para>
        <para>
    This chapter introduces the main product features and benefits of the
    &hasi;. Inside you will find several example clusters and learn about
    the components making up a cluster. The last section provides an
    overview of the architecture, describing the individual architecture
    layers and processes within the cluster.
   </para>
        <para>
    For explanations of some common terms used in the context of &ha;
    clusters, refer to <xref linkend="gl.heartb"/>.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec.ha.availability">
  <title>Availability as Extension</title>

  <para>
   The &hasi; is available as an extension to &sls; &productnumber;.
   Support for using &ha; clusters across unlimited distances is available with
   &hageo;.
  </para>
 </sect1>
 <sect1 xml:id="sec.ha.features">
  <title>Key Features</title>

  <para>
   &productnamereg; helps you ensure and manage the availability of your
   network resources. The following sections highlight some of the key
   features:
  </para>

  <sect2 xml:id="sec.ha.features.scenarios">
   <title>Wide Range of Clustering Scenarios</title>
   <para>
    The &hasi; supports the following scenarios:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Active/active configurations
     </para>
    </listitem>
    <listitem>
     <para>
      Active/passive configurations: N+1, N+M, N to 1, N to M
     </para>
    </listitem>
    <listitem>
     <para>
      Hybrid physical and virtual clusters, allowing virtual servers to be
      clustered with physical servers. This improves service availability
      and resource usage.
     </para>
    </listitem>
    <listitem>
     <para>
      Local clusters
     </para>
    </listitem>
    <listitem>
     <para>
      Metro clusters (<quote>stretched</quote> local clusters)
     </para>
    </listitem>
    <listitem>
     <para>
      &geo; clusters (&geo.dispersed; clusters)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Your cluster can contain up to 32 Linux servers. Using 
    <literal>pacemaker_remote</literal>, the cluster can be extended to include
    additional Linux servers beyond this limit.
    Any server in the cluster can restart resources (applications, services, IP
    addresses, and file systems) from a failed server in the cluster.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.features.flexibility">
   <title>Flexibility</title>
   <para>
    The &hasi; ships with &corosync; messaging and membership layer
    and Pacemaker Cluster Resource Manager. Using Pacemaker, administrators
    can continually monitor the health and status of their resources, manage
    dependencies, and automatically stop and start services based on highly
    configurable rules and policies. The &hasi; allows you to tailor a
    cluster to the specific applications and hardware infrastructure that
    fit your organization. Time-dependent configuration enables services to
    automatically migrate back to repaired nodes at specified times.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.features.storage">
   <title>Storage and Data Replication</title>
   <para>
    With the &hasi; you can dynamically assign and reassign server
    storage as needed. It supports Fibre Channel or iSCSI storage area
    networks (SANs). Shared disk systems are also supported, but they are
    not a requirement. &productname; also comes with a cluster-aware file
    system (OCFS2) and the cluster Logical Volume Manager (cluster LVM2).
    For replication of your data, use DRBD* to mirror the data of
    a &ha; service from the active node of a cluster to its standby node.
    Furthermore, &productname; also supports CTDB (Cluster Trivial Database),
    a technology for Samba clustering.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.features.virtualized">
   <title>Support for Virtualized Environments</title>
   <para>
    &productname; supports the mixed clustering of both physical and
    virtual Linux servers. &sls; &productnumber; ships with &xen;,
    an open source virtualization hypervisor, and with &kvm; (Kernel-based
    Virtual Machine), a virtualization software for Linux which is based on
    hardware virtualization extensions. The cluster resource manager in the
    &hasi; can recognize, monitor, and manage services running within
    virtual servers and services running in physical servers. Guest
    systems can be managed as services by the cluster.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.features.geo">
   <title>Support of Local, Metro, and &geo; Clusters</title>
   <para>
    &productname; has been extended to support different geographical
    scenarios. Support for &geo.dispersed; clusters (&geo; clusters)
    is available with &hageo;.
   </para>
   <variablelist>
    <varlistentry>
     <term>Local Clusters</term>
     <listitem>
      <para>
       A single cluster in one location (for example, all nodes are located
       in one data center). The cluster uses multicast or unicast for
       communication between the nodes and manages failover internally.
       Network latency can be neglected. Storage is typically accessed
       synchronously by all nodes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Metro Clusters</term>
     <listitem>
      <para>
       A single cluster that can stretch over multiple buildings or data
       centers, with all sites connected by fibre channel. The cluster uses
       multicast or unicast for communication between the nodes and manages
       failover internally. Network latency is usually low (&lt;5 ms for
       distances of approximately 20 miles). Storage is frequently
       replicated (mirroring or synchronous replication).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&geo; Clusters (Multi-Site Clusters)</term>
     <listitem>
      <para>
       Multiple, &geo.dispersed; sites with a local cluster each. The
       sites communicate via IP. Failover across the sites is coordinated by
       a higher-level entity. &geo; clusters need to cope with limited
       network bandwidth and high latency. Storage is replicated
       asynchronously.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The greater the geographical distance between individual cluster nodes,
    the more factors may potentially disturb the high availability of
    services the cluster provides. Network latency, limited bandwidth and
    access to storage are the main challenges for long-distance clusters.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.features.ra">
   <title>Resource Agents</title>
   <para>
    &productname; includes a huge number of resource agents to manage
    resources such as Apache, IPv4, IPv6 and many more. It also ships with
    resource agents for popular third party applications such as IBM
    WebSphere Application Server. For an overview of Open Cluster Framework
    (OCF) resource agents included with your product, use the <command>crm
    ra</command> command as described in
    <xref linkend="sec.ha.manual_config.ocf"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.features.tools">
   <title>User-friendly Administration Tools</title>
   <para>
    The &hasi; ships with a set of powerful tools for basic installation
    and setup of your cluster as well as effective configuration and
    administration:
   </para>
   <variablelist>
    <varlistentry>
     <term>&yast; </term>
     <listitem>
      <para>
       A graphical user interface for general system installation and
       administration. Use it to install the &hasi; on top of &sls; as
       described in the &instquick;. &yast;
       also provides the following modules in the &ha; category to help
       configure your cluster or individual components:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Cluster: Basic cluster setup. For details, refer to
         <xref linkend="cha.ha.ycluster"/>.
        </para>
       </listitem>
       <listitem>
        <para>
         DRBD: Configuration of a Distributed Replicated Block Device.
        </para>
       </listitem>
       <listitem>
        <para>
         IP Load Balancing: Configuration of load balancing with &lvs; or
         &haproxy;. For details, refer to <xref linkend="cha.ha.lb"/>.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&hawk2;</term>
     <listitem>
      <para>
       A user-friendly Web-based interface with which you can monitor and
       administer your &ha; clusters from Linux or non-Linux machines alike.
       &hawk2; can be accessed from any machine inside or outside of the cluster
       by using a (graphical) Web browser. Therefore it is the ideal solution
       even if the system on which you are working only provides a minimal graphical
       user interface. For details, <xref linkend="cha.conf.hawk2"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crm</command> Shell
     </term>
     <listitem>
      <para>
       A powerful unified command line interface to configure resources and
       execute all monitoring or administration tasks. For details, refer to
       <xref linkend="cha.ha.manual_config"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ha.benefits">
  <title>Benefits</title>

  <para>
   The &hasi; allows you to configure up to 32 Linux servers into a
   high-availability cluster (HA cluster), where resources can be
   dynamically switched or moved to any server in the cluster. Resources can
   be configured to automatically migrate if a server fails, or they can be
   moved manually to troubleshoot hardware or balance the workload.
  </para>

  <para>
   The &hasi; provides high availability from commodity components. Lower
   costs are obtained through the consolidation of applications and
   operations onto a cluster. The &hasi; also allows you to centrally
   manage the complete cluster and to adjust resources to meet changing
   workload requirements (thus, manually <quote>load balance</quote> the
   cluster). Allowing clusters of more than two nodes also provides savings
   by allowing several nodes to share a <quote>hot spare</quote>.
  </para>

  <para>
   An equally important benefit is the potential reduction of unplanned
   service outages as well as planned outages for software and hardware
   maintenance and upgrades.
  </para>

  <para>
   Reasons that you would want to implement a cluster include:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Increased availability
    </para>
   </listitem>
   <listitem>
    <para>
     Improved performance
    </para>
   </listitem>
   <listitem>
    <para>
     Low cost of operation
    </para>
   </listitem>
   <listitem>
    <para>
     Scalability
    </para>
   </listitem>
   <listitem>
    <para>
     Disaster recovery
    </para>
   </listitem>
   <listitem>
    <para>
     Data protection
    </para>
   </listitem>
   <listitem>
    <para>
     Server consolidation
    </para>
   </listitem>
   <listitem>
    <para>
     Storage consolidation
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Shared disk fault tolerance can be obtained by implementing RAID on the
   shared disk subsystem.
  </para>

  <para>
   The following scenario illustrates some benefits the &hasi; can
   provide.
  </para>

  <bridgehead>Example Cluster Scenario</bridgehead>

  <para>
   Suppose you have configured a three-server cluster, with a Web server
   installed on each of the three servers in the cluster. Each of the
   servers in the cluster hosts two Web sites. All the data, graphics, and
   Web page content for each Web site are stored on a shared disk subsystem
   connected to each of the servers in the cluster. The following figure
   depicts how this setup might look.
  </para>

  <figure>
   <title>Three-Server Cluster</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example1.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example1.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   During normal cluster operation, each server is in constant communication
   with the other servers in the cluster and performs periodic polling of
   all registered resources to detect failure.
  </para>

  <para>
   Suppose Web Server 1 experiences hardware or software problems and the
   users depending on Web Server 1 for Internet access, e-mail, and
   information lose their connections. The following figure shows how
   resources are moved when Web Server 1 fails.
  </para>

  <figure>
   <title>Three-Server Cluster after One Server Fails</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example2.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example2.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Web Site A moves to Web Server 2 and Web Site B moves to Web Server 3. IP
   addresses and certificates also move to Web Server 2 and Web Server 3.
  </para>

  <para>
   When you configured the cluster, you decided where the Web sites hosted
   on each Web server would go should a failure occur. In the previous
   example, you configured Web Site A to move to Web Server 2 and Web Site B
   to move to Web Server 3. This way, the workload once handled by Web
   Server 1 continues to be available and is evenly distributed between any
   surviving cluster members.
  </para>

  <para>
   When Web Server 1 failed, the &hasi; software did the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Detected a failure and verified with &stonith; that Web Server 1 was
     really dead. &stonith; is an acronym for <quote>Shoot The Other Node
     In The Head</quote> and is a means of bringing down misbehaving nodes
     to prevent them from causing trouble in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Remounted the shared data directories that were formerly mounted on Web
     server 1 on Web Server 2 and Web Server 3.
    </para>
   </listitem>
   <listitem>
    <para>
     Restarted applications that were running on Web Server 1 on Web Server
     2 and Web Server 3.
    </para>
   </listitem>
   <listitem>
    <para>
     Transferred IP addresses to Web Server 2 and Web Server 3.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In this example, the failover process happened quickly and users regained
   access to Web site information within seconds, usually without needing to
   log in again.
  </para>

  <para>
   Now suppose the problems with Web Server 1 are resolved, and Web Server 1
   is returned to a normal operating state. Web Site A and Web Site B can
   either automatically fail back (move back) to Web Server 1, or they can
   stay where they are. This depends on how you configured the resources for
   them. Migrating the services back to Web Server 1 will incur some
   down-time, so the &hasi; also allows you to defer the migration until
   a period when it will cause little or no service interruption. There are
   advantages and disadvantages to both alternatives.
  </para>

  <para>
   The &hasi; also provides resource migration capabilities. You can move
   applications, Web sites, etc. to other servers in your cluster as
   required for system management.
  </para>

  <para>
   For example, you could have manually moved Web Site A or Web Site B from
   Web Server 1 to either of the other servers in the cluster. Use cases for
   this are upgrading or performing scheduled maintenance on Web Server 1,
   or increasing performance or accessibility of the Web sites.
  </para>
 </sect1>
 <sect1 xml:id="sec.ha.clusterconfig">
  <title>Cluster Configurations: Storage</title>

  <para>
   Cluster configurations with the &hasi; might or might not include a
   shared disk subsystem. The shared disk subsystem can be connected via
   high-speed Fibre Channel cards, cables, and switches, or it can be
   configured to use iSCSI. If a server fails, another designated server in
   the cluster automatically mounts the shared disk directories that were
   previously mounted on the failed server. This gives network users
   continuous access to the directories on the shared disk subsystem.
  </para>

  <important>
   <title>Shared Disk Subsystem with LVM2</title>
   <para>
    When using a shared disk subsystem with LVM2, that subsystem must be
    connected to all servers in the cluster from which it needs to be
    accessed.
   </para>
  </important>

  <para>
   Typical resources might include data, applications, and services. The
   following figure shows how a typical Fibre Channel cluster configuration
   might look.
  </para>

  <figure>
   <title>Typical Fibre Channel Cluster Configuration</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example3.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example3.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Although Fibre Channel provides the best performance, you can also
   configure your cluster to use iSCSI. iSCSI is an alternative to Fibre
   Channel that can be used to create a low-cost Storage Area Network (SAN).
   The following figure shows how a typical iSCSI cluster configuration
   might look.
  </para>

  <figure>
   <title>Typical iSCSI Cluster Configuration</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example4.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example4.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Although most clusters include a shared disk subsystem, it is also
   possible to create a cluster without a shared disk subsystem. The
   following figure shows how a cluster without a shared disk subsystem
   might look.
  </para>

  <figure>
   <title>Typical Cluster Configuration Without Shared Storage</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example5.png" width="80%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example5.png" width="100%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="sec.ha.architecture">
  <title>Architecture</title>

  <para>
   This section provides a brief overview of the &hasi; architecture. It
   identifies and provides information on the architectural components, and
   describes how those components interoperate.
  </para>

  <sect2 xml:id="sec.ha.architecture.layers">
   <title>Architecture Layers</title>
   <para>
    The &hasi; has a layered architecture.
    <xref linkend="fig.ha.architecture" xrefstyle="FigureXRef"/> illustrates
    the different layers and their associated components.
   </para>
   <figure xml:id="fig.ha.architecture">
    <title>Architecture</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cluster_stack_arch.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cluster_stack_arch.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3 xml:id="sec.ha.architecture.layers.messaging">
    <title>Messaging and Infrastructure Layer</title>
    <para>
     The primary or first layer is the messaging/infrastructure layer, also
     known as the &corosync; layer. This layer contains components that
     send out the messages containing <quote>I'm alive</quote> signals and
     other information.
    </para>
   </sect3>
   <sect3 xml:id="sec.ha.architecture.layers.allocation">
    <title>Resource Allocation Layer</title>
    <para>
     The next layer is the resource allocation layer. This layer is the most
     complex, and consists of the following components:
    </para>
    <variablelist>
     <varlistentry xml:id="vle.crm">
      <term>Cluster Resource Manager (CRM)</term>
      <listitem>
       <para>
        Every action taken in the resource allocation layer passes through
        the Cluster Resource Manager. If other components of the resource
        allocation layer (or components which are in a higher layer) need to
        communicate, they do so through the local CRM. On every node, the
        CRM maintains the
        <xref linkend="vle.cib" xrefstyle="select:nopage"/>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry xml:id="vle.cib">
      <term>Cluster Information Base (CIB)</term>
      <listitem>
       <para>
        The Cluster Information Base is an in-memory XML representation of
        the entire cluster configuration and current status. It contains
        definitions of all cluster options, nodes, resources, constraints
        and the relationship to each other. The CIB also synchronizes
        updates to all cluster nodes. There is one master CIB in the
        cluster, maintained by the
        <xref linkend="vle.dc" xrefstyle="select:nopage"/>. All other nodes
        contain a CIB replica.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry xml:id="vle.dc">
      <term>Designated Coordinator (DC)</term>
      <listitem>
       <para>
        One CRM in the cluster is elected as DC. The DC is the only entity
        in the cluster that can decide that a cluster-wide change needs to
        be performed, such as fencing a node or moving resources around. The
        DC is also the node where the master copy of the CIB is kept. All
        other nodes get their configuration and resource allocation
        information from the current DC. The DC is elected from all nodes in
        the cluster after a membership change.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry xml:id="vle.pe.and.te">
      <term>Policy Engine (PE)</term>
      <listitem>
       <para>
        Whenever the Designated Coordinator needs to make a cluster-wide
        change (react to a new CIB), the Policy Engine calculates the next
        state of the cluster based on the current state and configuration.
        The PE also produces a transition graph containing a list of
        (resource) actions and dependencies to achieve the next cluster
        state. The PE always runs on the DC.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry xml:id="vle.lrm">
      <term>Local Resource Manager (LRM)</term>
      <listitem>
       <para>
        The LRM calls the local Resource Agents (see
        <xref linkend="sec.ha.architecture.layers.resources"/>) on behalf of
        the CRM. It can thus perform start / stop / monitor operations and
        report the result to the CRM. The LRM is the authoritative source
        for all resource-related information on its local node.
<!--In &productname;, the LRM is
        implemented as daemon, interacting directly with resource agents.
        The daemon is not cluster-aware and presents a common interface to
        the supported resource types.-->
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="sec.ha.architecture.layers.resources">
    <title>Resource Layer</title>
    <para>
     The highest layer is the Resource Layer. The Resource Layer includes
     one or more Resource Agents (RA). Resource Agents are programs (usually
     shell scripts) that have been written to start, stop, and monitor a
     certain kind of service (a resource). Resource Agents are called only
     by the LRM. Third parties can include their own agents in a defined
     location in the file system and thus provide out-of-the-box cluster
     integration for their own software.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.ha.architecture.processflow">
   <title>Process Flow</title>
   <para>
    &productname; uses Pacemaker as CRM. The CRM is implemented as daemon
    (<literal>crmd</literal>) that has an instance on each cluster node.
    Pacemaker centralizes all cluster decision-making by electing one of the
    crmd instances to act as a master. Should the elected crmd process (or
    the node it is on) fail, a new one is established.
   </para>
   <para>
    A CIB, reflecting the cluster’s configuration and current state of all
    resources in the cluster is kept on each node. The contents of the CIB
    are automatically kept synchronous across the entire cluster.
   </para>
   <para>
    Many actions performed in the cluster will cause a cluster-wide change.
    These actions can include things like adding or removing a cluster
    resource or changing resource constraints. It is important to understand
    what happens in the cluster when you perform such an action.
   </para>
   <para>
    For example, suppose you want to add a cluster IP address resource. To
    do this, you can use one of the command line tools or the Web interface
    to modify the CIB. It is not required to perform the actions on the DC,
    you can use either tool on any node in the cluster and they will be
    relayed to the DC. The DC will then replicate the CIB change to all
    cluster nodes.
   </para>
   <para>
    Based on the information in the CIB, the PE then computes the ideal
    state of the cluster and how it should be achieved and feeds a list of
    instructions to the DC. The DC sends commands via the
    messaging/infrastructure layer which are received by the crmd peers on
    other nodes. Each crmd uses its LRM (implemented as lrmd) to perform
    resource modifications. The lrmd is not cluster-aware and interacts
    directly with resource agents (scripts).
   </para>
   <para>
    All peer nodes report the results of their operations back to the DC.
    After the DC concludes that all necessary operations are successfully
    performed in the cluster, the cluster will go back to the idle state and
    wait for further events. If any operation was not carried out as
    planned, the PE is invoked again with the new information recorded in
    the CIB.
   </para>
   <para>
    In some cases, it may be necessary to power off nodes to protect shared
    data or complete resource recovery. For this Pacemaker comes with a
    fencing subsystem, stonithd. &stonith; is an acronym for <quote>Shoot
    The Other Node In The Head</quote> and is usually implemented with a
    remote power switch. In Pacemaker, &stonith; devices are modeled as
    resources (and configured in the CIB) to enable them to be easily
    monitored for failure. When clients detect a failure they send a
    request to stonithd, which then fences the failed node.
   </para>
  </sect2>
 </sect1>
</chapter>
