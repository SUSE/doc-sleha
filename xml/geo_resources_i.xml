<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-geo-rsc">
 <title>Configuring cluster resources and constraints</title>
 <para>
  Apart from the resources and constraints that you need to define for your
  specific cluster setup, &geo; clusters require additional resources and
  constraints as described below. You can either configure them with the
  &crmshell; (&crmsh;) as demonstrated in the examples below, or with
  &hawk2;.
 </para>
 <para>
  This chapter focuses on tasks specific to &geo; clusters. For an introduction
  to your preferred cluster management tool and general instructions on how to
  configure resources and constraints with it, refer to one of the following
  chapters:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <xref linkend="cha-conf-hawk2"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="cha-ha-manual-config"/>
   </para>
  </listitem>
 </itemizedlist>
 <para>
  If you have set up your &geo; cluster with the bootstrap scripts, the cluster
  resources needed for booth have been configured already (including a resource
  group for boothd). In this case, you can skip
  <xref
  linkend="sec-ha-geo-rsc-boothd" xrefstyle="select:label"/> and only
  need to execute the remaining steps below to complete the cluster resource
  configuration.
 </para>
 <para>
  If you are setting up your &geo; cluster manually, you need to execute all of
  the following steps:
 </para>
 <itemizedlist>
  <listitem>
   <para>
<!--Configuring Ticket Dependencies-->
    <xref linkend="sec-ha-geo-rsc-ticket-dep"/>
   </para>
  </listitem>
  <listitem>
   <para>
<!--Configuring a Resource Group for <systemitem class="daemon"
     >boothd</systemitem>-->
    <xref linkend="sec-ha-geo-rsc-boothd"/>
   </para>
  </listitem>
  <listitem>
   <para>
<!--Adding an Ordering Constraint for <systemitem class="daemon"
     >boothd</systemitem> and the Resource Group-->
    <xref linkend="sec-ha-geo-rsc-order"/>
   </para>
  </listitem>
  <listitem>
   <para>
<!--Transferring the Resource Configuration to Other Cluster Sites-->
    <xref linkend="sec-ha-geo-rsc-sync-cib"/>
   </para>
  </listitem>
 </itemizedlist>
 <important>
  <title>No CIB synchronization across sites</title>
  <para>
   The CIB is <emphasis>not</emphasis> automatically synchronized across
   cluster sites of a &geo; cluster. All resources that must be highly
   available across the &geo; cluster need to be configured for each site
   accordingly or need to be transferred to the other site or sites.
  </para>
  <para>
   To simplify transfer, any resources with site-specific parameters can be
   configured in such a way that the parameters' values depend on the name of
   the cluster site where the resource is running (see also
   <xref
    linkend="cha-ha-geo-req"/>, <citetitle>Other Requirements and
   Recommendations</citetitle>).
  </para>
  <para>
   After you have configured the resources on one site, you can tag the
   resources that are needed on all cluster sites, export them from the current
   CIB, and import them into the CIB of another cluster site. For details, see
   <xref linkend="sec-ha-geo-rsc-sync-cib"/>.
  </para>
 </important>

 <sect1 xml:id="sec-ha-geo-rsc-ticket-dep">
  <title>Configuring ticket dependencies of resources</title>
  &ticket-dependency-loss-policy;
  <procedure xml:id="pro-ha-geo-setup-rsc-constraints">
   <title>Configuring ticket dependencies of resources with crmsh</title>
   <step>
    <para>
     On one of the nodes of cluster &cluster1;, start a shell and log in as
     &rootuser; or equivalent.
<!--<remark>toms 2011-10-04: Shorten it to "Start a root shell on one
       of the cluster nodes"? I guess we deal with admin-minded people here</remark>
       taroth 2011-10-06: because of new ACL support of the crm shell, also
       non-root users may have the rights to do so, therefore phrased it like
       that -->
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step xml:id="step-ha-geo-setup-rsc-constraints">
    <para>
     Configure constraints that define which resources depend on a certain
     ticket. For example, to make a primitive resource <literal>rsc1</literal>
     depend on <literal>&ticket1;</literal>:
    </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-&ticket1; &ticket1;: \
  rsc1 loss-policy="fence"</screen>
    <para>
     In case <literal>&ticket1;</literal> is revoked, the node running the
     resource should be fenced.
    </para>
   </step>
   <step>
    <para>
     If you want other resources to depend on further tickets, create as many
     constraints as necessary with <command>rsc_ticket</command>.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-geo-rsc-boothd">
  <title>Configuring a resource group for <systemitem class="daemon">boothd</systemitem></title>

  <para>
   If you have set up your &geo; cluster with the
   <systemitem>crm cluster init</systemitem> bootstrap scripts, you can skip the
   following procedure as the resources and the resource group for boothd have
   already been configured in this case.
  </para>
  &boothd-resource-group;
 <procedure>
   <step>
    <para>
     On one of the nodes of cluster <literal>&cluster1;</literal>, start a
     shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create both primitive resources and to add them to
     one group, <literal>g-booth</literal>:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip-booth ocf:heartbeat:IPaddr2 \
  params iflabel="ha" nic="eth1" cidr_netmask="24"
  params rule #cluster-name eq &cluster1; ip="192.168.201.100" \
  params rule #cluster-name eq &cluster2; ip="192.168.202.100"
&prompt.crm.conf;<command>primitive</command> booth-site ocf:pacemaker:booth-site \
  meta resource-stickiness="INFINITY" \
  params config="nfs" op monitor interval="10s"
&prompt.crm.conf;<command>group</command> g-booth ip-booth booth-site</screen>
    <para>
     With this configuration, each booth daemon will be available at its
     individual IP address, independent of the node the daemon is running on.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-geo-rsc-order">
  <title>Adding an ordering constraint</title>

<!--taroth 2012-02-14: fix for bnc#746863-->

  &booth-order-constraint;
  <procedure>
   <step>
    <para>
     On one of the nodes of cluster &cluster1;, start a shell and log in as
     &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Create an ordering constraint, for example:
    </para>
<screen>&prompt.crm.conf;<command>order</command> o-booth-before-rsc1 Mandatory: g-booth rsc1</screen>
    <para>
     It defines that <literal>rsc1</literal> (which depends on
     <literal>&ticket1;</literal>) can only be started after the
     <literal>g-booth</literal> resource group.
    </para>
   </step>
   <step>
    <para>
     For any other resources that depend on a certain ticket, define further
     ordering constraints.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>quit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-geo-rsc-sync-cib">
<!--taroth 2014-11-25: fate#316118: [BETA 7] CIB replication between sites-->

  <title>Transferring the resource configuration to other cluster sites</title>

  <para>
   After having completed or changed your resource configuration for one
   cluster site, transfer it to the other sites of your &geo; cluster.
  </para>

  <para>
   To simplify the transfer, you can tag any resources that are needed on all
   cluster sites, export them from the current CIB, and import them into the
   CIB of another cluster site. Tagging does not create any colocation or
   ordering relationship between the resources.
  </para>

  <para>
   <xref linkend="pro-ha-geo-rsc-sync-cib-export"/> and
   <xref
    linkend="pro-ha-geo-rsc-sync-cib-import"/> give an example of how
   to do so. They are based on the following prerequisites:
  </para>

  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     You have a &geo; cluster with two sites: cluster
     <literal>&cluster1;</literal> and cluster <literal>&cluster2;</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     The cluster names for each site are defined in the respective
     &corosync.conf; files:
    </para>
<screen>totem {
     [...]
     cluster_name: &cluster1;
     }</screen>
    <para>
     This can either be done manually (by editing &corosync.conf;) or with the
     &yast; cluster module (by switching to the <guimenu>Communication
     Channels</guimenu> category and defining a <guimenu>Cluster
     Name</guimenu>). Afterward, stop and start the <systemitem
      class="service">pacemaker</systemitem>
     service for the changes to take effect:
    </para>
    <screen>&prompt.root;<command>crm</command> cluster restart</screen></listitem>
   <listitem>
    <para>
     The necessary resources for booth and for all services that should be
     highly available across your &geo; cluster have been configured in the CIB
     on site <literal>&cluster1;</literal>. They will be imported to the CIB on
     site <literal>&cluster2;</literal>.
    </para>
   </listitem>
  </itemizedlist>

  <procedure xml:id="pro-ha-geo-rsc-sync-cib-export">
   <title>Tagging and exporting a resource configuration</title>
   <step>
    <para>
     Log in to one of the nodes of cluster <literal>&cluster1;</literal>.
    </para>
   </step>
   <step>
    <para>
     Start the cluster with:
    </para>
<screen>&prompt.root;<command>crm</command> cluster start</screen>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Review the current CIB configuration:
    </para>
<screen>&prompt.crm.conf;show</screen>
   </step>
   <step>
    <para>
     Mark the resources and constraints that are needed across the &geo;
     cluster with the tag <literal>geo_resources</literal>:
    </para>
<screen>&prompt.crm.conf;<command>tag</command> geo_resources: \
  <replaceable>LIST_OF_RESOURCES_and_CONSTRAINTS_FOR_REQUIRED_SERVICES</replaceable> <co xml:id="co-geo-rsc-any"/>\
  rsc1-req-&ticket1; ip-booth booth-site g-booth o-booth-before-rsc1 <co xml:id="co-geo-rsc-booth"/></screen>
    <calloutlist>
     <callout arearefs="co-geo-rsc-any">
      <para>
       Any resources and constraints of your specific setup that you need on
       all sites of the &geo; cluster (for example, resources for DRBD as
       described in the
       <link
        xlink:href="https://documentation.suse.com/sbp/all/html/SBP-DRBD/index.html"><citetitle>&sbp;</citetitle>
       document</link>).
      </para>
     </callout>
     <callout arearefs="co-geo-rsc-booth">
      <para>
       Resources and constraints for boothd (primitives, booth resource group,
       ticket dependency, additional ordering constraint), see
       <xref
          linkend="sec-ha-geo-rsc-ticket-dep" xrefstyle="select:label"/>
       to <xref linkend="sec-ha-geo-rsc-order" xrefstyle="select:label"/>.
      </para>
     </callout>
    </calloutlist>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If the configuration is according to your wishes, submit your changes with
     <command>submit</command> and leave the crm live shell with
     <command>quit</command>.
    </para>
   </step>
   <step xml:id="st-ha-geo-rsc-sync-cib-export-start">
    <para>
     Export the tagged resources and constraints to a file named
     <filename>exported.cib</filename>:
    </para>
<screen>&prompt.root;<command>crm configure show</command> tag:geo_resources geo_resources &gt; exported.cib</screen>
    <para>
     The command <command>crm configure show
     tag:</command><replaceable>TAGNAME</replaceable> shows all resources that
     belong to the tag <replaceable>TAGNAME</replaceable>.
    </para>
   </step>
  </procedure>

  <procedure xml:id="pro-ha-geo-rsc-sync-cib-import">
   <title>Importing a tagged resource configuration</title>
   <para>
    To import the saved configuration file into the CIB of the second cluster
    site, proceed as follows:
   </para>
   <step>
    <para>
     Log in to one of the nodes of cluster <literal>&cluster2;</literal>.
    </para>
   </step>
   <step>
    <para>
     Start the cluster with:
    </para>
<screen>&prompt.root;<command>crm</command> cluster start</screen>
   </step>
   <step>
    <para>
     Copy the file <filename>exported.cib</filename> from cluster
     <literal>&cluster1;</literal> to this node.
     <remark>taroth
        2014-11-26: alternatively, the CIB can be loaded from an URL - consider
        if to mention this, too</remark>
    </para>
   </step>
   <step>
    <para>
     Import the tagged resources and constraints from the file
     <filename>exported.cib</filename> into the CIB of cluster
     <literal>&cluster2;</literal>:
    </para>
<screen>&prompt.root;<command>crm configure load</command> update <replaceable>PATH_TO_FILE/exported.cib</replaceable></screen>
    <para>
     When using the <option>update</option> parameter for the <command>crm
     configure load</command> command, &crmsh; tries to integrate the contents
     of the file into the current CIB configuration (instead of replacing the
     current CIB with the file contents).
    </para>
   </step>
   <step xml:id="st-ha-geo-rsc-sync-cib-import-stop">
    <para>
     View the updated CIB configuration with the following command:
    </para>
<screen>&prompt.root;<command>crm configure show</command></screen>
    <para>
     The imported resources and constraints will appear in the CIB.
    </para>
   </step>
  </procedure>

<!--taroth 2017-07-31: commenting because the basis for this is no longer
      included in this document:
  <example xml:id="ex-ha-geo-rsc-refer-params">
   <title>Referencing site-dependent parameters in resources</title>
   <para>
    Based on the example in<!-\-
    <xref linkend="sec-ha-geo-rsc-drbd" xrefstyle="select:label"/>-\->, you can
    also create resources that reference site-specific parameters of another
    resource, for example, the IP parameters of <literal>ip_nfs</literal>.
    Proceed as follows:
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      On cluster <literal>&cluster1;</literal> create a dummy resource that
      references the IP parameters of <literal>ip_nfs</literal> and uses them
      as the value of its <literal>state</literal> parameter:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> dummy1 ocf:pacemaker:Dummy \
  params rule #cluster-name eq &cluster1; \
  @ip_nfs-instance_attributes-0-ip:state \
  params rule #cluster-name eq &cluster2; \
  @ip_nfs-instance_attributes-1-ip:state \
  op monitor interval=10</screen>
    </listitem>
    <listitem>
     <para>
      Add a constraint to make the <literal>dummy1</literal> resource depend on
      <literal>&ticket3;</literal>, too:
     </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> dummy1-dep-&ticket3; \
  &ticket3;: dummy1 loss-policy=stop</screen>
    </listitem>
    <listitem>
     <para>
      Tag the resource and the constraint:
     </para>
<screen>&prompt.crm.conf;<command>tag</command> geo_resources_2: dummy1 \
  dummy1-dep-&ticket3;</screen>
    </listitem>
    <listitem>
     <para>
      Review your changes with <command>show</command>, submit your changes
      with <command>submit</command>, and leave the crm live shell with
      <command>quit</command>.
     </para>
    </listitem>
    <listitem>
     <para>
      Export the resources tagged with <literal>geo_resources_2</literal> from
      cluster <literal>&cluster1;</literal> and import them into the CIB of
      cluster <literal>&cluster2;</literal>, similar to
      <xref linkend="st-ha-geo-rsc-sync-cib-export-start"/> through
      <xref linkend="st-ha-geo-rsc-sync-cib-import-stop"/> of
      <xref linkend="pro-ha-geo-rsc-sync-cib" xrefstyle="select:label"/>.
     </para>
    </listitem>
   </orderedlist>
   <para>
    This configuration will result in the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      When granting <literal>&ticket3;</literal> to cluster
      <literal>&cluster1;</literal>, the following file will be created on the
      node hosting the <literal>dummy</literal> resource:
      <filename>/var/lib/heartbeat/cores/192.168.201.151</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      When granting <literal>&ticket3;</literal> to cluster
      <literal>&cluster2;</literal>, the following file will be created on the
      node hosting the <literal>dummy</literal> resource:
      <filename>/var/lib/heartbeat/cores/192.168.202.151</filename>.
     </para>
    </listitem>
   </itemizedlist>
  </example>-->
 </sect1>
</chapter>
