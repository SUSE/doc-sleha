<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<sect1 xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="sec.ha.geo.rsc">
 <title>Configuring Cluster Resources and Constraints</title>

 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <para>
  Apart from the resources and constraints that you need to define for your
  specific cluster setup, &geo; clusters require additional resources and
  constraints as described below. You can either configure them with the
  &crmshell; (&crmsh;) as demonstrated in the examples below, or with
  the &haweb; (&hawk2;).
 </para>

 <para>
  This section focuses on tasks specific to &geo; clusters. For an
  introduction to your preferred cluster management tool and general
  instructions on how to configure resources and constraints with it, refer
  to one of the following chapters in the
  <citetitle>&haguide;</citetitle> for &productname;, available from
  <link xlink:href="http://www.suse.com/documentation/"/>:
 </para>

 <itemizedlist>
  <listitem>
   <para>
    &hawk2;: Chapter <citetitle>Configuring and Managing Cluster Resources
    (Web Interface)</citetitle>
   </para>
  </listitem>
  <listitem>
   <para>
    &crmsh;: Chapter <citetitle>Configuring and Managing Cluster
    Resources (Command Line)</citetitle>
   </para>
  </listitem>
 </itemizedlist>

 <important>
  <title>No CIB Synchronization Across Sites</title>
  <para>
   The CIB is <emphasis>not</emphasis> automatically synchronized across
   cluster sites of a &geo; cluster. This means you need to configure all
   resources that must be highly available across the &geo; cluster for
   each site accordingly.
  </para>
  <para>
   To simplify transfer of the configuration to other cluster sites, any
   resources with site-specific parameters can be configured in such a way
   that the parameters' values depend on the name of the cluster site where
   the resource is running.
  </para>
  <para>
   To make this work, the cluster names for each site must be defined in the
   respective &corosync.conf; files. For example, &corosync.conf; of
   site 1 (<literal>&cluster1;</literal>) must contain the following
   entry:
  </para>
<screen>totem {
   [...]
   cluster_name: &cluster1;
   }</screen>
  <para>
   After you have configured the resources on one site, you can tag the
   resources that are needed on all cluster sites, export them from the
   current CIB, and import them into the CIB of another cluster site. For
   details, see <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
 </important>

 <sect2 xml:id="sec.ha.geo.rsc.drbd">
  <title>Resources and Constraints for DRBD</title>
  <para>
   To complete the DRBD setup, you need to configure some resources and
   constraints as shown in
   <xref linkend="pro.ha.geo.rsc.drbd" xrefstyle="select:label"/> and
   transfer them to the other cluster sites as explained in
   <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
  <procedure xml:id="pro.ha.geo.rsc.drbd">
   <title>Configuring Resources for a DRBD Setup</title>
   <step>
    <para>
     On one of the nodes of cluster <literal>&cluster1;</literal>, start
     a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Configure the (site-dependent) service IP for NFS as a basic primitive:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip_nfs ocf:heartbeat:IPaddr2 \
  params iflabel="nfs" nic="eth1" cidr_netmask="24"
  params rule #cluster-name eq &cluster1; ip="192.168.201.151" \
  params rule #cluster-name eq &cluster2; ip="192.168.202.151" \
  op monitor interval=10</screen>
   </step>
   <step>
    <para>
     Configure a file system resource and a resource for the NFS server:
    </para>
    <screen>&prompt.crm.conf;<command>primitive</command> nfs_fs ocf:heartbeat:Filesystem \
  params device="/dev/drbd/by-res/nfs/0" directory="/mnt/nfs" \
  fstype="ext4"
&prompt.crm.conf;<command>primitive</command> nfs_service ocf:heartbeat:nfsserver</screen>
   </step>
   <step>
    <para>
     Configure the following primitives and multi-state resources for DRBD:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> drbd_nfs ocf:linbit:drbd \
  params drbd_resource="nfs-upper" \
  op monitor interval="31" role="Slave" \
  op monitor interval="30" role="Master"
&prompt.crm.conf;<command>primitive</command> drbd_nfs_lower ocf:linbit:drbd \
  params rule #cluster-name eq &cluster1; \
  drbd_resource="nfs-lower-&cluster1;" \
  params rule #cluster-name eq &cluster2; \
  drbd_resource="nfs-lower-&cluster2;" \                                
  op monitor interval="31" role="Slave" \
  op monitor interval="30" role="Master"
&prompt.crm.conf;<command>ms</command> ms_drbd_nfs drbd_nfs \
  meta master-max="1" master-node-max="1" \
  clone-max="1" clone-node-max="1" notify="true"
&prompt.crm.conf;<command>ms</command> ms_drbd_nfs_lower drbd_nfs_lower \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</screen>
   </step>
   <step>
    <para>
     Add a group with the following colocation and ordering constraints:
    </para>
<screen>&prompt.crm.conf;<command>group</command> g_nfs nfs_fs nfs_service
&prompt.crm.conf;<command>colocation</command> col_nfs_ip_with_lower \
   inf: ip_nfs:Started  ms_drbd_nfs_lower:Master
&prompt.crm.conf;<command>colocation</command> col_nfs_g_with_upper \
   inf: g_nfs:Started  ms_drbd_nfs:Master
&prompt.crm.conf;<command>colocation</command> col_nfs_upper_with_ip \
   inf: ms_drbd_nfs:Master  ip_nfs:Started
&prompt.crm.conf;<command>order</command> o_lower_drbd_before_ip_nfs \
   inf: ms_drbd_nfs_lower:promote  ip_nfs:start
&prompt.crm.conf;<command>order</command> o_ip_nfs_before_drbd \
   inf: ip_nfs:start  ms_drbd_nfs:promote
&prompt.crm.conf;<command>order</command> o_drbd_nfs_before_svc \
   inf: ms_drbd_nfs:promote  g_nfs:start</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec.ha.geo.rsc.booth">
  <title>Ticket Dependencies, Constraints and Resources for booth</title>
  <para>
   To complete the booth setup, you need to execute the following steps to
   configure the resources and constraints needed for booth and failover of
   resources:
  </para>
  <itemizedlist>
   <listitem>
    <para>
<!--Configuring Ticket Dependencies-->
     <xref linkend="pro.ha.geo.setup.rsc.constraints" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
<!--Configuring a Resource Group for <systemitem class="daemon"
      >boothd</systemitem>-->
     <xref linkend="pro.ha.geo.setup.rsc.boothd" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
<!--Adding an Ordering Constraint for <systemitem class="daemon"
       >boothd</systemitem> and the Resource Group-->
     <xref linkend="pro.ha.geo.setup.rsc.order" xrefstyle="select:title"/>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The resource configurations need to be available on each of the cluster
   sites. Transfer them to the other sites as described in
   <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
  <procedure xml:id="pro.ha.geo.setup.rsc.constraints">
   <title>Configuring Ticket Dependencies of Resources</title>
   &ticket-dependency-loss-policy; 
   <step>
    <para>
     On one of the nodes of cluster &cluster1;, start a shell and log in
     as &rootuser; or equivalent.
<!--<remark>toms 2011-10-04: Shorten it to "Start a root shell on one
       of the cluster nodes"? I guess we deal with admin-minded people here</remark>
       taroth 2011-10-06: because of new ACL support of the crm shell, also
       non-root users may have the rights to do so, therefore phrased it like
       that -->
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step xml:id="step.ha.geo.setup.rsc.constraints">
    <para>
     Configure constraints that define which resources depend on a certain
     ticket. For example, we need the following constraint for the DRBD
     scenario outlined in <xref linkend="sec.ha.geo.drbd.scenario"/>:
    </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> nfs-req-ticket-nfs ticket-nfs: \ 
   ms_drbd_nfs:Master loss-policy=demote</screen>
    <para>
     This command creates a constraint with the ID
     <literal>nfs-req-ticket-nfs</literal>. It defines that the multi-state
     resource <literal>ms_drbd_nfs</literal> depends on
     <literal>ticket-nfs</literal>. However, only the resource's master mode
     depends on the ticket. In case <literal>ticket-nfs</literal> is
     revoked, <literal>ms_drbd_nfs</literal> is automatically demoted to
     <literal>slave</literal> mode, which in return will put DRBD into
     <literal>Secondary</literal> mode. That way, it is ensured that DRBD
     replication is still running, even if a site does not have the ticket.
    </para>
   </step>
   <step>
    <para>
     If you want other resources to depend on further tickets, create as
     many constraints as necessary with <command>rsc_ticket</command>.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
  <example xml:id="ex.ha.geo.setup.rsc.ticket.dep">
   <title>Ticket Dependency for Primitives</title>
   <para>
    Here is another example for a constraint that makes a primitive resource
    <literal>rsc1</literal> depend on <literal>&ticket1;</literal>:
   </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-&ticket1; &ticket1;: \
   rsc1 loss-policy="fence"</screen>
   <para>
    In case <literal>&ticket1;</literal> is revoked, the node running the
    resource should be fenced.
   </para>
  </example>
  <procedure xml:id="pro.ha.geo.setup.rsc.boothd">
   <title>Configuring a Resource Group for <systemitem class="daemon">boothd</systemitem></title> 
   &boothd-resource-group; 
   <step>
    <para>
     On one of the nodes of cluster <literal>&cluster1;</literal>, start
     a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create both primitive resources and to add them
     to one group, <literal>g-booth</literal>:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip-booth ocf:heartbeat:IPaddr2 \
  params iflabel="ha" nic="eth1" cidr_netmask="24"
  params rule #cluster-name eq &cluster1; ip="192.168.201.151" \
  params rule #cluster-name eq &cluster2; ip="192.168.202.151" 
&prompt.crm.conf;<command>primitive</command> booth ocf:pacemaker:booth-site \
  meta resource-stickiness="INFINITY" \
  params config="nfs" op monitor interval="10s"
&prompt.crm.conf;<command>group</command> g-booth ip-booth booth</screen>
    <para>
     With this configuration, each booth daemon will be available at its
     individual IP address, independent of the node the daemon is running
     on.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
<!--taroth 2012-02-14: fix for bnc#746863-->
  <procedure xml:id="pro.ha.geo.setup.rsc.order">
   <title>Adding an Ordering Constraint</title> 
   &booth-order-constraint; 
   <step>
    <para>
     On one of the nodes of cluster &cluster1;, start a shell and log in
     as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Create an ordering constraint:
    </para>
<screen>&prompt.crm.conf;<command>order</command> o-booth-before-nfs inf: g-booth ms_drbd_nfs:promote</screen>
    <para>
     The ordering constraint <literal>o-booth-before-nfs</literal> defines
     that the resource <literal>ms_drbd_nfs</literal> can only be promoted
     to master mode after the <literal>g-booth</literal> resource group has
     started.
    </para>
   </step>
   <step>
    <para>
     For any other resources that depend on a certain ticket, define further
     ordering constraints.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
  <example xml:id="ex.ha.geo.rsc.order">
   <title>Ordering Constraint for Primitives</title>
   <para>
    If the resource that depends on a certain ticket is not a multi-state
    resource, but a primitive, the ordering constraint would look like the
    following:
   </para>
<screen>&prompt.crm.conf;<command>order</command> o-booth-before-rsc1 inf: g-booth rsc1</screen>
   <para>
    It defines that <literal>rsc1</literal> (which depends on
    <literal>&ticket1;</literal>) can only be started after the
    <literal>g-booth</literal> resource group.
   </para>
  </example>
 </sect2>

 <sect2 xml:id="sec.ha.geo.rsc.sync.cib">
<!--taroth 2014-11-25: fate#316118: [BETA 7] CIB replication between sites-->
  <title>Transferring the Resource Configuration to Other Cluster Sites</title>
  <para>
   If you have configured resources for one cluster site as described in
   <xref linkend="sec.ha.geo.rsc.drbd" xrefstyle="select:label"/> and
   <xref linkend="sec.ha.geo.rsc.booth" xrefstyle="select:label"/>, you are
   not done yet. You need to transfer the resource configuration to the
   other sites of your &geo; cluster.
  </para>
  <para>
   To simplify the transfer, you can tag any resources that are needed on
   <emphasis>all</emphasis> cluster sites, export them from the current CIB,
   and import them into the CIB of another cluster site.
   <xref linkend="pro.ha.geo.rsc.sync.cib"/> gives an example of how to do
   so. It is based on the following prerequisites:
  </para>
  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     You have a &geo; cluster with two sites: cluster
     <literal>&cluster1;</literal> and cluster
     <literal>&cluster2;</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     The cluster names for each site are defined in the respective
     &corosync.conf; files:
    </para>
<screen>totem {
     [...]
     cluster_name: &cluster1;
     }</screen>
    <para>
     This can either be done manually (by editing &corosync.conf;) or
     with the &yast; cluster module as described in the
     <citetitle>&admin;</citetitle> for &productname;
     &productnumber;, available at <link xlink:href="http://www.suse.com/documentation/"/>. Refer to the
     chapter <citetitle>Installation and Basic Setup</citetitle>, procedure
     <citetitle>Defining the First Communication Channel</citetitle>.
    </para>
   </listitem>
   <listitem>
    <para>
     You have configured the necessary resources for DRBD and booth as
     described in <xref linkend="sec.ha.geo.rsc.drbd"/> and
     <xref linkend="sec.ha.geo.rsc.booth"/>.
    </para>
   </listitem>
  </itemizedlist>
  <procedure xml:id="pro.ha.geo.rsc.sync.cib">
   <title>Transferring the Resource Configuration to Other Cluster Sites</title>
   <step>
    <para>
     Log in to one of the nodes of cluster <literal>&cluster1;</literal>.
    </para>
   </step>
   <step>
    <para>
     Start the cluster with:
    </para>
<screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Tag the resources and constraints that are needed across the &geo;
     cluster:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Review the current CIB configuration:
      </para>
<screen>&prompt.crm.conf;show</screen>
     </step>
     <step>
      <para>
       Enter the following command to group the &geo; cluster-related
       resources with the tag <literal>geo_resources</literal>:
      </para>
<screen>&prompt.crm.conf;<command>tag</command> geo_resources: \
  ip_nfs nfs_fs nfs_service drbd_nfs drbd_nfs_lower ms_drbd_nfs \
  ms_drbd_nfs_lower g_nfs <co xml:id="co.geo.rsc.drbd"/>\
  col_nfs_ip_with_lower  col_nfs_g_with_upper col_nfs_upper_with_ip <xref linkend="co.geo.rsc.drbd" xrefstyle="select:label"/>\
  o_lower_drbd_before_ip_nfs o_ip_nfs_before_drbd \
  o_drbd_nfs_before_svc <xref linkend="co.geo.rsc.drbd" xrefstyle="select:label"/>\
  nfs-req-ticket-nfs ip-booth booth g-booth o-booth-before-nfs <co xml:id="co.geo.rsc.booth"/>
  [...] <co xml:id="co.geo.rsc.any"/></screen>
      <para>
       Tagging does not create any colocation or ordering relationship
       between the resources.
      </para>
      <calloutlist>
       <callout arearefs="co.geo.rsc.drbd">
        <para>
         Resources and constraints for DRBD, see
         <xref linkend="sec.ha.geo.rsc.drbd"/>.
        </para>
       </callout>
       <callout arearefs="co.geo.rsc.booth">
        <para>
         Resources and constraints for boothd, see
         <xref linkend="sec.ha.geo.rsc.booth"/>.
        </para>
       </callout>
       <callout arearefs="co.geo.rsc.any">
        <para>
         Any other resources of your specific setup that you need on all
         sites of the &geo; cluster.
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Review your changes with <command>show</command>.
      </para>
     </step>
     <step>
      <para>
       If the configuration is according to your wishes, submit your changes
       with <command>submit</command> and leave the crm live shell with
       <command>exit</command>.
      </para>
     </step>
    </substeps>
   </step>
   <step xml:id="st.ha.geo.rsc.sync.cib.export.start">
    <para>
     Export the tagged resources and constraints to a file named
     <filename>exported.cib</filename>:
    </para>
<screen>&prompt.root;<command>crm configure show</command> tag:geo_resources geo_resources &gt; exported.cib</screen>
    <para>
     The command <command>crm configure show tag:</command><replaceable>TAGNAME</replaceable> 
     shows all resources that belong to
     the tag <replaceable>TAGNAME</replaceable>.
    </para>
   </step>
   <step>
    <para>
     Log in to one of the nodes of cluster <literal>&cluster2;</literal>
     and proceed as follows:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Start the cluster with:
      </para>
<screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
     </step>
     <step>
      <para>
       Copy the file <filename>exported.cib</filename> from cluster
       <literal>&cluster1;</literal> to this node.
       <remark>taroth
        2014-11-26: alternatively, the CIB can be loaded from an URL - consider
        if to mention this, too</remark>
      </para>
     </step>
     <step>
      <para>
       Import the tagged resources and constraints from the file
       <filename>exported.cib</filename> into the CIB of cluster
       <literal>&cluster2;</literal>:
      </para>
<screen>&prompt.root;<command>crm configure load</command> update <replaceable>PATH_TO_FILE/exported.cib</replaceable></screen>
      <para>
       When using the <option>update</option> parameter for the <command>crm
       configure load</command> command, &crmsh; tries to integrate the
       contents of the file into the current CIB configuration (instead of
       replacing the current CIB with the file contents).
      </para>
     </step>
     <step xml:id="st.ha.geo.rsc.sync.cib.import.stop">
      <para>
       View the updated CIB configuration with the following command:
      </para>
<screen>&prompt.root;<command>crm configure show</command></screen>
      <para>
       The imported resources and constraints will appear in the CIB.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   This configuration will result in the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     When granting <literal>ticket-nfs</literal> to cluster
     <literal>&cluster1;</literal>, the node hosting the resource
     <literal>ip_nfs</literal> will get the IP address
     <literal>192.168.201.151</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     When granting <literal>ticket-nfs</literal> to cluster
     <literal>&cluster2;</literal>, the node hosting the resource
     <literal>ip_nfs</literal> will get the IP address
     <literal>192.168.202.151</literal>.
    </para>
   </listitem>
  </itemizedlist>
  <example xml:id="ex.ha.geo.rsc.refer.params">
   <title>Referencing Site-Dependent Parameters in Resources</title>
   <para>
    Based on the example in
    <xref linkend="pro.ha.geo.rsc.drbd" xrefstyle="select:label"/>, you can
    also create resources that reference site-specific parameters of another
    resource, for example, the IP parameters of <literal>ip_nfs</literal>.
    Proceed as follows:
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      On cluster <literal>&cluster1;</literal> create a dummy resource
      that references the IP parameters of <literal>ip_nfs</literal> and
      uses them as the value of its <literal>state</literal> parameter:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> dummy1 ocf:pacemaker:Dummy \
  params rule #cluster-name eq &cluster1; \
  @ip_nfs-instance_attributes-0-ip:state \
  params rule #cluster-name eq &cluster2; \
  @ip_nfs-instance_attributes-1-ip:state \
  op monitor interval=10</screen>
    </listitem>
    <listitem>
     <para>
      Add a constraint to make the <literal>dummy1</literal> resource depend
      on <literal>ticket-nfs</literal>, too:
     </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> dummy1-dep-ticket-nfs \
  ticket-nfs: dummy1 loss-policy=stop</screen>
    </listitem>
    <listitem>
     <para>
      Tag the resource and the constraint:
     </para>
<screen>&prompt.crm.conf;<command>tag</command> geo_resources_2: dummy1 \
  dummy1-dep-ticket-nfs</screen>
    </listitem>
    <listitem>
     <para>
      Review your changes with <command>show</command>, submit your changes
      with <command>submit</command>, and leave the crm live shell with
      <command>exit</command>.
     </para>
    </listitem>
    <listitem>
     <para>
      Export the resources tagged with <literal>geo_resources_2</literal>
      from cluster <literal>&cluster1;</literal> and import them into the
      CIB of cluster <literal>&cluster2;</literal>, similar to
      <xref linkend="st.ha.geo.rsc.sync.cib.export.start"/> through
      <xref linkend="st.ha.geo.rsc.sync.cib.import.stop"/> of
      <xref linkend="pro.ha.geo.rsc.sync.cib" xrefstyle="select:label"/>.
     </para>
    </listitem>
   </orderedlist>
   <para>
    This configuration will result in the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      When granting <literal>ticket-nfs</literal> to cluster
      <literal>&cluster1;</literal>, the following file will be created
      on the node hosting the <literal>dummy</literal> resource:
      <filename>/var/lib/heartbeat/cores/192.168.201.151</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      When granting <literal>ticket-nfs</literal> to cluster
      <literal>&cluster2;</literal>, the following file will be created
      on the node hosting the <literal>dummy</literal> resource:
      <filename>/var/lib/heartbeat/cores/192.168.202.151</filename>.
     </para>
    </listitem>
   </itemizedlist>
  </example>
 </sect2>
</sect1>
