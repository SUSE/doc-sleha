<?xml version="1.0"?>
<!DOCTYPE chapter [
  <!ENTITY % entities SYSTEM "generic-entities.ent">
  %entities;
]>

<!-- Converted by suse-upgrade version 1.1 -->
<!--taroth 2012-01-11: todo for next revision: how to define constraints via
rule editor-->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-config-hawk">
 <title>Configuring and Managing Cluster Resources (Web Interface)</title>
 <info xmlns:d="http://docbook.org/ns/docbook">
      <abstract>
        <para>
    To configure and manage cluster resources, either use &haweb;
    (&hawk;), a Web-based user interface, or the &crmshell;
    (&crmsh;) command line utility.
   </para>
        <para>
    &hawk; allows you to monitor and administer your Linux cluster from
    non-Linux machines as well. Furthermore, it is the ideal solution in
    case your system only provides a minimal graphical user interface.
   </para>
        <para>
    This chapter introduces &hawk; and covers basic tasks for configuring
    and managing cluster resources: modifying global cluster options,
    creating basic and advanced types of resources (groups and clones),
    configuring constraints, specifying failover nodes and failback nodes,
    configuring resource monitoring, starting, cleaning up or removing
    resources, and migrating resources manually. For detailed analysis of
    the cluster status, &hawk; generates a cluster report
    (<literal>hb_report</literal>). You can view the cluster history or
    explore potential failure scenarios with the simulator.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    <revhistory xml:id="rh-cha-ha-config-hawk">
   <revision>
     <date>2022-10-12</date>
     <revdescription>
       <para/>
     </revdescription>
   </revision>
 </revhistory>
</info>
    <sect1 xml:id="sec-ha-config-hawk-intro">
  <title>&hawk;&mdash;Overview</title>

  <para>
   &hawk;'s Web interface allows you to monitor and administer your Linux
   cluster from non-Linux machines as well. Furthermore, it is the ideal
   solution in case your system only provides a minimal graphical user
   interface.
  </para>

  <sect2 xml:id="sec-conf-hawk2-login">
   <title>Starting &hawk; and Logging In</title>
   <para>
    The Web interface is included in the
    <systemitem class="resource">hawk</systemitem> package. It must be
    installed on all cluster nodes you want to connect to with &hawk;. On
    the machine from which you want to access a cluster node using
    &hawk;, you only need a (graphical) Web browser with JavaScript and
    cookies enabled to establish the connection.
   </para>
   <para>
    To use &hawk;, the respective Web service must be started on the node
    that you want to connect to via the Web interface.
   </para>
   <para>
    If you have set up your cluster with the scripts from the
    <systemitem class="resource">ha-cluster-bootstrap</systemitem> package,
    the &hawk; service is already started. In that case, skip
    <xref linkend="pro-ha-hawk-service"/> and proceed with
    <xref linkend="pro-ha-hawk-login"/>.
   </para>
   <procedure xml:id="pro-ha-hawk-service">
    <title>Starting &hawk; Services</title>
    <step>
     <para>
      On the node you want to connect to, open a shell and log in as
      &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Check the status of the service by entering
     </para>
<screen>&prompt.root;<command>systemctl</command> status hawk</screen>
    </step>
    <step>
     <para>
      If the service is not running, start it with
     </para>
<screen>&prompt.root;<command>systemctl</command> start hawk</screen>
     <para>
      If you want &hawk; to start automatically at boot time, execute the
      following command:
     </para>
<screen>&prompt.root;<command>systemctl</command> enable hawk</screen>
    </step>
   </procedure>
   <note>
    <title>User Authentication</title>
    <para>
     &hawk; users must be members of the
     <systemitem class="groupname">haclient</systemitem> group. The
     installation creates a Linux user named
     <systemitem class="username">hacluster</systemitem>, who is added to
     the <systemitem class="groupname">haclient</systemitem> group. When
     using the <command>ha-cluster-init </command>script for setup, a
     default password is set for the
     <systemitem class="username">hacluster</systemitem> user.
    </para>
    <para>
     Before starting &hawk;, set or change the password for the
     <systemitem>hacluster</systemitem> user. Alternatively, create a new
     user which is a member of the
     <systemitem class="groupname">haclient</systemitem> group.
    </para>
    <para>
     Do this on every node you will connect to with &hawk;.
    </para>
   </note>
   <procedure xml:id="pro-ha-hawk-login">
    <title>Logging In to the &hawk; Web Interface</title>
    <para>
     The &hawk; Web interface uses the HTTPS protocol and port
     <literal>7630</literal>.
    </para>
    <!--taroth 2015-08-10: fate#318183 (SLE HA 12 SP1)-->
    <note>
     <title>Accessing &hawk; Via a Virtual IP</title>
     <para>To access &hawk; even in case the cluster node you usually connect to is down, a
      virtual IP address (<literal>IPaddr</literal> or <literal>IPaddr2</literal>) can be configured
      for &hawk; as cluster resource. It does not need any special configuration.</para>
    </note>
    <step>
     <para>
      On any machine, start a Web browser and make sure that JavaScript and
      cookies are enabled.
     </para>
    </step>
    <step>
     <para>
      As URL, enter the IP address or host name of any cluster node running
      the &hawk; Web service. Alternatively, enter the address of any
      <literal>IPaddr(2)</literal> resource that the cluster operator may
      have configured:
     </para>
<screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
     <note>
      <title>Certificate Warning</title>
      <para>
       If a certificate warning appears when you try to access the URL for
       the first time, a self-signed certificate is in use. Self-signed
       certificates are not considered trustworthy by default.
      </para>
      <para>
       Ask your cluster operator for the certificate details to verify the
       certificate.
      </para>
      <para>
       To proceed anyway, you can add an exception in the browser to bypass
       the warning.
      </para>
      <para>
       For information on how to replace the self-signed certificate with a
       certificate signed by an official Certificate Authority, refer to
       <xref linkend="vle-ha-hawk-certificate"/>.
      </para>
     </note>
    </step>
    <step>
     <para>
      On the &hawk; login screen, enter the <guimenu>Username</guimenu>
      and <guimenu>Password</guimenu> of the
      <systemitem class="username">hacluster</systemitem> user (or of any
      other user that is a member of the
      <systemitem class="groupname">haclient</systemitem> group).
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Log In</guimenu>.
     </para>
     <para>
      The <guimenu>Cluster Status</guimenu> screen appears, displaying the
      status of your cluster nodes and resources. The information shown is
      similar to the output of <command>crm status</command> in the
      &crmshell;.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-config-hawk-intro-main">
   <title>Main Screen: Cluster Status</title>
   <remark>taroth 2014-07-31: todo: this sections needs a serious overhaul and restructuring as a
    lot of new functions and views have been added to Hawk during the last releases - however, it
    makes more sense to postpone this until the next general UI overhaul for Hawk, so
    let's live with the current state for one more release... ;(</remark>
   <para>
    After logging in, &hawk; displays the <guimenu>Cluster
    Status</guimenu> screen. It shows a summary with the most important
    global cluster parameters and the status of your cluster nodes and
    resources. The following color code is used for status display of nodes
    and resources:
   </para>
   <itemizedlist xml:id="il-ha-config-hawk-colors">
    <title>&hawk; Color Code</title>
    <listitem>
     <para>
      Green: OK. For example, the resource is running or the node is online.
     </para>
    </listitem>
    <listitem>
     <para>
      Red: Bad, unclean. For example, the resource has failed or the node
      was not shut down cleanly.
     </para>
    </listitem>
    <listitem>
     <para>
      Yellow: In transition. For example, the node is currently being shut
      down or a resource is currently being started or stopped. If you click
      a pending resource to view its details, &hawk; also displays the
      state to which the resource is currently changing
      (<literal>Starting</literal>, <literal>Stopping</literal>,
      <literal>Moving</literal>, <literal>Promoting</literal>, or
      <literal>Demoting</literal>).
     </para>
<!--https://bugzilla.novell.com/show_bug.cgi?id=883529: 
      [doc] mention record-pending needs to be set for hawk to display resources in transitional state -->
     <note>
      <title>Displaying Transitional State for Resources</title>
      <para>
       The transitional state for resources is only shown if the operation
       property <varname>record-pending</varname> is set to
       <literal>true</literal>. If you have set up your cluster with the
       <command>ha-cluster-init</command> script, this property is turned on
       globally by default. To enable it manually, either use the &hawk;
       <guimenu>Cluster Configuration</guimenu> screen to add and enable the
       property below <guimenu>Operation Defaults</guimenu> or use the
       following command:
      </para>
<screen>&prompt.root;<command>crm</command> configure op_defaults record-pending=true</screen>
     </note>
    </listitem>
    <listitem>
     <para>
      Gray: Not running, but the cluster expects it to be running. For
      example, nodes that the administrator has stopped or put into
      <literal>standby</literal> mode. Also nodes that are offline are
      displayed in gray (if they have been shut down cleanly).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In addition to the color code, &hawk; also displays icons for the
    state of nodes, resources, tickets and for error messages in all views
    of the <guimenu>Cluster Status</guimenu> screen.
   </para>
   <para>
    If a resource has failed, an error message with the details is shown in
    red at the top of the screen. To analyze the causes for the failure,
    click the error message. This automatically takes you to &hawk;'s
    <guimenu>History Explorer</guimenu> and triggers the collection of data
    for a time span of 20 minutes (10 minutes before and 10 minutes after
    the failure occurred). For more details, refer to
    <xref linkend="pro-ha-config-hawk-history-explorer"/>.
   </para>
   <figure pgwide="0">
    <title>&hawk;&mdash;Cluster Status (Summary View)</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="hawk-cluster-status-main.png" width="90%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="hawk-cluster-status-main.png" width="80%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    The <guimenu>Cluster Status</guimenu> screen refreshes itself in near
    real-time. Choose between the following views, which you can access with
    the three icons in the upper right corner:
   </para>
<!--fate#309100-->
   <variablelist>
    <title>&hawk; Cluster Status Views</title>
    <varlistentry>
     <term>Summary View</term>
     <listitem>
      <para>
       Shows the most important global cluster parameters and the status of
       your cluster nodes and resources at the same time. If your setup
       includes &geo; clusters (multi-site clusters), the summary view
       also shows tickets. To view details about all elements belonging to a
       certain category (tickets, nodes, or resources), click the category
       title, which is marked as a link. Otherwise click the individual
       elements for details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Tree View</term>
     <listitem>
      <para>
       Presents an expandable view of the most important global cluster
       parameters and the status of your cluster nodes and resources. If
       your setup includes &geo; clusters (multi-site clusters), the tree
       view also shows tickets. Click the arrows to expand or collapse the
       elements belonging to the respective category. In contrast to the
       <guimenu>Summary View</guimenu> this view not only shows the IDs and
       status of resources but also the type (for example, primitive, clone,
       or group).
      </para>
<!--https://fate.suse.com/316303: HAWK resource summary on group level-->
      <para>
       For groups, it is also possible to switch to a view where resources
       of the same type (within a group) are presented together. Press the
       <literal>ab</literal> icon in the resources category to toggle
       between the normal view where the resources are displayed
       individually and a view where they are coalesced by type. For
       example, if you have three resources of the type
       <literal>ocf:pacemaker:Dummy</literal> in one group, and only one of
       them is running, the view-by-type view would show <literal>1/3
       ocf:pacemaker:Dummy<replaceable>&nbsp;NODENAME</replaceable></literal>
       on a gray background. This indicates that only 1 of 3 resources has
       already started.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Table View</term>
     <listitem>
      <para>
       This view is especially useful for larger clusters, because it shows
       in a concise way which resources are currently running on which node.
       Inactive nodes or resources are also displayed.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The top-level row of the main screen shows the user name with which you
    are logged in. It also allows you to <guimenu>Log Out</guimenu> of the
    Web interface, and to access the following <guimenu>Tools</guimenu> from
    the wrench icon next to the user name:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <guimenu>Simulator</guimenu>. For details, refer to
      <xref linkend="sec-ha-config-hawk-simulator"/>.
     </para>
    </listitem>
<!--fate#313718 (SP3)-->
    <listitem>
     <para>
      <guimenu>Cluster Diagram</guimenu>. Select this entry for a graphical
      representation of the nodes and the resources configured in the CIB.
      The diagram also shows the ordering and colocation between resources
      and node assignments (scores).
     </para>
     <figure xml:id="fig-ha-cluster-diagram" pgwide="0">
      <title>&hawk;&mdash;Cluster Diagram</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="hawk-cluster-diagram.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="hawk-cluster-diagram.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </listitem>
    <listitem>
     <para>
      <guimenu>Cluster Report</guimenu> (<command>hb_report</command>). For
      details, refer to <xref linkend="sec-ha-config-hawk-crmreport"/>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To perform basic operator tasks on nodes and resources (like starting or
    stopping resources, bringing nodes online, or viewing details), click
    the wrench icon next to the node or resource. This will display a
    context menu.
<!--fate#314388 (SP3)-->
    For any clone, group or multi-state child resource on any of the status
    screens, select the <guimenu>Parent</guimenu> menu item from the context
    menu. Clicking this will let you start, stop, etc. the top-level clone
    or group to which that primitive belongs.
   </para>
   <para>
    For more complex tasks like configuring resources, constraints, or
    global cluster options, use the navigation bar on the left hand side.
    From there, you can access the following screens:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <guimenu>Cluster Status</guimenu>: See
      <xref linkend="sec-ha-config-hawk-intro-main"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      <guimenu>History Explorer</guimenu>: See
      <xref linkend="pro-ha-config-hawk-history-explorer"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      <guimenu>Setup Wizard</guimenu>: See
      <xref linkend="sec-ha-config-hawk-wizard"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      <guimenu>Cluster Configuration</guimenu>: See
      <xref linkend="sec-ha-config-hawk-global"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      <guimenu>Access Control Lists</guimenu>: See
      <xref linkend="cha-ha-acl"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      <guimenu>Resources</guimenu>: See
      <xref linkend="sec-ha-config-hawk-rsc"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      <guimenu>Constraints</guimenu>: See
      <xref linkend="sec-ha-config-hawk-rsc"/> for details.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <title>Available Functions in &hawk;</title>
    <para>
     By default, users logged in as &rootuser; or
     <systemitem class="username">hacluster</systemitem> have full
     read-write access to all cluster configuration tasks. However,
     <xref linkend="cha-ha-acl" xrefstyle="select:title"/> can be used to
     define fine-grained access permissions.
    </para>
    <para>
     If ACLs are enabled in the CRM, the available functions in &hawk;
     depend on the user role and access permissions assigned to you. In
     addition, the following functions in &hawk; can only be executed by
     the user <systemitem class="username">hacluster</systemitem>:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Generating an <literal>hb_report</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Using the <guimenu>History Explorer</guimenu>.
      </para>
     </listitem>
     <listitem>
      <para>
       Viewing recent events for nodes or resources.
      </para>
     </listitem>
    </itemizedlist>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ha-config-hawk-global">
  <title>Configuring Global Cluster Options</title>

  <para>
   Global cluster options control how the cluster behaves when confronted
   with certain situations. They are grouped into sets and can be viewed and
   modified with cluster management tools like &hawk;, and
   <command>crm</command> shell. The predefined values can usually be kept.
   However, to make key functions of your cluster work correctly, you need
   to adjust the following parameters after basic cluster setup:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <xref linkend="sec-ha-config-basics-global-quorum" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec-ha-config-basics-global-stonith" xrefstyle="select:title"/>
    </para>
   </listitem>
  </itemizedlist>

  <procedure xml:id="pro-ha-config-hawk-global">
   <title>Modifying Global Cluster Options</title>
   <step>
    <para>
     Start a Web browser and log in to the cluster as described in
     <xref linkend="sec-conf-hawk2-login"/>.
    </para>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Cluster
     Properties</guimenu> to view the global cluster options and their
     current values. &hawk; displays the most important parameters with
     regard to <guimenu>CRM Configuration</guimenu>, <guimenu>Resource
     Defaults</guimenu>, and <guimenu>Operation Defaults</guimenu>.
<!--taroth 2013-03-12: I'm also wondering if it would make sense to simply display the
      default values as a text to the right of the field? this would help to reset the values to the
      defaults in case I misconfigured them... if the input fields only reflect the current value, I
      will never now that the default value has been after I changed the value once... tserong
      2013-04-15: Good idea, I've made a note to do that (although I don't know if I'll get it in
      for this release or not, maybe don't mention it, it'll be obvious anyway...) The main crm
      config properties, we have online help for, because I can extract it from pacemaker somehow
      (by querying the PE, or whatever). Unfortunately this metadata isn't exposed anywhere for
      rsc_defaults and op_defaults (or, if it is, I'm not aware of it). 
      - taroth 2014-07-31: filed https://bugzilla.novell.com/show_bug.cgi?id=889767 for this-->
    </para>
    <figure pgwide="0">
     <title>&hawk;&mdash;Cluster Configuration</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk-cluster-props.png" width="80%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk-cluster-props.png" width="80%"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     Depending on your cluster requirements, adjust the <guimenu>CRM
     Configuration</guimenu>:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Set <guimenu>no-quorum-policy</guimenu> to the appropriate value.
      </para>
     </step>
     <step>
      <para>
       If you need to disable fencing for any reasons, deselect
       <guimenu>stonith-enabled</guimenu>.
      </para>
      <important>
       <title>No Support Without &stonith;</title>
       <para>
        A cluster without &stonith; is not supported.
       </para>
      </important>
     </step>
     <step>
      <para>
       To remove a property from the CRM configuration, click the minus icon
       next to the property. If a property is deleted, the cluster will
       behave as if that property had the default value. For details of the
       default values, refer to
       <xref linkend="sec-ha-config-basics-meta-attr"/>.
      </para>
     </step>
     <step>
      <para>
       To add a new property for the CRM configuration, choose one from the
       drop-down box and click the plus icon.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     If you need to change <guimenu>Resource Defaults</guimenu> or
     <guimenu>Operation Defaults</guimenu>, proceed as follows:
    </para>
    <substeps performance="required">
     <step>
      <para>
       To change the value of defaults that are already displayed, edit the
       value in the respective input field.
      </para>
     </step>
     <step>
      <para>
       To add a new resource default or operation default, choose one from
       the empty drop-down list, click the plus icon and enter a value. If
       there are default values defined, &hawk; proposes them
       automatically.
      </para>
     </step>
     <step>
      <para>
       To remove a resource or operation default, click the minus icon next
       to the parameter. If no values are specified for <guimenu>Resource
       Defaults</guimenu> and <guimenu>Operation Defaults</guimenu>, the
       cluster uses the default values that are documented in
       <xref linkend="sec-ha-config-basics-meta-attr"/> and
       <xref linkend="sec-ha-config-basics-operations"/>.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Confirm your changes.
    </para>
   </step>
  </procedure>

<!-- taroth 2012-01-09: switching back to *default* options is not possible with
Hawk, the Revert button will switch back to the former config as long as you
haven't applied the changes yet
-->
 </sect1>
 <xi:include href="ha_hawk_rsc_config_i.xml"/>
 <xi:include href="ha_hawk_rsc_manage_i.xml"/>
 <sect1 xml:id="sec-ha-config-hawk-dashboard">
<!--fate#310306 (SP3)-->

  <title>Monitoring Multiple Clusters</title>

  <para>
   You can use &hawk; as a single point of administration for monitoring
   multiple clusters. &hawk;'s <guimenu>Cluster Dashboard</guimenu>
   allows you to view a summary of multiple clusters, with each summary
   listing the number of nodes, resources, tickets (if you use &geo;
   clusters), and their state. The summary also shows whether any failures
   have appeared in the respective cluster.
  </para>

  <para>
   The cluster information displayed in the <guimenu>Cluster
   Dashboard</guimenu> is stored in a persistent cookie. This means you need
   to decide which &hawk; instance you want to view the <guimenu>Cluster
   Dashboard</guimenu> on, and always use that one. The machine you are
   running &hawk; on does not even need to be part of any cluster for
   that purpose&mdash;it can be a separate, unrelated system.
  </para>

  <procedure xml:id="pro-ha-config-hawk-dashboard">
   <title>Monitoring Multiple Clusters with &hawk;</title>
   <itemizedlist>
    <title>Prerequisites</title>
    <listitem>
     <para>
      All clusters to be monitored from &hawk;'s <guimenu>Cluster
      Dashboard</guimenu> must be running &productname;
      &productnumber;. It is not possible to monitor clusters that are
      running earlier versions of &productname;.
     </para>
    </listitem>
    <listitem>
     <para>
      If you did not replace the self-signed certificate for &hawk; on
      every cluster node with your own certificate (or a certificate signed
      by an official Certificate Authority), log in to &hawk; on
      <emphasis>every</emphasis> node in <emphasis>every</emphasis> cluster
      at least once. Verify the certificate (and add an exception in the
      browser to bypass the warning).
     </para>
    </listitem>
    <listitem>
     <para>
      If you are using Mozilla Firefox, you must change its preferences to
      <guimenu>Accept third-party cookies</guimenu>. Otherwise cookies from
      monitored clusters will not be set, thus preventing login to the
      clusters you are trying to monitor.
     </para>
    </listitem>
   </itemizedlist>
   <step>
    <para>
     Start the &hawk; Web service on a machine you want to use for
     monitoring multiple clusters.
    </para>
   </step>
   <step>
    <para>
     Start a Web browser and as URL enter the IP address or host name of the
     machine that runs &hawk;:
    </para>
<screen>https://<replaceable>IPaddress</replaceable>:7630/</screen>
   </step>
   <step>
    <para>
     On the &hawk; login screen, click the <guimenu>Dashboard</guimenu>
     link in the right upper corner.
    </para>
    <para>
     The <guimenu>Add Cluster</guimenu> dialog appears.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk-dashboard-add-cluster.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk-dashboard-add-cluster.png" width="80%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Enter a custom <guimenu>Cluster Name</guimenu> with which to identify
     the cluster in the <guimenu>Cluster Dashboard</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Enter the <guimenu>Host Name</guimenu> of one of the cluster nodes and
     confirm your changes.
    </para>
    <para>
     The <guimenu>Cluster Dashboard</guimenu> opens and shows a summary of
     the cluster you have added.
    </para>
   </step>
   <step>
    <para>
     To add more clusters to the dashboard, click the plus icon and enter
     the details for the next cluster.
    </para>
    <figure>
     <title>&hawk;&mdash;Cluster Dashboard</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk-dashboard-multiple-clusters.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk-dashboard-multiple-clusters.png" width="80%"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     To remove a cluster from the dashboard, click the <literal>x</literal>
     icon next to the cluster's summary.
    </para>
   </step>
   <step>
    <para>
     To view more details about a cluster, click somewhere in the cluster's
     box on the dashboard.
    </para>
    <para>
     This opens a new browser window or new browser tab. If you are not
     currently logged in to the cluster, this takes you to the &hawk;
     login screen. After having logged in, &hawk; shows the
     <guimenu>Cluster Status</guimenu> of that cluster in the summary view.
     From here, you can administer the cluster with &hawk; as usual.
    </para>
   </step>
   <step>
    <para>
     As the <guimenu>Cluster Dashboard</guimenu> stays open in a separate
     browser window or tab, you can easily switch between the dashboard and
     the administration of individual clusters in &hawk;.
    </para>
   </step>
  </procedure>

  <para>
   Any status changes for nodes or resources are reflected almost
   immediately within the <guimenu>Cluster Dashboard</guimenu>.
  </para>

<!--from bnc#808703:  
   Hawk's status screen is for viewing detailed status of one cluster (which is
   why you can't add other clusters to monitor when you're on the status screen).
   
   The dashboard is for showing summary details of many clusters, any of which you
   can click to get to the (more detailed) status screen showing that cluster
   only.
   
   You don't need to log in to hawk to visit the dashboard screen.  But you do
   need to be logged in to any clusters that are being viewed in the dashboard
   (since beta3 the dashboard will prompt for login details for each monitored
   cluster if you're not already logged in).  This is probably where the confusion
   starts.  That and the fact that any Hawk instance can also show a dashboard...
   
   It might help to look at it this way.  Assume you have two clusters (c0 and
   c1), and happen to have a separate unrelated system running Hawk, which you
   only use for its dashboard functionality (in fact there doesn't even need to be
   a cluster running on that system at all).  If you view the dashboard on that
   system, you don't need to log into hawk on that node, as you're not interested
   in that system itself - you just want to use its dashboard to monitor clusters
   c0 and c1.-->
 </sect1>
 <sect1 xml:id="sec-ha-config-hawk-geo">
  <title>&hawk; for &geo; Clusters</title>

  <para>
   For more details on &hawk; features that relate to &geo.dispersed;
   clusters (&geo; clusters), see the <citetitle>&geoquick; &geo;
   Clustering for &productname;</citetitle>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-config-hawk-trouble">
  <title>Troubleshooting</title>

  <variablelist>
   <varlistentry>
    <term>&hawk; Log Files</term>
    <listitem>
     <para>
      Find the &hawk; log files in
      <filename>/srv/www/hawk/log</filename>. Check these files in case you
      cannot access &hawk;.
     </para>
     <para>
      If you have trouble starting or stopping a resource with &hawk;,
      check the Pacemaker log messages. Where Pacemaker logs to is specified
      in the <literal>logging</literal> section of
      <filename>/etc/corosync/corosync.conf</filename>.
<!--By default, Pacemaker logs to <filename>/var/log/messages</filename>.-->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Authentication Fails</term>
    <listitem>
     <para>
      If you cannot log in to &hawk; with a new user that is a member of
      the <systemitem class="groupname">haclient</systemitem> group (or if
      you experience delays until &hawk; accepts logins from this user),
      stop the <systemitem class="daemon">nscd</systemitem> daemon:
     </para>
<screen><command>systemctl</command> stop nscd</screen>
     <para>
      Try again.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-hawk-certificate">
    <term>Replacing the Self-Signed Certificate</term>
    <listitem>
     <para>
      To avoid the warning about the self-signed certificate on first
      &hawk; start-up, replace the automatically created certificate with
      your own certificate or a certificate that was signed by an official
      Certificate Authority (CA).
     </para>
     <para>
      The certificate is stored in
      <filename>/etc/lighttpd/certs/hawk-combined.pem</filename> and
      contains both key and certificate.
     </para>
     <para>
      Change the permissions to make the file only accessible by
      &rootuser;:
     </para>
<screen>chown root.root /etc/lighttpd/certs/hawk-combined.pem
      chmod 600 /etc/lighttpd/certs/hawk-combined.pem</screen>
     <para>
      After you have created or received your new key and certificate,
      combine them by executing the following command:
     </para>
<screen>cat <replaceable>keyfile</replaceable>&nbsp;<replaceable>certificatefile</replaceable> &gt; /etc/lighttpd/certs/hawk-combined.pem</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Login to &hawk; Fails After Using History Explorer/hb_report</term>
    <listitem>
     <para>
<!--taroth 2011-11-18: filed https://bugzilla.novell.com/show_bug.cgi?
      id=731282 - if that can be fixed in the software, remove this 
      troubleshooting topic - taroth 2012-01-11: some progress with the bug
      but maybe still too early to remove the topic completely-->
      Depending on the period of time you defined in the <guimenu>History
      Explorer</guimenu> or <guimenu>hb_report</guimenu> and the events that
      took place in the cluster during this time, &hawk; might collect an
      extensive amount of information. It is stored in log files in the
      <filename>/tmp</filename> directory. This might consume the remaining
      free disk space on your node. In case &hawk; should not respond
      after using the <guimenu>History Explorer</guimenu> or
      <guimenu>hb_report</guimenu>, check the hard disk of your cluster node
      and remove the respective log files.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Cluster Dashboard</guimenu>: Unable to connect to host</term>
    <listitem>
     <para>
      If adding clusters to &hawk;'s dashboard fails, check the
      prerequisites listed in
      <xref linkend="pro-ha-config-hawk-dashboard"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Cluster Dashboard</guimenu>: Node Not Accessible</term>
    <listitem>
     <para>
      The <guimenu>Cluster Dashboard</guimenu> only polls one node in each
      cluster for status. If the node being polled goes down, the dashboard
      will cycle to poll another node. In that case, &hawk; briefly
      displays a warning message about that node being inaccessible. The
      message will disappear after &hawk; has found another node to
      contact to.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
