<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xml:id="sec-ha-config-basics-remote" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Managing services on remote hosts</title>
 <info>
  <abstract>
   <para>
    The possibilities for monitoring and managing services on remote hosts
    has become increasingly important during the last few years.
    &productname; 11 SP3 offered fine-grained monitoring of services on
    remote hosts via monitoring plug-ins. The recent addition of the
    <literal>pacemaker_remote</literal> service now allows &productname;
    &productnumber; to fully manage and monitor resources on remote hosts
    just as if they were a real cluster node&mdash;without the need to
    install the cluster stack on the remote machines.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

  <sect1 xml:id="sec-ha-config-basics-remote-nagios">
   <title>Monitoring services on remote hosts with monitoring plug-ins</title>
   <para>
    Monitoring of virtual machines can be done with the VM agent (which only
    checks if the guest shows up in the hypervisor), or by external scripts
    called from the VirtualDomain or Xen agent. Up to now, more fine-grained
    monitoring was only possible with a full setup of the &ha; stack
    within the virtual machines.
   </para>
   <para>
    By providing support for monitoring plug-ins (formerly named Nagios
    plug-ins), the &hasi; now also allows you to monitor services on
    remote hosts. You can collect external statuses on the guests without
    modifying the guest image. For example, VM guests might run Web services
    or simple network resources that need to be accessible. With the Nagios
    resource agents, you can now monitor the Web service or the network
    resource on the guest. In case these services are not reachable anymore,
    the &hasi; will trigger a restart or migration of the respective
    guest.
   </para>
   <para>
    If your guests depend on a service (for example, an NFS server to be
    used by the guest), the service can either be an ordinary resource,
    managed by the cluster, or an external service that is monitored with
    Nagios resources instead.
   </para>
   <para>
    To configure the Nagios resources, the following packages must be
    installed on the host:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <package>monitoring-plugins</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>monitoring-plugins-metadata</package>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    &yast; or Zypper will resolve any dependencies on further packages,
    if required.
   </para>
   <para>
    A typical use case is to configure the monitoring plug-ins as resources
    belonging to a resource container, which usually is a VM. The container
    will be restarted if any of its resources has failed. Refer to
    <xref linkend="ex-ha-nagios-config"/> for a configuration example.
    Alternatively, Nagios resource agents can also be configured as ordinary
    resources to use them for monitoring hosts or services via the network.
   </para>
   <example xml:id="ex-ha-nagios-config">
    <title>Configuring resources for monitoring plug-ins</title>
<screen>primitive vm1 VirtualDomain \
    params hypervisor="qemu:///system" config="/etc/libvirt/qemu/vm1.xml" \
    op start interval="0" timeout="90" \
    op stop interval="0" timeout="90" \
    op monitor interval="10" timeout="30"
primitive vm1-sshd nagios:check_tcp \
    params hostname="vm1" port="22" \ <co xml:id="co-nagios-hostname"/>
    op start interval="0" timeout="120" \ <co xml:id="co-nagios-startinterval"/>
    op monitor interval="10"
group g-vm1-and-services vm1 vm1-sshd \
    meta container="vm1" <co xml:id="co-nagios-container"/></screen>
    <calloutlist>
     <callout arearefs="co-nagios-hostname">
      <para>
       The supported parameters are the same as the long options of a
       monitoring plug-in. Monitoring plug-ins connect to services with the
       parameter <literal>hostname</literal>. Therefore the attribute's
       value must be a resolvable host name or an IP address.
      </para>
     </callout>
     <callout arearefs="co-nagios-startinterval">
      <para>
       As it takes some time to get the guest operating system up and its
       services running, the start timeout of the monitoring resource must
       be long enough.
      </para>
     </callout>
     <callout arearefs="co-nagios-container">
      <para>
       A cluster resource container of type
       <literal>ocf:heartbeat:Xen</literal>,
       <literal>ocf:heartbeat:VirtualDomain</literal> or
       <literal>ocf:heartbeat:lxc</literal>. It can either be a VM or a
       Linux Container.
      </para>
     </callout>
    </calloutlist>
    <para>
     The example above contains only one resource for the
     <literal>check_tcp</literal>plug-in, but multiple resources for
     different plug-in types can be configured (for example,
     <literal>check_http</literal> or <literal>check_udp</literal>).
    </para>
    <para>
     If the host names of the services are the same, the
     <literal>hostname</literal> parameter can also be specified for the
     group, instead of adding it to the individual primitives. For example:
    </para>
<screen>group g-vm1-and-services vm1 vm1-sshd vm1-httpd \
     meta container="vm1" \
     params hostname="vm1" </screen>
    <para>
     If any of the services monitored by the monitoring plug-ins fail within
     the VM, the cluster will detect that and restart the container resource
     (the VM). Which action to take in this case can be configured by
     specifying the <literal>on-fail</literal> attribute for the service's
     monitoring operation. It defaults to
     <literal>restart-container</literal>.
    </para>
    <para>
     Failure counts of services will be taken into account when considering
     the VM's migration-threshold.
    </para>
   </example>
  </sect1>

  <sect1 xml:id="sec-ha-config-basics-remote-pace-remote">
   <title>Managing services on remote nodes with <literal>pacemaker_remote</literal></title>
   <para>
    With the <literal>pacemaker_remote</literal> service, &ha; clusters
    can be extended to virtual nodes or remote bare-metal machines. They do
    not need to run the cluster stack to become members of the cluster.
   </para>
   <para>
    The &hasi; can now launch virtual environments (KVM and LXC), plus
    the resources that live within those virtual environments without
    requiring the virtual environments to run &pace; or &corosync;.
   </para>
   <para>
    For the use case of managing both virtual machines as cluster resources
    plus the resources that live within the VMs, you can now use the
    following setup:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The <quote>normal</quote> (bare-metal) cluster nodes run the
      &hasi;.
     </para>
    </listitem>
    <listitem>
     <para>
      The virtual machines run the <literal>pacemaker_remote</literal>
      service (almost no configuration required on the VM's side).
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster stack on the <quote>normal</quote> cluster nodes launches
      the VMs and connects to the <literal>pacemaker_remote</literal>
      service running on the VMs to integrate them as remote nodes into the
      cluster.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    As the remote nodes do not have the cluster stack installed, this has
    the following implications:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Remote nodes do not take part in quorum.
     </para>
    </listitem>
    <listitem>
     <para>
      Remote nodes cannot become the DC.
     </para>
    </listitem>
    <listitem>
     <para>
      Remote nodes are not bound by the scalability limits (&corosync;
      has a member limit of 32 nodes).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Find more information about the <literal>remote_pacemaker</literal>
    service, including multiple use cases with detailed setup instructions
    in <xref linkend="article-pacemaker-remote"/>.
   </para>
  </sect1>
</chapter>
