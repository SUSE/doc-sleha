<?xml version="1.0"?>
<!DOCTYPE chapter [
  <!ENTITY % entities SYSTEM "generic-entities.ent">
  %entities;
]>

<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-storage-protect">
 <title>Storage protection and SBD</title>
 <info xmlns:d="http://docbook.org/ns/docbook">
  <abstract>
   <para>
    SBD (&stonith; Block Device) provides a node fencing mechanism for
    Pacemaker-based clusters through the exchange of messages via shared block
    storage (SAN, iSCSI, FCoE, etc.). This isolates the fencing
    mechanism from changes in firmware version or dependencies on specific
    firmware controllers. SBD needs a watchdog on each node to ensure that misbehaving
    nodes are really stopped. Under certain conditions, it is also possible to use
    SBD without shared storage, by running it in diskless mode.
   </para>
   <para>
    The cluster bootstrap scripts provide an automated
    way to set up a cluster with the option of using SBD as fencing mechanism.
    For details, see the <xref linkend="article-installation"/>. However,
    manually setting up SBD provides you with more options regarding the
    individual settings.
   </para>
   <para>
    This chapter explains the concepts behind SBD. It guides you through
    configuring the components needed by SBD to protect your cluster from
    potential data corruption in case of a split-brain scenario.
   </para>
   <para>
    In addition to node level fencing, you can use additional mechanisms for storage
    protection, such as LVM exclusive activation or OCFS2 file locking support
    (resource level fencing). They protect your system against administrative or
    application faults.
   </para>
  </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    <revhistory xml:id="rh-cha-ha-storage-protect">
   <revision>
     <date>2025-07-21</date>
     <revdescription>
       <para/>
     </revdescription>
   </revision>
 </revhistory>
</info>
    <sect1 xml:id="sec-ha-storage-protect-overview">
      <title>Conceptual overview</title>
      <para>SBD expands to <emphasis>Storage-Based Death</emphasis> or
        <emphasis>STONITH Block Device</emphasis>.
      </para>
      <para>
        The highest priority of the &ha; cluster stack is to protect the integrity
        of data. This is achieved by preventing uncoordinated concurrent access
        to data storage. The cluster stack takes care of this using several
        control mechanisms.
      </para>
      <para>
        However, network partitioning or software malfunction could potentially
        cause scenarios where several DCs are elected in a cluster. This
        split-brain scenario can cause data corruption.
      </para>
      <para>
        Node fencing via &stonith; is the primary mechanism to prevent split brain.
        Using SBD as a node fencing mechanism is one way of shutting down nodes
        without using an external power off device in case of a split-brain scenario.
      </para>

  <variablelist>
   <title>SBD components and mechanisms</title>
   <varlistentry>
    <term>SBD device</term>
    <listitem>
     <para> In an environment where all nodes have access to shared storage, a
      small logical unit (or a small partition on a logical unit) is formatted
      for use with SBD. The size of
      the SBD device depends on the block size of the used disk (for example,
      1&nbsp;MB for standard SCSI disks with 512&nbsp;byte block size or
      4&nbsp;MB for DASD disks with 4&nbsp;kB block size). The initialization
      process creates a message layout on the device with slots for up to 255
      nodes.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD daemon</term>
    <listitem>
     <para> After the respective SBD daemon is configured, it is brought online
      on each node before the rest of the cluster stack is started. It is
      terminated after all other cluster components have been shut down, thus
      ensuring that cluster resources are never activated without SBD
      supervision. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Messages</term>
    <listitem>
     <para>
      The daemon automatically allocates one of the message slots on the
      partition to itself, and constantly monitors it for messages addressed
      to itself. Upon receipt of a message, the daemon immediately complies
      with the request, such as initiating a power-off or reboot cycle for
      fencing.
     </para>
     <para>
      Also, the daemon constantly monitors connectivity to the storage device, and
      terminates itself if the partition becomes unreachable. This
      guarantees that it is not disconnected from fencing messages. If the
      cluster data resides on the same logical unit in a different partition,
      this is not an additional point of failure; the workload terminates
      anyway if the storage connectivity is lost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Watchdog</term>
     <listitem>
      <para>
      Whenever SBD is used, a correctly working watchdog is crucial.
      Modern systems support a <emphasis>hardware watchdog</emphasis>
      that needs to be <quote>tickled</quote> or <quote>fed</quote> by a
      software component. The software component (in this case, the SBD daemon)
      <quote>feeds</quote> the watchdog by regularly writing a service pulse
      to the watchdog. If the daemon stops feeding the watchdog, the hardware
      enforces a system restart. This protects against failures of the SBD
      process itself, such as dying, or becoming stuck on an I/O error.
     </para>
     </listitem>
   </varlistentry>
  </variablelist>
  <para>
   If Pacemaker integration is activated, loss of device majority alone does not
   trigger self-fencing. For example, your cluster contains three nodes: A, B, and
   C. Because of a network split, A can only see itself while B and C can
   still communicate. In this case, there are two cluster partitions: one
   with quorum because of being the majority (B, C), and one without (A).
   If this happens while the majority of fencing devices are unreachable,
   node A would self-fence, but nodes B and C would continue to run.
   </para>
  </sect1>

 <sect1 xml:id="sec-ha-storage-protect-steps">
 <title>Overview of manually setting up SBD</title>
 <para>
  The following steps are necessary to manually set up storage-based protection.
  They must be executed as &rootuser;. Before you start, check <xref linkend="sec-ha-storage-protect-req" xrefstyle="sec.ha.storage.protect.req"/>.
  </para>
 <procedure>
   <step>
    <para>
     <xref linkend="sec-ha-storage-protect-watchdog" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>Depending on your scenario, either use SBD with one to three devices or in diskless mode.
     For an outline, see <xref linkend="sec-ha-storage-protect-fencing-number"/>. The detailed setup
     is described in:</para>
    <itemizedlist>
     <listitem>
      <para>
       <xref linkend="sec-ha-storage-protect-fencing-setup" xrefstyle="select:title"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="sec-ha-storage-protect-diskless-sbd" xrefstyle="select:title"/>
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     <xref linkend="sec-ha-storage-protect-test" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-req">
  <title>Requirements and restrictions</title>
   <itemizedlist>
   <listitem>
    <para>You can use up to three SBD devices for storage-based fencing.
     When using one to three devices, the shared storage must be accessible from all nodes.</para>
   </listitem>
   <listitem>
    <para>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
     </para>
   </listitem>
   <listitem>
    <para>The shared storage can be connected via Fibre Channel (FC),
     Fibre Channel over Ethernet (FCoE), or even iSCSI. </para>
   </listitem>
   <listitem>
    <para> The shared storage segment <emphasis>must not</emphasis>
     use host-based RAID, LVM, or DRBD*. DRBD can be split, which is
     problematic for SBD, as there cannot be two states in SBD.
     Cluster multi-device (Cluster MD) cannot be used for SBD.
    </para>
   </listitem>
   <listitem>
    <para> However, using storage-based RAID and multipathing is
     recommended for increased reliability. </para>
   </listitem>
   <listitem>
    <para>An SBD device can be shared between different clusters, as
     long as no more than 255 nodes share the device. </para>
   </listitem>
   <listitem>
     <para>
       Fencing does not work with an asymmetric SBD setup. When using more
       than one SBD device, all nodes must have a slot in all SBD devices.
     </para>
   </listitem>
   <listitem>
     <para>
       When using more than one SBD device, all devices must have the same configuration,
       for example, the same timeout values.
     </para>
   </listitem>
   <listitem>
    <para>For clusters with more than two nodes, you can also use SBD in
    <emphasis>diskless</emphasis> mode. For two-node clusters, you can only use
    diskless SBD if you also configure &qdevice; to help handle split-brain scenarios.
   </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-fencing-number">
  <title>Number of SBD devices</title>
  <para> SBD supports the use of up to three devices: </para>
  <variablelist>
   <varlistentry>
    <term>One device</term>
    <listitem>
     <para>
      The most simple implementation. It is appropriate for clusters where
      all of your data is on the same shared storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Two devices</term>
    <listitem>
     <para>
      This configuration is primarily useful for environments that use
      host-based mirroring, but where no third storage device is available.
      SBD does not terminate itself if it loses access to one mirror leg,
      allowing the cluster to continue. However, since SBD does not have
      enough knowledge to detect an asymmetric split of the storage, it
      does not fence the other side while only one mirror leg is available.
      Thus, it cannot automatically tolerate a second failure while one of
      the storage arrays is down.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Three devices</term>
    <listitem>
     <para>
      The most reliable configuration. It is resilient against outages of
      one device, be it because of failures or maintenance. SBD
      terminates itself only if more than one device is lost and if required,
      depending on the status of the cluster partition or node. If at least
      two devices are still accessible, fencing messages can be successfully
      transmitted.
     </para>
     <para>
      This configuration is suitable for more complex scenarios where
      storage is not restricted to a single array. Host-based mirroring
      solutions can have one SBD per mirror leg (not mirrored itself), and
      an additional tie-breaker on iSCSI.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Diskless</term>
    <listitem>
     <para>&sbd-diskless;</para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-watchdog-timings">
   <title>Calculation of timeouts</title>
    <para>
      When using SBD as a fencing mechanism, it is vital to consider the timeouts
      of all components, because they depend on each other. When using more than one
      SBD device, all devices must have the same timeout values.
    </para>
    <variablelist>
     <varlistentry>
      <term><literal>watchdog</literal> timeout</term>
      <listitem>
       <para>
        This timeout is set during initialization of the SBD device. It depends
        mostly on your storage latency. The majority of devices must be successfully
        read within this time. Otherwise, the node might self-fence.
       </para>
       <note>
        <title>Multipath or iSCSI setup for disk-based SBD</title>
          <para>
          If your SBD devices reside on a multipath setup or iSCSI, the timeout
          should be set to the time required to detect a path failure and switch
          to the next path.
          </para>
          <para>
           This also means that in <filename>/etc/multipath.conf</filename>, the
           value of <literal>max_polling_interval</literal> must be less than the
           <literal>watchdog</literal> timeout set during initialization of the SBD device.
         </para>
       </note>
       <para>
        For diskless SBD, the watchdog timeout is set by the <literal>SBD_WATCHDOG_TIMEOUT</literal>
        parameter in <filename>/etc/sysconfig/sbd</filename>. <literal>SBD_WATCHDOG_TIMEOUT</literal>
        does not need to be set for disk-based SBD because the watchdog setting in the SBD device's
        metadata takes precedence over <filename>/etc/sysconfig/sbd</filename>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>msgwait</literal> timeout</term>
      <listitem>
       <para>
        This timeout is set during initialization of the SBD device. It defines
        the time after which a message written to a node's slot on the SBD device
        is considered delivered. The timeout should be long enough for the node to
        detect that it needs to self-fence.
       </para>
       <para>
        However, if the <literal>msgwait</literal> timeout is long,
        a fenced cluster node might rejoin before the fencing action returns.
        This can be mitigated by setting the <varname>SBD_DELAY_START</varname>
        parameter in the SBD configuration, as described in
        <xref linkend="pro-ha-storage-protect-sbd-config" xrefstyle="select:label"/>
        in
        <xref linkend="st-ha-storage-protect-sbd-delay-start"/>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>stonith-timeout</literal> in the CIB</term>
      <listitem>
       <para>
        This timeout is set in the CIB as a global cluster property. It defines
        how long to wait for the &stonith; action (reboot, on, off) to complete.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>stonith-watchdog-timeout</literal> in the CIB</term>
      <listitem>
       <para>
        This timeout is set in the CIB as a global cluster property. If not set
        explicitly, it defaults to <literal>0</literal>, which is appropriate for
        using SBD with one to three devices. For SBD in diskless mode, this timeout
        must <emphasis>not</emphasis> be <literal>0</literal>.</para>
      </listitem>
     </varlistentry>
    </variablelist>
  <para>
   If you change the watchdog timeout, you must adjust the other timeouts
   as well. The following formula expresses the relationship between these values
   for disk-based SBD:
  </para>
   <example xml:id="ex-ha-storage-protect-disk-based-sbd-timings">
    <title>Formula for disk-based SBD timeout calculation</title>
    <screen>Timeout (msgwait) &gt;= (Timeout (watchdog) * 2)
stonith-timeout &gt;= Timeout (msgwait) + 20%</screen>
    <para>
      For example, if you set the watchdog timeout to <literal>30</literal>,
      set the <literal>msgwait</literal> timeout to at least <literal>60</literal> and
      <literal>stonith-timeout</literal> to at least <literal>72</literal>.
    </para>
   </example>
   <para>
    The following formula expresses the relationship between these values
    for diskless SBD:
   </para>
   <example xml:id="ex-ha-storage-protect-diskless-sbd-timings">
    <title>Formula for diskless SBD timeout calculation</title>
    <screen>stonith-watchdog-timeout &gt;= (SBD_WATCHDOG_TIMEOUT * 2)
stonith-timeout &gt;= stonith-watchdog-timeout + 20%
    </screen>
    <para>
      For example, if you set <literal>SBD_WATCHDOG_TIMEOUT</literal> to <literal>10</literal>,
      set <literal>stonith-watchdog-timeout</literal> to at least <literal>20</literal> and
      <literal>stonith-timeout</literal> to at least <literal>24</literal>.
    </para>
   </example>
    <para>
     If you use the bootstrap scripts provided by the &crmshell; to set up a
     cluster and to initialize the SBD device, the relationship between these
     timeouts is automatically considered.
    </para>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-watchdog">
  <title>Setting up the watchdog</title>
  <para> &productname; ships with several kernel modules that provide
   hardware-specific watchdog drivers. For clusters in production environments,
   we recommend using a hardware-specific watchdog driver. However, if no watchdog
   matches your hardware, <systemitem class="resource">softdog</systemitem> can
   be used as kernel watchdog module.
 </para>
 <para>
   &productname; uses the SBD daemon as the software component that <quote>feeds</quote>
   the watchdog.</para>

  <sect2 xml:id="sec-ha-storage-protect-hw-watchdog">
   <title>Using a hardware watchdog</title>

   <para>Finding the right watchdog kernel module for a given system is not
    trivial. Automatic probing fails often. As a result, many modules
    are already loaded before the right one gets a chance.</para>
    <para>
     The following table lists some commonly used watchdog drivers. However, this is
     not a complete list of supported drivers. If your hardware is not listed below,
     you can also find a list of choices in the directories
     <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/watchdog</filename>
     and
     <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/ipmi</filename>.
     Alternatively, ask your hardware or
     system vendor for details on system-specific watchdog configuration.
    </para>
   <table xml:id="tab-ha-storage-protect-watchdog-drivers">
    <title>Commonly used watchdog drivers</title>
    <tgroup cols="2">
     <thead>
      <row>
       <entry>Hardware</entry>
       <entry>Driver</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>HP</entry>
       <entry><systemitem class="resource">hpwdt</systemitem></entry>
      </row>
      <row>
       <entry>Dell, Lenovo (Intel TCO)</entry>
       <entry><systemitem class="resource">iTCO_wdt</systemitem></entry>
      </row>
      <row>
       <entry>Fujitsu</entry>
       <entry><systemitem class="resource">ipmi_watchdog</systemitem></entry>
      </row>
      <row>
       <entry>LPAR on IBM Power</entry>
       <entry><systemitem class="resource">pseries-wdt</systemitem></entry>
      </row>
      <row>
       <entry>VM on IBM z/VM</entry>
       <entry><systemitem class="resource">vmwatchdog</systemitem></entry>
      </row>
      <row>
       <entry>Xen VM (DomU)</entry>
       <entry><systemitem class="resource">xen_xdt</systemitem></entry>
      </row>
      <row>
       <entry>VM on VMware vSphere</entry>
       <entry><systemitem class="resource">wdat_wdt</systemitem></entry>
      </row>
      <row>
       <entry>Generic</entry>
       <entry><systemitem class="resource">softdog</systemitem></entry>
      </row>
     </tbody>
    </tgroup>
   </table>

 <important>
    <title>Accessing the watchdog timer</title>
    <para>Some hardware vendors ship systems management software that uses the
     watchdog for system resets (for example, HP ASR daemon). If the watchdog is
     used by SBD, disable such software. No other software must access the
     watchdog timer. </para>
   </important>

   <procedure xml:id="pro-ha-storage-protect-watchdog">
    <title>Loading the correct kernel module</title>
    <para>To make sure the correct watchdog module is loaded, proceed as follows:</para>
     <step>
      <para>List the drivers that have been installed with your kernel version:</para>
       <screen>&prompt.root;<command>rpm -ql kernel-<replaceable>VERSION</replaceable> | grep watchdog</command></screen>
      </step>
      <step xml:id="st-ha-storage-listwatchdog-modules">
       <para>List any watchdog modules that are currently loaded in the kernel:</para>
       <screen>&prompt.root;<command>lsmod | egrep "(wd|dog)"</command></screen>
      </step>
      <step>
       <para>If you get a result, unload the wrong module:</para>
       <screen>&prompt.root;<command>rmmod <replaceable>WRONG_MODULE</replaceable></command></screen>
      </step>
      <step>
     <para> Enable the watchdog module that matches your hardware: </para>
     <screen>&prompt.root;<command>echo <replaceable>WATCHDOG_MODULE</replaceable> &gt; /etc/modules-load.d/watchdog.conf</command>
&prompt.root;<command>systemctl restart systemd-modules-load</command></screen>
    </step>
    <step>
     <para>Test whether the watchdog module is loaded correctly:</para>
     <screen>&prompt.root;<command>lsmod | grep dog</command></screen>
    </step>
    <step>
     <para>Verify if the watchdog device is available and works:</para>
     <screen>&prompt.root;<command>ls -l /dev/watchdog*</command>
&prompt.root;<command>sbd query-watchdog</command></screen>
     <para> If your watchdog device is not available, check the
      module name and options. Maybe use another driver. </para>
    </step>
    <step>
     <para>
      Verify if the watchdog device works:
     </para>
     <screen>&prompt.root;<command>sbd -w <replaceable>WATCHDOG_DEVICE</replaceable> test-watchdog</command></screen>
    </step>
    <step>
     <para>
      Reboot your machine to make sure there are no conflicting kernel modules. For example,
      if you find the message <literal>cannot register ...</literal> in your log, this would indicate
      such conflicting modules. To ignore such modules, refer to <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-mod.html#sec-mod-modprobe-blacklist"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-storage-protect-sw-watchdog">
   <title>Using the software watchdog (softdog)</title>
   <para>
    For clusters in production environments we recommend to use a hardware-specific watchdog
    driver. However, if no watchdog matches your hardware, <systemitem class="resource">softdog</systemitem> can be used as kernel watchdog module. </para>

   <important>
    <title>Softdog limitations</title>
    <para>
     The softdog driver assumes that at least one CPU is still running. If all
     CPUs are stuck, the code in the softdog driver that should reboot the system
     will never be executed. In contrast, hardware watchdogs keep working even
     if all CPUs are stuck.
    </para>
   </important>

   <procedure xml:id="pro-ha-storage-protect-sw-watchdog">
    <title>Loading the softdog kernel module</title>
    <step>
     <para>Enable the softdog watchdog:</para>
<screen>&prompt.root;<command>echo softdog &gt; /etc/modules-load.d/watchdog.conf</command>
&prompt.root;<command>systemctl restart systemd-modules-load</command></screen>
    </step>
    <step>
     <para>Test whether the softdog watchdog module is loaded correctly:</para>
     <screen>&prompt.root;<command>lsmod | grep softdog</command></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-fencing-setup">
  <title>Setting up SBD with devices</title>
  <para>
   The following steps are necessary for setup:
  </para>
 <procedure>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-sbd-create" xrefstyle="select:title"/>
        </para>
   </step>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-sbd-config" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-sbd-services" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-sbd-test" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-fencing" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>
  <para>
    Before you start, make sure the block device or devices you want to use for
    SBD meet the requirements specified in <xref linkend="sec-ha-storage-protect-req" xrefstyle="select:label"/>.
  </para>
  <para>
   When setting up the SBD devices, you need to take several timeout values into
   account. For details, see <xref linkend="sec-ha-storage-protect-watchdog-timings"/>.
  </para>
  <para>
   The node terminates itself if the SBD daemon running on it has not
   updated the watchdog timer fast enough. After having set the timeouts, test
   them in your specific environment.
  </para>

  <procedure xml:id="pro-ha-storage-protect-sbd-create">
   <title>Initializing the SBD devices</title>
   <para>
    To use SBD with shared storage, you must first create the messaging
    layout on one to three block devices. The <command>sbd create</command> command
    writes a metadata header to the specified device or devices. It also
    initializes the messaging slots for up to 255 nodes. If executed without any
    further options, the command uses the default timeout settings.</para>
    <warning>
     <title>Overwriting existing data</title>
      <para> Make sure the device or devices you want to use for SBD do not hold any
       important data. When you execute the <command>sbd create</command>
       command, roughly the first megabyte of the specified block devices
       is overwritten without further requests or backup.
      </para>
    </warning>
    <step>
     <para>Decide which block device or block devices to use for SBD.</para>
    </step>
    <step>
     <para>Initialize the SBD device: </para>
     <itemizedlist>
       <listitem>
         <para>
           To use one device with the default timeout values, run the following command:
         </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> create</command></screen>
       </listitem>
       <listitem>
         <para>
           To use more than one device for SBD, specify the <option>-d</option> option multiple
           times, for example:
         </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID1</replaceable> -d /dev/disk/by-id/<replaceable>DEVICE_ID2</replaceable> -d /dev/disk/by-id/<replaceable>DEVICE_ID3</replaceable> create</command></screen>
       </listitem>
       <listitem>
       <para>To adjust the timeouts to use for SBD (for example, if your SBD device resides on a
        multipath group), use the <option>-1</option> and <option>-4</option> options. If you
        initialize more than one device, you must set the same timeout values for all devices. For
        details, see <xref linkend="sec-ha-storage-protect-watchdog-timings"/>.
        All timeouts are given in seconds:</para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> -1 30</command><co xml:id="co-ha-sbd-watchdog"/> <command>-4 60</command><co xml:id="co-ha-sbd-msgwait"/> <command>create</command></screen>
       <calloutlist>
         <callout arearefs="co-ha-sbd-watchdog">
         <para> The <option>-1</option> option is used to specify the
          <literal>watchdog</literal> timeout. In the example above, it is set
          to <literal>30</literal> seconds. The minimum allowed value for the
          emulated watchdog is <literal>15</literal> seconds. </para>
        </callout>
        <callout arearefs="co-ha-sbd-msgwait">
        <para> The <option>-4</option> option is used to specify the
          <literal>msgwait</literal> timeout. In the example above, it is set to
          <literal>60</literal> seconds. </para>
        </callout>
      </calloutlist>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>Check what is written on the device: </para>
     <screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> dump</command>
Header version     : 2.1
UUID               : 619127f4-0e06-434c-84a0-ea82036e144c
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 15
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 30
==Header on disk /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> is dumped</screen>
    <para> As you can see, the timeouts are also stored in the header to ensure
    that all participating nodes agree on them. </para>
    </step>
   </procedure>
   <para>
    After you have initialized the SBD devices, edit the SBD configuration file,
    then enable and start the respective services for the changes to take effect.
   </para>

   <procedure xml:id="pro-ha-storage-protect-sbd-config">
   <title>Editing the SBD configuration file</title>
    <step>
     <para>Open the file <filename>/etc/sysconfig/sbd</filename>.</para>
    </step>
    <step>
     <para>Search for the following parameter: <parameter>SBD_DEVICE</parameter>.
     </para>
     <para>It specifies the devices to monitor and to use for exchanging SBD messages.
     </para>
    <para> Edit this line by replacing /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>
     with your SBD device:</para>
    <screen>SBD_DEVICE="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>"</screen>
    <para> If you need to specify multiple devices in the first line, separate them with semicolons
     (the order of the devices does not matter):</para>
    <screen>SBD_DEVICE="/dev/disk/by-id/<replaceable>DEVICE_ID1</replaceable>;/dev/disk/by-id/<replaceable>DEVICE_ID2</replaceable>;/dev/disk/by-id/<replaceable>DEVICE_ID3</replaceable>"</screen>
    <para> If the SBD device is not accessible, the daemon fails to start and inhibits
     cluster start-up. </para>
   </step>
   <step xml:id="st-ha-storage-protect-sbd-delay-start">
    <para>Search for the following parameter: <varname>SBD_DELAY_START</varname>.</para>
    <para>
      Enables or disables a delay. Set <varname>SBD_DELAY_START</varname>
      to <literal>yes</literal> if <literal>msgwait</literal> is
      long, but your cluster nodes boot quickly.
      Setting this parameter to <literal>yes</literal> delays the start of
      SBD on boot. This is sometimes necessary with virtual machines.
    </para>
    <para>
      The default delay length is the same as the <literal>msgwait</literal> timeout value.
      Alternatively, you can specify an integer, in seconds, instead of <literal>yes</literal>.
      For help calculating an appropriate value, see <command>man sbd</command>, section
      <literal>Configuration via environment</literal>.
    </para>
    <para>
      If you enable <varname>SBD_DELAY_START</varname>, you must also check the SBD service file
      to ensure that the value of <literal>TimeoutStartSec</literal> is greater than the value of
      <varname>SBD_DELAY_START</varname>. For more information, see
      <link xlink:href="https://www.suse.com/support/kb/doc/?id=000019356"/>.
    </para>
   </step>
   <step>
     <para>
       Copy the configuration file to all nodes by using <command>csync2</command>:
     </para>
<screen>&prompt.root;<command>csync2 -xv</command></screen>
     <para>
       For more information, see <xref linkend="sec-ha-installation-setup-csync2"/>.
     </para>
   </step>
  </procedure>

 <para>After you have added your SBD devices to the SBD configuration file,
  enable the SBD daemon. The SBD daemon is a critical piece
  of the cluster stack. It needs to be running when the cluster stack is running.
  Thus, the <systemitem>sbd</systemitem> service is started as a dependency whenever
  the cluster services are started.</para>

  <procedure xml:id="pro-ha-storage-protect-sbd-services">
   <title>Enabling and starting the SBD service</title>
   <step>
    <para>On each node, enable the SBD service:</para>
    <screen>&prompt.root;<command>systemctl enable sbd</command></screen>
    <para>SBD starts together with the Corosync service whenever the
     cluster services are started.</para>
   </step>
   <step>
    <para>Restart the cluster services on all nodes at once by using the <option>--all</option>
     option:</para>
    <screen>&prompt.root;<command>crm cluster restart --all</command></screen>
    <para> This automatically triggers the start of the SBD daemon. </para>
    <important>
     <title>Restart cluster services for SBD changes</title>
     <para>
       If any SBD metadata changes, you must restart the cluster services again. To keep critical
       cluster resources running during the restart, consider putting the cluster into maintenance
       mode first. For more information, see <xref linkend="cha-ha-maintenance"/>.
     </para>
   </important>
   </step>
  </procedure>

  <para>
   As a next step, test the SBD devices as described in <xref linkend="pro-ha-storage-protect-sbd-test" xrefstyle="select:label"/>.
  </para>

  <procedure xml:id="pro-ha-storage-protect-sbd-test">
   <title>Testing the SBD devices</title>
    <step>
     <para> The following command dumps the node slots and their current
      messages from the SBD device: </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> list</command></screen>
    <para> Now you should see all cluster nodes that have ever been started with SBD listed here.
     For example, if you have a two-node cluster, the message slot should show
      <literal>clear</literal> for both nodes:</para>
     <screen>0       &node1;        clear
1       &node2;          clear</screen>
    </step>
    <step>
     <para> Try sending a test message to one of the nodes: </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> message &node1; test</command></screen>
    </step>
    <step>
     <para> The node acknowledges the receipt of the message in the system
      log files: </para>
<screen>May 03 16:08:31 &node1; sbd[66139]: /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>: notice: servant:
Received command test from &node2; on disk /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></screen>
     <para> This confirms that SBD is indeed up and running on the node and
      that it is ready to receive messages. </para>
    </step>
   </procedure>

  <para>
   As a final step, you need to adjust the cluster configuration as described in
   <xref linkend="pro-ha-storage-protect-fencing" xrefstyle="select:label"/>.
  </para>

<procedure xml:id="pro-ha-storage-protect-fencing">
 <title>Configuring the cluster to use SBD</title>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm configure</command>.
    </para>
   </step>
   <step>
    <para>Enter the following:</para>
    <screen>
&prompt.crm.conf;<command>property stonith-enabled="true"</command><co xml:id="co-ha-sbd-st-enabled"/>
&prompt.crm.conf;<command>property stonith-watchdog-timeout=0</command><co xml:id="co-ha-sbd-watchdog-timeout"/>
&prompt.crm.conf;<command>property stonith-timeout="40s"</command><co xml:id="co-ha-sbd-st-timeout"/></screen>
    <calloutlist>
     <callout arearefs="co-ha-sbd-st-enabled">
      <para>
       This is the default configuration, because clusters without &stonith; are not supported.
       But in case &stonith; has been deactivated for testing purposes,
       make sure this parameter is set to <literal>true</literal> again.</para>
     </callout>
     <callout arearefs="co-ha-sbd-watchdog-timeout">
      <para>If not explicitly set, this value defaults to <literal>0</literal>,
        which is appropriate for use of SBD with one to three devices.
      </para>
     </callout>
     <callout arearefs="co-ha-sbd-st-timeout">
      <para>
       To calculate the <parameter>stonith-timeout</parameter>, refer to
       <xref linkend="sec-ha-storage-protect-watchdog-timings"/>.
       A <systemitem>stonith-timeout</systemitem> value of <literal>40</literal>
       would be appropriate if the <literal>msgwait</literal> timeout value for
       SBD was set to <literal>30</literal> seconds.</para>
     </callout>
   </calloutlist>
  </step>
  <step xml:id="st-ha-storage-protect-fencing-static-random">
   <para>
    Configure the SBD &stonith; resource. You do not need to clone this resource.
   </para>
   <para>
    For a two-node cluster, in case of split brain, fencing is issued from
    each node to the other as expected. To prevent both nodes from being reset at practically
    the same time, it is recommended to apply the following fencing
    delays to help one of the nodes, or even the preferred node, win the fencing match.
    For clusters with more than two nodes, you do not need to apply these delays.
   </para>
   <variablelist>
    <varlistentry>
     <term>Priority fencing delay</term>
     <listitem>
       <para>
        The <literal>priority-fencing-delay</literal> cluster property is disabled by
        default. By configuring a delay value, if the other node is lost and it has
        the higher total resource priority, the fencing targeting it is delayed
        for the specified amount of time. This means that in case of split-brain,
        the more important node wins the fencing match.
      </para>
      <para>
        Resources that matter can be configured with priority meta attribute. On
        calculation, the priority values of the resources or instances that are running
        on each node are summed up to be accounted. A promoted resource instance takes the
        configured base priority plus one so that it receives a higher value than any
        unpromoted instance.
      </para>
      <screen>&prompt.root;<command>crm configure property priority-fencing-delay=30</command></screen>
       <para>
        Even if <literal>priority-fencing-delay</literal> is used, we still
        recommend also using <literal>pcmk_delay_base</literal> or
        <literal>pcmk_delay_max</literal> as described below to address any
        situations where the nodes happen to have equal priority.
        The value of <literal>priority-fencing-delay</literal> should be significantly
        greater than the maximum of <literal>pcmk_delay_base</literal> / <literal>pcmk_delay_max</literal>,
        and preferably twice the maximum.
       </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Predictable static delay</term>
     <listitem>
      <para>This parameter adds a static delay before executing &stonith; actions.
      To prevent the nodes from being reset at the same time under split-brain of
      a two-node cluster, configure separate fencing resources with different delay values.
      The preferred node can be marked with the parameter to be targeted with a longer
      fencing delay so that it wins any fencing match.
      To make this succeed, each node must have two primitive &stonith;
      devices. In the following configuration, &node1; wins
      and survives in a split-brain scenario:
      </para>
<screen>&prompt.crm.conf;<command>primitive st-sbd-&node1; stonith:external/sbd \
  params pcmk_host_list=&node1; pcmk_delay_base=20</command>
&prompt.crm.conf;<command>primitive st-sbd-&node2; stonith:external/sbd \
  params pcmk_host_list=&node2; pcmk_delay_base=0</command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Dynamic random delay</term>
     <listitem>
      <para>This parameter adds a random delay for &stonith; actions on the fencing device.
       Rather than a static delay targeting a specific node, the parameter
       <parameter>pcmk_delay_max</parameter> adds a random delay for any fencing
       with the fencing resource to prevent double reset. Unlike
       <parameter>pcmk_delay_base</parameter>, this parameter can be specified for
       a unified fencing resource targeting multiple nodes.
      </para>
<screen>&prompt.crm.conf;<command>primitive stonith_sbd stonith:external/sbd \
  params pcmk_delay_max=30</command></screen>
      <warning>
       <title><parameter>pcmk_delay_max</parameter> might not prevent double reset
       in a split-brain scenario</title>
       <para>
        The lower the value of <parameter>pcmk_delay_max</parameter>, the higher
        the chance that a double reset might still occur.
       </para>
       <para>
        If your aim is to have a predictable survivor, use a priority fencing delay
        or predictable static delay.
       </para>
      </warning>
     </listitem>
    </varlistentry>
   </variablelist>
  </step>
  <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     Submit your changes with <command>commit</command> and leave the crm live
     configuration with <command>quit</command>.
    </para>
   </step>
  </procedure>

   <para> After the resource starts, your cluster is successfully
    configured to use SBD if a node needs to be fenced.</para>
  </sect1>

  <sect1 xml:id="sec-ha-storage-protect-diskless-sbd">
   <title>Setting up diskless SBD</title>
   <para>SBD can be operated in a diskless mode. In this mode, a watchdog device
    is used to reset the node in the following cases: if it loses quorum,
    if any monitored daemon is lost and not recovered, or if Pacemaker decides
    that the node requires fencing. Diskless SBD is based on
    <quote>self-fencing</quote> of a node, depending on the status of the cluster,
    the quorum and certain reasonable assumptions. No &stonith; SBD resource
    primitive is needed in the CIB.
   </para>
   <important>
    <title>Do not block &corosync; traffic in the local firewall</title>
    <para>
     Diskless SBD relies on reformed membership and loss of quorum to achieve
     fencing. &corosync; traffic must be able to pass through all network interfaces,
     including the loopback interface, and must not be blocked by a local firewall.
     Otherwise, &corosync; cannot reform a new membership, which can cause a
     split-brain scenario that cannot be handled by diskless SBD fencing.
    </para>
   </important>
    <important>
     <title>Number of cluster nodes</title>
       <para>
        SBD in diskless mode cannot handle split-brain scenarios for two-node clusters.
        Do <emphasis>not</emphasis> use diskless SBD as a fencing mechanism
        for two-node clusters unless you also configure &qdevice; to participate in
        quorum decisions, as described in <xref linkend="cha-ha-qdevice"/>.
      </para>
   </important>

   <procedure xml:id="pro-ha-storage-protect-confdiskless">
    <title>Configuring diskless SBD</title>
    <step>
     <para>Open the file <filename>/etc/sysconfig/sbd</filename> and use
      the following entries:</para>
     <screen>SBD_PACEMAKER=yes
SBD_STARTMODE=always
SBD_DELAY_START=no
SBD_WATCHDOG_DEV=/dev/watchdog
SBD_WATCHDOG_TIMEOUT=5</screen>
      <para>
       The <varname>SBD_DEVICE</varname> entry is not needed as no shared
       disk is used. When this parameter is missing, the <systemitem>sbd</systemitem>
       service does not start any watcher process for SBD devices.
      </para>
      <para>
        If you need to delay the start of SBD on boot, change <varname>SBD_DELAY_START</varname>
        to <literal>yes</literal>. The default delay length is double the value of
        <varname>SBD_WATCHDOG_TIMEOUT</varname>. Alternatively, you can specify an integer,
        in seconds, instead of <literal>yes</literal>. For help calculating an appropriate value,
        see <command>man sbd</command>, section <literal>Configuration via environment</literal>.
      </para>
      <important>
       <title><literal>SBD_WATCHDOG_TIMEOUT</literal> for diskless SBD and &qdevice;</title>
       <para>
        If you use &qdevice; with diskless SBD, the <literal>SBD_WATCHDOG_TIMEOUT</literal>
        value must be greater than &qdevice;'s <literal>sync_timeout</literal> value,
        or SBD will time out and fail to start.
       </para>
       <para>
        The default value for <literal>sync_timeout</literal> is 30 seconds.
        Therefore, set <literal>SBD_WATCHDOG_TIMEOUT</literal> to a greater value,
        such as <literal>35</literal>.
       </para>
      </important>
    </step>
    <step>
     <para>On each node, enable the SBD service:</para>
     <screen>&prompt.root;<command>systemctl enable sbd</command></screen>
     <para>SBD starts together with the Corosync service whenever the
      cluster services are started.</para>
    </step>
    <step>
     <para>Restart the cluster services on all nodes at once by using the <option>--all</option>
     option:</para>
    <screen>&prompt.root;<command>crm cluster restart --all</command></screen>
    <para> This automatically triggers the start of the SBD daemon. </para>
    <important>
     <title>Restart cluster services for SBD changes</title>
     <para>
       If any SBD metadata changes, you must restart the cluster services again. To keep critical
       cluster resources running during the restart, consider putting the cluster into maintenance
       mode first. For more information, see <xref linkend="cha-ha-maintenance"/>.
     </para>
   </important>
    </step>
    <step>
      <para>
       Check if the parameter <parameter>have-watchdog=true</parameter> has
       been automatically set:
      </para>
      <screen>&prompt.root;<command>crm configure show | grep have-watchdog</command>
         have-watchdog=true</screen>
    </step>
    <step xml:id="pro-ha-sbd-diskless-cib">
     <para>Run <command>crm configure</command> and set the following cluster
      properties on the &crmshell;:</para>
<screen>&prompt.crm.conf;<command>property stonith-enabled="true"</command><co xml:id="co-ha-sbd-stonith-enabled"/>
&prompt.crm.conf;<command>property stonith-watchdog-timeout=10</command><co xml:id="co-ha-sbd-diskless-watchdog-timeout"/>
&prompt.crm.conf;<command>property stonith-timeout=15</command><co xml:id="co-ha-sbd-diskless-stonith-timeout"/></screen>
    <calloutlist>
     <callout arearefs="co-ha-sbd-stonith-enabled">
      <para>
       This is the default configuration, because clusters without &stonith; are not supported.
       But in case &stonith; has been deactivated for testing purposes,
       make sure this parameter is set to <literal>true</literal> again.</para>
     </callout>
     <callout arearefs="co-ha-sbd-diskless-watchdog-timeout">
      <para>For diskless SBD, this parameter must not equal zero.
       It defines after how long it is assumed that the fencing target has already
       self-fenced. Use the following formula to calculate this timeout:
      </para>
      <screen>stonith-watchdog-timeout &gt;= (SBD_WATCHDOG_TIMEOUT * 2)</screen>
     </callout>
     <callout arearefs="co-ha-sbd-diskless-stonith-timeout">
       <para>
         This parameter must allow sufficient time for fencing to complete.
         For diskless SBD, use the following formula to calculate this timeout:
       </para>
       <screen>stonith-timeout &gt;= stonith-watchdog-timeout + 20%</screen>
       <important>
        <title>Diskless SBD timeouts</title>
        <para>
          With diskless SBD, if the <literal>stonith-timeout</literal> value is smaller than the
          <literal>stonith-watchdog-timeout</literal> value, failed nodes can become stuck
          in an <literal>UNCLEAN</literal> state and block failover of active resources.
        </para>
       </important>
     </callout>
    </calloutlist>
   </step>
  <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     Submit your changes with <command>commit</command> and leave the crm live
     configuration with <command>quit</command>.
    </para>
   </step>
  </procedure>
  </sect1>

  <sect1 xml:id="sec-ha-storage-protect-test">
   <title>Testing SBD and fencing</title>
   <para>To test whether SBD works as expected for node fencing purposes, use one or all
    of the following methods:
   </para>
  <variablelist>
   <varlistentry>
    <term>Manually triggering fencing of a node</term>
    <listitem>
     <para>To trigger a fencing action for node <replaceable>NODENAME</replaceable>:</para>
 <screen>&prompt.root;<command>crm node fence <replaceable>NODENAME</replaceable></command></screen>
     <para>Check if the node is fenced and if the other nodes consider the node as fenced
      after the <parameter>stonith-watchdog-timeout</parameter>.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Simulating an SBD failure</term>
    <listitem>
     <procedure>
      <step>
       <para>Identify the process ID of the SBD inquisitor:</para>
       <screen>&prompt.root;<command>systemctl status sbd</command>
● sbd.service - Shared-storage based fencing daemon

   Loaded: loaded (/usr/lib/systemd/system/sbd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-04-17 15:24:51 CEST; 6 days ago
     Docs: man:sbd(8)
  Process: 1844 ExecStart=/usr/sbin/sbd $SBD_OPTS -p /var/run/sbd.pid watch (code=exited, status=0/SUCCESS)
 Main PID: 1859 (sbd)
    Tasks: 4 (limit: 4915)
   CGroup: /system.slice/sbd.service
           ├─<emphasis role="strong">1859 sbd: inquisitor</emphasis>
[...]</screen>
      </step>
      <step>
       <para>Simulate an SBD failure by terminating the SBD inquisitor process.
       In our example, the process ID of the SBD inquisitor is
         <literal>1859</literal>:</para>
       <screen>&prompt.root;<command>kill -9 1859</command></screen>
       <para>
        The node proactively self-fences. The other nodes notice the loss of
        the node and consider it has self-fenced after the
        <parameter>stonith-watchdog-timeout</parameter>.
       </para>
      </step>
     </procedure>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Triggering fencing through a monitor operation failure</term>
    <listitem>
     <para>With a normal configuration, a failure of a resource <emphasis>stop operation</emphasis>
      triggers fencing. To trigger fencing manually, you can produce a failure
      of a resource stop operation. Alternatively, you can temporarily change
      the configuration of a resource <emphasis>monitor operation</emphasis>
      and produce a monitor failure as described below:</para>
     <procedure>
      <step>
       <para>Configure an <literal>on-fail=fence</literal> property for a resource monitor
        operation:</para>
       <screen>op monitor interval=10 on-fail=fence</screen>
      </step>
      <step>
       <para>Let the monitoring operation fail (for example, by terminating the respective
        daemon, if the resource relates to a service).</para>
       <para>This failure triggers a fencing action.</para>
      </step>
     </procedure>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-rsc-fencing">
  <title>Additional mechanisms for storage protection</title>
  <remark>toms 2018-04-20: this can be improved...</remark>
  <para>Apart from node fencing via &stonith; there are other methods to achieve
    storage protection at a resource level. For example, SCSI-3 and SCSI-4 use
    persistent reservations whereas <literal>sfex</literal> provides a locking
    mechanism. Both methods are explained in the following subsections.
  </para>
  <sect2 xml:id="sec-ha-storage-protect-sgpersist">
   <title>Configuring an sg_persist resource</title>
   <para>
    The SCSI specifications 3 and 4 define <emphasis>persistent reservations</emphasis>.
    These are SCSI protocol features and can be used for I/O fencing and failover.
    This feature is implemented in the <command>sg_persist</command> Linux
    command.
   </para>
   <note>
    <title>SCSI disk compatibility</title>
    <para> Any backing disks for <literal>sg_persist</literal> must be SCSI
     disk compatible. <literal>sg_persist</literal> only works for devices like
     SCSI disks or iSCSI LUNs.
     <remark>toms 2018-04-20: What about FCoE, FC, iSER, SRP, Serial Attached SCSI (SAR)?</remark>
     Do <emphasis>not</emphasis> use it for IDE, SATA, or any block devices
     which do not support the SCSI protocol. </para>
   </note>
   <para>Before you proceed, check if your disk supports
    persistent reservations. Use the following command (replace
     <replaceable>DEVICE_ID</replaceable> with your device name):</para>
    <screen>&prompt.root;<command>sg_persist -n --in --read-reservation -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
   <para>The result shows whether your disk supports persistent reservations:</para>
    <itemizedlist>
     <listitem>
      <para>Supported disk:</para>
      <screen>PR generation=0x0, there is NO reservation held</screen>
     </listitem>
     <listitem>
      <para>Unsupported disk:</para>
      <screen>PR in (Read reservation): command not supported
Illegal request, Invalid opcode</screen>
     </listitem>
    </itemizedlist>

   <remark>toms 2018-04-20: Do we need to prepare anything else with sg_persists?</remark>
   <para>If you get an error message (like the one above), replace the old
    disk with an SCSI compatible disk. Otherwise proceed as follows:</para>
   <procedure>
    <step>
     <para>
      Create the primitive resource <literal>sg_persist</literal>,
      using a stable device name for the disk:
    </para>
     <screen>&prompt.root;<command>crm configure</command>
&prompt.crm.conf;<command>primitive sg sg_persist \
    params devs="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>" reservation_type=3 \
    op monitor interval=60 timeout=60</command></screen>
    </step>
    <step>
     <para> Create a promotable clone of the <literal>sg_persist</literal> primitive:
     </para>
     <screen>&prompt.crm.conf;<command>clone clone-sg sg \
    meta promotable=true promoted-max=1 notify=true</command></screen>
    </step>
    <step>
     <para>Test the setup. When the resource is promoted, you can
      mount and write to the disk partitions on the cluster node where
      the primary instance is running, but you cannot write on the cluster node
      where the secondary instance is running.</para>
    </step>
    <step>
     <para> Add a file system primitive for Ext4, using a stable device name for
     the disk partition: </para>
     <screen>&prompt.crm.conf;<command>primitive ext4 Filesystem \
    params device="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>" directory="/mnt/ext4" fstype=ext4 \
    op monitor timeout=40s interval=20s</command></screen>
    </step>
    <step>
     <para> Add the following order relationship plus a collocation between the
      <literal>sg_persist</literal> clone and the file system resource: </para>
     <screen>&prompt.crm.conf;<command>order o-clone-sg-before-ext4 Mandatory: clone-sg:promote ext4:start</command>
&prompt.crm.conf;<command>colocation col-ext4-with-sg-persist inf: ext4 clone-sg:Promoted</command></screen>
    </step>
    <step>
     <para> Check all your changes with the <command>show changed</command> command.
     </para>
    </step>
    <step>
     <para> Commit your changes. </para>
    </step>
   </procedure>
   <para>For more information, refer to the <command>sg_persist</command> man
    page.</para>
  </sect2>

  <sect2 xml:id="sec-ha-storage-protect-exstoract">
   <title>Ensuring exclusive storage activation with <literal>sfex</literal></title>
    <para>
     <remark>taroth 2018-04-26: ToDo - for next release, revise this section, too,
     and flatten its structure</remark>
    This section introduces <literal>sfex</literal>, an additional low-level
    mechanism to lock access to shared storage exclusively to one node. Note
    that sfex does not replace &stonith;. As sfex requires shared
    storage, it is recommended that the SBD node fencing mechanism described
    above is used on another partition of the storage.
   </para>

   <para>
    By design, sfex cannot be used with workloads that require concurrency
    (such as &ocfs;). It serves as a layer of protection for classic failover
    style workloads. This is similar to an SCSI-2 reservation in effect, but
    more general.
   </para>

   <sect3 xml:id="sec-ha-storage-protect-exstoract-description">
    <title>Overview</title>
    <para>
     In a shared storage environment, a small partition of the storage is set
     aside for storing one or more locks.
    </para>
    <para>
     Before acquiring protected resources, the node must first acquire the
     protecting lock. The ordering is enforced by Pacemaker. The sfex
     component ensures that even if Pacemaker is subject to a split-brain
     situation, the lock is never granted more than once.
    </para>
    <para>
     These locks must also be refreshed periodically, so that a node's death
     does not permanently block the lock and other nodes can proceed.
    </para>
   </sect3>

   <sect3 xml:id="sec-ha-storage-protect-exstoract-requirements">
    <title>Setup</title>
    <para>
     In the following, learn how to create a shared partition for use with
     sfex and how to configure a resource for the sfex lock in the CIB. A
     single sfex partition can hold any number of locks, and needs 1 KB
     of storage space allocated per lock.
     By default, <command>sfex_init</command> creates one lock on the partition.
    </para>
    <important>
     <title>Requirements</title>
     <itemizedlist>
      <listitem>
       <para>
        The shared partition for sfex should be on the same logical unit as
        the data you want to protect.
       </para>
      </listitem>
      <listitem>
       <para>
        The shared sfex partition must not use host-based RAID or DRBD.
       </para>
      </listitem>
      <listitem>
       <para>
        Using an LVM logical volume is possible.
       </para>
      </listitem>
     </itemizedlist>
    </important>
    <procedure>
     <title>Creating an sfex partition</title>
     <step>
      <para>
       Create a shared partition for use with sfex. Note the name of this
       partition and use it as a substitute for
       <filename>/dev/sfex</filename> below.
      </para>
     </step>
     <step>
      <para>
       Create the sfex metadata with the following command:
      </para>
      <screen>&prompt.root;<command>sfex_init -n 1 /dev/sfex</command></screen>
     </step>
     <step>
      <para>
       Verify that the metadata has been created correctly:
      </para>
      <screen>&prompt.root;<command>sfex_stat -i 1 /dev/sfex ; echo $?</command></screen>
      <para>
       This should return <literal>2</literal>, since the lock is not
       currently held.
      </para>
     </step>
    </procedure>
    <procedure>
     <title>Configuring a resource for the sfex lock</title>
     <step>
      <para>
       The sfex lock is represented via a resource in the CIB, configured as
       follows:
      </para>
      <screen>&prompt.crm.conf;<command>primitive sfex_1 ocf:heartbeat:sfex \
      params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
      op monitor interval="10s" timeout="30s" on-fail="fence"</command></screen>
     </step>
     <step>
      <para>
       To protect resources via an sfex lock, create mandatory order and
       placement constraints between the resources to protect the sfex resource. If
       the resource to be protected has the ID
       <literal>filesystem1</literal>:
      </para>
      <screen>&prompt.crm.conf;<command>order order-sfex-1 Mandatory: sfex_1 filesystem1</command>
&prompt.crm.conf;<command>colocation col-sfex-1 inf: filesystem1 sfex_1</command></screen>
     </step>
     <step>
      <para>
       If using group syntax, add the sfex resource as the first resource to
       the group:
      </para>
      <screen>&prompt.crm.conf;<command>group LAMP sfex_1 filesystem1 apache ipaddr</command></screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-sbd-redeploy">
  <title>Changing SBD configuration</title>
  <para>
    You might need to change the cluster's SBD configuration for various reasons. For example:
  </para>
  <itemizedlist>
   <listitem>
     <para>
       Changing disk-based SBD to diskless SBD
     </para>
   </listitem>
   <listitem>
     <para>
       Changing diskless SBD to disk-based SBD
     </para>
   </listitem>
   <listitem>
     <para>
       Replacing an SBD device with a new device
     </para>
   </listitem>
   <listitem>
     <para>
       Changing timeout values and other settings
     </para>
   </listitem>
  </itemizedlist>
  <para>
    You can use &crmsh; to change the SBD configuration. This method uses
    &crmsh;'s default settings, including timeout values.
  </para>
  <itemizedlist>
    <listitem>
      <para>
        <xref linkend="pro-ha-storage-protect-sbd-redeploy-disk-based-crmsh"/>
      </para>
    </listitem>
    <listitem>
      <para>
        <xref linkend="pro-ha-storage-protect-sbd-redeploy-diskless-crmsh"/>
      </para>
    </listitem>
  </itemizedlist>
  <para>
    If you need to change any settings,
    or if you use custom settings and need to retain them when replacing a device, you must
    manually edit the SBD configuration.
  </para>
  <itemizedlist>
    <listitem>
      <para>
        <xref linkend="pro-ha-storage-protect-sbd-redeploy-disk-based-manual"/>
      </para>
    </listitem>
    <listitem>
      <para>
        <xref linkend="pro-ha-storage-protect-sbd-redeploy-diskless-manual"/>
      </para>
    </listitem>
  </itemizedlist>

  <procedure xml:id="pro-ha-storage-protect-sbd-redeploy-disk-based-crmsh">
    <title>Redeploying disk-based SBD with &crmsh;</title>
    <para>
      Use this procedure to change diskless SBD to disk-based SBD, or to replace an existing
      SBD device with a new device.
    </para>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen>&prompt.root;<command>crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources, so the services managed
        by the resources can continue to run even if the cluster services stop.
      </para>
    </step>
    <step>
      <para>
        Configure the new device:
      </para>
<screen>&prompt.root;<command>crm -F cluster init sbd -s /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
      <para>
        The <option>-F</option> option allows &crmsh; to reconfigure SBD even if the SBD service is
        already running.
      </para>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen>&prompt.root;<command>crm status</command></screen>
      <para>
        Initially, the nodes have a status of <literal>UNCLEAN (offline)</literal>,
        but after a short time they change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        Check the SBD configuration. First, check the device metadata:
      </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> dump</command>
</screen>
      <para>
        Then check that all nodes in the cluster are assigned to a slot in the device:
      </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> list</command></screen>
    </step>
    <step>
      <para>
        Check the status of the SBD service:
      </para>
<screen>&prompt.root;<command>systemctl status sbd</command></screen>
      <para>
        If you changed diskless SBD to disk-based SBD, check that the following section
        includes a device ID:
      </para>
<screen>CGroup: /system/.slice/sbd.service
        |&mdash;23314 "sbd: inquisitor"
        |&mdash;23315 "sbd: watcher: /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> - slot: 0 - uuid: <replaceable>DEVICE_UUID</replaceable>"
        |&mdash;23316 "sbd: watcher: Pacemaker"
        |&mdash;23317 "sbd: watcher: Cluster"</screen>
    </step>
    <step>
      <para>
        When the nodes are back online, move the cluster out of maintenance mode and back
        into normal operation:
      </para>
<screen>&prompt.root;<command>crm maintenance off</command></screen>
    </step>
  </procedure>

  <procedure xml:id="pro-ha-storage-protect-sbd-redeploy-disk-based-manual">
    <title>Redeploying disk-based SBD manually</title>
    <para>
      Use this procedure to change diskless SBD to disk-based SBD, to replace an existing
      SBD device with a new device, or to change the timeout settings for disk-based SBD.
    </para>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen>&prompt.root;<command>crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources, so the services managed
        by the resources can continue to run even when you stop the cluster services.
      </para>
    </step>
    <step>
      <para>
        Stop the cluster services, including SBD, on all nodes:
      </para>
<screen>&prompt.root;<command>crm cluster stop --all</command></screen>
    </step>
    <step>
      <para>
        Reinitialize the device metadata, specifying the new device ID and timeouts as required:
      </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> -1 <replaceable>VALUE</replaceable> -4 <replaceable>VALUE</replaceable> create</command></screen>
      <para>
        The <option>-1</option> option specifies the <literal>watchdog</literal> timeout.
      </para>
      <para>
        The <option>-4</option> option specifies the <literal>msgwait</literal> timeout. This must
        be at least double the <literal>watchdog</literal> timeout.
      </para>
      <para>
        For more information, see <xref linkend="pro-ha-storage-protect-sbd-create"/>.
      </para>
    </step>
    <step>
      <para>
        Open the file <filename>/etc/sysconfig/sbd</filename>.
      </para>
    </step>
    <step>
      <para>
        If you are changing diskless SBD to disk-based SBD, add the following line and
        specify the device ID. If you are replacing an SBD device, change the value of
        this line to the new device ID:
      </para>
<screen>SBD_DEVICE="/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>"</screen>
    </step>
    <step>
      <para>
        Adjust the other settings as required. For more information,
        see <xref linkend="pro-ha-storage-protect-sbd-config"/>.
      </para>
    </step>
    <step>
      <para>
        Copy the configuration file to all nodes:
      </para>
<screen>&prompt.root;<command>csync2 -xv</command></screen>
    </step>
    <step>
      <para>
        Start the cluster services on all nodes:
      </para>
<screen>&prompt.root;<command>crm cluster start --all</command></screen>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen>&prompt.root;<command>crm status</command></screen>
      <para>
        Initially, the nodes have a status of <literal>UNCLEAN (offline)</literal>,
        but after a short time they change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        Check the SBD configuration. First, check the device metadata:
      </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> dump</command>
</screen>
      <para>
        Then check that all nodes in the cluster are assigned to a slot in the device:
      </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> list</command></screen>
    </step>
    <step>
      <para>
        Check the status of the SBD service:
      </para>
<screen>&prompt.root;<command>systemctl status sbd</command></screen>
      <para>
        If you changed diskless SBD to disk-based SBD, check that the following section
        includes a device ID:
      </para>
<screen>CGroup: /system/.slice/sbd.service
        |&mdash;23314 "sbd: inquisitor"
        |&mdash;23315 "sbd: watcher: /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> - slot: 0 - uuid: <replaceable>DEVICE_UUID</replaceable>"
        |&mdash;23316 "sbd: watcher: Pacemaker"
        |&mdash;23317 "sbd: watcher: Cluster"</screen>
    </step>
    <step>
      <para>
        If you changed any timeouts, or if you changed diskless SBD to disk-based SBD,
        you might also need to change the CIB properties <literal>stonith-timeout</literal> and
        <literal>stonith-watchdog-timeout</literal>. For disk-based SBD,
        <literal>stonith-watchdog-timeout</literal> should be <literal>0</literal> or defaulted.
        For more information, see <xref linkend="sec-ha-storage-protect-watchdog-timings"/>.
      </para>
      <para>
        To check the current values, run the following command:
      </para>
<screen>&prompt.root;<command>crm configure show</command></screen>
      <para>
        If you need to change the values, use the following commands:
      </para>
<screen>&prompt.root;<command>crm configure property stonith-watchdog-timeout=0</command>
&prompt.root;<command>crm configure property stonith-timeout=<replaceable>VALUE</replaceable></command></screen>
    </step>
    <step>
      <para>
        If you changed diskless SBD to disk-based SBD, you must configure a &stonith; resource
        for SBD. For example:
      </para>
<screen>&prompt.root;<command>crm configure primitive stonith-sbd stonith:external/sbd</command></screen>
      <para>
        For more information, see <xref linkend="st-ha-storage-protect-fencing-static-random"/> in
        <xref linkend="pro-ha-storage-protect-fencing"/>.
      </para>
    </step>
    <step>
      <para>
        When the nodes are back online, move the cluster out of maintenance mode and back
        into normal operation:
      </para>
<screen>&prompt.root;<command>crm maintenance off</command></screen>
    </step>
  </procedure>

  <procedure xml:id="pro-ha-storage-protect-sbd-redeploy-diskless-crmsh">
    <title>Redeploying diskless SBD with &crmsh;</title>
    <para>
      Use this procedure to change disk-based SBD to diskless SBD.
    </para>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen>&prompt.root;<command>crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources, so the services managed
        by the resources can continue to run even if the cluster services stop.
      </para>
    </step>
    <step>
      <para>
        Configure diskless SBD:
          </para>
<screen>&prompt.root;<command>crm -F cluster init sbd -S</command></screen>
      <para>
        The <option>-F</option> option allows &crmsh; to reconfigure SBD even if the SBD service is
        already running.
      </para>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen>&prompt.root;<command>crm status</command></screen>
      <para>
        Initially, the nodes have a status of <literal>UNCLEAN (offline)</literal>,
        but after a short time they change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        Check the status of the SBD service:
      </para>
<screen>&prompt.root;<command>systemctl status sbd</command></screen>
      <para>
        The following section should <emphasis>not</emphasis> include a device ID:
      </para>
<screen>CGroup: /system/.slice/sbd.service
        |&mdash;23314 "sbd: inquisitor"
        |&mdash;23315 "sbd: watcher: Pacemaker"
        |&mdash;23316 "sbd: watcher: Cluster"</screen>
    </step>
    <step>
      <para>
        When the nodes are back online, move the cluster out of maintenance mode and back
        into normal operation:
      </para>
<screen>&prompt.root;<command>crm maintenance off</command></screen>
    </step>
  </procedure>

  <procedure xml:id="pro-ha-storage-protect-sbd-redeploy-diskless-manual">
    <title>Redeploying diskless SBD manually</title>
    <para>
      Use this procedure to change disk-based SBD to diskless SBD, or to change
      the timeout values for diskless SBD.
    </para>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen>&prompt.root;<command>crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources, so the services managed
        by the resources can continue to run even when you stop the cluster services.
      </para>
    </step>
    <step>
      <para>
        Stop the cluster services, including SBD, on all nodes:
      </para>
<screen>&prompt.root;<command>crm cluster stop --all</command></screen>
    </step>
    <step>
      <para>
        Open the file <filename>/etc/sysconfig/sbd</filename>.
      </para>
    </step>
    <step>
      <para>
        If you are changing disk-based SBD to diskless SBD, remove or comment out the
        <literal>SBD_DEVICE</literal> entry.
      </para>
    </step>
    <step>
      <para>
        Adjust the other settings as required. For more information,
        see <xref linkend="sec-ha-storage-protect-diskless-sbd"/>.
      </para>
    </step>
    <step>
      <para>
        Copy the configuration file to all nodes:
      </para>
<screen>&prompt.root;<command>csync2 -xv</command></screen>
    </step>
    <step>
      <para>
        Start the cluster services on all nodes:
      </para>
<screen>&prompt.root;<command>crm cluster start --all</command></screen>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen>&prompt.root;<command>crm status</command></screen>
      <para>
        Initially, the nodes have a status of <literal>UNCLEAN (offline)</literal>,
        but after a short time they change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        Check the status of the SBD service:
      </para>
<screen>&prompt.root;<command>systemctl status sbd</command></screen>
      <para>
        If you changed disk-based SBD to diskless SBD, check that the following section does
        <emphasis>not</emphasis> include a device ID:
      </para>
<screen>CGroup: /system/.slice/sbd.service
        |&mdash;23314 "sbd: inquisitor"
        |&mdash;23315 "sbd: watcher: Pacemaker"
        |&mdash;23316 "sbd: watcher: Cluster"</screen>
    </step>
    <step>
      <para>
        If you changed any timeouts, or if you changed disk-based SBD to diskless SBD, you might
        also need to change the CIB properties <literal>stonith-timeout</literal> and
        <literal>stonith-watchdog-timeout</literal>. For more information, see
      <xref linkend="pro-ha-sbd-diskless-cib"/> of <xref linkend="pro-ha-storage-protect-confdiskless"/>.
      </para>
      <para>
        To check the current values, run the following command:
      </para>
<screen>&prompt.root;<command>crm configure show</command></screen>
      <para>
        If you need to change the values, use the following commands:
      </para>
<screen>&prompt.root;<command>crm configure property stonith-watchdog-timeout=<replaceable>VALUE</replaceable></command>
&prompt.root;<command>crm configure property stonith-timeout=<replaceable>VALUE</replaceable></command></screen>
    </step>
    <step>
      <para>
        When the nodes are back online, move the cluster out of maintenance mode and back
        into normal operation:
      </para>
<screen>&prompt.root;<command>crm maintenance off</command></screen>
    </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-moreinfo">
  <title>For more information</title>
   <para>
    For more details, see <command>man sbd</command>.
   </para>
 </sect1>
</chapter>
