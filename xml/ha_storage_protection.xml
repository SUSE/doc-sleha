<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="cha.ha.storage.protect"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Storage Protection and SBD</title>
 <info>
  <abstract>
   <para>
    SBD (&stonith; Block Device) provides a node fencing mechanism for
    Pacemaker-based clusters through the exchange of messages via shared block
    storage such as a SAN, iSCSI, or FCoE for example. This isolates the fencing
    mechanism from changes in firmware version or dependencies on specific
    firmware controllers. SBD can be used as a &stonith; mechanism in all
    configurations that have reliable shared storage, but it can also used in
    <emphasis>diskless</emphasis> mode, that means, without any shored storage.
   </para>
   <para>
    The <package>ha-cluster-bootstrap</package> scripts provide a quick way to
    set up a cluster with the option of using SBD as fencing mechanism. The
    bootstrap scripts automatically execute all required steps to set up SBD as
    fencing mechanism. For automatic setup, see the
    <xref linkend="art.sleha.install.quick"/>.
   </para>
   <para>
    However, manually setting up SBD provides you with more options regarding
    the individual settings. This chapter explains the concepts behind SBD and
    guides you through configuring the components that SBD needs to protect
    your cluster from potential data corruption in case of a split brain scenario.
   </para>
  </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec.ha.storage.protect.overview">
      <title>Conceptual Overview</title>
      <para>SBD expands to <emphasis>Storage-Based Death</emphasis> or
        <emphasis>STONITH Block Device</emphasis>.
      <remark>taroth 2018-04-18: in ha_glossary.xml, we even introduce a different
      term: '&stonith; by disk' - we should consolidate this. I would suggest to
      use the term that is mentioned in 'man sbd': STONITH Block Device</remark></para>
      <para>
        The highest priority of the &ha; cluster stack is to protect the integrity
        of data. This is achieved by preventing uncoordinated concurrent access
        to data storage. The cluster stack takes care of this using several
        control mechanisms.
      </para>
      <para>
        However, network partitioning or software malfunction could potentially
        cause scenarios where several DCs are elected in a cluster. If this
        so-called split brain scenarios were allowed to unfold, data corruption
        might occur.
      </para>
      <para>
        Server fencing via &stonith; is the primary mechanism to prevent this.
        Using SBD as server fencing mechanism is one way of shutting down nodes
        without external power off device in case of a split brain scenario.
      </para>
      <para>
        Additional mechanisms for storage protection are LVM2 exclusive activation
        or OCFS2 file locking support.
        <remark>taroth 2018-04-20: FIXME add link!</remark>
        They protect your system against administrative or application faults.
        Combined appropriately for your setup, they can reliably prevent split
        brain scenarios from causing harm.
      </para>

  <variablelist>
   <title>SBD Components and Mechanisms</title>
   <varlistentry>
    <term>SBD Partition</term>
    <listitem>
     <para> In an environment where all nodes have access to shared storage, a
      small partition of the device is formatted for use with SBD. The size of
      the partition depends on the block size of the used disk (for example,
      1&nbsp;MB for standard SCSI disks with 512&nbsp;Byte block size or
      4&nbsp;MB for DASD disks with 4&nbsp;kB block size). The initialization
      process creates a message layout on the device with slots for up to 255
      nodes.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD Daemon</term>
    <listitem>
     <para> After the respective SBD daemon is configured, it is brought online
      on each node before the rest of the cluster stack is started. It is
      terminated after all other cluster components have been shut down, thus
      ensuring that cluster resources are never activated without SBD
      supervision. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Messages</term>
    <listitem>
     <para>
      The daemon automatically allocates one of the message slots on the
      partition to itself, and constantly monitors it for messages addressed
      to itself. Upon receipt of a message, the daemon immediately complies
      with the request, such as initiating a power-off or reboot cycle for
      fencing.
     </para>
     <para>
      The daemon constantly monitors connectivity to the storage device, and
      terminates itself in case the partition becomes unreachable. This
      guarantees that it is not disconnected from fencing messages. If the
      cluster data resides on the same logical unit in a different partition,
      this is not an additional point of failure: The work-load will terminate
      anyway if the storage connectivity has been lost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Watchdog</term>
     <listitem>
      <para>
      Increased protection is used by combining SBD with watchdog.
      Whenever SBD is used, a correctly working watchdog is crucial.
      Modern systems support a <emphasis>hardware watchdog</emphasis>
      that needs to be <quote>tickled</quote> or <quote>fed</quote> by a
      software component. The software component (in this case, the SBD daemon)
      <quote>feeds</quote> the watchdog, by regularly writing a service pulse
      to the watchdog. If the daemon stops feeding the watchdog, the hardware
      will enforce a system restart. This protects against failures of the SBD
      process itself, such as dying, or becoming stuck on an IO error.
     </para>
     </listitem>
   </varlistentry>
  </variablelist>
  <para>
   If Pacemaker integration is activated, SBD will not self-fence if device
   majority is lost. For example, your cluster contains 3 nodes: A, B, and
   C. Because of a network split, A can only see itself while B and C can
   still communicate. In this case, there are two cluster partitions, one
   with quorum because of being the majority (B, C), and one without (A).
   If this happens while the majority of fencing devices are unreachable,
   node A would immediately commit suicide, but the nodes B and C would
   continue to run.
   </para>
  </sect1>

 <sect1 xml:id="sec.ha.storage.protect.steps">
 <title>Steps to Set Up Storage-based Protection</title>
 <para>
  The following steps are necessary to manually set up storage-based protection.
  They must be executed as &rootuser;. Before you start, check the <xref
  linkend="sec.ha.storage.protect.req" xrefstyle="sec.ha.storage.protect.req"/>.
  <remark>taroth 2018-04-19: ToDo - modify and re-add step overview after the chapter
  has reached its final structure!</remark>
 </para>
  <!--<procedure>
   <step>
    <para>
     <xref linkend="sec.ha.storage.protect.watchdog" xrefstyle="select:title"/>.
     For clusters in production environments we recommend to use a watchdog
     kernel module that matches your hardware. However, if no watchdog matches
     your hardware, <literal>softdog</literal> can be used as kernel watchdog module.
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.create" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.daemon" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.test" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.fencing" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>-->
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.req">
  <title>Requirements</title>
   <itemizedlist>
   <listitem>
    <para>You can use up to three SBD devices for storage-based fencing.
     When using one to three devices, the shared storage must be accessible from all nodes.</para>
   </listitem>
   <listitem>
    <para>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
     </para>
   </listitem>
   <listitem>
    <para>The shared storage can be connected via Fibre Channel (FC),
     Fibre Channel over Ethernet (FCoE), or even iSCSI. </para>
   </listitem>
   <listitem>
    <para> The shared storage segment <emphasis>must not</emphasis>
     use host-based RAID, LVM2, nor DRBD*. DRBD can be splitted, which is
     problematic for SBD, as there cannot be two states in SBD.
     Cluster multi-device (Cluster MD) cannot be used for SBD.
    </para>
   </listitem>
   <listitem>
    <para> However, using storage-based RAID and multipathing is
     recommended for increased reliability. </para>
   </listitem>
   <listitem>
    <para>An SBD device can be shared between different clusters, as
     long as no more than 255 nodes share the device. </para>
   </listitem>
   <listitem>
    <para>For clusters with more than two nodes, you can also use SBD in
    <emphasis>diskless</emphasis> mode.<remark>taroth 2018-04-18: add link to
    this (new) section after it has been added</remark>
   </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.fencing.number">
  <title>Number of SBD Devices</title>
  <!--fate#309375-->
  <para> SBD supports the use of up to three devices: </para>
  <variablelist>
   <varlistentry>
    <term>Diskless</term>
    <listitem>
     <para>&sbd-diskless;</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>One Device</term>
    <listitem>
     <para>
      The most simple implementation. It is appropriate for clusters where
      all of your data is on the same shared storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Two Devices</term>
    <listitem>
     <para>
      This configuration is primarily useful for environments that use
      host-based mirroring but where no third storage device is available.
      SBD will not terminate itself if it loses access to one mirror leg,
      allowing the cluster to continue. However, since SBD does not have
      enough knowledge to detect an asymmetric split of the storage, it
      will not fence the other side while only one mirror leg is available.
      Thus, it cannot automatically tolerate a second failure while one of
      the storage arrays is down.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Three Devices</term>
    <listitem>
     <para>
      The most reliable configuration. It is resilient against outages of
      one device&mdash;be it because of failures or maintenance. SBD
      will only terminate itself if more than one device is lost. If at least
      two devices are still accessible, fencing messages can be successfully be
      transmitted.
     </para>
     <para>
      This configuration is suitable for more complex scenarios where
      storage is not restricted to a single array. Host-based mirroring
      solutions can have one SBD per mirror leg (not mirrored itself), and
      an additional tie-breaker on iSCSI.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.watchdog.timings">
   <title>Calculation of Timeouts</title>
    <para>
      When using SBD as fencing mechanism, it is vital to consider the timeouts
      of all components that need to work together, because they depend on each
      other.
    </para>
    <variablelist>
     <varlistentry>
      <term>Watchdog Timeout</term>
      <listitem>
       <para>
        This timeout is set during initialization of the SBD device. It depends
        mostly on your storage latency. The majority of devices must be successfully
        read within this time. Otherwise, the node will self-fence.
       </para>
       <note>
        <title>Multipath or iSCSI Setup</title>
          <para>
          If your SBD device(s) reside on a multipath setup or iSCSI, the timeout
          should be  set to the time required to detect a path failure and switch
          to the next path.
          </para>
          <para>
           This also means that in <filename>/etc/multipath.conf</filename> the
           value of  <literal>max_polling_interval</literal> must be less than
           <literal>watchdog</literal> timeout.
         </para>
       </note>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>msgwait</literal> Timeout</term>
      <listitem>
       <para>
        This timeout is set during initialization of the SBD device. It defines
        the time after which a message written to a node's slot on the SBD device
        is considered delivered. The timeout should be long enough for the node to
        detect that it needs to self-fence.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>stonith-timeout</literal> in the CIB</term>
      <listitem>
       <para>
        This timeout is set in the CIB as global cluster property. It defines
        how long to wait for the &stonith; action (reboot, on, off) to complete.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>stonith-watchdog-timeout</literal> in the CIB</term>
      <listitem>
       <para>
        This timeout is set in the CIB as global cluster property. If not set
        explicitly, it defaults to <literal>0</literal>, which is appropriate for
        using SBD with one to three devices. For use of SBD in diskless mode, see <xref
        linkend="pro.ha.storage.protect.confdiskless"/> for more details.</para>
      </listitem>
     </varlistentry>
    </variablelist>
  <para>
   If you change the watchdog timeout, you need to adjust the other two timeouts
   as well. The following <quote>formula</quote> expresses the relationship
   between these three values:
  </para>
   <example xml:id="ex.ha.storage.protect.sbd-timings">
    <title>Formula for Timeout Calculation</title>
    <screen>Timeout (msgwait) = (Timeout (watchdog) * 2)
stonith-timeout = Timeout (msgwait) + 20%</screen>
   </example>
   <para>
    For example, if you set the watchdog timeout to <literal>120</literal>,
    set the <literal>msgwait</literal> timeout to <literal>240</literal> and the
    <literal>stonith-timeout</literal> to <literal>288</literal>.
   </para>
    <para>
     If you use the <package>ha-cluster-bootstrap</package> scripts to set up a
     cluster and to initialize the SBD device, the relationship between these
     timeouts is automatically considered.
    </para>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.watchdog">
  <title>Setting Up the Watchdog</title>
  <para> &productname; ships with several different kernel modules that provide
   hardware-specific watchdog drivers. For a list of the most commonly used
   ones, see <xref
   linkend="tab.ha.storage.protect.watchdog.drivers" xrefstyle="select:title
   nopage"/>.
 </para>
 <para>
  For clusters in production environments we recommend to use a hardware-specific
  watchdog driver. However, if no watchdog matches your hardware,
  <systemitem class="resource">softdog</systemitem> can be used as kernel
  watchdog module.
 </para>
 <para>
   The &hasi; uses the SBD daemon as the software component that <quote>feeds</quote>
   the watchdog.</para>

  <sect2 xml:id="sec.ha.storage.protect.hw-watchdog">
   <title>Using a Hardware Watchdog</title>

   <para>Finding the right watchdog kernel module for a given system is not
    trivial. Automatic probing fails very often. As a result, lots of modules
    are already loaded before the right one gets a chance.</para>

  <para>
   <xref linkend="tab.ha.storage.protect.watchdog.drivers" xrefstyle="select:label"/>
   lists the most commonly used watchdog drivers. If your hardware is not listed there,
   the directory
   <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/watchdog</filename>
   gives you a list of choices, too. Alternatively, ask your hardware vendor for
   the name.</para>

     <table xml:id="tab.ha.storage.protect.watchdog.drivers">
        <title>Commonly Used Watchdog Drivers</title>
        <tgroup cols="2">
         <thead>
          <row>
           <entry>Hardware</entry>
           <entry>Driver</entry>
          </row>
         </thead>
         <tbody>
          <row>
           <entry>HP</entry>
           <entry><systemitem class="resource">hpwdt</systemitem></entry>
          </row>
          <row>
           <entry>Dell, Fujitsu, Lenovo (Intel TCO)</entry>
           <entry><systemitem class="resource">iTCO_wdt</systemitem></entry>
          </row>
          <row>
           <entry>VM on z/VM on IBM mainframe</entry>
           <entry><systemitem class="resource">vmwatchdog</systemitem></entry>
          </row>
          <row>
           <entry>Xen VM (DomU)</entry>
           <entry><systemitem class="resource">xen_xdt</systemitem></entry>
          </row>
          <row>
           <entry>Generic</entry>
           <entry><systemitem class="resource">softdog</systemitem></entry>
          </row>
         </tbody>
        </tgroup>
       </table>

 <important>
    <title>Accessing the Watchdog Timer</title>
    <para>Some hardware vendors ship systems management software that uses the
     watchdog for system resets (for example, HP ASR daemon). If watchdog is
     used by SBD, disable such software. No other software must access the
     watchdog timer. </para>
   </important>

   <procedure xml:id="pro.ha.storage.protect.watchdog">
    <title>Loading the Correct Kernel Module</title>
    <para>To make sure the correct watchdog model is loaded, proceed as follows:</para>
     <step>
      <para>List the drivers that have been installed with your kernel version:</para>
       <screen>&prompt.root;<command>rpm</command> -ql kernel-<replaceable>VERSION</replaceable> | <command>grep</command> watchdog</screen>
      </step>
      <step xml:id="st.ha.storage.listwatchdog.modules">
       <para>List any watchdog modules that are currently loaded in the kernel:</para>
       <screen>&prompt.root;<command>lsmod</command> | <command>egrep</command> "(wd|dog)"</screen>
      </step>
      <step>
       <para>If you get a result, unload the wrong module:</para>
       <screen>&prompt.root;<command>rmmod</command> <replaceable>WRONG_MODULE</replaceable></screen>
      </step>
      <step>
     <para> Enable the watchdog module that matches your hardware: </para>
     <screen>&prompt.root;<command>echo</command> <replaceable>WATCHDOG_MODULE</replaceable> > /etc/modules-load.d/watchdog.conf
&prompt.root;<command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
     <para>Test if the watchdog module is loaded correctly:</para>
     <screen>&prompt.root;<command>lsmod</command> | <command>grep</command> dog</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.storage.protect.sw-watchdog">
   <title>Using the Software Watchdog (softdog)</title>
   <para>
    For clusters in production environments we recommend to use a hardware-specific watchdog
    driver. However, if no watchdog matches your hardware, <systemitem class="resource"
    >softdog</systemitem> can be used as kernel watchdog module. </para>

   <important>
    <title>Softdog Limitations</title>
    <para>
     The softdog driver assumes that at least one CPU is still running. If all
     CPUs are stuck, the code in the softdog driver that should reboot the system
     will never be executed. In contrast, hardware watchdogs keep working even
     if all CPUs are stuck.
    </para>
   </important>

   <procedure xml:id="pro.ha.storage.protect.sw-watchdog">
    <title>Loading the Softdog Kernel Module</title>
    <step>
     <para>Enable the softdog driver:</para>
     <screen>&prompt.root;<command>echo</command> softdog > /etc/modules-load.d/watchdog.conf</screen>
    </step>
    <step>
     <para>Add the <systemitem class="resource">softdog</systemitem>
          module in <filename>/etc/modules-load.d/watchdog.conf</filename>
          and restart a service:</para>
         <screen>&prompt.root;<command>echo</command> softdog > /etc/modules-load.d/watchdog.conf
&prompt.root;<command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
     <para>Test if the softdog watchdog module is loaded correctly:</para>
     <screen>&prompt.root;<command>lsmod</command> | <command>grep</command> softdog</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.fencing.setup">
  <title>Setting Up SBD with Devices</title>
  <para>
   The following steps are necessary for setup:
  </para>
 <procedure>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.create" xrefstyle="select:title"/>
        </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.config" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.services" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.test" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.fencing" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>
  <para>
    Before you start, make sure the block device or devices you want to use for
    SBD meet the requirements specified in <xref linkend="sec.ha.storage.protect.req"
    xrefstyle="select:label"/>.
  </para>
  <para>
   When setting up the SBD devices, you need to take several timeout values into
   account. For details, see <xref linkend="sec.ha.storage.protect.watchdog.timings"/>.
  </para>
  <para>
   The node will terminate itself if the SBD daemon running on it has not
   updated the watchdog timer fast enough. After having set the timeouts, test
   them in your specific environment.
  </para>

  <procedure xml:id="pro.ha.storage.protect.sbd.create">
   <title>Initializing the SBD Devices</title>
   <para>
    To use SBD with shared storage, you must first create the messaging
    layout on one to three block devices. The <command>sbd create</command> command
    will write a metadata header to the specified device or devices. It will also
    initialize the messaging slots for up to 255 nodes. If executed without any
    further options, the command will use the default timeout settings.</para>
    <warning>
     <title>Overwriting Existing Data</title>
      <para> Make sure the device or devices you want to use for SBD do not hold any
       important data. As soon as you execute the <command>sbd create</command>
       command, roughly the first megabyte of the specified block devices
       will be overwritten without further requests nor backup.
      </para>
    </warning>
    <step>
     <para>Decide which block device or block devices to use for SBD.</para>
    </step>
    <step>
     <para>Initialize the SBD device with the following command: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> create</screen>
     <para>(replace <filename>/dev/<replaceable>SBD</replaceable></filename>
       with your actual path name, for example:
       <filename>/dev/disk/by-id/scsi-ST2000DM001-0123456_Wabcdefg</filename>)</para>
        <para> If you want to use more than one device for SBD, specify the <option>-d</option> option multiple times, for
      example: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD1</replaceable> -d /dev/<replaceable>SBD2</replaceable> -d /dev/<replaceable>SBD3</replaceable> create</screen>
    </step>
    <step>
     <para>If your SBD device resides on a multipath group, use the <option>-1</option>
      and <option>-4</option> options to adjust the timeouts to use for SBD (for
      details, see <xref linkend="sec.ha.storage.protect.watchdog.timings"/>).
      All timeouts are given in seconds:</para>
      <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> -4 180<co xml:id="co.ha.sbd.msgwait"/> -1 90<co xml:id="co.ha.sbd.watchdog"/> create</screen>
     <calloutlist>
      <callout arearefs="co.ha.sbd.msgwait">
       <para> The <option>-4</option> option is used to specify the
         <literal>msgwait</literal> timeout. In the example above, it is set to
         <literal>180</literal> seconds. </para>
      </callout>
      <callout arearefs="co.ha.sbd.watchdog">
       <para> The <option>-1</option> option is used to specify the
         <literal>watchdog</literal> timeout. In the example above, it is set
        to <literal>90</literal> seconds. The minimum allowed value for the
        emulated watchdog is <literal>15</literal> seconds. </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>Check what has been written to the device: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> dump
Header version     : 2.1
UUID               : 619127f4-0e06-434c-84a0-ea82036e144c
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 5
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 10
==Header on disk /dev/<replaceable>SBD</replaceable> is dumped</screen>
    <para> As you can see, the timeouts are also stored in the header, to ensure
    that all participating nodes agree on them. </para>
    </step>
   </procedure>
   <para>
    After you have initialized the SBD devices, edit the SBD configuration file,
    then enable and start the respective services for the changes to take effect.
   </para>

   <procedure xml:id="pro.ha.storage.protect.sbd.config">
   <title>Editing the SBD Configuration File</title>
    <step>
     <para>Open the file <filename>/etc/sysconfig/sbd</filename>.</para>
    </step>
    <step>
     <para>Search for the following parameter: <parameter>SBD_DEVICE</parameter>.
     </para>
     <para>It specifies the devices to monitor and to use for exchanging SBD messages.
     </para>
    </step>
   <step>
    <para> Edit this line by replacing <replaceable>SBD</replaceable> with your SBD device:</para>
    <screen>SBD_DEVICE="/dev/<replaceable>SBD</replaceable>"</screen>
    <para> If you need to specify multiple devices in the first line, separate them by a semicolon
     (the order of the devices does not matter):</para>
    <screen>SBD_DEVICE="/dev/<replaceable>SBD1</replaceable>; /dev/<replaceable>SBD2</replaceable>; /dev/<replaceable>SBD3</replaceable>"</screen>
    <para> If the SBD device is not accessible, the daemon will fail to start and inhibit
     &corosync; start-up. </para>
   </step>
  </procedure>

 <para>After you have added your SBD devices to the SBD configuration file,
  enable the SBD daemon. The SBD daemon is a critical piece
  of the cluster stack. It needs to be running when the cluster stack is running.
  Thus, the <systemitem>sbd</systemitem> service is started as a dependency whenever
  the <systemitem>pacemaker</systemitem> service is started.</para>

  <procedure xml:id="pro.ha.storage.protect.sbd.services">
   <title>Enabling and Starting the SBD Service</title>
   <step>
    <para>On each node, enable the SBD service:</para>
    <screen>&prompt.root;<command>systemctl</command> enable sbd</screen>
    <para>It will be started together with the Corosync service whenever the Pacemaker
     service is started.</para>
   </step>
   <step>
    <para>Restart the cluster stack on each node:</para>
    <screen>&prompt.root;<command>systemctl</command> stop pacemaker
&prompt.root;<command>systemctl</command> start pacemaker</screen>
    <para> This automatically triggers the start of the SBD daemon. </para>
   </step>
  </procedure>

  <para>
   As next step, test the SBD devices as described in <xref
   linkend="pro.ha.storage.protect.sbd.test" xrefstyle="select:label"/>.
  </para>

  <procedure xml:id="pro.ha.storage.protect.sbd.test">
   <title>Testing the SBD Devices</title>
    <step>
     <para> The following command will dump the node slots and their current
      messages from the SBD device: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> list</screen>
    <para> Now you should see all cluster nodes that have ever been started with SBD listed here.
     For example, if you have a two-node cluster, the message slot should show
      <literal>clear</literal> for both nodes:</para>
     <screen>0       &node1;        clear
1       &node2;          clear</screen>
    </step>
    <step>
     <para> Try sending a test message to one of the nodes: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> message &node1; test</screen>
    </step>
    <step>
     <para> The node will acknowledge the receipt of the message in the system
      log files: </para>
     <screen>Aug 29 14:10:00 &node1; sbd: [13412]: info: Received command test from &node2;</screen>
     <para> This confirms that SBD is indeed up and running on the node and
      that it is ready to receive messages. </para>
    </step>
   </procedure>

  <para>
   As final step, you need to adjust the cluster configuration as described in
   <xref linkend="pro.ha.storage.protect.fencing" xrefstyle="select:label"/>.
  </para>

<procedure xml:id="pro.ha.storage.protect.fencing">
 <title>Configuring the Cluster to Use SBD</title>
   <para>
    To configure the use of SBD in the cluster, you need to do the following in
    the cluster configuration:
   </para>
   <itemizedlist>
    <listitem>
     <para>
       set the <parameter>stonith-timeout</parameter> parameter to a value that
       matches your setting
     </para>
    </listitem>
    <listitem>
     <para>
      configure the SBD &stonith; resource
     </para>
    </listitem>
   </itemizedlist>
   <para>
     For the calculation of the <parameter>stonith-timeout</parameter> refer to
     <xref linkend="sec.ha.storage.protect.watchdog.timings"/>.
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>Enter the following:</para>
    <screen>
&prompt.crm.conf;<command>property</command> stonith-enabled="true" <co xml:id="co.ha.sbd.st.enabled"/>
&prompt.crm.conf;<command>property</command> stonith-watchdog-timeout=0 <co xml:id="co.ha.sbd.watchdog-timeout"/>
&prompt.crm.conf;<command>property</command> stonith-timeout="40s" <co xml:id="co.ha.sbd.st.timeout"/>
&prompt.crm.conf;<command>primitive</command> stonith_sbd stonith:external/sbd <co xml:id="co.ha.sbd.st.rsc"/>\
  params pcmk_delay_max=30 <co xml:id="co.ha.sbd.pm_delay_max"/></screen>
    <calloutlist>
     <callout arearefs="co.ha.sbd.st.enabled">
      <para>
       This is the default configuration, because clusters with &stonith; are not supported.
       But in case &stonith; had been deactivated for testing purposes,
       make sure this parameter is set to <literal>true</literal>.</para>
     </callout>
     <callout arearefs="co.ha.sbd.watchdog-timeout">
      <para>If not explicitly set, this value default to <literal>0</literal>,
        which is appropriate for use of SBD with one to three devices.
      </para>
     </callout>
     <callout arearefs="co.ha.sbd.st.timeout">
      <para>
       A <systemitem>stonith-timeout</systemitem> value of <literal>40</literal>
       would be appropriate if the <literal>msgwait</literal> timeout value for
       SBD was set to <literal>30</literal> seconds.</para>
     </callout>
     <callout arearefs="co.ha.sbd.st.rsc">
      <para>
       The &stonith; resource to use for SBD.</para>
     </callout>
     <callout arearefs="co.ha.sbd.pm_delay_max">
      <para>This parameter prevents double fencing when using slow devices such
       as SBD. It adds a random delay for &stonith; actions on the fencing device.
       It is especially important for two-node clusters where otherwise both nodes
       might try to fence each other in case of a split brain scenario.</para>
     </callout>
   </calloutlist>
   </step>
  <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     Submit your changes with <command>commit</command> and leave the crm live
     configuration with <command>exit</command>.
    </para>
   </step>
  </procedure>

   <para> After the resource has started, your cluster is successfully
    configured for use of SBD and will use this method in case a
    node needs to be fenced.</para>
  </sect1>

  <sect1 xml:id="sec.ha.storage.protect.diskless-sbd">
   <title>Setting Up Diskless SBD</title>
   <para>SBD can be operated in a diskless mode. In this mode, a watchdog device
    will be used to reset the node in the following cases: if it loses quorum,
    if any monitored daemon is lost and not recovered, or if Pacemaker decides
    that the node requires fencing. Diskless SBD is basically based on
    <quote>self-fencing</quote> of a node, depending on the status of the cluster,
    the quorum and some reasonable assumptions.</para>
    <important>
     <title>Number of Cluster Nodes</title>
       <para>
         Do <emphasis>not</emphasis> use diskless SBD as fencing mechanism
         for two-node clusters. Use it only in clusters with three or more
         nodes. SBD in diskless mode cannot handle split brain
         scenarios for two-node clusters.
      </para>
   </important>

   <procedure xml:id="pro.ha.storage.protect.confdiskless">
    <title>Configuring Diskless SBD</title>
    <step>
     <para>Open the file <filename>/etc/sysconfig/sbd</filename> and use
      the following entries:</para>
     <screen>SBD_PACEMAKER=yes
SBD_STARTMODE=always
SBD_DELAY_START=no
SBD_WATCHDOG_DEV=/dev/watchdog
SBD_WATCHDOG_TIMEOUT=5</screen>
      <para>
       The <varname>SBD_DEVICE</varname> entry is not needed as no shared
       disk is used. When this parameter is missing, the <systemitem>sbd</systemitem>
       service does not start any watcher process for SBD devices.
      </para>
    </step>
    <step>
     <para>On each node, enable the SBD service:</para>
     <screen>&prompt.root;<command>systemctl</command> enable sbd</screen>
     <para>It will be started together with the Corosync service whenever the Pacemaker
      service is started.</para>
    </step>
    <step>
     <para>Restart the cluster stack on each node:</para>
     <screen>&prompt.root;<command>systemctl</command> stop pacemaker
&prompt.root;<command>systemctl</command> start pacemaker</screen>
     <para> This automatically triggers the start of the SBD daemon. </para>
    </step>
    <step>
      <para>
       Check if the parameter <parameter>have-watchdog=true</parameter> has
       been automatically set:
      </para>
      <screen>&prompt.root;<command>crm</command> configure show | grep have-watchdog
         have-watchdog=true</screen>
    </step>
    <step>
     <para>Run <command>crm configure</command> and set the following cluster
      properties on the &crmshell;:</para>
<screen>&prompt.crm.conf;<command>property</command> stonith-enabled="true" <xref
        linkend="co.ha.sbd.st.enabled" xrefstyle="select:label"/>
&prompt.crm.conf;<command>property</command> stonith-watchdog-timeout=10 <co
        xml:id="co.ha.sbd.diskless.watchdog-timeout"/></screen>
    <calloutlist>
     <callout arearefs="co.ha.sbd.st.enabled">
      <para>
       This is the default configuration, because clusters with &stonith; are not supported.
       But in case &stonith; had been deactivated for testing purposes,
       make sure this parameter is set to <literal>true</literal>.</para>
     </callout>
     <callout arearefs="co.ha.sbd.diskless.watchdog-timeout">
      <para>For diskless SBD, this parameter must not equal zero.
       It defines after how long it is assumed that the fencing target has already
       self-fenced. Therefore its value needs to be &gt;= the value of
       <varname>SBD_WATCHDOG_TIMEOUT</varname> in <filename>/etc/sysconfig/sbd</filename>.
       Starting with &productname; 15, if you set it to a negative value, Pacemaker
       will automatically calculate an appropriate timeout. In that case, it will
       be set to twice the value of <parameter>SBD_WATCHDOG_TIMEOUT</parameter>
       that is configured in <filename>/etc/sysconfig/sbd</filename>.
      </para>
     </callout>
    </calloutlist>
   </step>
  <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     Submit your changes with <command>commit</command> and leave the crm live
     configuration with <command>exit</command>.
    </para>
   </step>
  </procedure>
  </sect1>

  <sect1 xml:id="sec.ha.storage.protect.test">
   <title>Testing SBD and Fencing</title>
   <para>To test if SBD works as expected for node fencing purposes, use one or all
    of the following methods:
   </para>
  <variablelist>
   <varlistentry>
    <term>Manually Triggering Fencing of a Node</term>
    <listitem>
     <para>To trigger a fencing action for node <replaceable>NODENAME</replaceable>:</para>
 <screen>&prompt.root;<command>crm</command> node fence <replaceable>NODENAME</replaceable></screen>
     <para>Check if the node is fenced and if the other nodes consider the node as fenced
      after the <parameter>stonith-watchdog-timeout</parameter>.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Simulating an SBD Failure</term>
    <listitem>
     <procedure>
      <step>
       <para>Identify the process ID of the SBD inquisitor:</para>
       <screen>&prompt.root;<command>systemctl</command> status sbd
● sbd.service - Shared-storage based fencing daemon

   Loaded: loaded (/usr/lib/systemd/system/sbd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-04-17 15:24:51 CEST; 6 days ago
     Docs: man:sbd(8)
  Process: 1844 ExecStart=/usr/sbin/sbd $SBD_OPTS -p /var/run/sbd.pid watch (code=exited, status=0/SUCCESS)
 Main PID: 1859 (sbd)
    Tasks: 4 (limit: 4915)
   CGroup: /system.slice/sbd.service
           ├─<emphasis role="strong">1859 sbd: inquisitor</emphasis>
[...]</screen>
      </step>
      <step>
       <para>Simulate an SBD failure by killing the SBD inquisitor process.
       In our example, the process ID of the SBD inquisitor is
         <literal>1859</literal>):</para>
       <screen>&prompt.root;<command>kill</command> -9 1859 </screen>
       <para>
        <remark>taroth 2018-04-28: @DEVs, What is the expected result?</remark>
       </para>
      </step>
     </procedure>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Producing a Failure of a Resource Operation</term>
    <listitem>
     <procedure>
      <step>
       <para>Configure a <property>on-fail=fence</property> property for a resource monitor
        operation:</para>
       <screen>op monitor interval=10 on-fail=fence</screen>
      </step>
      <step>
       <para> Produce a failure of the monitoring operation (for example, by killing the respective
        daemon, if the resource relates to a service).</para>
       <para>This failure should trigger a fencing action.</para>
      </step>
     </procedure>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.morestuff">
  <title>Additional Mechanisms for Storage Protection</title>
  <remark>toms 2018-04-20: this can be improved...</remark>
  <para>Apart from &stonith;, there are other methods to reach storage
  protection. SCSI-3 and SCSI-4 use persistent reservations whereas
  <literal>sfex</literal> provides a locking mechanism. Both methods are
   explained in the following subsections.
  </para>
  <sect2 xml:id="sec.ha.storage.protect.sgpersist">
   <title>Configuring an sg_persist Resource</title>
   <remark>toms 2018-04-20: I would like to see a little bit more background
   information. </remark>
   <para>
    The SCSI specification 3 and 4 define <emphasis>persistent reservations</emphasis>.
    These are SCSI protocol features and can be used for I/O fencing and failover.
    This feature is implemented in the <command>sg_persist</command> Linux
    command.
   </para>
   <note>
    <title>SCSI Disk Compatibility</title>
    <para> Any backing disks for <literal>sg_persist</literal> must be SCSI
     disk compatible. <literal>sg_persist</literal> only works for devices like
     SCSI disks or iSCSI LUNs.
     <remark>toms 2018-04-20: What about FCoE, FC, iSER, SRP, Serial Attached SCSI (SAR)?</remark>
     Do <emphasis>not</emphasis> use it for IDE, SATA, or any block devices
     which do not support the SCSI protocol. </para>
   </note>
   <para>Before you proceed, check if your disk supports
    persistent reservations. Use the following command (replace
     <replaceable>DISK</replaceable> with your device name):</para>
    <screen>&prompt.root;<command>sg_persist</command> -n --in --read-reservation -d /dev/<replaceable>DISK</replaceable></screen>
   <para>The result shows if your disk supports persistent reservations:</para>
    <itemizedlist>
     <listitem>
      <para>Supported disk:</para>
      <screen>PR generation=0x0, there is NO reservation held</screen>
     </listitem>
     <listitem>
      <para>Unsupported disk:</para>
      <screen>PR in (Read reservation): command not supported
Illegal request, Invalid opcode</screen>
     </listitem>
    </itemizedlist>

   <remark>toms 2018-04-20: Do we need to prepare anything else with sg_persists?</remark>
   <para>If you get an error message (like the one above), replace the old
    disk with a SCSI compatible disk. Otherwise proceed as follows:</para>
   <procedure>
    <step>
     <para>
      To create the primitive resource <literal>sg_persist</literal>,
      run the following commands as &rootuser;: </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> sg sg_persist \
    params devs="/dev/sdc" reservation_type=3 \
    op monitor interval=60 timeout=60</screen>
    </step>
    <step>
     <para> Add the <literal>sg_persist</literal> primitive to a master-slave
      group: <remark>taroth 2018-03-05: ygao, should 'master-max' be replaced
       with 'promoted-max' or does the screen below need more changes? - ygao
       2018-03-13: the new names are not explicitly promoted in crmsh
       yet</remark>
     </para>
     <screen>&prompt.crm.conf;<command>ms</command> ms-sg sg \
    meta master-max=1 notify=true</screen>
    </step>
    <step>
     <para> Do some tests. When the resource is in master/slave status, on the
      master server, you can mount and write on <filename>/dev/sdc1</filename>,
      while on the slave server you cannot write. </para>
    </step>
    <step>
     <para> Add a file system primitive for Ext4: </para>
     <screen>&prompt.crm.conf;<command>primitive</command> ext4 ocf:heartbeat:Filesystem \
    params device="/dev/sdc1" directory="/mnt/ext4" fstype=ext4</screen>
    </step>
    <step>
     <para> Add the following order relationship plus a collocation between the
      <literal>sg_persist</literal> master and the file system resource: </para>
     <screen>&prompt.crm.conf;<command>order</command> o-ms-sg-before-ext4 inf: ms-sg:promote ext4:start
&prompt.crm.conf;<command>colocation</command> col-ext4-with-sg-persist inf: ext4 ms-sg:Master</screen>
    </step>
    <step>
     <para> Check all your changes with the <command>show</command> command.
     </para>
    </step>
    <step>
     <para> Commit your changes. </para>
    </step>
   </procedure>
   <para>For more information, refer to the <command>sg_persist</command> man
    page.</para>
  </sect2>

  <sect2 xml:id="sec.ha.storage.protect.exstoract">
   <title>Ensuring Exclusive Storage Activation</title>
   <remark>toms 2018-04-18: Hmn... I would have liked it more if this chapter
    describes only SBD. Describing another "low-level mechanism" is a bit
    distracting. Probably there is no better location...</remark>
   <para>
    This section introduces <literal>sfex</literal>, an additional low-level
    mechanism to lock access to shared storage exclusively to one node. Note
    that sfex does not replace &stonith;. Since sfex requires shared
    storage, it is recommended that the <literal>external/sbd</literal>
    fencing mechanism described above is used on another partition of the
    storage.
   </para>

   <para>
    By design, sfex cannot be used with workloads that require concurrency
    (such as &ocfs;), but serves as a layer of protection for classic failover
    style workloads. This is similar to a SCSI-2 reservation in effect, but
    more general.
   </para>

   <sect3 xml:id="sec.ha.storage.protect.exstoract.description">
    <title>Overview</title>
    <para>
     In a shared storage environment, a small partition of the storage is set
     aside for storing one or more locks.
    </para>
    <para>
     Before acquiring protected resources, the node must first acquire the
     protecting lock. The ordering is enforced by Pacemaker, and the sfex
     component ensures that even if Pacemaker were subject to a split brain
     situation, the lock will never be granted more than once.
    </para>
    <para>
     These locks must also be refreshed periodically, so that a node's death
     does not permanently block the lock and other nodes can proceed.
    </para>
   </sect3>

   <sect3 xml:id="sec.ha.storage.protect.exstoract.requirements">
    <title>Setup</title>
    <para>
     In the following, learn how to create a shared partition for use with
     sfex and how to configure a resource for the sfex lock in the CIB. A
     single sfex partition can hold any number of locks, and needs 1 KB
     of storage space allocated per lock.
     By default, <command>sfex_init</command> creates one lock on the partition.
    </para>
    <important>
     <title>Requirements</title>
     <itemizedlist>
      <listitem>
       <para>
        The shared partition for sfex should be on the same logical unit as
        the data you want to protect.
       </para>
      </listitem>
      <listitem>
       <para>
        The shared sfex partition must not use host-based RAID, nor DRBD.
       </para>
      </listitem>
      <listitem>
       <para>
        Using a LVM2 logical volume is possible.
       </para>
      </listitem>
     </itemizedlist>
    </important>
    <procedure>
     <title>Creating an sfex Partition</title>
     <step>
      <para>
       Create a shared partition for use with sfex. Note the name of this
       partition and use it as a substitute for
       <filename>/dev/sfex</filename> below.
      </para>
     </step>
     <step>
      <para>
       Create the sfex meta data with the following command:
      </para>
      <screen>&prompt.root;<command>sfex_init</command> -n 1 /dev/sfex</screen>
     </step>
     <step>
      <para>
       Verify that the meta data has been created correctly:
      </para>
      <screen>&prompt.root;<command>sfex_stat</command> -i 1 /dev/sfex ; echo $?</screen>
      <para>
       This should return <literal>2</literal>, since the lock is not
       currently held.
      </para>
     </step>
    </procedure>
    <procedure>
     <title>Configuring a Resource for the sfex Lock</title>
     <step>
      <para>
       The sfex lock is represented via a resource in the CIB, configured as
       follows:
      </para>
      <screen>&prompt.crm.conf;<command>primitive</command> sfex_1 ocf:heartbeat:sfex \
#	params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
#	op monitor interval="10s" timeout="30s" on-fail="fence"</screen>
     </step>
     <step>
      <para>
       To protect resources via an sfex lock, create mandatory ordering and
       placement constraints between the protectees and the sfex resource. If
       the resource to be protected has the ID
       <literal>filesystem1</literal>:
      </para>
      <screen>&prompt.crm.conf;<command>order</command> order-sfex-1 inf: sfex_1 filesystem1
&prompt.crm.conf;<command>colocation</command> colo-sfex-1 inf: filesystem1 sfex_1</screen>
     </step>
     <step>
      <para>
       If using group syntax, add the sfex resource as the first resource to
       the group:
      </para>
      <screen>&prompt.crm.conf;<command>group</command> LAMP sfex_1 filesystem1 apache ipaddr</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.moreinfo">
  <title>For More Information</title>

  <itemizedlist>
    <listitem>
      <para><link xlink:href="http://www.linux-ha.org/wiki/SBD_Fencing"/></para>
    </listitem>
    <listitem>
      <para><command>man sbd</command></para>
    </listitem>
  </itemizedlist>
 </sect1>
</chapter>
