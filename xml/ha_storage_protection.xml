<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="cha-ha-storage-protect"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Storage protection and SBD</title>
 <info>
  <abstract>
   <para>
    SBD (&stonith; Block Device) provides a node fencing mechanism for
    Pacemaker-based clusters through the exchange of messages via shared block
    storage (SAN, iSCSI, FCoE, etc.). This isolates the fencing
    mechanism from changes in firmware version or dependencies on specific
    firmware controllers. SBD needs a watchdog on each node to ensure that misbehaving
    nodes are really stopped. Under certain conditions, it is also possible to use
    SBD without shared storage, by running it in diskless mode.
   </para>
   <para>
    The <package>ha-cluster-bootstrap</package> scripts provide an automated
    way to set up a cluster with the option of using SBD as fencing mechanism.
    For details, see the <xref linkend="article-installation"/>. However,
    manually setting up SBD provides you with more options regarding the
    individual settings.
   </para>
   <para>
    This chapter explains the concepts behind SBD. It guides you through
    configuring the components needed by SBD to protect your cluster from
    potential data corruption in case of a split brain scenario.
   </para>
   <para>
    In addition to node level fencing, you can use additional mechanisms for storage
    protection, such as LVM2 exclusive activation or OCFS2 file locking support
    (resource level fencing). They protect your system against administrative or
    application faults.
   </para>
  </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-storage-protect-overview">
      <title>Conceptual overview</title>
      <para>SBD expands to <emphasis>Storage-Based Death</emphasis> or
        <emphasis>STONITH Block Device</emphasis>.
      </para>
      <para>
        The highest priority of the &ha; cluster stack is to protect the integrity
        of data. This is achieved by preventing uncoordinated concurrent access
        to data storage. The cluster stack takes care of this using several
        control mechanisms.
      </para>
      <para>
        However, network partitioning or software malfunction could potentially
        cause scenarios where several DCs are elected in a cluster. If this
        so-called split brain scenario were allowed to unfold, data corruption
        might occur.
      </para>
      <para>
        Node fencing via &stonith; is the primary mechanism to prevent this.
        Using SBD as a node fencing mechanism is one way of shutting down nodes
        without using an external power off device in case of a split brain scenario.
      </para>

  <variablelist>
   <title>SBD components and mechanisms</title>
   <varlistentry>
    <term>SBD partition</term>
    <listitem>
     <para> In an environment where all nodes have access to shared storage, a
      small partition of the device is formatted for use with SBD. The size of
      the partition depends on the block size of the used disk (for example,
      1&nbsp;MB for standard SCSI disks with 512&nbsp;byte block size or
      4&nbsp;MB for DASD disks with 4&nbsp;kB block size). The initialization
      process creates a message layout on the device with slots for up to 255
      nodes.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD daemon</term>
    <listitem>
     <para> After the respective SBD daemon is configured, it is brought online
      on each node before the rest of the cluster stack is started. It is
      terminated after all other cluster components have been shut down, thus
      ensuring that cluster resources are never activated without SBD
      supervision. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Messages</term>
    <listitem>
     <para>
      The daemon automatically allocates one of the message slots on the
      partition to itself, and constantly monitors it for messages addressed
      to itself. Upon receipt of a message, the daemon immediately complies
      with the request, such as initiating a power-off or reboot cycle for
      fencing.
     </para>
     <para>
      Also, the daemon constantly monitors connectivity to the storage device, and
      terminates itself in case the partition becomes unreachable. This
      guarantees that it is not disconnected from fencing messages. If the
      cluster data resides on the same logical unit in a different partition,
      this is not an additional point of failure: The workload will terminate
      anyway if the storage connectivity has been lost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Watchdog</term>
     <listitem>
      <para>
      Whenever SBD is used, a correctly working watchdog is crucial.
      Modern systems support a <emphasis>hardware watchdog</emphasis>
      that needs to be <quote>tickled</quote> or <quote>fed</quote> by a
      software component. The software component (in this case, the SBD daemon)
      <quote>feeds</quote> the watchdog by regularly writing a service pulse
      to the watchdog. If the daemon stops feeding the watchdog, the hardware
      will enforce a system restart. This protects against failures of the SBD
      process itself, such as dying, or becoming stuck on an I/O error.
     </para>
     </listitem>
   </varlistentry>
  </variablelist>
  <para>
   If Pacemaker integration is activated, SBD will not self-fence if device
   majority is lost. For example, your cluster contains three nodes: A, B, and
   C. Because of a network split, A can only see itself while B and C can
   still communicate. In this case, there are two cluster partitions: one
   with quorum because of being the majority (B, C), and one without (A).
   If this happens while the majority of fencing devices are unreachable,
   node A would immediately commit suicide, but nodes B and C would
   continue to run.
   </para>
  </sect1>

 <sect1 xml:id="sec-ha-storage-protect-steps">
 <title>Overview of manually setting up SBD</title>
 <para>
  The following steps are necessary to manually set up storage-based protection.
  They must be executed as &rootuser;. Before you start, check <xref
  linkend="sec-ha-storage-protect-req" xrefstyle="sec.ha.storage.protect.req"/>.
  </para>
 <procedure>
   <step>
    <para>
     <xref linkend="sec-ha-storage-protect-watchdog" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>Depending on your scenario, either use SBD with one to three devices or in diskless mode.
     For an outline, see <xref linkend="sec-ha-storage-protect-fencing-number"/>. The detailed setup
     is described in:</para>
    <itemizedlist>
     <listitem>
      <para>
       <xref linkend="sec-ha-storage-protect-fencing-setup" xrefstyle="select:title"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="sec-ha-storage-protect-diskless-sbd" xrefstyle="select:title"/>
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     <xref linkend="sec-ha-storage-protect-test" xrefstyle="select:title"
     />
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-req">
  <title>Requirements</title>
   <itemizedlist>
   <listitem>
    <para>You can use up to three SBD devices for storage-based fencing.
     When using one to three devices, the shared storage must be accessible from all nodes.</para>
   </listitem>
   <listitem>
    <para>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
     </para>
   </listitem>
   <listitem>
    <para>The shared storage can be connected via Fibre Channel (FC),
     Fibre Channel over Ethernet (FCoE), or even iSCSI. </para>
   </listitem>
   <listitem>
    <para> The shared storage segment <emphasis>must not</emphasis>
     use host-based RAID, LVM2, or DRBD*. DRBD can be split, which is
     problematic for SBD, as there cannot be two states in SBD.
     Cluster multi-device (Cluster MD) cannot be used for SBD.
    </para>
   </listitem>
   <listitem>
    <para> However, using storage-based RAID and multipathing is
     recommended for increased reliability. </para>
   </listitem>
   <listitem>
    <para>An SBD device can be shared between different clusters, as
     long as no more than 255 nodes share the device. </para>
   </listitem>
   <listitem>
    <para>For clusters with more than two nodes, you can also use SBD in
    <emphasis>diskless</emphasis> mode.
   </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-fencing-number">
  <title>Number of SBD devices</title>
  <!--fate#309375-->
  <para> SBD supports the use of up to three devices: </para>
  <variablelist>
   <varlistentry>
    <term>One device</term>
    <listitem>
     <para>
      The most simple implementation. It is appropriate for clusters where
      all of your data is on the same shared storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Two devices</term>
    <listitem>
     <para>
      This configuration is primarily useful for environments that use
      host-based mirroring but where no third storage device is available.
      SBD will not terminate itself if it loses access to one mirror leg,
      allowing the cluster to continue. However, since SBD does not have
      enough knowledge to detect an asymmetric split of the storage, it
      will not fence the other side while only one mirror leg is available.
      Thus, it cannot automatically tolerate a second failure while one of
      the storage arrays is down.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Three devices</term>
    <listitem>
     <para>
      The most reliable configuration. It is resilient against outages of
      one device&mdash;be it because of failures or maintenance. SBD
      will terminate itself only if more than one device is lost and if required,
      depending on the status of the cluster partition or node. If at least
      two devices are still accessible, fencing messages can be successfully
      transmitted.
     </para>
     <para>
      This configuration is suitable for more complex scenarios where
      storage is not restricted to a single array. Host-based mirroring
      solutions can have one SBD per mirror leg (not mirrored itself), and
      an additional tie-breaker on iSCSI.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Diskless</term>
    <listitem>
     <para>&sbd-diskless;</para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-watchdog-timings">
   <title>Calculation of timeouts</title>
    <para>
      When using SBD as a fencing mechanism, it is vital to consider the timeouts
      of all components, because they depend on each other.
    </para>
    <variablelist>
     <varlistentry>
      <term>Watchdog timeout</term>
      <listitem>
       <para>
        This timeout is set during initialization of the SBD device. It depends
        mostly on your storage latency. The majority of devices must be successfully
        read within this time. Otherwise, the node might self-fence.
       </para>
       <note>
        <title>Multipath or iSCSI setup</title>
          <para>
          If your SBD device(s) reside on a multipath setup or iSCSI, the timeout
          should be  set to the time required to detect a path failure and switch
          to the next path.
          </para>
          <para>
           This also means that in <filename>/etc/multipath.conf</filename> the
           value of  <literal>max_polling_interval</literal> must be less than
           <literal>watchdog</literal> timeout.
         </para>
       </note>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>msgwait</literal> Timeout</term>
      <listitem>
       <para>
        This timeout is set during initialization of the SBD device. It defines
        the time after which a message written to a node's slot on the SBD device
        is considered delivered. The timeout should be long enough for the node to
        detect that it needs to self-fence.
       </para>
       <para>
        However, if the <literal>msgwait</literal> timeout is relatively long,
        a fenced cluster node might rejoin before the fencing action returns.
        This can be mitigated by setting the <varname>SBD_DELAY_START</varname>
        parameter in the SBD configuration, as described in
        <xref linkend="pro-ha-storage-protect-sbd-config" xrefstyle="select:label"/>
        in
        <xref linkend="st-ha-storage-protect-sbd-delay-start"/>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>stonith-timeout</literal> in the CIB</term>
      <listitem>
       <para>
        This timeout is set in the CIB as a global cluster property. It defines
        how long to wait for the &stonith; action (reboot, on, off) to complete.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>stonith-watchdog-timeout</literal> in the CIB</term>
      <listitem>
       <para>
        This timeout is set in the CIB as a global cluster property. If not set
        explicitly, it defaults to <literal>0</literal>, which is appropriate for
        using SBD with one to three devices. For use of SBD in diskless mode, see <xref
        linkend="pro-ha-storage-protect-confdiskless"/> for more details.</para>
      </listitem>
     </varlistentry>
    </variablelist>
  <para>
   If you change the watchdog timeout, you need to adjust the other two timeouts
   as well. The following <quote>formula</quote> expresses the relationship
   between these three values:
  </para>
   <example xml:id="ex-ha-storage-protect-sbd-timings">
    <title>Formula for timeout calculation</title>
    <screen>Timeout (msgwait) >= (Timeout (watchdog) * 2)
stonith-timeout = Timeout (msgwait) + 20%</screen>
   </example>
   <para>
    For example, if you set the watchdog timeout to <literal>120</literal>,
    set the <literal>msgwait</literal> timeout to <literal>240</literal> and the
    <literal>stonith-timeout</literal> to <literal>288</literal>.
   </para>
    <para>
     If you use the <package>ha-cluster-bootstrap</package> scripts to set up a
     cluster and to initialize the SBD device, the relationship between these
     timeouts is automatically considered.
    </para>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-watchdog">
  <title>Setting up the watchdog</title>
  <para> &productname; ships with several kernel modules that provide
   hardware-specific watchdog drivers. For a list of the most commonly used
   ones, see <xref
   linkend="tab-ha-storage-protect-watchdog-drivers" xrefstyle="select:title
   nopage"/>.
 </para>
 <para>
  For clusters in production environments we recommend to use a hardware-specific
  watchdog driver. However, if no watchdog matches your hardware,
  <systemitem class="resource">softdog</systemitem> can be used as kernel
  watchdog module.
 </para>
 <para>
   The &hasi; uses the SBD daemon as the software component that <quote>feeds</quote>
   the watchdog.</para>

  <sect2 xml:id="sec-ha-storage-protect-hw-watchdog">
   <title>Using a hardware watchdog</title>

   <para>Finding the right watchdog kernel module for a given system is not
    trivial. Automatic probing fails very often. As a result, lots of modules
    are already loaded before the right one gets a chance.</para>

  <para>
   <xref linkend="tab-ha-storage-protect-watchdog-drivers" xrefstyle="select:label"/>
   lists the most commonly used watchdog drivers. If your hardware is not listed there,
   the directories
   <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/watchdog</filename>
   or
   <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/ipmi</filename>
   give you a list of choices, too. Alternatively, ask your hardware or
   system vendor for details on system specific watchdog configuration.
   </para>

     <table xml:id="tab-ha-storage-protect-watchdog-drivers">
        <title>Commonly used watchdog drivers</title>
        <tgroup cols="2">
         <thead>
          <row>
           <entry>Hardware</entry>
           <entry>Driver</entry>
          </row>
         </thead>
         <tbody>
          <row>
           <entry>HP</entry>
           <entry><systemitem class="resource">hpwdt</systemitem></entry>
          </row>
          <row>
           <entry>Dell, Lenovo (Intel TCO)</entry>
           <entry><systemitem class="resource">iTCO_wdt</systemitem></entry>
          </row>
          <row>
           <entry>Fujitsu</entry>
           <entry><systemitem class="resource">ipmi_watchdog</systemitem></entry>
          </row>
          <row>
           <entry>VM on z/VM on IBM mainframe</entry>
           <entry><systemitem class="resource">vmwatchdog</systemitem></entry>
          </row>
          <row>
           <entry>Xen VM (DomU)</entry>
           <entry><systemitem class="resource">xen_xdt</systemitem></entry>
          </row>
          <row>
           <entry>Generic</entry>
           <entry><systemitem class="resource">softdog</systemitem></entry>
          </row>
         </tbody>
        </tgroup>
       </table>

 <important>
    <title>Accessing the watchdog timer</title>
    <para>Some hardware vendors ship systems management software that uses the
     watchdog for system resets (for example, HP ASR daemon). If the watchdog is
     used by SBD, disable such software. No other software must access the
     watchdog timer. </para>
   </important>

   <procedure xml:id="pro-ha-storage-protect-watchdog">
    <title>Loading the correct kernel module</title>
    <para>To make sure the correct watchdog module is loaded, proceed as follows:</para>
     <step>
      <para>List the drivers that have been installed with your kernel version:</para>
       <screen>&prompt.root;<command>rpm</command> -ql kernel-<replaceable>VERSION</replaceable> | <command>grep</command> watchdog</screen>
      </step>
      <step xml:id="st-ha-storage-listwatchdog-modules">
       <para>List any watchdog modules that are currently loaded in the kernel:</para>
       <screen>&prompt.root;<command>lsmod</command> | <command>egrep</command> "(wd|dog)"</screen>
      </step>
      <step>
       <para>If you get a result, unload the wrong module:</para>
       <screen>&prompt.root;<command>rmmod</command> <replaceable>WRONG_MODULE</replaceable></screen>
      </step>
      <step>
     <para> Enable the watchdog module that matches your hardware: </para>
     <screen>&prompt.root;<command>echo</command> <replaceable>WATCHDOG_MODULE</replaceable> > /etc/modules-load.d/watchdog.conf
&prompt.root;<command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
     <para>Test whether the watchdog module is loaded correctly:</para>
     <screen>&prompt.root;<command>lsmod</command> | <command>grep</command> dog</screen>
    </step>
    <step>
     <para>Verify if the watchdog device is available and works:</para>
     <screen>&prompt.root;<command>ls -l</command> /dev/watchdog*
&prompt.root;<command>sbd</command> query-watchdog</screen>
     <para> If your watchdog device is not available, stop here and check the
      module name and options. Maybe use another driver. </para>
    </step>
    <step>
     <para>
      Verify if the watchdog device works:
     </para>
     <screen>&prompt.root;<command>sbd</command> -w <replaceable>WATCHDOG_DEVICE</replaceable> test-watchdog</screen>
    </step>
    <step>
     <para>
      Reboot your machine to make sure there are no conflicting kernel modules. For example,
      if you find the message <literal>cannot register ...</literal> in your log, this would indicate
      such conflicting modules. To ignore such modules, refer to <link
       xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-mod.html#sec-mod-modprobe-blacklist"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-storage-protect-sw-watchdog">
   <title>Using the software watchdog (softdog)</title>
   <para>
    For clusters in production environments we recommend to use a hardware-specific watchdog
    driver. However, if no watchdog matches your hardware, <systemitem class="resource"
    >softdog</systemitem> can be used as kernel watchdog module. </para>

   <important>
    <title>Softdog limitations</title>
    <para>
     The softdog driver assumes that at least one CPU is still running. If all
     CPUs are stuck, the code in the softdog driver that should reboot the system
     will never be executed. In contrast, hardware watchdogs keep working even
     if all CPUs are stuck.
    </para>
   </important>

   <procedure xml:id="pro-ha-storage-protect-sw-watchdog">
    <title>Loading the softdog kernel module</title>
    <step>
     <para>Enable the softdog driver:</para>
     <screen>&prompt.root;<command>echo</command> softdog > /etc/modules-load.d/watchdog.conf</screen>
    </step>
    <step>
     <para>Add the <systemitem class="resource">softdog</systemitem>
          module in <filename>/etc/modules-load.d/watchdog.conf</filename>
          and restart a service:</para>
         <screen>&prompt.root;<command>echo</command> softdog > /etc/modules-load.d/watchdog.conf
&prompt.root;<command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
     <para>Test whether the softdog watchdog module is loaded correctly:</para>
     <screen>&prompt.root;<command>lsmod</command> | <command>grep</command> softdog</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-fencing-setup">
  <title>Setting up SBD with devices</title>
  <para>
   The following steps are necessary for setup:
  </para>
 <procedure>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-sbd-create" xrefstyle="select:title"/>
        </para>
   </step>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-sbd-config" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-sbd-services" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-sbd-test" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro-ha-storage-protect-fencing" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>
  <para>
    Before you start, make sure the block device or devices you want to use for
    SBD meet the requirements specified in <xref linkend="sec-ha-storage-protect-req"
    xrefstyle="select:label"/>.
  </para>
  <para>
   When setting up the SBD devices, you need to take several timeout values into
   account. For details, see <xref linkend="sec-ha-storage-protect-watchdog-timings"/>.
  </para>
  <para>
   The node will terminate itself if the SBD daemon running on it has not
   updated the watchdog timer fast enough. After having set the timeouts, test
   them in your specific environment.
  </para>

  <procedure xml:id="pro-ha-storage-protect-sbd-create">
   <title>Initializing the SBD devices</title>
   <para>
    To use SBD with shared storage, you must first create the messaging
    layout on one to three block devices. The <command>sbd create</command> command
    will write a metadata header to the specified device or devices. It will also
    initialize the messaging slots for up to 255 nodes. If executed without any
    further options, the command will use the default timeout settings.</para>
    <warning>
     <title>Overwriting existing data</title>
      <para> Make sure the device or devices you want to use for SBD do not hold any
       important data. When you execute the <command>sbd create</command>
       command, roughly the first megabyte of the specified block devices
       will be overwritten without further requests or backup.
      </para>
    </warning>
    <step>
     <para>Decide which block device or block devices to use for SBD.</para>
    </step>
    <step>
     <para>Initialize the SBD device with the following command: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> create</screen>
     <para>(Replace <filename>/dev/<replaceable>SBD</replaceable></filename>
       with your actual path name, for example:
       <filename>/dev/disk/by-id/scsi-ST2000DM001-0123456_Wabcdefg</filename>.)</para>
        <para> To use more than one device for SBD, specify the <option>-d</option> option multiple times, for
      example: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD1</replaceable> -d /dev/<replaceable>SBD2</replaceable> -d /dev/<replaceable>SBD3</replaceable> create</screen>
    </step>
    <step>
     <para>If your SBD device resides on a multipath group, use the <option>-1</option>
      and <option>-4</option> options to adjust the timeouts to use for SBD. For
      details, see <xref linkend="sec-ha-storage-protect-watchdog-timings"/>.
      All timeouts are given in seconds:</para>
      <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> -4 180<co xml:id="co-ha-sbd-msgwait"/> -1 90<co xml:id="co-ha-sbd-watchdog"/> create</screen>
     <calloutlist>
      <callout arearefs="co-ha-sbd-msgwait">
       <para> The <option>-4</option> option is used to specify the
         <literal>msgwait</literal> timeout. In the example above, it is set to
         <literal>180</literal> seconds. </para>
      </callout>
      <callout arearefs="co-ha-sbd-watchdog">
       <para> The <option>-1</option> option is used to specify the
         <literal>watchdog</literal> timeout. In the example above, it is set
        to <literal>90</literal> seconds. The minimum allowed value for the
        emulated watchdog is <literal>15</literal> seconds. </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>Check what has been written to the device: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> dump
Header version     : 2.1
UUID               : 619127f4-0e06-434c-84a0-ea82036e144c
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 5
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 10
==Header on disk /dev/<replaceable>SBD</replaceable> is dumped</screen>
    <para> As you can see, the timeouts are also stored in the header, to ensure
    that all participating nodes agree on them. </para>
    </step>
   </procedure>
   <para>
    After you have initialized the SBD devices, edit the SBD configuration file,
    then enable and start the respective services for the changes to take effect.
   </para>

   <procedure xml:id="pro-ha-storage-protect-sbd-config">
   <title>Editing the SBD configuration file</title>
    <step>
     <para>Open the file <filename>/etc/sysconfig/sbd</filename> and use
      the following entries:</para>
     <screen>SBD_PACEMAKER=yes
SBD_STARTMODE=always
SBD_DELAY_START=no
SBD_WATCHDOG_DEV=/dev/watchdog
SBD_WATCHDOG_TIMEOUT=5</screen>
      <para>
       The <varname>SBD_DEVICE</varname> entry is not needed as no shared
       disk is used. When this parameter is missing, the <systemitem>sbd</systemitem>
       service does not start any watcher process for SBD devices.
      </para>
    </step>
    <step>
     <para>Search for the following parameter: <parameter>SBD_DEVICE</parameter>.
     </para>
     <para>It specifies the devices to monitor and to use for exchanging SBD messages.
     </para>
   </step>
   <step>
    <para> Edit this line by replacing <replaceable>SBD</replaceable> with your SBD device:</para>
    <screen>SBD_DEVICE="/dev/<replaceable>SBD</replaceable>"</screen>
    <para> If you need to specify multiple devices in the first line, separate them with semicolons
     (the order of the devices does not matter):</para>
    <screen>SBD_DEVICE="/dev/<replaceable>SBD1</replaceable>;/dev/<replaceable>SBD2</replaceable>;/dev/<replaceable>SBD3</replaceable>"</screen>
    <para> If the SBD device is not accessible, the daemon will fail to start and inhibit
     cluster start-up. </para>
   </step>
   <step xml:id="st-ha-storage-protect-sbd-delay-start">
    <para>Search for the following parameter: <parameter>SBD_DELAY_START</parameter>.</para>
    <para>
      Enables or disables a delay. Set <parameter>SBD_DELAY_START</parameter>
      to <literal>yes</literal> if <literal>msgwait</literal> is relatively
      long, but your cluster nodes boot very fast.
      Setting this parameter to <literal>yes</literal> delays the start of
      SBD on boot.  This is sometimes necessary with virtual machines.
    </para>
   </step>
  </procedure>

 <para>After you have added your SBD devices to the SBD configuration file,
  enable the SBD daemon. The SBD daemon is a critical piece
  of the cluster stack. It needs to be running when the cluster stack is running.
  Thus, the <systemitem>sbd</systemitem> service is started as a dependency whenever
  the <systemitem>pacemaker</systemitem> service is started.</para>

  <procedure xml:id="pro-ha-storage-protect-sbd-services">
   <title>Enabling and starting the SBD service</title>
   <step>
    <para>On each node, enable the SBD service:</para>
    <screen>&prompt.root;<command>systemctl</command> enable sbd</screen>
    <para>It will be started together with the Corosync service whenever the Pacemaker
     service is started.</para>
   </step>
   <step>
    <para>Restart the cluster stack on each node:</para>
    <screen>&prompt.root;<command>crm</command> cluster restart</screen>
    <para> This automatically triggers the start of the SBD daemon. </para>
   </step>
  </procedure>

  <para>
   As a next step, test the SBD devices as described in <xref
   linkend="pro-ha-storage-protect-sbd-test" xrefstyle="select:label"/>.
  </para>

  <procedure xml:id="pro-ha-storage-protect-sbd-test">
   <title>Testing the SBD devices</title>
    <step>
     <para> The following command will dump the node slots and their current
      messages from the SBD device: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> list</screen>
    <para> Now you should see all cluster nodes that have ever been started with SBD listed here.
     For example, if you have a two-node cluster, the message slot should show
      <literal>clear</literal> for both nodes:</para>
     <screen>0       &node1;        clear
1       &node2;          clear</screen>
    </step>
    <step>
     <para> Try sending a test message to one of the nodes: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> message &node1; test</screen>
    </step>
    <step>
     <para> The node will acknowledge the receipt of the message in the system
      log files: </para>
     <screen>May 03 16:08:31 &node1; sbd[66139]: /dev/<replaceable>SBD</replaceable>: notice: servant: Received command test from &node2; on disk /dev/<replaceable>SBD</replaceable></screen>
     <para> This confirms that SBD is indeed up and running on the node and
      that it is ready to receive messages. </para>
    </step>
   </procedure>

  <para>
   As a final step, you need to adjust the cluster configuration as described in
   <xref linkend="pro-ha-storage-protect-fencing" xrefstyle="select:label"/>.
  </para>

<procedure xml:id="pro-ha-storage-protect-fencing">
 <title>Configuring the cluster to use SBD</title>
   <para>
    To configure the use of SBD in the cluster, you need to do the following in
    the cluster configuration:
   </para>
   <itemizedlist>
    <listitem>
     <para>
       Set the <parameter>stonith-timeout</parameter> parameter to a value that
       matches your setting.
     </para>
    </listitem>
    <listitem>
     <para>
      Configure the SBD &stonith; resource.
     </para>
    </listitem>
   </itemizedlist>
   <para>
     For the calculation of the <parameter>stonith-timeout</parameter> refer to
     <xref linkend="sec-ha-storage-protect-watchdog-timings"/>.
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>Enter the following:</para>
    <screen>
&prompt.crm.conf;<command>property</command> stonith-enabled="true" <co xml:id="co-ha-sbd-st-enabled"/>
&prompt.crm.conf;<command>property</command> stonith-watchdog-timeout=0 <co xml:id="co-ha-sbd-watchdog-timeout"/>
&prompt.crm.conf;<command>property</command> stonith-timeout="40s" <co xml:id="co-ha-sbd-st-timeout"
/></screen>
    <calloutlist>
     <callout arearefs="co-ha-sbd-st-enabled">
      <para>
       This is the default configuration, because clusters without &stonith; are not supported.
       But in case &stonith; has been deactivated for testing purposes,
       make sure this parameter is set to <literal>true</literal> again.</para>
     </callout>
     <callout arearefs="co-ha-sbd-watchdog-timeout">
      <para>If not explicitly set, this value defaults to <literal>0</literal>,
        which is appropriate for use of SBD with one to three devices.
      </para>
     </callout>
     <callout arearefs="co-ha-sbd-st-timeout">
      <para>
       A <systemitem>stonith-timeout</systemitem> value of <literal>40</literal>
       would be appropriate if the <literal>msgwait</literal> timeout value for
       SBD was set to <literal>30</literal> seconds.</para>
     </callout>
   </calloutlist>
  </step>
  <step xml:id="st-ha-storage-protect-fencing-static-random">
   <para>
    For a two-node cluster, in case of split-brain, there will be fencing issued from
    each node to the other as expected. To prevent both nodes from being reset at practically
    the same time, it is recommended to apply the following fencing
    delays to help one of the nodes, or even the preferred node, win the fencing match.
    For clusters with more than two nodes, you do not need to apply these delays.
   </para>
   <variablelist>
    <varlistentry>
     <term>Priority fencing delay</term>
     <listitem>
       <para>
        The <literal>priority-fencing-delay</literal> cluster property is disabled by
        default. By configuring a delay value, if the other node is lost and it has
        the higher total resource priority, the fencing targeting it will be delayed
        for the specified amount of time. This means that in case of split-brain,
        the more important node wins the fencing match .
      </para>
      <para>
        Resources that matter can be configured with priority meta attribute. On
        calculation, the priority values of the resources or instances that are running
        on each node are summed up to be accounted. A promoted resource instance takes the
        configured base priority plus one, so that it receives a higher value than any
        unpromoted instance.
      </para>
      <screen>&prompt.root;<command>crm</command> configure property priority-fencing-delay=30</screen>
       <para>
        Even if <literal>priority-fencing-delay</literal> is used, we still
        recommend also using <literal>pcmk_delay_base</literal> or
        <literal>pcmk_delay_max</literal> as described below to address any
        situations where the nodes happen to have equal priority.
        The value of <literal>priority-fencing-delay</literal> should be significantly
        greater than the maximum of <literal>pcmk_delay_base</literal> / <literal>pcmk_delay_max</literal>, 
        and preferably twice the maximum.
       </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Predictable static delays</term>
     <listitem>
      <para>This parameter adds a static delay before executing &stonith; actions.
      To prevent the nodes from getting reset at the same time under split-brain of
      a two-node cluster, configure separate fencing resources with different delay values.
      The preferred node can be marked with the parameter to be targeted with a longer
      fencing delay, so that it wins any fencing match.
      To make this succeed, it is essential to create two primitive &stonith;
      devices for each node. In the following configuration, &node1; will win
      and survive in case of a split brain scenario:
      </para>
      <screen>&prompt.crm.conf;<command>primitive</command> st-sbd-&node1; stonith:external/sbd params \
       pcmk_host_list=&node1; pcmk_delay_base=20
&prompt.crm.conf;<command>primitive</command> st-sbd-&node2; stonith:external/sbd params \
       pcmk_host_list=&node2; pcmk_delay_base=0</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Dynamic random delays</term>
     <listitem>
      <para>This parameter adds a random delay for &stonith; actions on the fencing device.
       Rather than a static delay targeting a specific node, the parameter
       <parameter>pcmk_delay_max</parameter> adds a random delay for any fencing
       with the fencing resource to prevent double reset. Unlike
       <parameter>pcmk_delay_base</parameter>, this parameter can be specified for
       an unified fencing resource targeting multiple nodes.
      </para>
      <screen>&prompt.crm.conf;<command>primitive</command> stonith_sbd stonith:external/sbd
  params pcmk_delay_max=30</screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </step>
  <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     Submit your changes with <command>commit</command> and leave the crm live
     configuration with <command>quit</command>.
    </para>
   </step>
  </procedure>

   <para> After the resource has started, your cluster is successfully
    configured for use of SBD. It will use this method in case a
    node needs to be fenced.</para>
  </sect1>

  <sect1 xml:id="sec-ha-storage-protect-diskless-sbd">
   <title>Setting up diskless SBD</title>
   <para>SBD can be operated in a diskless mode. In this mode, a watchdog device
    will be used to reset the node in the following cases: if it loses quorum,
    if any monitored daemon is lost and not recovered, or if Pacemaker decides
    that the node requires fencing. Diskless SBD is based on
    <quote>self-fencing</quote> of a node, depending on the status of the cluster,
    the quorum and some reasonable assumptions. No &stonith; SBD resource
    primitive is needed in the CIB.
   </para>
    <important>
     <title>Number of cluster nodes</title>
       <remark>toms 2020-05-14: yan: there are still some self-contradictions
        here, but I don't know how to make it better :-)</remark>
       <para>
         Do <emphasis>not</emphasis> use diskless SBD as a fencing mechanism
         for two-node clusters.
         Use diskless SBD only for clusters with three or more nodes.
         SBD in diskless mode cannot handle split brain scenarios for two-node clusters.
         If you want to use diskless SBD for two-node clusters, use &qdevice; as
         described in <xref linkend="cha-ha-qdevice"/>.
      </para>
   </important>

   <procedure xml:id="pro-ha-storage-protect-confdiskless">
    <title>Configuring diskless SBD</title>
    <step>
     <para>Open the file <filename>/etc/sysconfig/sbd</filename> and use
      the following entries:</para>
     <screen>SBD_PACEMAKER=yes
SBD_STARTMODE=always
SBD_DELAY_START=no
SBD_WATCHDOG_DEV=/dev/watchdog
SBD_WATCHDOG_TIMEOUT=5</screen>
      <para>
       The <varname>SBD_DEVICE</varname> entry is not needed as no shared
       disk is used. When this parameter is missing, the <systemitem>sbd</systemitem>
       service does not start any watcher process for SBD devices.
      </para>
    </step>
    <step>
     <para>On each node, enable the SBD service:</para>
     <screen>&prompt.root;<command>systemctl</command> enable sbd</screen>
     <para>It will be started together with the Corosync service whenever the Pacemaker
      service is started.</para>
    </step>
    <step>
     <para>Restart the cluster stack on each node:</para>
     <screen>&prompt.root;<command>crm</command> cluster restart</screen>
     <para> This automatically triggers the start of the SBD daemon. </para>
    </step>
    <step>
      <para>
       Check if the parameter <parameter>have-watchdog=true</parameter> has
       been automatically set:
      </para>
      <screen>&prompt.root;<command>crm</command> configure show | grep have-watchdog
         have-watchdog=true</screen>
    </step>
    <step>
     <para>Run <command>crm configure</command> and set the following cluster
      properties on the &crmshell;:</para>
<screen>&prompt.crm.conf;<command>property</command> stonith-enabled="true" <co
        xml:id="co-ha-sbd-stonith-enabled"/>
&prompt.crm.conf;<command>property</command> stonith-watchdog-timeout=10 <co
        xml:id="co-ha-sbd-diskless-watchdog-timeout"/></screen>
    <calloutlist>
     <callout arearefs="co-ha-sbd-stonith-enabled">
      <para>
       This is the default configuration, because clusters without &stonith; are not supported.
       But in case &stonith; has been deactivated for testing purposes,
       make sure this parameter is set to <literal>true</literal> again.</para>
     </callout>
     <callout arearefs="co-ha-sbd-diskless-watchdog-timeout">
      <para>For diskless SBD, this parameter must not equal zero.
       It defines after how long it is assumed that the fencing target has already
       self-fenced. Therefore its value needs to be &gt;= the value of
       <varname>SBD_WATCHDOG_TIMEOUT</varname> in <filename>/etc/sysconfig/sbd</filename>.
       Starting with &productname; 15, if you set <parameter>stonith-watchdog-timeout</parameter>
       to a negative value, Pacemaker will automatically calculate this timeout
       and set it to twice the value of <parameter>SBD_WATCHDOG_TIMEOUT</parameter>.
      </para>
     </callout>
    </calloutlist>
   </step>
  <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     Submit your changes with <command>commit</command> and leave the crm live
     configuration with <command>quit</command>.
    </para>
   </step>
  </procedure>
  </sect1>

  <sect1 xml:id="sec-ha-storage-protect-test">
   <title>Testing SBD and fencing</title>
   <para>To test whether SBD works as expected for node fencing purposes, use one or all
    of the following methods:
   </para>
  <variablelist>
   <varlistentry>
    <term>Manually triggering fencing of a node</term>
    <listitem>
     <para>To trigger a fencing action for node <replaceable>NODENAME</replaceable>:</para>
 <screen>&prompt.root;<command>crm</command> node fence <replaceable>NODENAME</replaceable></screen>
     <para>Check if the node is fenced and if the other nodes consider the node as fenced
      after the <parameter>stonith-watchdog-timeout</parameter>.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Simulating an SBD failure</term>
    <listitem>
     <procedure>
      <step>
       <para>Identify the process ID of the SBD inquisitor:</para>
       <screen>&prompt.root;<command>systemctl</command> status sbd
● sbd.service - Shared-storage based fencing daemon

   Loaded: loaded (/usr/lib/systemd/system/sbd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-04-17 15:24:51 CEST; 6 days ago
     Docs: man:sbd(8)
  Process: 1844 ExecStart=/usr/sbin/sbd $SBD_OPTS -p /var/run/sbd.pid watch (code=exited, status=0/SUCCESS)
 Main PID: 1859 (sbd)
    Tasks: 4 (limit: 4915)
   CGroup: /system.slice/sbd.service
           ├─<emphasis role="strong">1859 sbd: inquisitor</emphasis>
[...]</screen>
      </step>
      <step>
       <para>Simulate an SBD failure by terminating the SBD inquisitor process.
       In our example, the process ID of the SBD inquisitor is
         <literal>1859</literal>):</para>
       <screen>&prompt.root;<command>kill</command> -9 1859 </screen>
       <para>
        The node proactively self-fences. The other nodes notice the loss of
        the node and consider it has self-fenced after the
        <parameter>stonith-watchdog-timeout</parameter>.
       </para>
      </step>
     </procedure>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Triggering fencing through a monitor operation failure</term>
    <listitem>
     <para>With a normal configuration, a failure of a resource <emphasis>stop operation</emphasis>
      will trigger fencing. To trigger fencing manually, you can produce a failure
      of a resource stop operation. Alternatively, you can temporarily change
      the configuration of a resource <emphasis>monitor operation</emphasis>
      and produce a monitor failure as described below:</para>
     <procedure>
      <step>
       <para>Configure an <literal>on-fail=fence</literal> property for a resource monitor
        operation:</para>
       <screen>op monitor interval=10 on-fail=fence</screen>
      </step>
      <step>
       <para>Let the monitoring operation fail (for example, by terminating the respective
        daemon, if the resource relates to a service).</para>
       <para>This failure triggers a fencing action.</para>
      </step>
     </procedure>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-rsc-fencing">
  <title>Additional mechanisms for storage protection</title>
  <remark>toms 2018-04-20: this can be improved...</remark>
  <para>Apart from node fencing via &stonith; there are other methods to achieve
    storage protection at a resource level. For example, SCSI-3 and SCSI-4 use
    persistent reservations whereas <literal>sfex</literal> provides a locking
    mechanism. Both methods are explained in the following subsections.
  </para>
  <sect2 xml:id="sec-ha-storage-protect-sgpersist">
   <title>Configuring an sg_persist resource</title>
   <remark>toms 2018-04-20: I would like to see a little bit more background
   information. </remark>
   <para>
    The SCSI specifications 3 and 4 define <emphasis>persistent reservations</emphasis>.
    These are SCSI protocol features and can be used for I/O fencing and failover.
    This feature is implemented in the <command>sg_persist</command> Linux
    command.
   </para>
   <note>
    <title>SCSI disk compatibility</title>
    <para> Any backing disks for <literal>sg_persist</literal> must be SCSI
     disk compatible. <literal>sg_persist</literal> only works for devices like
     SCSI disks or iSCSI LUNs.
     <remark>toms 2018-04-20: What about FCoE, FC, iSER, SRP, Serial Attached SCSI (SAR)?</remark>
     Do <emphasis>not</emphasis> use it for IDE, SATA, or any block devices
     which do not support the SCSI protocol. </para>
   </note>
   <para>Before you proceed, check if your disk supports
    persistent reservations. Use the following command (replace
     <replaceable>DISK</replaceable> with your device name):</para>
    <screen>&prompt.root;<command>sg_persist</command> -n --in --read-reservation -d /dev/<replaceable>DISK</replaceable></screen>
   <para>The result shows whether your disk supports persistent reservations:</para>
    <itemizedlist>
     <listitem>
      <para>Supported disk:</para>
      <screen>PR generation=0x0, there is NO reservation held</screen>
     </listitem>
     <listitem>
      <para>Unsupported disk:</para>
      <screen>PR in (Read reservation): command not supported
Illegal request, Invalid opcode</screen>
     </listitem>
    </itemizedlist>

   <remark>toms 2018-04-20: Do we need to prepare anything else with sg_persists?</remark>
   <para>If you get an error message (like the one above), replace the old
    disk with an SCSI compatible disk. Otherwise proceed as follows:</para>
   <procedure>
    <step>
     <para>
      To create the primitive resource <literal>sg_persist</literal>,
      run the following commands as &rootuser;: </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> sg sg_persist \
    params devs="/dev/sdc" reservation_type=3 \
    op monitor interval=60 timeout=60</screen>
    </step>
    <step>
     <para> Add the <literal>sg_persist</literal> primitive to a master-slave
      group: <remark>taroth 2018-03-05: ygao, should 'master-max' be replaced
       with 'promoted-max' or does the screen below need more changes? - ygao
       2018-03-13: the new names are not explicitly promoted in crmsh
       yet</remark>
     </para>
     <screen>&prompt.crm.conf;<command>ms</command> ms-sg sg \
    meta master-max=1 notify=true</screen>
    </step>
    <step>
     <para> Do some tests. When the resource is in master/slave status, you can
      mount and write on <filename>/dev/sdc1</filename> on the cluster node where
      the master instance is running, while you cannot write on the cluster node
      where the slave instance is running.</para>
    </step>
    <step>
     <para> Add a file system primitive for Ext4: </para>
     <screen>&prompt.crm.conf;<command>primitive</command> ext4 Filesystem \
    params device="/dev/sdc1" directory="/mnt/ext4" fstype=ext4</screen>
    </step>
    <step>
     <para> Add the following order relationship plus a collocation between the
      <literal>sg_persist</literal> master and the file system resource: </para>
     <screen>&prompt.crm.conf;<command>order</command> o-ms-sg-before-ext4 Mandatory: ms-sg:promote ext4:start
&prompt.crm.conf;<command>colocation</command> col-ext4-with-sg-persist inf: ext4 ms-sg:Master</screen>
    </step>
    <step>
     <para> Check all your changes with the <command>show changed</command> command.
     </para>
    </step>
    <step>
     <para> Commit your changes. </para>
    </step>
   </procedure>
   <para>For more information, refer to the <command>sg_persist</command> man
    page.</para>
  </sect2>

  <sect2 xml:id="sec-ha-storage-protect-exstoract">
   <title>Ensuring exclusive storage activation with <literal>sfex</literal></title>
    <para>
     <remark>taroth 2018-04-26: ToDo - for next release, revise this section, too,
     and flatten its structure</remark>
    This section introduces <literal>sfex</literal>, an additional low-level
    mechanism to lock access to shared storage exclusively to one node. Note
    that sfex does not replace &stonith;. As sfex requires shared
    storage, it is recommended that the SBD node fencing mechanism described
    above is used on another partition of the storage.
   </para>

   <para>
    By design, sfex cannot be used with workloads that require concurrency
    (such as &ocfs;). It serves as a layer of protection for classic failover
    style workloads. This is similar to an SCSI-2 reservation in effect, but
    more general.
   </para>

   <sect3 xml:id="sec-ha-storage-protect-exstoract-description">
    <title>Overview</title>
    <para>
     In a shared storage environment, a small partition of the storage is set
     aside for storing one or more locks.
    </para>
    <para>
     Before acquiring protected resources, the node must first acquire the
     protecting lock. The ordering is enforced by Pacemaker. The sfex
     component ensures that even if Pacemaker were subject to a split brain
     situation, the lock will never be granted more than once.
    </para>
    <para>
     These locks must also be refreshed periodically, so that a node's death
     does not permanently block the lock and other nodes can proceed.
    </para>
   </sect3>

   <sect3 xml:id="sec-ha-storage-protect-exstoract-requirements">
    <title>Setup</title>
    <para>
     In the following, learn how to create a shared partition for use with
     sfex and how to configure a resource for the sfex lock in the CIB. A
     single sfex partition can hold any number of locks, and needs 1 KB
     of storage space allocated per lock.
     By default, <command>sfex_init</command> creates one lock on the partition.
    </para>
    <important>
     <title>Requirements</title>
     <itemizedlist>
      <listitem>
       <para>
        The shared partition for sfex should be on the same logical unit as
        the data you want to protect.
       </para>
      </listitem>
      <listitem>
       <para>
        The shared sfex partition must not use host-based RAID, nor DRBD.
       </para>
      </listitem>
      <listitem>
       <para>
        Using an LVM2 logical volume is possible.
       </para>
      </listitem>
     </itemizedlist>
    </important>
    <procedure>
     <title>Creating an sfex partition</title>
     <step>
      <para>
       Create a shared partition for use with sfex. Note the name of this
       partition and use it as a substitute for
       <filename>/dev/sfex</filename> below.
      </para>
     </step>
     <step>
      <para>
       Create the sfex metadata with the following command:
      </para>
      <screen>&prompt.root;<command>sfex_init</command> -n 1 /dev/sfex</screen>
     </step>
     <step>
      <para>
       Verify that the metadata has been created correctly:
      </para>
      <screen>&prompt.root;<command>sfex_stat</command> -i 1 /dev/sfex ; echo $?</screen>
      <para>
       This should return <literal>2</literal>, since the lock is not
       currently held.
      </para>
     </step>
    </procedure>
    <procedure>
     <title>Configuring a resource for the sfex lock</title>
     <step>
      <para>
       The sfex lock is represented via a resource in the CIB, configured as
       follows:
      </para>
      <screen>&prompt.crm.conf;<command>primitive</command> sfex_1 ocf:heartbeat:sfex \
#	params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
#	op monitor interval="10s" timeout="30s" on-fail="fence"</screen>
     </step>
     <step>
      <para>
       To protect resources via an sfex lock, create mandatory ordering and
       placement constraints between the resources to protect the sfex resource. If
       the resource to be protected has the ID
       <literal>filesystem1</literal>:
      </para>
      <screen>&prompt.crm.conf;<command>order</command> order-sfex-1 Mandatory: sfex_1 filesystem1
&prompt.crm.conf;<command>colocation</command> col-sfex-1 inf: filesystem1 sfex_1</screen>
     </step>
     <step>
      <para>
       If using group syntax, add the sfex resource as the first resource to
       the group:
      </para>
      <screen>&prompt.crm.conf;<command>group</command> LAMP sfex_1 filesystem1 apache ipaddr</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-storage-protect-moreinfo">
  <title>For more information</title>

  <itemizedlist>
    <listitem>
      <para><command>man sbd</command></para>
    </listitem>
     <listitem>
      <para><link xlink:href="http://www.linux-ha.org/wiki/SBD_Fencing"/></para>
    </listitem>
  </itemizedlist>
 </sect1>
</chapter>
