<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!--taroth 2010-03-19: some sections have IDs that do not match our style guide
 conventions because of the last minute changes down here, no time left to fix this now-->
<!--taroth 2012-01-16: for next revision, check completely against 
    http://www.linux-ha.org/wiki/SBD_Fencing-->
<chapter version="5.0" xml:id="cha.ha.storage.protect"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Storage Protection</title>
 <info>
  <abstract>
   <para>
    SBD (&stonith; Block Device) provides a node fencing mechanism for
    Pacemaker-based clusters through the exchange of messages via shared block
    storage such as a SAN, iSCSI, or FCoE for example. This isolates the fencing
    mechanism from changes in firmware version or dependencies on specific
    firmware controllers. SBD can be used as a &stonith; mechanism in all
    configurations that have reliable shared storage, but it can also used in
    <literal>diskless mode</literal>, that means, without any shored storage.
   </para>
    <!--taroth 2018-04-20: commenting too not make the abstract too long,
     maybe it can be sued in the diskless SBD section?
     In this mode, a watchdog device will be used to reset the node in the following cases:
    if it loses quorum, if any monitored daemon is lost and not recovered or if
    Pacemaker decides that the node requires fencing.-->
   <para>
    The <package>ha-cluster-bootstrap</package> scripts provide a quick way to
    set up a cluster with the option of using SBD as fencing mechanism. The
    bootstrap scripts automatically execute all required steps to set up SBD as
    fencing mechanism. For automatic setup, see the
    <xref linkend="art.sleha.install.quick"/>.
   </para>
   <para>
    However, manually setting up SBD provides you with more options regarding
    the individual settings. This chapter explains the concepts behind SBD and
    guides you through configuring the components that SBD needs to protect
    your cluster from potential data corruption in case of a split brain scenario.
   </para>
  </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec.ha.storage.overview">
      <title>Conceptual Overview</title>
      <para>SBD expands to <emphasis>Storage-Based Death</emphasis> or
        <emphasis>STONITH Block Device</emphasis>.
      <remark>taroth 2018-04-18: in ha_glossary.xml, we even introduce a different
      term: '&stonith; by disk' - we should consolidate this. I would suggest to
      use the term that is mentioned in 'man sbd': STONITH Block Device</remark></para>
      <para>
        The highest priority of the &ha; cluster stack is to protect the integrity
        of data. This is achieved by preventing uncoordinated concurrent access
        to data storage. The cluster stack takes care of this using several
        control mechanisms.
      </para>
      <para>
        However, network partitioning or software malfunction could potentially
        cause scenarios where several DCs are elected in a cluster. If this
        so-called split brain scenarios were allowed to unfold, data corruption
        might occur.
      </para>
      <para>
        Server fencing via &stonith; is the primary mechanism to prevent this.
        Using SBD as server fencing mechanism is one way of shutting down nodes
        without external power off device in case of a split brain scenario.
      </para>
      <para>
        Additional mechanisms for storage protection are LVM2 exclusive activation
        or OCFS2 file locking support.
        <remark>taroth 2018-04-20: FIXME add link!</remark>
        They protect your system against administrative or application faults.
        Combined appropriately for your setup, they can reliably prevent split
        brain scenarios from causing harm.
      </para>

  <variablelist>
   <title>SBD Components and Mechanisms</title>
   <varlistentry>
    <term>SBD Partition</term>
    <listitem>
     <para> In an environment where all nodes have access to shared storage, a
      small partition of the device is formatted for use with SBD. The size of
      the partition depends on the block size of the used disk (for example,
      1&nbsp;MB for standard SCSI disks with 512&nbsp;Byte block size or
      4&nbsp;MB for DASD disks with 4&nbsp;kB block size). The initialization
      process creates a message layout on the device with slots for up to 255
      nodes.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD Daemon</term>
    <listitem>
     <para> After the respective SBD daemon is configured, it is brought online
      on each node before the rest of the cluster stack is started. It is
      terminated after all other cluster components have been shut down, thus
      ensuring that cluster resources are never activated without SBD
      supervision. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Messages</term>
    <listitem>
     <para>
      The daemon automatically allocates one of the message slots on the
      partition to itself, and constantly monitors it for messages addressed
      to itself. Upon receipt of a message, the daemon immediately complies
      with the request, such as initiating a power-off or reboot cycle for
      fencing.
     </para>
     <para>
      The daemon constantly monitors connectivity to the storage device, and
      terminates itself in case the partition becomes unreachable. This
      guarantees that it is not disconnected from fencing messages. If the
      cluster data resides on the same logical unit in a different partition,
      this is not an additional point of failure: The work-load will terminate
      anyway if the storage connectivity has been lost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Watchdog</term>
     <listitem>
      <para>
      Increased protection is used by combining SBD with watchdog.
      Whenever SBD is used, a correctly working watchdog is crucial.
      Modern systems support a <emphasis>hardware watchdog</emphasis>
      that needs to be <quote>tickled</quote> or <quote>fed</quote> by a
      software component. The software component (in this case, the SBD daemon)
      <quote>feeds</quote> the watchdog, by regularly writing a service pulse
      to the watchdog. If the daemon stops feeding the watchdog, the hardware
      will enforce a system restart. This protects against failures of the SBD
      process itself, such as dying, or becoming stuck on an IO error.
     </para>
     </listitem>
   </varlistentry>
  </variablelist>
  <para>
   If Pacemaker integration is activated, SBD will not self-fence if device
   majority is lost. For example, your cluster contains 3 nodes: A, B, and
   C. Because of a network split, A can only see itself while B and C can
   still communicate. In this case, there are two cluster partitions, one
   with quorum because of being the majority (B, C), and one without (A).
   If this happens while the majority of fencing devices are unreachable,
   node A would immediately commit suicide, but the nodes B and C would
   continue to run.
   </para>
  </sect1>

 <sect1 xml:id="sec.ha.storage.protect.procedure">
 <title>Steps to Set Up Storage-based Protection</title>
 <para>
  The following steps are necessary to manually set up storage-based protection.
  They must be executed as &rootuser;. Before you start, check the <xref
  linkend="sec.ha.storage.protect.req" xrefstyle="sec.ha.storage.protect.req"/>.
  <remark>taroth 2018-04-19: ToDo - modify and re-add step overview after the chapter
  has reached its final structure!</remark>
 </para>
  <!--<procedure>
   <step>
    <para>
     <xref linkend="sec.ha.storage.protect.watchdog" xrefstyle="select:title"/>.
     For clusters in production environments we recommend to use a watchdog
     kernel module that matches your hardware. However, if no watchdog matches
     your hardware, <literal>softdog</literal> can be used as kernel watchdog module.
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.create" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.daemon" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.test" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.fencing" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>-->
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.req">
  <title>Requirements</title>
   <itemizedlist>
   <listitem>
    <para>You can use up to three SBD devices for storage-based fencing.
     When using 1-3 devices, the shared storage must be accessible from all nodes.</para>
   </listitem>
   <listitem>
    <para>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
     </para>
   </listitem>
   <listitem>
    <para>The shared storage can be connected via Fibre Channel (FC),
     Fibre Channel over Ethernet (FCoE), or even iSCSI. </para>
   </listitem>
   <listitem>
    <para> The shared storage segment <emphasis>must not</emphasis>
     use host-based RAID, LVM2, nor DRBD*. DRBD can be splitted, which is
     problematic for SBD, as there cannot be two states in SBD.
     Cluster multi-device (Cluster MD) cannot be used for SBD.
    </para>
   </listitem>
   <listitem>
    <para> However, using storage-based RAID and multipathing is
     recommended for increased reliability. </para>
   </listitem>
   <listitem>
    <para>An SBD device can be shared between different clusters, as
     long as no more than 255 nodes share the device. </para>
   </listitem>
   <listitem>
    <para>For clusters with more than two nodes, you can also use SBD in
    <emphasis>diskless</emphasis> mode.<remark>taroth 2018-04-18: add link to
    this (new) section after it has been added</remark>
   </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.fencing.number">
  <title>Number of SBD Devices</title>
  <!--fate#309375-->
  <para> SBD supports the use of up to three devices: </para>
  <variablelist>
   <varlistentry>
    <term>Diskless</term>
    <listitem>
     <para>&sbd-diskless;</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>One Device</term>
    <listitem>
     <para>
      The most simple implementation. It is appropriate for clusters where
      all of your data is on the same shared storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Two Devices</term>
    <listitem>
     <para>
      This configuration is primarily useful for environments that use
      host-based mirroring but where no third storage device is available.
      SBD will not terminate itself if it loses access to one mirror leg,
      allowing the cluster to continue. However, since SBD does not have
      enough knowledge to detect an asymmetric split of the storage, it
      will not fence the other side while only one mirror leg is available.
      Thus, it cannot automatically tolerate a second failure while one of
      the storage arrays is down.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Three Devices</term>
    <listitem>
     <para>
      The most reliable configuration. It is resilient against outages of
      one device&mdash;be it because of failures or maintenance. SBD
      will only terminate itself if more than one device is lost. If at least
      two devices are still accessible, fencing messages can be successfully be
      transmitted.
     </para>
     <para>
      This configuration is suitable for more complex scenarios where
      storage is not restricted to a single array. Host-based mirroring
      solutions can have one SBD per mirror leg (not mirrored itself), and
      an additional tie-breaker on iSCSI.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.watchdog.timings">
   <title>Calculation of Timeouts</title>
    <para>
      When using SBD as fencing mechanism, it is vital to consider the timeouts
      of all components that need to work together, because they depend on each
      other.
    </para>
    <variablelist>
     <varlistentry>
      <term>Watchdog Timeout</term>
      <listitem>
       <para>
        This timeout is set during initialization of the SBD device. It depends
        mostly on your storage latency. The majority of devices must be successfully
        read within this time. Otherwise, the node will self-fence.
       </para>
       <note>
        <title>Multipath or iSCSI Setup</title>
          <para>
          If your SBD device(s) reside on a multipath setup or iSCSI, the timeout
          should be  set to the time required to detect a path failure and switch
          to the next path.
          </para>
          <para>
           This also means that in <filename>/etc/multipath.conf</filename> the
           value of  <literal>max_polling_interval</literal> must be less than
           <literal>watchdog</literal> timeout.
         </para>
       </note>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>msgwait</literal> Timeout</term>
      <listitem>
       <para>
        This timeout is set during initialization of the SBD device. It defines
        the time after which a message written to a node's slot on the SBD device
        is considered delivered. The timeout should be long enough for the node to
        detect that it needs to self-fence.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>stonith-timeout</literal> in the CIB</term>
      <listitem>
       <para>
        This timeout is set in the CIB as global cluster property. It defines
        how long to wait for the &stonith; action (reboot, on, off) to complete.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
  <para>
   If you change the watchdog timeout, you need to adjust the other two timeouts
   as well. The following <quote>formula</quote> expresses the relationship
   between these three values:
  </para>
   <example xml:id="ex.ha.storage.protect.sbd-timings">
    <title>Formula for Timeout Calculation</title>
    <screen>Timeout (msgwait) = (Timeout (watchdog) * 2)
stonith-timeout = Timeout (msgwait) + 20%</screen>
   </example>
   <para>
    For example, if you set the watchdog timeout to <literal>120</literal>,
    set the <literal>msgwait</literal> timeout to <literal>240</literal> and the
    <literal>stonith-timeout</literal> to <literal>288</literal>.
   </para>
    <para>
     If you use the <package>ha-cluster-bootstrap</package> scripts to set up a
     cluster and to initialize the SBD device, the relationship between these
     timeouts is automatically considered.
    </para>
 </sect1>

 <sect1 xml:id="sec.ha.storage.protect.watchdog">
  <title>Setting Up the Watchdog</title>
  <para> &productname; ships with several different kernel modules that provide
   hardware-specific watchdog drivers. For a list of the most commonly used
   ones, see <xref
   linkend="tab.ha.storage.protect.watchdog.drivers" xrefstyle="select:title
   nopage"/>.
 </para>
 <para>
  For clusters in production environments we recommend to use a hardware-specific
  watchdog driver. However, if no watchdog matches your hardware,
  <systemitem class="resource">softdog</systemitem> can be used as kernel
  watchdog module.
 </para>
 <para>
   The &hasi; uses the SBD daemon as the software component that <quote>feeds</quote>
   the watchdog.</para>

  <sect2 xml:id="sec.ha.storage.protect.hw-watchdog">
   <title>Using a Hardware Watchdog</title>

   <para>Finding the right watchdog kernel module for a given system is not
    trivial. Automatic probing fails very often. As a result, lots of modules
    are already loaded before the right one gets a chance.</para>

  <para>
   <xref linkend="tab.ha.storage.protect.watchdog.drivers" xrefstyle="select:label"/>
   lists the most commonly used watchdog drivers. If your hardware is not listed there,
   the directory
   <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/watchdog</filename>
   gives you a list of choices, too. Alternatively, ask your hardware vendor for
   the name.</para>

     <table xml:id="tab.ha.storage.protect.watchdog.drivers">
        <title>Commonly Used Watchdog Drivers</title>
        <tgroup cols="2">
         <thead>
          <row>
           <entry>Hardware</entry>
           <entry>Driver</entry>
          </row>
         </thead>
         <tbody>
          <row>
           <entry>HP</entry>
           <entry><systemitem class="resource">hpwdt</systemitem></entry>
          </row>
          <row>
           <entry>Dell, Fujitsu, Lenovo (Intel TCO)</entry>
           <entry><systemitem class="resource">iTCO_wdt</systemitem></entry>
          </row>
          <row>
           <entry>VM on z/VM on IBM mainframe</entry>
           <entry><systemitem class="resource">vmwatchdog</systemitem></entry>
          </row>
          <row>
           <entry>Xen VM (DomU)</entry>
           <entry><systemitem class="resource">xen_xdt</systemitem></entry>
          </row>
          <row>
           <entry>Generic</entry>
           <entry><systemitem class="resource">softdog</systemitem></entry>
          </row>
         </tbody>
        </tgroup>
       </table>

 <important>
    <title>Accessing the Watchdog Timer</title>
    <para>Some hardware vendors ship systems management software that uses the
     watchdog for system resets (for example, HP ASR daemon). If watchdog is
     used by SBD, disable such software. No other software must access the
     watchdog timer. </para>
   </important>

   <procedure xml:id="pro.ha.storage.protect.watchdog">
    <title>Loading the Correct Kernel Module</title>
    <para>To make sure the correct watchdog model is loaded, proceed as follows:</para>
     <step>
      <para>List the drivers that have been installed with your kernel version:</para>
       <screen>&prompt.root;<command>rpm</command> -ql kernel-<replaceable>VERSION</replaceable> | <command>grep</command> watchdog</screen>
      </step>
      <step xml:id="st.ha.storage.listwatchdog.modules">
       <para>List any watchdog modules that are currently loaded in the kernel:</para>
       <screen>&prompt.root;<command>lsmod</command> | <command>egrep</command> "(wd|dog)"</screen>
      </step>
      <step>
       <para>If you get a result, unload the wrong module:</para>
       <screen>&prompt.root;<command>rmmod</command> <replaceable>WRONG_MODULE</replaceable></screen>
      </step>
      <step>
     <para> Enable the watchdog module that matches your hardware: </para>
     <screen>&prompt.root;<command>echo</command> <replaceable>WATCHDOG_MODULE</replaceable> > /etc/modules-load.d/watchdog.conf
&prompt.root;<command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
     <para>Test if the watchdog module is loaded correctly:</para>
     <screen>&prompt.root;<command>lsmod</command> | <command>grep</command> dog</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.storage.protect.sw-watchdog">
   <title>Using the Software Watchdog (softdog)</title>
   <para>
    For clusters in production environments we recommend to use a hardware-specific watchdog
    driver. However, if no watchdog matches your hardware, <systemitem class="resource"
    >softdog</systemitem> can be used as kernel watchdog module. </para>

   <important>
    <title>Softdog Limitations</title>
    <para>
     The softdog driver assumes that at least one CPU is still running. If all
     CPUs are stuck, the code in the softdog driver that should reboot the system
     will never be executed. In contrast, hardware watchdogs keep working even
     if all CPUs are stuck.
    </para>
   </important>

   <procedure xml:id="pro.ha.storage.protect.sw-watchdog">
    <title>Loading the Softdog Kernel Module</title>
    <step>
     <para>Enable the softdog driver:</para>
     <screen>&prompt.root;<command>echo</command> softdog > /etc/modules-load.d/watchdog.conf</screen>
    </step>
    <step>
     <para>Reload all modules:</para>
     <screen>&prompt.root;<command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
     <para>Test if the softdog watchdog module is loaded correctly:</para>
     <screen>&prompt.root;<command>lsmod</command> | <command>grep</command> softdog</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.storageprotection.fencing.setup">
  <title>Setting Up SBD with 1-3 Devices</title>
  <para>
   The following steps are necessary for setup:
  </para>
 <procedure>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.create" xrefstyle="select:title"/>
        </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.config" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.services" xrefstyle="select:title"
     />
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.ha.storage.protect.sbd.test" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>
  <para>
    Before you start, make sure the block device or devices you want to use for
    SBD meet the requirements specified in <xref linkend="sec.ha.storage.protect.req"
    xrefstyle="select:label"/>.
  </para>
  <para>
   When setting up the SBD devices, you need to take several timeout values into
   account. For details, see <xref linkend="sec.ha.storage.protect.watchdog.timings"/>.
  </para>
  <para>
   The node will terminate itself if the SBD daemon running on it has not
   updated the watchdog timer fast enough. After having set the timeouts, test
   them in your specific environment.
  </para>

  <procedure xml:id="pro.ha.storage.protect.sbd.create">
   <title>Initializing the SBD Devices</title>
   <para>
    To use SBD with shared storage, you must first create the messaging
    layout on one to three block devices. The <command>sbd create</command> command
    will write a metadata header to the specified device or devices. It will also
    initialize the messaging slots for up to 255 nodes. If executed without any
    further options, the command will use the default timeout settings.</para>
    <warning>
     <title>Overwriting Existing Data</title>
      <para> Make sure the device or devices you want to use for SBD do not hold any
       important data. As soon as you execute the <command>sbd create</command>
       command, roughly the first megabyte of the specified block devices
       will be overwritten without further requests nor backup.
      </para>
    </warning>
    <step>
     <para>Decide which block device or block devices to use for SBD.</para>
    </step>
    <step>
     <para>Initialize the SBD device with the following command: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> create</screen>
     <para>(replace <filename>/dev/<replaceable>SBD</replaceable></filename>
       with your actual path name, for example:
       <filename>/dev/disk/by-id/scsi-ST2000DM001-0123456_Wabcdefg</filename>)</para>
        <para> If you want to use more than one device for SBD, specify the <option>-d</option> option multiple times, for
      example: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD1</replaceable> -d /dev/<replaceable>SBD2</replaceable> -d /dev/<replaceable>SBD3</replaceable> create</screen>
    </step>
    <step>
     <para>If your SBD device resides on a multipath group, use the <option>-1</option>
      and <option>-4</option> options to adjust the timeouts to use for SBD (for
      details, see <xref linkend="sec.ha.storage.protect.watchdog.timings"/>).
      All timeouts are given in seconds:</para>
      <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> -4 180<co xml:id="co.msgwait"/> -1 90<co xml:id="co.watchdog"/> create</screen>
     <calloutlist>
      <callout arearefs="co.msgwait">
       <para> The <option>-4</option> option is used to specify the
         <literal>msgwait</literal> timeout. In the example above, it is set to
         <literal>180</literal> seconds. </para>
      </callout>
      <callout arearefs="co.watchdog">
       <para> The <option>-1</option> option is used to specify the
         <literal>watchdog</literal> timeout. In the example above, it is set
        to <literal>90</literal> seconds. The minimum allowed value for the
        emulated watchdog is <literal>15</literal> seconds. </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>Check what has been written to the device: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> dump
Header version     : 2.1
UUID               : 619127f4-0e06-434c-84a0-ea82036e144c
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 5
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 10
==Header on disk /dev/<replaceable>SBD</replaceable> is dumped</screen>
    <para> As you can see, the timeouts are also stored in the header, to ensure
    that all participating nodes agree on them. </para>
    </step>
   </procedure>
   <para>
    After you have initialized the SBD devices, edit the SBD configuration file,
    then enable and start the respective services for the changes to take effect.
   </para>
   <procedure xml:id="pro.ha.storage.protect.sbd.config">
   <title>Adding the Devices to the SBD Configuration File</title>
    <step>
     <para>Open the file <filename>/etc/sysconfig/sbd</filename>.</para>
    </step>
    <step>
     <para>Search for the following parameter: <parameter>SBD_DEVICE</parameter>.
     </para>
     <para>It specifies the devices to monitor and to use for exchanging SBD messages.
     </para>
    </step>
   <step>
    <para> Edit this line by replacing <replaceable>SBD</replaceable> with your SBD device:</para>
    <screen>SBD_DEVICE="/dev/<replaceable>SBD</replaceable>"</screen>
    <para> If you need to specify multiple devices in the first line, separate them by a semicolon
     (the order of the devices does not matter):</para>
    <screen>SBD_DEVICE="/dev/<replaceable>SBD1</replaceable>; /dev/<replaceable>SBD2</replaceable>; /dev/<replaceable>SBD3</replaceable>"</screen>
    <para> If the SBD device is not accessible, the daemon will fail to start and inhibit
     &corosync; start-up. </para>
   </step>
 </procedure>

 <para>After you have added your SBD devices to the SBD configuration file,
  enable the SBD daemon to start at boot time. The SBD daemon is a critical piece
  of the cluster stack. It needs to be running when the cluster stack is running.
  Thus, the <systemitem>sbd</systemitem> service is started as a dependency whenever
  the <systemitem>pacemaker</systemitem> service is started.</para>

  <procedure xml:id="pro.ha.storage.protect.sbd.services">
   <title>Enabling and Starting the SBD Service</title>
   <step>
    <para>On each node, enable the SBD service to start at boot time: </para>
    <screen>&prompt.root;<command>systemctl</command> enable sbd</screen>
   </step>
   <step>
    <para>Restart the cluster stack on each node to ensure that the
     <systemitem>sbd</systemitem> service is started:</para>
    <screen>&prompt.root;<command>systemctl</command> stop pacemaker
&prompt.root;<command>systemctl</command> start pacemaker</screen>
    <para> This automatically triggers the start of the SBD daemon. </para>
   </step>
  </procedure>

  <para>
   As a final step, test the SBD devices as described in <xref
   linkend="pro.ha.storage.protect.sbd.test" xrefstyle="select:label"/>.
  </para>

  <procedure xml:id="pro.ha.storage.protect.sbd.test">
   <title>Testing the SBD Devices</title>
    <step>
     <para> The following command will dump the node slots and their current
      messages from the SBD device: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> list</screen>
    <para> Now you should see all cluster nodes that have ever been started with SBD listed here.
     For example, if you have a two-node cluster, the message slot should show
      <literal>clear</literal> for both nodes:</para>
     <screen>0       &node1;        clear
1       &node2;          clear</screen>
    </step>
    <step>
     <para> Try sending a test message to one of the nodes: </para>
     <screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> message &node1; test</screen>
    </step>
    <step>
     <para> The node will acknowledge the receipt of the message in the system
      log files: </para>
     <screen>Aug 29 14:10:00 &node1; sbd: [13412]: info: Received command test from &node2;</screen>
     <para> This confirms that SBD is indeed up and running on the node and
      that it is ready to receive messages. </para>
    </step>
   </procedure>
  </sect1>

  <sect1 xml:id="pro.ha.storage.protect.fencing">
   <title>Configuring the Cluster to Use SBD</title>
   <para>
    To configure the use of SBD in the cluster, you need to do the following in
    the cluster configuration:
   </para>
   <itemizedlist>
    <listitem>
     <para>
       set the <parameter>stonith-timeout</parameter> parameter to a value that
       matches your setting
     </para>
    </listitem>
    <listitem>
     <para>
      configure the SBD &stonith; resource
     </para>
    </listitem>
   </itemizedlist>
   <para>
     For the calculation of the <parameter>stonith-timeout</parameter> refer to
     <xref linkend="sec.ha.storage.protect.watchdog.timings"/>.
   </para>
 <procedure>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>Enter the following:</para>
    <screen>
&prompt.crm.conf;<command>property</command> stonith-enabled="true" <co xml:id="co.ha.sbd.st.enabled"/>
&prompt.crm.conf;<command>property</command> stonith-timeout="40s" <co xml:id="co.ha.sbd.st.timeout"/>
&prompt.crm.conf;<command>primitive</command> stonith_sbd stonith:external/sbd <co xml:id="co.ha.sbd.st.rsc"/>\
  params pcmk_delay_max=30 <co xml:id="co.ha.sbd.pm_delay_max"/></screen>
    <calloutlist>
     <callout arearefs="co.ha.sbd.st.enabled">
      <para>
       This is the default configuration, because clusters with &stonith; are not supported.
       But in case &stonith; had been deactivated for testing purposes,
       make sure this parameter is set to <literal>true</literal>.</para>
     </callout>
     <callout arearefs="co.ha.sbd.st.timeout">
      <para>
       A <systemitem>stonith-timeout</systemitem> value of <literal>40</literal>
       would be appropriate if the <literal>msgwait</literal> timeout value for
       SBD was set to <literal>30</literal> seconds.</para>
     </callout>
     <callout arearefs="co.ha.sbd.st.rsc">
      <para>
       The &stonith; resource to use for SBD.</para>
     </callout>
     <callout arearefs="co.ha.sbd.pm_delay_max">
      <para>This parameter prevents double fencing when using slow devices such
       as SBD. It adds a random delay for &stonith; actions on the fencing device.
       It is especially important for two-node clusters where otherwise both nodes
       might try to fence each other in case of a split brain scenario.</para>
     </callout>
   </calloutlist>
   </step>
  <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     Submit your changes with <command>commit</command> and leave the crm live
     configuration with <command>exit</command>.
    </para>
   </step>
  </procedure>

   <para> After the resource has started, your cluster is successfully
    configured for use of SBD and will use this method in case a
    node needs to be fenced. </para>
  </sect1>

  <sect1 xml:id="sec.ha..storageprotection.sgpersist">
   <title>Configuring an sg_persist Resource</title>
   <remark>toms 2018-04-18: Not sure why this section is here. It does not
    belong to the described procedure introduced in
    <xref linkend="sec.ha.storageprotection.fencing.setup"/>.
    I'm tempted to move it...</remark>
   <note>
    <title>SCSI Disk Compatibility</title>
    <para> Any backing disks for <literal>sg_persist</literal> must be SCSI
     disk compatible. <literal>sg_persist</literal> only works for devices like
     SCSI disks or iSCSI LUNs. Do <emphasis>not</emphasis> use it for IDE,
     SATA, or any block devices which do not support the SCSI protocol. </para>
   </note>
   <remark>toms 2014-09-10: FATE#312345</remark>
   <procedure>
    <step>
     <para> Log in as &rootuser; and start a shell. </para>
    </step>
    <step>
     <para> Run the following commands to create the primitive resources
       <literal>sg_persist</literal>: </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> sg sg_persist \
    params devs="/dev/sdc" reservation_type=3 \
    op monitor interval=60 timeout=60</screen>
    </step>
    <step>
     <para> Add the <literal>sg_persist</literal> primitive to a master-slave
      group: <remark>taroth 2018-03-05: ygao, should 'master-max' be replaced
       with 'promoted-max' or does the screen below need more changes? - ygao
       2018-03-13: the new names are not explicitly promoted in crmsh
       yet</remark>
     </para>
     <screen>&prompt.crm.conf;<command>ms</command> ms-sg sg \
    meta master-max=1 notify=true</screen>
    </step>
    <step>
     <para> Do some tests. When the resource is in master/slave status, on the
      master server, you can mount and write on <filename>/dev/sdc1</filename>,
      while on the slave server you cannot write. </para>
    </step>
   </procedure>
   <para>For more information, refer to the <command>sg_persist</command> man
    page. Usually you should use the above resource with a
     <literal>Filesystem</literal> resource, for example, Ext2/3/4/XFS. </para>
   <procedure>
    <step>
     <para> Add a file system primitive for Ext4: </para>
     <screen>&prompt.crm.conf;<command>primitive</command> ext4 ocf:heartbeat:Filesystem \
    params device="/dev/sdc1" directory="/mnt/ext4" fstype=ext4</screen>
    </step>
    <step>
     <para> Add the following order relationship plus a collocation between the
       <literal>sg_persist</literal> master and the file system resource: </para>
     <screen>&prompt.crm.conf;<command>order</command> o-ms-sg-before-ext4 inf: ms-sg:promote ext4:start
&prompt.crm.conf;<command>colocation</command> col-ext4-with-sg-persist inf: ext4 ms-sg:Master</screen>
    </step>
    <step>
     <para> Check all your changes with the <command>show</command> command.
     </para>
    </step>
    <step>
     <para> Commit your changes. </para>
    </step>
   </procedure>
  </sect1>


<!--  <sect1 xml:id="sec.ha.storage.protect.fencing">
  <title>Storage-based Fencing</title>
  <para>
   You can reliably avoid split brain scenarios by using one or more
   &stonith; Block Devices (SBD), <literal>watchdog</literal> support and
   the <literal>external/sbd</literal> &stonith; agent.
  </para>
 </sect1>-->

 <sect1 xml:id="sec.ha.storageprotection.exstoract">
  <title>Ensuring Exclusive Storage Activation</title>
  <remark>toms 2018-04-18: Hmn... I would have liked it more if this chapter
  describes only SBD. Describing another "low-level mechanism" is a bit
  distracting. Probably there is no better location...</remark>
  <para>
   This section introduces <literal>sfex</literal>, an additional low-level
   mechanism to lock access to shared storage exclusively to one node. Note
   that sfex does not replace &stonith;. Since sfex requires shared
   storage, it is recommended that the <literal>external/sbd</literal>
   fencing mechanism described above is used on another partition of the
   storage.
  </para>

  <para>
   By design, sfex cannot be used with workloads that require concurrency
   (such as OCFS2), but serves as a layer of protection for classic failover
   style workloads. This is similar to a SCSI-2 reservation in effect, but
   more general.
  </para>

  <sect2 xml:id="sec.ha.storageprotection.exstoract.description">
   <title>Overview</title>
   <para>
    In a shared storage environment, a small partition of the storage is set
    aside for storing one or more locks.
   </para>
   <para>
    Before acquiring protected resources, the node must first acquire the
    protecting lock. The ordering is enforced by Pacemaker, and the sfex
    component ensures that even if Pacemaker were subject to a split brain
    situation, the lock will never be granted more than once.
   </para>
   <para>
    These locks must also be refreshed periodically, so that a node's death
    does not permanently block the lock and other nodes can proceed.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.storageprotection.exstoract.requirements">
   <title>Setup</title>
   <para>
    In the following, learn how to create a shared partition for use with
    sfex and how to configure a resource for the sfex lock in the CIB. A
    single sfex partition can hold any number of locks, and needs 1 KB 
    of storage space allocated per lock.
    By default, sfex_init creates one lock on the partition.
   </para>
   <important>
    <title>Requirements</title>
    <itemizedlist>
     <listitem>
      <para>
       The shared partition for sfex should be on the same logical unit as
       the data you want to protect.
      </para>
     </listitem>
     <listitem>
      <para>
       The shared sfex partition must not use host-based RAID, nor DRBD.
      </para>
     </listitem>
     <listitem>
      <para>
       Using a LVM2 logical volume is possible.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <procedure>
    <title>Creating an sfex Partition</title>
    <step>
     <para>
      Create a shared partition for use with sfex. Note the name of this
      partition and use it as a substitute for
      <filename>/dev/sfex</filename> below.
     </para>
    </step>
    <step>
     <para>
      Create the sfex meta data with the following command:
     </para>
<screen>&prompt.root;<command>sfex_init</command> -n 1 /dev/sfex</screen>
    </step>
    <step>
     <para>
      Verify that the meta data has been created correctly:
     </para>
<screen>&prompt.root;<command>sfex_stat</command> -i 1 /dev/sfex ; echo $?</screen>
     <para>
      This should return <literal>2</literal>, since the lock is not
      currently held.
     </para>
    </step>
   </procedure>
   <procedure>
    <title>Configuring a Resource for the sfex Lock</title>
    <step>
     <para>
      The sfex lock is represented via a resource in the CIB, configured as
      follows:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> sfex_1 ocf:heartbeat:sfex \
#	params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
#	op monitor interval="10s" timeout="30s" on-fail="fence"</screen>
    </step>
    <step>
     <para>
      To protect resources via an sfex lock, create mandatory ordering and
      placement constraints between the protectees and the sfex resource. If
      the resource to be protected has the id
      <literal>filesystem1</literal>:
     </para>
<screen>&prompt.crm.conf;<command>order</command> order-sfex-1 inf: sfex_1 filesystem1
&prompt.crm.conf;<command>colocation</command> colo-sfex-1 inf: filesystem1 sfex_1</screen>
    </step>
    <step>
     <para>
      If using group syntax, add the sfex resource as the first resource to
      the group:
     </para>
<screen>&prompt.crm.conf;<command>group</command> LAMP sfex_1 filesystem1 apache ipaddr</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ha.storage.moreinfo">
  <title>For More Information</title>

  <itemizedlist>
    <listitem>
      <para><link xlink:href="http://www.linux-ha.org/wiki/SBD_Fencing"/></para>
    </listitem>
    <listitem>
      <para><command>man sbd</command></para>
    </listitem>
  </itemizedlist>
 </sect1>
</chapter>
