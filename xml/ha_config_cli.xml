<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!--
 
 Future TODOs:
 * Correct IDs
 * FATE 304867 Multilevel administration rights for CIB
   => 2010-02-23: According to Lars, not fully functionaly yet

 http://www.clusterlabs.org/wiki/Example_configurations
-->
<chapter xml:id="cha.ha.manual_config" version="5.0"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
<!--<sect1 id="sec.ha.manual_config.adminrights">
  <title>Setting Multilevel Administration Rights</title>
  <remark>FATE#304867 Multilevel administration rights for CIB</remark>
  <para></para>
 </sect1>-->
<!-- FATE#309125 -->
<!-- FATE#310358, #310174, #310172 -->
 <title>Configuring and Managing Cluster Resources (Command Line)</title>
 <info>
      <abstract>
        <para>
    To configure and manage cluster resources, either use the &crmshell;
    (&crmsh;) command line utility or &haweb; (&hawk;), a Web-based
    user interface.
   </para>
        <para>
    This chapter introduces <command>crm</command>, the command line tool
    and covers an overview of this tool, how to use templates, and mainly
    configuring and managing cluster resources: creating basic and advanced
    types of resources (groups and clones), configuring constraints,
    specifying failover nodes and failback nodes, configuring resource
    monitoring, starting, cleaning up or removing resources, and migrating
    resources manually.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer></dm:maintainer>
        <dm:status>editing</dm:status>
        <dm:deadline></dm:deadline>
        <dm:priority></dm:priority>
        <dm:translation></dm:translation>
        <dm:languages></dm:languages>
        <dm:release></dm:release>
        <dm:repository></dm:repository>
      </dm:docmanager>
    </info>
    <note>
  <title>User Privileges</title>
  <para>
   Sufficient privileges are necessary to manage a cluster. The
   <command>crm</command> command and its subcommands need to be run either
   as &rootuser; user or as the CRM owner user (typically the user
   <systemitem class="username">hacluster</systemitem>).
  </para>
  <para>
   However, the <option>user</option> option allows you to run
   <command>crm</command> and its subcommands as a regular (unprivileged)
   user and to change its ID using <command>sudo</command> whenever
   necessary. For example, with the following command <command>crm</command>
   will use <systemitem class="username">hacluster</systemitem> as the
   privileged user ID:
  </para>
<screen>&prompt.root;<command>crm</command> options user hacluster</screen>
  <para>
   Note that you need to set up <filename>/etc/sudoers</filename> so that
   <command>sudo</command> does not ask for a password.
  </para>
 </note>
 <sect1 xml:id="sec.ha.manual_config.crm">
  <title>&crmsh;&mdash;Overview</title>

  <para>
   The <command>crm</command> command has several subcommands which manage
   resources, CIBs, nodes, resource agents, and others. It offers a thorough
   help system with embedded examples. All examples follow a naming
   convention described in
   <xref linkend="app.naming" xrefstyle="select:label"/>.
  </para>

<!-- toms 2014-02-27: 
      Should we add that to the section "Typographical Conventions"?
    -->

  <tip>
   <title>Distinguish Between Shell Prompt and Interactive crm Prompt</title>
   <para>
    To make all the code and examples more readable, this chapter uses the
    following notations between shell prompts and the interactive crm
    prompt:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Shell prompt for user &rootuser;:
     </para>
<screen>&prompt.root;</screen>
    </listitem>
    <listitem>
     <para>
      Interactive &crmsh; prompt (displayed in green, if terminal
      supports colors):
     </para>
<screen>&prompt.crm;</screen>
    </listitem>
   </itemizedlist>
  </tip>

  <sect2 xml:id="sec.ha.manual_config.crm.help">
   <title>Getting Help</title>
   <para>
    Help can be accessed in several ways:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      To output the usage of <command>crm</command> and its command line
      options:
     </para>
<screen>&prompt.root;<command>crm</command> --help</screen>
    </listitem>
    <listitem>
     <para>
      To give a list of all available commands:
     </para>
<screen>&prompt.root;<command>crm</command> help</screen>
    </listitem>
    <listitem>
     <para>
      To access other help sections, not just the command reference:
     </para>
<screen>&prompt.root;<command>crm</command> help topics</screen>
    </listitem>
    <listitem>
     <para>
      To view the extensive help text of the <command>configure</command>
      subcommand:
     </para>
<screen>&prompt.root;<command>crm</command> configure help</screen>
    </listitem>
    <listitem>
     <para>
      To print the syntax, its usage, and examples of a subcommand of
      <command>configure</command>:
     </para>
<screen>&prompt.root;<command>crm</command> configure help group</screen>
     <para>
      This is also possible:
     </para>
<screen>&prompt.root;<command>crm</command> help configure group</screen>
    </listitem>
   </itemizedlist>
   <para>
    Almost all output of the <command>help</command> subcommand (do not mix
    it up with the <option>--help</option> option) opens a text viewer. This
    text viewer allows you to scroll up or down and read the help text more
    comfortably. To leave the text viewer, press the <keycap>Q</keycap> key.
   </para>
   <tip xml:id="tip.crm.tabcompletion">
    <title>Use Tab Completion in Bash and Interactive Shell</title>
    <para>
     The &crmsh; supports full tab completion in Bash directly, not only
     for the interactive shell. For example, typing <literal>crm help
     config</literal><keycap function="tab"/> will complete the word 
     like in the interactive shell.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.crm.run">
   <title>Executing &crmsh;'s Subcommands</title>
   <para>
    The <command>crm</command> command itself can be used in the following
    ways:
   </para>
   <itemizedlist>
    <listitem>
     <formalpara>
      <title>Directly:</title>
      <para>
       Concatenate all subcommands to <command>crm</command>, press
       <keycap function="enter"/> and you see the output immediately. For
       example, enter <command>crm</command> <option>help ra</option> to get
       information about the <command>ra</command> subcommand (resource
       agents).
      </para>
     </formalpara>
      <para>It is possible to abbreviate subcommands as long as they are
        unique. For example, you can shorten <command>status</command> as
      <command>st</command> and &crmsh; will know what you have meant.
      </para>
      <para>Another feature is to shorten parameters. Usually, you add
        parameters through the <command>params</command> keyword.
        You can leave out the params section if it is the first and only section.
        For example, this line:
      </para>
      <screen>&prompt.root;<command>crm</command> primitive ipaddr ocf:heartbeat:IPaddr2 params ip=192.168.0.55</screen>
      <para>is equivalent to this line:</para>
      <screen>&prompt.root;<command>crm</command> primitive ipaddr ocf:heartbeat:IPaddr2 ip=192.168.0.55</screen>
    </listitem>
    <listitem>
     <formalpara>
      <title>As crm Shell Script:</title>
      <para>
       Use <command>crm</command> and its subcommands in a script. This can
       be done in two ways:
      </para>
     </formalpara>
<screen>&prompt.root;<command>crm</command> -f example.cli
&prompt.root;<command>crm</command> &lt; example.cli</screen>
     <para>
      The script can contain any subcommand from <command>crm</command>. For
      example:
     </para>
<screen># A small example file with some crm subcommands
<command>status</command>
<command>node</command> list</screen>
     <para>
      Any line starting with the hash symbol (#) is a comment and is
      ignored. If a line is too long, insert a backslash (\) at the end and
      continue in the next line. It is recommended to indent lines that
      belong to a certain subcommand to improve readability.
     </para>
    </listitem>
    <listitem>
      <formalpara>
        <title>As &crmsh; Cluster Scripts:</title>
        <para>These are a collection of metadata, references to RPM packages,
          configuration files, and &crmsh; subcommands bundled under a single,
          yet descriptive name. They are managed through the
          <command>crm script</command> command.
        </para>
      </formalpara>
      <para>Do not confuse them with crm shell scripts: although both share
        some common objectives, the crm shell scripts only contain subcommands
        whereas cluster scripts incorporate much more than a simple
        enumeration of commands. For more information, see <xref
          linkend="sec.ha.manual_config.clusterscripts"/>.
      </para>
    </listitem>
    <listitem>
     <formalpara>
      <title>Interactive as Internal Shell:</title>
      <para>
       Type <command>crm</command> to enter the internal shell. The prompt
       changes to <literal>&crm.live;</literal>. With
       <command>help</command> you can get an overview of the available
       subcommands. As the internal shell has different levels of
       subcommands, you can <quote>enter</quote> one by typing this
       subcommand and press <keycap function="enter"/>.
      </para>
     </formalpara>
     <para>
      For example, if you type <command>resource</command> you enter the
      resource management level. Your prompt changes to
      <literal>&crm.live;resource#</literal>. If you want to leave the
      internal shell, use the commands <command>quit</command>,
      <command>bye</command>, or <command>exit</command>. If you need to go
      one level back, use <command>back</command>, <command>up</command>,
      <command>end</command>, or <command>cd</command>.
     </para>
     <para>
      You can enter the level directly by typing <command>crm</command> and
      the respective subcommand(s) without any options and press
      <keycap function="enter"/>.
     </para>
     <para>
      The internal shell supports also tab completion for subcommands and
      resources. Type the beginning of a command, press
      <keycap function="tab"/> and <command>crm</command> completes the
      respective object.
     </para>
    </listitem>
   </itemizedlist>
<!-- Fate#310303: -->
   <para>
    In addition to previously explained methods, &crmsh; also supports
    synchronous command execution. Use the <option>-w</option> option to
    activate it. If you have started <command>crm</command> without
    <option>-w</option>, you can enable it later with the user preference's
    <command>wait</command> set to <literal>yes</literal> (<command>options
    wait yes</command>). If this option is enabled, <command>crm</command>
    waits until the transition is finished. Whenever a transaction is
    started, dots are printed to indicate progress. Synchronous command
    execution is only applicable for commands like <command>resource
    start</command>.
   </para>
   <note>
    <title>Differentiate Between Management and Configuration Subcommands</title>
    <para>
     The <command>crm</command> tool has management capability (the
     subcommands <command>resource</command> and <command>node</command>)
     and can be used for configuration (<command>cib</command>,
     <command>configure</command>).
    </para>
   </note>
   <para>
    The following subsections give you an overview of some important aspects
    of the <command>crm</command> tool.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.ocf">
   <title>Displaying Information about OCF Resource Agents</title>
   <para>
    As you need to deal with resource agents in your cluster configuration
    all the time, the <command>crm</command> tool contains the
    <command>ra</command> command. Use it to show information about resource
    agents and to manage them (for additional information, see also
    <xref linkend="sec.ha.config.basics.raclasses"/>):
   </para>
<screen>&prompt.root;<command>crm</command> ra
&prompt.crm.ra;</screen>
   <para>
    The command <command>classes</command> lists all classes and providers:
   </para>
<screen>&prompt.crm.ra;<command>classes</command>
 lsb
 ocf / heartbeat linbit lvm2 ocfs2 pacemaker
 service
 stonith
 systemd
 </screen>
   <para>
    To get an overview of all available resource agents for a class (and
    provider) use the <command>list</command> command:
   </para>
<screen>&prompt.crm.ra;<command>list</command> ocf
AoEtarget           AudibleAlarm        CTDB                ClusterMon
Delay               Dummy               EvmsSCC             Evmsd
Filesystem          HealthCPU           HealthSMART         ICP
IPaddr              IPaddr2             IPsrcaddr           IPv6addr
LVM                 LinuxSCSI           MailTo              ManageRAID
ManageVE            Pure-FTPd           Raid1               Route
SAPDatabase         SAPInstance         SendArp             ServeRAID
...</screen>
   <para>
    An overview of a resource agent can be viewed with
    <command>info</command>:
   </para>
<screen>&prompt.crm.ra;<command>info</command> ocf:linbit:drbd
This resource agent manages a DRBD* resource
as a master/slave resource. DRBD is a shared-nothing replicated storage
device. (ocf:linbit:drbd)

Master/Slave OCF Resource Agent for DRBD

Parameters (* denotes required, [] the default):

drbd_resource* (string): drbd resource name
    The name of the drbd resource from the drbd.conf file.

drbdconf (string, [/etc/drbd.conf]): Path to drbd.conf
    Full path to the drbd.conf file.

Operations' defaults (advisory minimum):

    start         timeout=240
    promote       timeout=90 
    demote        timeout=90 
    notify        timeout=90 
    stop          timeout=100
    monitor_Slave_0 interval=20 timeout=20 start-delay=1m
    monitor_Master_0 interval=10 timeout=20 start-delay=1m</screen>
   <para>
    Leave the viewer by pressing <keycap>Q</keycap>.
   </para>
   <tip>
    <title>Use <command>crm</command> Directly</title>
    <para>
     In the former example we used the internal shell of the
     <command>crm</command> command. However, you do not necessarily need to
     use it. You get the same results if you add the respective subcommands
     to <command>crm</command>. For example, you can list all the OCF
     resource agents by entering <command>crm</command> <option>ra list
     ocf</option> in your shell.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.clusterscripts">
    <title>Using &crmsh;'s Cluster Scripts</title>
    <remark>toms 2015-10-14: FATE#318211</remark>
    <para>Collecting information from all cluster nodes and deploying any
      changes is a key cluster administration task. Instead of performing
      the same procedures manually on different nodes (which is error-prone),
      you can use the &crmsh; cluster scripts.
    </para>

    <para>The &crmsh; cluster scripts have the following objectives:</para>
    <itemizedlist>
      <listitem>
        <para>Installing software is required for a specific task.</para>
      </listitem>
      <listitem>
        <para>Creating or modifying any configuration files.</para>
      </listitem>
      <listitem>
        <para>Collecting information and reporting potential problems with the
          cluster.</para>
      </listitem>
      <listitem>
        <para>Deploying the changes to all nodes.</para>
      </listitem>
    </itemizedlist>

    <para>&crmsh; cluster scripts do not replace other tools for managing
      clusters&mdash;they provide an integrated way to perform the above
      tasks across the cluster. Find detailed information at <link
        xlink:href="http://crmsh.github.io/scripts/"
        xlink:show="new"/>.
    </para>

    <sect3 xml:id="sec.ha.manual_config.clusterscripts.usage">
      <title>Usage</title>
      <para>To get a list of all available cluster scripts, run:</para>
      <screen>&prompt.root;<command>crm</command> script list</screen>

      <para>To view the components of a script, use the
        <command>show</command> command and the name of the cluster script,
        for example:</para>
      <screen>&prompt.root;<command>crm</command> script show mailto
mailto (Basic)
MailTo

 This is a resource agent for MailTo. It sends email to a sysadmin
whenever  a takeover occurs.

1. Notifies recipients by email in the event of resource takeover

  id (required)  (unique)
      Identifier for the cluster resource
  email (required)
      Email address
  subject
      Subject</screen>

      <para>The output of <command>show</command> contains a title, a
          short description, and a procedure. Each procedure is divided
          into a series of steps, performed in the given order. </para>
      <para>Each step contains a list of required and optional parameters,
        along with a short description and its default value.</para>

      <para>Each cluster script understands a set of common parameters.
        These parameters can be passed to any script:</para>

      <table>
        <title>Common Parameters</title>
        <tgroup cols="3">
          <thead>
            <row>
              <entry>Parameter</entry>
              <entry>Argument</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><parameter>action</parameter></entry>
              <entry><replaceable>INDEX</replaceable></entry>
              <entry>If set, only execute a single action (index, as
                returned by verify)</entry>
            </row>
            <row>
              <entry><parameter>dry_run</parameter></entry>
              <entry><replaceable>BOOL</replaceable></entry>
              <entry>If set, simulate execution only (default: no) </entry>
            </row>
            <row>
              <entry><parameter>nodes</parameter></entry>
              <entry><replaceable>LIST</replaceable></entry>
              <entry>List of nodes to execute the script for</entry>
            </row>
            <row>
              <entry><parameter>port</parameter></entry>
              <entry><replaceable>NUMBER</replaceable></entry>
              <entry>Port to connect to</entry>
            </row>
            <row>
              <entry><parameter>statefile</parameter></entry>
              <entry><replaceable>FILE</replaceable></entry>
              <entry>When single-stepping, the state is saved in the given
                file </entry>
            </row>
            <row>
              <entry><parameter>sudo</parameter></entry>
              <entry><replaceable>BOOL</replaceable></entry>
              <entry>If set, crm will prompt for a sudo password and use sudo
                where appropriate (default: no) </entry>
            </row>
            <row>
              <entry><parameter>timeout</parameter></entry>
              <entry><replaceable>NUMBER</replaceable></entry>
              <entry>Execution timeout in seconds (default: 600) </entry>
            </row>
            <row>
              <entry><parameter>user</parameter></entry>
              <entry><replaceable>USER</replaceable></entry>
              <entry>Run script as the given user </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </sect3>

    <sect3 xml:id="sec.ha.manual_config.clusterscripts.verify_run">
      <title>Verifying and Running a Cluster Script</title>
      <para>Before running a cluster script, review the actions that it will
          perform and verify its parameters to avoid problems. A cluster script
          can potentially perform a series of actions and may fail for
          various reasons. Thus, verifying your parameters before
          running it helps to avoid problems.</para>
      <para>For example, the <systemitem>mailto</systemitem> resource agent
        requires unique identifier and an e-mail address. To verify these
        parameters, run:</para>
      <screen>&prompt.root;<command>crm</command> verify mailto id=sysadmin email=&exampleuser;@example.org
1. Ensure mail package is installed

        mailx

2. Configure cluster resources

        primitive sysadmin ocf:heartbeat:MailTo
                email="tux@example.org"
                op start timeout="10"
                op stop timeout="10"
                op monitor interval="10" timeout="10"

        clone c-sysadmin sysadmin</screen>
      <para>The <command>verify</command> prints the steps and replaces
        any placeholders with your given parameters. If <command>verify</command>
        finds any problems, it will report it.
        If everything is ok, replace the <command>verify</command>
        command with <command>run</command>:</para>
      <screen>&prompt.root;<command>crm</command> run mailto id=sysadmin email=&exampleuser;@example.org
INFO: MailTo
INFO: Nodes: &node1;, &node2;
OK: Ensure mail package is installed
OK: Configure cluster resources</screen>
      <para>Check whether your resource is integrated into your cluster
          with <command>crm status</command>:</para>
      <screen>&prompt.root;<command>crm</command> status
[...]
 Clone Set: c-sysadmin [sysadmin]
     Started: [ &node1; &node2; ]</screen>
    </sect3>

  </sect2>

  <sect2 xml:id="sec.ha.manual_config.template">
   <title>Using Configuration Templates</title>
<!-- Info from https://bugzilla.novell.com/show_bug.cgi?id=541490 -->
    <remark>toms 2015-10-14: will be replaced by the crm cluster scripts</remark>
    <note>
      <title>Deprecation Notice</title>
      <para>The use of configuration templates is deprecated and will
        be removed in the future. Configuration templates will be replaced
        by cluster scripts, see <xref
          linkend="sec.ha.manual_config.clusterscripts"/>.
      </para>
    </note>
   <para>
    Configuration templates are ready-made cluster configurations for
    &crmsh;. Do not confuse them with the <emphasis>resource
    templates</emphasis> (as described in
    <xref linkend="sec.ha.manual_config.rsc_template"/>). Those are
    templates for the <emphasis>cluster</emphasis> and not for the crm
    shell.
   </para>
   <para>
    Configuration templates require minimum effort to be tailored to the
    particular user's needs. Whenever a template creates a configuration,
    warning messages give hints which can be edited later for further
    customization.
   </para>
   <para>
    The following procedure shows how to create a simple yet functional
    Apache configuration:
   </para>
   <procedure xml:id="pro.ha.manual_config.template">
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command>
      interactive shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Create a new configuration from a configuration template:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Switch to the <command>template</command> subcommand:
       </para>
<screen>&prompt.crm.conf;<command>template</command></screen>
      </step>
      <step>
       <para>
        List the available configuration templates:
       </para>
<screen>&prompt.crm.conf.templ;<command>list</command> templates
gfs2-base   filesystem  virtual-ip  apache   clvm     ocfs2    gfs2</screen>
      </step>
      <step>
       <para>
        Decide which configuration template you need. As we need an Apache
        configuration, we select the <literal>apache</literal> template and
        name it <literal>g-intranet</literal>:
       </para>
<screen>&prompt.crm.conf.templ;<command>new</command> g-intranet apache
INFO: pulling in template apache
INFO: pulling in template virtual-ip</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Define your parameters:
     </para>
     <substeps performance="required">
      <step>
       <para>
        List the configuration you have created:
       </para>
<screen>&prompt.crm.conf.templ;<command>list</command>
g-intranet</screen>
      </step>
      <step xml:id="st.config_cli.show">
       <para>
        Display the minimum required changes that need to be filled out by
        you:
       </para>
<screen>&prompt.crm.conf.templ;<command>show</command>
ERROR: 23: required parameter ip not set
ERROR: 61: required parameter id not set
ERROR: 65: required parameter configfile not set</screen>
      </step>
      <step xml:id="st.config_cli.edit">
       <para>
        Invoke your preferred text editor and fill out all lines that have
        been displayed as errors in <xref linkend="st.config_cli.show"/>:
       </para>
<screen>&prompt.crm.conf.templ;<command>edit</command></screen>
      </step>
<!--<step>
      <para>Edit all lines starting with <literal>%%</literal>. If you
       need an overview, use the following command in another &rootuser;
       shell:</para>
      <screen>grep -n "^%%" /root/.crmconf/intranet
23:%% ip
31:%% netmask
35:%% lvs_support
61:%% id  intranet
65:%% configfile
71:%% options
76:%% envfiles</screen>
     </step>-->
     </substeps>
    </step>
    <step>
     <para>
      Show the configuration and check whether it is valid (bold text
      depends on the configuration you have entered in
      <xref linkend="st.config_cli.edit" xrefstyle="select:label"/>):
     </para>
<screen>&prompt.crm.conf.templ;<command>show</command>
primitive virtual-ip ocf:heartbeat:IPaddr \
    params ip=<emphasis role="strong">"192.168.1.101"</emphasis>
primitive apache ocf:heartbeat:apache \
    params configfile=<emphasis role="strong">"/etc/apache2/httpd.conf"</emphasis>
    monitor apache 120s:60s
group <emphasis role="strong">g-intranet</emphasis> \
    apache virtual-ip</screen>
    </step>
    <step>
     <para>
      Apply the configuration:
     </para>
<screen>&prompt.crm.conf.templ;<command>apply</command>
&prompt.crm.conf;<command>cd ..</command>
&prompt.crm.conf;<command>show</command></screen>
    </step>
    <step>
     <para>
      Submit your changes to the CIB:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    It is possible to simplify the commands even more, if you know the
    details. The above procedure can be summarized with the following
    command on the shell:
   </para>
<screen>&prompt.root;<command>crm</command> configure template \
   new g-intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</screen>
   <para>
    If you are inside your internal <command>crm</command> shell, use the
    following command:
   </para>
<screen>&prompt.crm.conf.templ;<command>new</command> intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</screen>
   <para>
    However, the previous command only creates its configuration from the
    configuration template. It does not apply nor commit it to the CIB.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.shadowconfig">
   <title>Testing with Shadow Configuration</title>
   <para>
    A shadow configuration is used to test different configuration
    scenarios. If you have created several shadow configurations, you can
    test them one by one to see the effects of your changes.
   </para>
   <para>
    The usual process looks like this:
   </para>
   <procedure>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command>
      interactive shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Create a new shadow configuration:
     </para>
<screen>&prompt.crm.conf;<command>cib</command> new myNewConfig
INFO: myNewConfig shadow CIB created</screen>
     <para>
      If you omit the name of the shadow CIB, a temporary name
      <literal>@tmp@</literal> is created.
     </para>
    </step>
    <step>
     <para>
      If you want to copy the current live configuration into your shadow
      configuration, use the following command, otherwise skip this step:
     </para>
<screen>crm(myNewConfig)# <command>cib</command> reset myNewConfig</screen>
     <para>
      The previous command makes it easier to modify any existing resources
      later.
     </para>
    </step>
    <step>
     <para>
      Make your changes as usual. After you have created the shadow
      configuration, all changes go there. To save all your changes, use the
      following command:
     </para>
<screen>crm(myNewConfig)# <command>commit</command></screen>
    </step>
    <step>
     <para>
      If you need the live cluster configuration again, switch back with the
      following command:
     </para>
<screen>crm(myNewConfig)configure# <command>cib</command> use live
&prompt.crm;</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.debugging">
   <title>Debugging Your Configuration Changes</title>
   <para>
    Before loading your configuration changes back into the cluster, it is
    recommended to review your changes with <command>ptest</command>. The
    <command>ptest</command> command can show a diagram of actions that will
    be induced by committing the changes. You need the
    <systemitem>graphviz</systemitem> package to display the diagrams. The
    following example is a transcript, adding a monitor operation:
   </para>
<screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>show</command> fence-&node2; 
primitive fence-&node2; stonith:apcsmart \
        params hostlist="&node2;"
&prompt.crm.conf;<command>monitor</command> fence-&node2; 120m:60s
&prompt.crm.conf;<command>show</command> changed
primitive fence-&node2; stonith:apcsmart \
        params hostlist="&node2;" \
        op monitor interval="120m" timeout="60s"
&prompt.crm.conf;<emphasis role="strong">ptest</emphasis>
&prompt.crm.conf;commit</screen>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.diagram">
   <title>Cluster Diagram</title>
   <para>
    To output a cluster diagram as shown in
    <xref linkend="fig.ha.cluster.diagram"/>, use the command
    <command>crm</command> <command>configure graph</command>. It displays
    the current configuration on its current window, therefore requiring
    X11.
   </para>
   <para>
    If you prefer Scalable Vector Graphics (SVG), use the following command:
   </para>
<screen>&prompt.root;<command>crm</command> configure graph dot config.svg svg</screen>
  </sect2>
 </sect1>
<!--<sect1 id="sec.ha.manual_config.adminrights">
  <title>Setting Multilevel Administration Rights</title>
  <remark>FATE#304867 Multilevel administration rights for CIB</remark>
  <para></para>
 </sect1>-->
 <sect1 xml:id="sec.ha.manual.config.crm.corosync">
  <title>Managing &corosync; Configuration</title>

  <remark>toms 2014-05-08: See FATE#316332</remark>

  <para>
   &corosync; is the underlying messaging layer for most HA clusters. The
   <command>corosync</command> subcommand provides commands for editing and
   managing the &corosync; configuration.
  </para>

  <para>
   For example, to list the status of the cluster, use
   <command>status</command>:
  </para>

<screen>&prompt.root;<command>crm</command> corosync status
Printing ring status.
Local node ID 175704363
RING ID 0
        id      = 10.121.9.43
        status  = ring 0 active with no faults
Quorum information
------------------
Date:             Thu May  8 16:41:56 2014
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          175704363
Ring ID:          4032
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   2
Highest expected: 2
Total votes:      2
Quorum:           2  
Flags:            Quorate 

Membership information
----------------------
    Nodeid      Votes Name
 175704363          1 &node1;.&exampledomain; (local)
 175704619          1 &node2;.&exampledomain;</screen>

  <para>
   The <command>diff</command> command is very helpful: It compares the
   &corosync; configuration on all nodes (if not stated otherwise) and
   prints the difference between:
  </para>

<screen>&prompt.root;<command>crm</command> corosync diff
--- &node2;
+++ &node1;
@@ -46,2 +46,2 @@
-       expected_votes: 2
-       two_node: 1
+       expected_votes: 1
+       two_node: 0</screen>

  <para>
   For more details, see
   <link xlink:href="http://crmsh.nongnu.org/crm.8.html#cmdhelp_corosync"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec.ha.config.crm.global">
  <title>Configuring Global Cluster Options</title>

  <para>
   Global cluster options control how the cluster behaves when confronted
   with certain situations. The predefined values can usually be kept.
   However, to make key functions of your cluster work correctly, you need
   to adjust the following parameters after basic cluster setup:
  </para>

  <procedure>
   <title>Modifying Global Cluster Options With <command>crm</command></title>
   <step>
    <para>
     Log in as &rootuser; and start the <command>crm</command> tool:
    </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
   </step>
   <step>
    <para>
     Use the following commands to set the options for two-node clusters
     only:
    </para>
<screen>&prompt.crm.conf;<command>property</command> no-quorum-policy=ignore
&prompt.crm.conf;<command>property</command> stonith-enabled=true</screen>
    <important>
     <title>No Support Without &stonith;</title>
     <para>
      A cluster without &stonith; is not supported.
     </para>
    </important>
   </step>
   <step>
    <para>
     Show your changes:
    </para>
<screen>&prompt.crm.conf;<command>show</command>
property $id="cib-bootstrap-options" \
   dc-version="1.1.1-530add2a3721a0ecccb24660a97dbfdaa3e68f51" \
   cluster-infrastructure="corosync" \
   expected-quorum-votes="2" \
   no-quorum-policy="ignore" \
   stonith-enabled="true"</screen>
   </step>
   <step>
    <para>
     Commit your changes and exit:
    </para>
<screen>&prompt.crm.conf;<command>commit</command>
&prompt.crm.conf;<command>exit</command></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ha.config.crm.resources">
  <title>Configuring Cluster Resources</title>

  <para>
   As a cluster administrator, you need to create cluster resources for
   every resource or application you run on servers in your cluster. Cluster
   resources can include Web sites, e-mail servers, databases, file systems,
   virtual machines, and any other server-based applications or services you
   want to make available to users at all times.
  </para>

  <para>
   For an overview of resource types you can create, refer to
   <xref linkend="sec.ha.config.basics.resources.types"/>.
  </para>

  <sect2 xml:id="sec.ha.manual_config.create">
   <title>Creating Cluster Resources</title>
   <para>
    There are three types of RAs (Resource Agents) available with the
    cluster (for background information, see
    <xref linkend="sec.ha.config.basics.raclasses"/>). To add a new resource
    to the cluster, proceed as follows:
   </para>
   <procedure xml:id="pro.ha.manual_config.create">
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> tool:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Configure a primitive IP address:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> myIP ocf:heartbeat:IPaddr \
     params ip=127.0.0.99 op monitor interval=60s</screen>
     <para>
      The previous command configures a <quote>primitive</quote> with the
      name <literal>myIP</literal>. You need to choose a class (here
      <literal>ocf</literal>), provider (<literal>heartbeat</literal>), and
      type (<literal>IPaddr</literal>). Furthermore, this primitive expects
      other parameters like the IP address. Change the address to your
      setup.
     </para>
    </step>
    <step>
     <para>
      Display and review the changes you have made:
     </para>
<screen>&prompt.crm.conf;<command>show</command></screen>
    </step>
    <step>
     <para>
      Commit your changes to take effect:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
<!--<sect2 id="sec.ha.manual_config.lsb">
    <title>LSB Initialization Scripts</title>
    <para>
    All LSB scripts are commonly found in the directory
    <filename>/etc/init.d</filename>. They must have several actions
    implemented, which are at least <literal>start</literal>,
    <literal>stop</literal>, <literal>restart</literal>,
    <literal>reload</literal>, <literal>force-reload</literal>, and
    <literal>status</literal> as explained in
    <ulink
    url="http://www.linux-foundation.org/spec/refspecs/LSB_1.3.0/gLSB/gLSB/iniscrptact.html"/>.
    </para>
    
    </sect2>-->
  </sect2>

<!-- FATE#310319 -->

  <sect2 xml:id="sec.ha.manual_config.rsc_template">
   <title>Creating Resource Templates</title>
   <para>
    If you want to create several resources with similar configurations, a
    resource template simplifies the task. See also
    <xref linkend="sec.ha.config.basics.constraints.templates"/> for some
    basic background information. Do not confuse them with the
    <quote>normal</quote> templates from
    <xref linkend="sec.ha.manual_config.template"/>. Use the
    <command>rsc_template</command> command to get familiar with the syntax:
   </para>
<screen>&prompt.root;<command>crm</command> configure rsc_template
usage: rsc_template &lt;name&gt; [&lt;class&gt;:[&lt;provider&gt;:]]&lt;type&gt;
        [params &lt;param&gt;=&lt;value&gt; [&lt;param&gt;=&lt;value&gt;...]]
        [meta &lt;attribute&gt;=&lt;value&gt; [&lt;attribute&gt;=&lt;value&gt;...]]
        [utilization &lt;attribute&gt;=&lt;value&gt; [&lt;attribute&gt;=&lt;value&gt;...]]
        [operations id_spec
            [op op_type [&lt;attribute&gt;=&lt;value&gt;...] ...]]</screen>
   <para>
    For example, the following command creates a new resource template with
    the name <literal>BigVM</literal> derived from the
    <literal>ocf:heartbeat:Xen</literal> resource and some default values
    and operations:
   </para>
<screen>&prompt.crm.conf;<command>rsc_template</command> BigVM ocf:heartbeat:Xen \
   params allow_mem_management="true" \
   op monitor timeout=60s interval=15s \
   op stop timeout=10m \
   op start timeout=10m</screen>
   <para>
    Once you defined the new resource template, you can use it in primitives
    or reference it in order, colocation, or rsc_ticket constraints. To
    reference the resource template, use the <literal>@</literal> sign:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> MyVM1 @BigVM \
   params xmfile="/etc/xen/shared-vm/MyVM1" name="MyVM1"</screen>
   <para>
    The new primitive MyVM1 is going to inherit everything from the BigVM
    resource templates. For example, the equivalent of the above two would
    be:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> MyVM1 ocf:heartbeat:Xen \
   params xmfile="/etc/xen/shared-vm/MyVM1" name="MyVM1" \
   params allow_mem_management="true" \
   op monitor timeout=60s interval=15s \
   op stop timeout=10m \
   op start timeout=10m</screen>
   <para>
    If you want to overwrite some options or operations, add them to your
    (primitive) definition. For instance, the following new primitive MyVM2
    doubles the timeout for monitor operations but leaves others untouched:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> MyVM2 @BigVM \
   params xmfile="/etc/xen/shared-vm/MyVM2" name="MyVM2" \
   op monitor timeout=120s interval=30s    </screen>
   <para>
    A resource template may be referenced in constraints to stand for all
    primitives which are derived from that template. This helps to produce a
    more concise and clear cluster configuration. Resource template
    references are allowed in all constraints except location constraints.
    Colocation constraints may not contain more than one template reference.
   </para>
  </sect2>

<!--  <sect2 id="sec.ha.manual_config.example">
   <title>Example Configuration for an NFS Server</title>
   <para>
    To set up the NFS server, you need to complete the following:
   </para>
   <procedure>
    <step>
     <para>
      Configure DRBD.
     </para>
    </step>
    <step>
     <para>
      Set up a file system Resource.
     </para>
    </step>
    <step>
     <para>
      Set up the NFS server and configure the IP address.
     </para>
    </step>
   </procedure>
   <para>
    Learn how to achieve this in the following subsection.
   </para>
   <sect3 id="sec.ha.manual_config.example.drbd">
    <title>Configuring DRBD</title>
<!-\- See also DocComment#8052 -\->
    <para>
     Before starting with the DRBD &ha; configuration, set up a DRBD device
     manually. Basically this is configuring DRBD and letting it
     synchronize. The exact procedure is described in
     <xref linkend="cha.ha.drbd"/>. For now, assume that you configured a
     resource <literal>r0</literal> that may be accessed at the device
     <filename>/dev/drbd_r0</filename> on both of your cluster nodes.
    </para>
    <para>
     The DRBD resource is an OCF master/slave resource. This can be found in
     the description of the metadata of the DRBD resource agent. However, it
     is important that the actions <literal>promote</literal> and
     <literal>demote</literal> exist in the <literal>actions</literal>
     section of the metadata. These are mandatory for master/slave resources
     and commonly not available to other resources.
    </para>
    <para>
     For &ha;, master/slave resources may have multiple masters on different
     nodes. It is even possible to have a master and slave on the same node.
     Therefore, configure this resource in a way that there is exactly one
     master and one slave, each running on different nodes. Do this with the
     meta attributes of the <literal>master</literal> resource. Master/slave
     resources are special types of clone resources in &ha;. Every master
     and every slave counts as a clone.
    </para>
    <para>
     Proceed as follows to configure a DRBD resource:
    </para>
    <procedure>
     <step>
      <para>
       Open a shell and become &rootuser;.
      </para>
     </step>
     <step>
      <para>
       Enter <command>crm</command> <option>configure</option> to open the
       internal shell.
      </para>
     </step>
     <step>
      <para>
       If you have a two-node cluster, set the following properties per
       <literal>ms</literal> resource:
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> my-stonith stonith:external/ipmi ...
&prompt.crm.conf;<command>ms</command> ms_drbd_r0 res_drbd_r0 meta \
   globally-unique=false ...
&prompt.crm.conf;<command>property</command> no-quorum-policy=ignore
&prompt.crm.conf;<command>property</command> stonith-enabled=true</screen>
     </step>
     <step>
      <para>
       Create a primitive DRBD resource:
      </para>
<screen>&prompt.crm.conf;<command
 >primitive</command> drbd_r0 ocf:linbit:drbd params \
 drbd drbd_resource=r0 op monitor interval="30s"</screen>
     </step>
     <step>
      <para>
       Create a master/slave resource:
      </para>
<screen>&prompt.crm.conf;<command
>ms</command> ms_drbd_r0 res_drbd_r0 meta master-max=1 \
 master-node-max=1 clone-max=2 clone-node-max=1 notify=true</screen>
     </step>
     <step>
      <para>
       Specify an colocation and order constraint:
      </para>
<screen>&prompt.crm.conf;<command
>colocation</command> fs_on_drbd_r0 inf: res_fs_r0 ms_drbd_r0:Master
&prompt.crm.conf;<command
>order</command> fs_after_drbd_r0 inf: ms_drbd_r0:promote<!-\-
      -\-> res_fs_r0:start</screen>
     </step>
     <step>
      <para>
       Display your changes with the <command>show</command> command.
      </para>
     </step>
     <step>
      <para>
       Commit your changes with the <command>commit</command> command.
      </para>
     </step>
    </procedure>
<!-\-<screen>&prompt.crm;<command>configure</command>
     &prompt.crm.conf;<command
     >primitive</command> drbd_r0 ocf:linbit:drbd params \
     drbd drbd_resource=r0 op monitor interval="30s"
     &prompt.crm.conf;<command>ms</command> drbd_resource drbd_r0 \
     meta clone_max=2 clone_node_max=1 master_max=1 master_node_max=1 notify=true
     &prompt.crm.conf;<command>commit</command></screen>-\->
   </sect3>
   <sect3 id="sec.ha.manual_config.example.filesystem">
    <title>Setting Up a File System Resource</title>
    <para>
     The <literal>filesystem</literal> resource is configured as an OCF
     primitive resource with DRBD. It has the task of mounting and
     unmounting a device to a directory on start and stop requests. In this
     case, the device is <filename>/dev/drbd_r0</filename> and the directory
     to use as mount point is <filename>/srv/failover</filename>. The file
     system used is <literal>xfs</literal>.
    </para>
    <para>
     Use the following commands in the <command>crm</command> shell to
     configure a file system resource:
    </para>
<!-\- 
     According to Philip Reiser (Linbit/DRBD inventor)
     ocf:heartbeat:Filesystem is unmaintained, therefor we need to use
     ocf:linbit:drbd
    -\->
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> filesystem_resource \
    ocf:linbit:drbd \
    params device=/dev/drbd_r0 directory=/srv/failover fstype=xfs</screen>
   </sect3>
   <sect3 id="sec.ha.manual_config.example.nfs">
    <title>NFS Server and IP Address</title>
    <para>
     To make the NFS server always available at the same IP address, use an
     additional IP address as well as the ones the machines use for their
     normal operations. This IP address is then assigned to the active NFS
     server in addition to the system's IP address.
    </para>
    <para>
     The NFS server and the IP address of the NFS server should always be
     active on the same machine. In this case, the start sequence is not
     very important. They may even be started at the same time. These are
     the typical requirements for a group resource.
    </para>
    <para>
     Before starting the &ha; RA configuration, configure the NFS server
     with &yast;. Do not let the system start the NFS server. Just set up
     the configuration file. If you want to do that manually, see the manual
     page exports(5) (<command>man 5 exports</command>). The configuration
     file is <filename>/etc/exports</filename>. The NFS server is configured
     as an LSB resource.
    </para>
    <para>
     Configure the IP address completely with the &ha; RA configuration. No
     additional modification is necessary in the system. The IP address RA
     is an OCF RA.
    </para>
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> nfs_resource ocf:nfsserver \
     params nfs_ip=10.10.0.1  nfs_shared_infodir=/shared
&prompt.crm.conf;<command>primitive</command> ip_resource ocf:heartbeat:IPaddr \
     params ip=10.10.0.1
&prompt.crm.conf;<command>group</command> nfs_group nfs_resource ip_resource
&prompt.crm.conf;<command>show</command>
primitive ip_res ocf:heartbeat:IPaddr \
        params ip="192.168.1.10"
primitive nfs_res ocf:heartbeat:nfsserver \
        params nfs_ip="192.168.1.10" nfs_shared_infodir="/shared"
group nfs_group nfs_res ip_res
&prompt.crm.conf;<command>commit</command>
&prompt.crm.conf;<command>end</command>
&prompt.crm;<command>quit</command></screen>
   </sect3>
  </sect2>
-->

  <sect2 xml:id="sec.ha.manual_create.stonith">
   <title>Creating a &stonith; Resource</title>
   <para>
    From the <command>crm</command> perspective, a &stonith; device is
    just another resource. To create a &stonith; resource, proceed as
    follows:
   </para>
   <procedure>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command>
      interactive shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Get a list of all &stonith; types with the following command:
     </para>
<screen>&prompt.crm;<command>ra</command> list stonith
apcmaster                  apcmastersnmp              apcsmart
baytech                    bladehpi                   cyclades
drac3                      external/drac5             external/dracmc-telnet
external/hetzner           external/hmchttp           external/ibmrsa
external/ibmrsa-telnet     external/ipmi              external/ippower9258
external/kdumpcheck        external/libvirt           external/nut
external/rackpdu           external/riloe             external/sbd
external/vcenter           external/vmware            external/xen0
external/xen0-ha           fence_legacy               ibmhmc
ipmilan                    meatware                   nw_rpc100s
rcd_serial                 rps10                      suicide
wti_mpc                    wti_nps</screen>
    </step>
    <step xml:id="st.ha.manual_create.stonith.type">
     <para>
      Choose a &stonith; type from the above list and view the list of
      possible options. Use the following command:
     </para>
<screen>&prompt.crm;<command>ra</command> info stonith:external/ipmi
IPMI STONITH external device (stonith:external/ipmi)

ipmitool based power management. Apparently, the power off
method of ipmitool is intercepted by ACPI which then makes
a regular shutdown. If case of a split brain on a two-node
it may happen that no node survives. For two-node clusters
use only the reset method.

Parameters (* denotes required, [] the default):

hostname (string): Hostname
    The name of the host to be managed by this STONITH device.
...<!--
ipaddr (string): IP Address
    The IP address of the STONITH device.

userid (string): Login
    The username used for logging in to the STONITH device.

passwd (string): Password
    The password used for logging in to the STONITH device.

interface (string, [lan]): IPMI interface
    IPMI interface to use, such as "lan" or "lanplus".

stonith-timeout (time, [60s]):
    How long to wait for the STONITH action to complete. Overrides the stonith-timeout cluster property

priority (integer, [0]):
    The priority of the stonith resource. The lower the number, the higher the priority.

Operations' defaults (advisory minimum):

    start         timeout=15
    stop          timeout=15
    status        timeout=15
    monitor_0     interval=15 timeout=15 start-delay=15--></screen>
    </step>
    <step>
     <para>
      Create the &stonith; resource with the <literal>stonith</literal>
      class, the type you have chosen in
      <xref linkend="st.ha.manual_create.stonith.type" xrefstyle="select:label nopage"/>,
      and the respective parameters if needed, for example:
     </para>
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> my-stonith stonith:external/ipmi \
    params hostname="&node1;" \
    ipaddr="&subnetI;.221" \
    userid="admin" passwd="secret" \
    op monitor interval=60m timeout=120s  </screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.constraints">
   <title>Configuring Resource Constraints</title>
   <para>
    Having all the resources configured is only one part of the job. Even if
    the cluster knows all needed resources, it might still not be able to
    handle them correctly. For example, try not to mount the file system on
    the slave node of DRBD (in fact, this would fail with DRBD). Define
    constraints to make these kind of information available to the cluster.
   </para>
   <para>
    For more information about constraints, see
    <xref linkend="sec.ha.config.basics.constraints"/>.
   </para>
   <sect3 xml:id="sec.ha.manual_config.constraints.locational">
    <title>Locational Constraints</title>
    <para>
     The <command>location</command> command defines on which nodes a
     resource may be run, may not be run or is preferred to be run.
    </para>
    <para>
     This type of constraint may be added multiple times for each resource.
     All <literal>location</literal> constraints are evaluated for a given
     resource. A simple example that expresses a preference to run the
     resource <literal>fs1</literal> on the node with the name
     <systemitem class="server">&node1;</systemitem> to 100 would be the
     following:
    </para>
<!-- 
     location ID RSC {node_pref|rules}
     
     Grammar:
     node_pref :: <score>: <node>
     
     rules ::
     rule [id_spec] [$role=<role>] <score>: <expression>
     [rule [id_spec] [$role=<role>] <score>: <expression> ...]
     
     id_spec :: $id=<id> | $id-ref=<id>
     score :: <number> | <attribute> | [-]inf
     expression :: <single_exp> [bool_op <simple_exp> ...]
     | <date_expr>
     bool_op :: or | and
     single_exp :: <attribute> [type:]<binary_op> <value>
     | <unary_op> <attribute>
     type :: string | version | number
     binary_op :: lt | gt | lte | gte | eq | ne
     unary_op :: defined | not_defined
     
     date_expr :: date_op <start> [<end>]  (TBD) 
    -->
<screen>&prompt.crm.conf;<command>location</command> loc-fs1 fs1 100: &node1;</screen>
    <para>
     Another example is a location with pingd:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> pingd pingd \
    params name=pingd dampen=5s multiplier=100 host_list="r1 r2"
&prompt.crm.conf;<command>location</command> loc-node_pref internal_www \
    rule 50: #uname eq &node1; \
    rule pingd: defined pingd</screen>
    <para>
     Another use case for location constraints are grouping primitives as a
     <emphasis>resource set</emphasis>. This can be useful if several
     resources depend on, for example, a ping attribute for network
     connectivity. In former times, the <literal>-inf/ping</literal> rules
     needed to be duplicated several times in the configuration, making it
     unnecessarily complex.
    </para>
    <para>
     The following example creates a resource set
     <varname>loc-&node1;</varname>, referencing the virtual IP addresses
     <varname>vip1</varname> and <varname>vip2</varname>:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> vip1 ocf:heartbeat:IPaddr2 params ip=&subnetI;.5
&prompt.crm.conf;<command>primitive</command> vip1 ocf:heartbeat:IPaddr2 params ip=&subnetI;.6
&prompt.crm.conf;<command>location</command> loc-&node1; { vip1 vip2 } inf: &node1; </screen>
    <para>
     In some cases it is much more efficient and convenient to use resource
     patterns for your <command>location</command> command. A resource
     pattern is a regular expression between two slashes. For example, the
     above virtual IP addresses can be all matched with the following:
    </para>
<screen>&prompt.crm.conf;<command>location</command>  loc-&node1; /vip.*/ inf: &node1;</screen>
   </sect3>
   <sect3 xml:id="sec.ha.manual_config.constraints.collocational">
    <title>Colocational Constraints</title>
    <para>
     The <command>colocation</command> command is used to define what
     resources should run on the same or on different hosts.
    </para>
    <para>
     It is only possible to set a score of either +inf or -inf, defining
     resources that must always or must never run on the same node. It is
     also possible to use non-infinite scores. In that case the colocation
     is called <emphasis>advisory</emphasis> and the cluster may decide not
     to follow them in favor of not stopping other resources if there is a
     conflict.
    </para>
    <para>
     For example, to run the resources with the IDs
     <literal>filesystem_resource</literal> and <literal>nfs_group</literal>
     always on the same host, use the following constraint:
    </para>
<!-- 
     colocation ID SCORE: RSC[:ROLE] RSC[:ROLE]
     
     Example: colocation dummy_and_apache -inf: apache dummy
    -->
<screen>&prompt.crm.conf;<command>colocation</command> nfs_on_filesystem inf: nfs_group filesystem_resource</screen>
    <para>
     For a master slave configuration, it is necessary to know if the
     current node is a master in addition to running the resource locally.
    </para>
   </sect3>
   <sect3 xml:id="sec.ha.manual_config.constraints.weak-bond">
    <title>Collocating Sets for Resources Without Dependency</title>
<!-- FATE#314917 -->
    <para>
     Sometimes it is useful to be able to place a group of resources on the
     same node (defining a colocation constraint), but without having hard
     dependencies between the resources.
    </para>
    <para>
     Use the command <command>weak-bond</command> if you want to place
     resources on the same node, but without any action if one of them
     fails.
    </para>
<screen>&prompt.root;<command>crm</command> configure assist weak-bond RES1 RES2</screen>
    <para>
     The implementation of <command>weak-bond</command> creates a dummy
     resource and a colocation constraint with the given resources
     automatically.
    </para>
   </sect3>
   <sect3 xml:id="sec.ha.manual_config.constraints.ordering">
    <title>Ordering Constraints</title>
    <para>
     The <command>order</command> command defines a sequence of action.
    </para>
    <para>
     Sometimes it is necessary to provide an order of resource actions or
     operations. For example, you cannot mount a file system before the
     device is available to a system. Ordering constraints can be used to
     start or stop a service right before or after a different resource
     meets a special condition, such as being started, stopped, or promoted
     to master.
    </para>
    <para>
     Use the following command in the <command>crm</command> shell to
     configure an ordering constraint:
    </para>
<!-- order ID score-type: FIRST-RSC[:ACTION] THEN-RSC[:ACTION] 
     
     score-type :: advisory | mandatory | <score>
     
     Example: order c_apache_1 mandatory: apache:start ip_1
    -->
<screen>&prompt.crm.conf;<command>order</command> nfs_after_filesystem mandatory: filesystem_resource nfs_group</screen>
   </sect3>
   <sect3 xml:id="sec.ha.manual_config.constraints.example">
    <title>Constraints for the Example Configuration</title>
    <para>
     The example used for this section would not work without additional
     constraints. It is essential that all resources run on the same machine
     as the master of the DRBD resource. The DRBD resource must be master
     before any other resource starts. Trying to mount the DRBD device when
     it is not the master simply fails. The following constraints must be
     fulfilled:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The file system must always be on the same node as the master of the
       DRBD resource.
      </para>
<screen>&prompt.crm.conf;<command>colocation</command> filesystem_on_master inf: \
    filesystem_resource drbd_resource:Master</screen>
     </listitem>
     <listitem>
      <para>
       The NFS server and the IP address must be on the same node as the
       file system.
      </para>
<screen>&prompt.crm.conf;<command>colocation</command> nfs_with_fs inf: \
   nfs_group filesystem_resource</screen>
     </listitem>
     <listitem>
      <para>
       The NFS server and the IP address start after the file system is
       mounted:
      </para>
<screen>&prompt.crm.conf;<command>order</command> nfs_second mandatory: \
   filesystem_resource:start nfs_group</screen>
     </listitem>
     <listitem>
      <para>
       The file system must be mounted on a node after the DRBD resource is
       promoted to master on this node.
      </para>
<screen>&prompt.crm.conf;<command>order</command> drbd_first inf: \
    drbd_resource:promote filesystem_resource:start</screen>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.failover">
   <title>Specifying Resource Failover Nodes</title>
   <para>
    To determine a resource failover, use the meta attribute
    migration-threshold. In case failcount exceeds migration-threshold on
    all nodes, the resource will remain stopped. For example:
   </para>
<!-- Example (rsc is r1): -->
<screen>&prompt.crm.conf;<command>location</command> rsc1-&node1; rsc1 100: &node1;</screen>
   <para>
    Normally, rsc1 prefers to run on &node1;. If it fails there,
    migration-threshold is checked and compared to the failcount. If
    failcount &gt;= migration-threshold then it is migrated to the node with
    the next best preference.
   </para>
   <para>
    Start failures set the failcount to inf depend on the
    <option>start-failure-is-fatal</option> option. Stop failures cause
    fencing. If there is no STONITH defined, the resource will not migrate.
   </para>
   <para>
    For an overview, refer to
    <xref linkend="sec.ha.config.basics.failover"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.failback">
   <title>Specifying Resource Failback Nodes (Resource Stickiness)</title>
   &failback-nodes;
   <para>
    For an overview, refer to
    <xref linkend="sec.ha.config.basics.failback"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.utilization">
   <title>Configuring Placement of Resources Based on Load Impact</title>
   <para>
    Some resources may have specific capacity requirements such as minimum
    amount of memory. Otherwise, they may fail to start completely or run
    with degraded performance.
   </para>
   <para>
    To take this into account, the &hasi; allows you to specify the
    following parameters:
   </para>
   <remark>dejan 2011-11-24: It is not clear whether location 
      constraints have any influence on resource placement in case 
      placement-strategy is set to something other than default. 
      Or is it explained elsewhere?</remark>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      The capacity a certain node <emphasis>provides</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      The capacity a certain resource <emphasis>requires</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      An overall strategy for placement of resources.
     </para>
    </listitem>
   </orderedlist>
   <para>
    For detailed background information about the parameters and a
    configuration example, refer to
    <xref linkend="sec.ha.config.basics.utilization"/>.
   </para>
   <para>
    To configure the resource's requirements and the capacity a node
    provides, use utilization attributes.
<!-- as described in
    <xref linkend="pro.ha.config.gui.capacity"/>.-->
    <remark>taroth 2013-12-04:
     todo - check if this can be done with hawk and adjust link accordingly</remark>
    You can name the utilization attributes according to your preferences
    and define as many name/value pairs as your configuration needs. In
    certain cases, some agents update the utilization themselves, for
    example the <systemitem class="resource">VirtualDomain</systemitem>.
   </para>
   <para>
    In the following example, we assume that you already have a basic
    configuration of cluster nodes and resources. You now additionally want
    to configure the capacities a certain node provides and the capacity a
    certain resource requires.
<!--The procedure of adding utilization
     attributes is basically the same and only differs in
     <xref
      linkend="step.ha.config.gui.capacity.node"/> and
     <xref
      linkend="step.ha.config.gui.capacity.resource"/>.-->
   </para>
   <procedure>
    <title>Adding Or Modifying Utilization Attributes With <command>crm</command></title>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command>
      interactive shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      To specify the capacity a node <emphasis>provides</emphasis>, use the
      following command and replace the placeholder
      <replaceable>NODE_1</replaceable> with the name of your node:
     </para>
<screen>&prompt.crm.conf;<command>node</command>
<!--
     --><replaceable>NODE_1</replaceable> utilization memory=16384<!--
      --> cpu=8</screen>
     <para>
      With these values, <replaceable>NODE_1</replaceable> would be assumed
      to provide 16GB of memory and 8 CPU cores to resources.
     </para>
    </step>
    <step>
     <para>
      To specify the capacity a resource <emphasis>requires</emphasis>, use:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> xen1 ocf:heartbeat:Xen ... \
     utilization memory=4096 cpu=4</screen>
     <para>
      This would make the resource consume 4096 of those memory units from
      <replaceable>NODE_1</replaceable>, and 4 of the CPU units.
     </para>
    </step>
    <step>
     <para>
      Configure the placement strategy with the <command>property</command>
      command:
     </para>
<screen>&prompt.crm.conf;<command>property</command> ...</screen>
     <para>
      The following values are available:
     </para>
     &placement-strategy-values;
    </step>
    <step>
     <para>
      Commit your changes before leaving &crmsh;:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    The following example demonstrates a three node cluster of equal nodes,
    with 4 virtual machines:
   </para>
<screen>&prompt.crm.conf;<command>node</command> &node1; utilization memory="4000"
&prompt.crm.conf;<command>node</command> &node2; utilization memory="4000"
&prompt.crm.conf;<command>node</command> &node3; utilization memory="4000"
&prompt.crm.conf;<command>primitive</command> xenA ocf:heartbeat:Xen \
    utilization hv_memory="3500" meta priority="10" \
    params xmfile="/etc/xen/shared-vm/vm1"
&prompt.crm.conf;<command>primitive</command> xenB ocf:heartbeat:Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm2"
&prompt.crm.conf;<command>primitive</command> xenC ocf:heartbeat:Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm3"
&prompt.crm.conf;<command>primitive</command> xenD ocf:heartbeat:Xen \
    utilization hv_memory="1000" meta priority="5" \
    params xmfile="/etc/xen/shared-vm/vm4"
&prompt.crm.conf;<command>property</command> placement-strategy="minimal"</screen>
   <para>
    With all three nodes up, xenA will be placed onto a node first, followed
    by xenD. xenB and xenC would either be allocated together or one of them
    with xenD.
   </para>
   <para>
    If one node failed, too little total memory would be available to host
    them all. xenA would be ensured to be allocated, as would xenD. However,
    only one of xenB or xenC could still be placed, and since their priority
    is equal, the result is not defined yet. To resolve this ambiguity as
    well, you would need to set a higher priority for either one.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.monitor">
   <title>Configuring Resource Monitoring</title>
<!--<remark>Add the new monitor command: $ crm configure monitor usage: monitor &lt;rsc>[:&lt;role>]
    &lt;interval>[:&lt;timeout>] </remark>-->
   <para>
    To monitor a resource, there are two possibilities: either define a
    monitor operation with the <command>op</command> keyword or use the
    <command>monitor</command> command. The following example configures an
    Apache resource and monitors it every 60 seconds with the
    <literal>op</literal> keyword:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> apache apache \
  params ... \
  <emphasis>op monitor interval=60s timeout=30s</emphasis></screen>
   <para>
    The same can be done with:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> apache apache \
   params ...
&prompt.crm.conf;<command>monitor</command> apache 60s:30s</screen>
   <para>
    For an overview, refer to
    <xref linkend="sec.ha.config.basics.monitoring"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.group">
   <title>Configuring a Cluster Resource Group</title>
<!--Text mostly copied from "Advanced Configuration"-->
   <para>
    One of the most common elements of a cluster is a set of resources that
    needs to be located together. Start sequentially and stop in the reverse
    order. To simplify this configuration we support the concept of groups.
    The following example creates two primitives (an IP address and an
    e-mail resource):
   </para>
   <procedure>
    <step>
     <para>
      Run the <command>crm</command> command as system administrator. The
      prompt changes to <literal>&crm.live;</literal>.
     </para>
    </step>
    <step>
     <para>
      Configure the primitives:
     </para>
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> Public-IP ocf:heartbeat:IPaddr \
   params ip=1.2.3.4 id= Public-IP
&prompt.crm.conf;<command>primitive</command> Email systemd:postfix \
   params id=Email</screen>
    </step>
    <step>
     <para>
      Group the primitives with their relevant identifiers in the correct
      order:
     </para>
<screen>&prompt.crm.conf;<command>group</command> g-mailsvc Public-IP Email</screen>
    </step>
   </procedure>
   <remark>toms 2013-03-28: FATE#313193:</remark>
   <para>
    To change the order of a group member, use the
    <command>modgroup</command> command from the
    <command>configure</command> subcommand. Use the following commands to
    move the primitive <literal>Email</literal> before
    <literal>Public-IP</literal>. (This is just to demonstrate the feature):
   </para>
<screen>&prompt.crm.conf;<command>modgroup</command> g-mailsvc add Email before Public-IP</screen>
   <para>
    In case you want to remove a resource from a group (for example,
    <literal>Email</literal>), use this command:
   </para>
<screen>&prompt.crm.conf;<command>modgroup</command> g-mailsvc remove Email</screen>
   <para>
    For an overview, refer to
    <xref linkend="sec.ha.config.basics.resources.advanced.groups"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.clone">
   <title>Configuring a Clone Resource</title>
<!-- Text mostly copied from "Advanced Configuration" -->
   <para>
<!-- FIXME -->
    Clones were initially conceived as a convenient way to start N instances
    of an IP resource and have them distributed throughout the cluster for
    load balancing. They have turned out to be useful for several other
    purposes, including integrating with DLM, the fencing subsystem and
    OCFS2. You can clone any resource, provided the resource agent supports
    it.
   </para>
   <para>
    Learn more about cloned resources in
    <xref linkend="sec.ha.config.basics.resources.advanced.clones"/>.
   </para>
   <sect3 xml:id="sec.ha.manual_config.clone.anonymous">
    <title>Creating Anonymous Clone Resources</title>
    <para>
     To create an anonymous clone resource, first create a primitive
     resource and then refer to it with the <command>clone</command>
     command. Do the following:
    </para>
    <procedure>
     <step>
      <para>
       Log in as &rootuser; and start the <command>crm</command>
       interactive shell:
      </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
     </step>
     <step>
      <para>
       Configure the primitive, for example:
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> Apache ocf:heartbeat:apache</screen>
     </step>
     <step>
      <para>
       Clone the primitive:
      </para>
<screen>&prompt.crm.conf;<command>clone</command> cl-apache Apache </screen>
     </step>
    </procedure>
   </sect3>
<!-- 
    <sect2 id="sec.ha.manual_config.clone.globally" os="notdefinied">
    <title>Creating Globally Unique Clone Resources</title>
    <remark>Haven't found an example. Any idea?</remark>
    <para> FIXME </para>
    </sect2>
   -->
   <sect3 xml:id="sec.ha.manual_config.clone.stateful">
    <title>Creating Stateful/Multi-State Clone Resources</title>
    <para>
     To create a stateful clone resource, first create a primitive resource
     and then the multi-state resource. The multi-state resource must
     support at least promote and demote operations.
    </para>
    <procedure>
     <step>
      <para>
       Log in as &rootuser; and start the <command>crm</command>
       interactive shell:
      </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
     </step>
     <step>
      <para>
       Configure the primitive. Change the intervals if needed:
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> my-rsc ocf:myCorp:myAppl \
    op monitor interval=60 \
    op monitor interval=61 role=Master</screen>
     </step>
     <step>
      <para>
       Create the multi-state resource:
      </para>
<screen>&prompt.crm.conf;<command>ms</command> ms-rsc my-rsc</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ha.config.crm">
  <title>Managing Cluster Resources</title>

  <para>
   Apart from the possibility to configure your cluster resources, the
   <command>crm</command> tool also allows you to manage existing resources.
   The following subsections gives you an overview.
  </para>

  <sect2 xml:id="sec.ha.manual_config.start">
   <title>Starting a New Cluster Resource</title>
   <para>
    To start a new cluster resource you need the respective identifier.
    Proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command>
      interactive shell:
     </para>
<screen>&prompt.root;<command>crm</command></screen>
    </step>
    <step>
     <para>
      Switch to the resource level:
     </para>
<screen>&prompt.crm;<command>resource</command></screen>
    </step>
    <step>
     <para>
      Start the resource with <command>start</command> and press the
      <keycap function="tab"/> key to show all known resources:
     </para>
<screen>&prompt.crm.res;<command>start</command>&nbsp;<replaceable>ID</replaceable></screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.cleanup">
   <title>Cleaning Up Resources</title>
   <para>
    A resource will be automatically restarted if it fails, but each failure
    raises the resource's failcount. If a
    <literal>migration-threshold</literal> has been set for that resource,
    the node will no longer be allowed to run the resource as soon as the
    number of failures has reached the migration threshold.
   </para>
   <procedure>
    <step>
     <para>
      Open a shell and log in as user &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Get a list of all your resources:
     </para>
<screen>&prompt.root;<command>crm</command> resource list
  ...
Resource Group: dlm-clvm:1
         dlm:1  (ocf::pacemaker:controld) Started 
         clvm:1 (ocf::lvm2:clvmd) Started
         cmirrord:1     (ocf::lvm2:cmirrord) Started</screen>
    </step>
    <step>
     <para>
      To clean up the resource <literal>dlm</literal>, for example:
     </para>
<screen>&prompt.root;<command>crm</command> resource cleanup dlm</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.remove">
   <title>Removing a Cluster Resource</title>
   <para>
    Proceed as follows to remove a cluster resource:
   </para>
   <procedure>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command>
      interactive shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Run the following command to get a list of your resources:
     </para>
<screen>&prompt.crm;<command>resource</command> status</screen>
     <para>
      For example, the output can look like this (whereas myIP is the
      relevant identifier of your resource):
     </para>
<screen>myIP    (ocf::IPaddr:heartbeat) ...</screen>
    </step>
    <step>
     <para>
      Delete the resource with the relevant identifier (which implies a
      <command>commit</command> too):
     </para>
<screen>&prompt.crm;<command>configure</command> delete <replaceable>YOUR_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Commit the changes:
     </para>
<screen>&prompt.crm;<command>configure</command> commit</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.migrate">
   <title>Migrating a Cluster Resource</title>
   <para>
    Although resources are configured to automatically fail over (or
    migrate) to other nodes of the cluster if a hardware or
    software failure occurs, you can also manually move a resource to another node
    using either &hawk; or the command line.
   </para>
   <para>
    Use the <command>migrate</command> command for this task. For example,
    to migrate the resource <literal>ipaddress1</literal> to a cluster node
    named <systemitem class="domainname">&node2;</systemitem>, use these
    commands:
   </para>
<screen>&prompt.root;<command>crm</command> resource
&prompt.crm.res;<command>migrate</command> ipaddress1 &node2;</screen>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.tag">
   <title>Grouping/Tagging Resources</title>
<!-- https://fate.suse.com/315101 -->
   <para>
    Tags are a way to refer to multiple resources at once, without creating
    any colocation or ordering relationship between them. This can be useful
    for grouping conceptually related resources. For example, if you have
    several resources related to a database, create a tag called
    <literal>databases</literal> and add all resources related to the
    database to this tag:
   </para>
<screen>&prompt.root;<command>crm</command> configure databases: db1 db2 db3</screen>
   <para>
    This allows you to start them all with a single command:
   </para>
<screen>&prompt.root;<command>crm</command> resource start databases</screen>
   <para>
    Similarly, you can stop them all too:
   </para>
<screen>&prompt.root;<command>crm</command> resource stop databases</screen>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.cli.maint.mode">
   <title>Using Maintenance Mode</title>
<!--   <remark>toms 2013-03-28: FATE#313381</remark>-->
   
   &maint-mode-basics;
   <para>
    With regard to that, &hasi; provides <literal>maintenance</literal>
    options on several levels:
   </para>
   <variablelist>
    <varlistentry>
     <term>Applying Maintenance Mode to your Cluster</term>
     <listitem>
      <para>
       In case you want to put the whole cluster in maintenance mode, use
       the following command:
      </para>
<screen>&prompt.root;<command>crm</command> configure property maintenance-mode=true</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Applying Maintenance Mode to Nodes</term>
     <listitem>
      <para>
<!--taroth 2014-10-01: commenting because of bnc#869684: 
       If your cluster consists of more than 3 nodes, you can 
       easily set one node to maintenance mode, while the other nodes 
       continue their normal operation.-->
       For example, to put the node <literal>&node1;</literal> into
       maintenance mode:
      </para>
<screen>&prompt.root;<command>crm</command> node maintenance &node1;</screen>
      <para>
       The <command>crm status</command> command will show the maintenance
       mode for &node1; and no more resources will be allocated to that
       node. To remove the maintenance flag from the node, use:
      </para>
<screen>&prompt.root;<command>crm</command> node ready &node1;</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Applying Maintenance Mode to Resources</term>
     <listitem>
      <para>
       If you need to set a specific resource into maintenance mode, use the
       <command>meta</command> command. For example, to put the resource
       <literal>ipaddress</literal> into maintenance mode, enter:
      </para>
<screen>&prompt.root;<command>crm</command> meta ipaddress set maintenance true</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   
   &warning-maint-mode;
   
   <para>
    For more details on what happens to the resources and the cluster while
    in maintenance mode, see
    <xref linkend="sec.ha.config.basics.maint.mode"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.manual_config.cli.health">
   <title>Getting Health Status</title>
   <remark>toms 2014-05-08: FATE#316464</remark>
   <para>
    The <quote>health</quote> status of a cluster or node can be displayed
    with so called <emphasis>scripts</emphasis>. A script can perform
    different tasks&mdash;they are not targeted to health. However, for
    this subsection, we focus on how to get the health status.
   </para>
   <para>
    To get all the details about the <command>health</command> command, use
    <command>describe</command>:
   </para>
<screen>&prompt.root;<command>crm</command> script describe health</screen>
   <para>
    It shows a description and a list of all parameters and their default
    values. To execute a script, use <command>run</command>:
   </para>
   <remark>toms 2014-05-08: see bnc#876882</remark>
<screen>&prompt.root;<command>crm</command> script run health</screen>
   <para>
    If you prefer to run only one step from the suite, the
    <command>describe</command> command lists all available steps in the
    <citetitle>Steps</citetitle> category.
   </para>
   <para>
    For example, the following command executes the first step of the
    <command>health</command> command. The output is stored in the
    <filename>health.json</filename> file for further investigation:
   </para>
<screen>&prompt.root;<command>crm</command> script run health
    statefile='health.json'</screen>
   <para/>
   <para>
    For additional information regarding scripts, see
    <link xlink:href="http://crmsh.github.io/scripts/"/>.
   </para>
  </sect2>
 </sect1>
<!-- FATE#309125 -->
 <sect1 xml:id="sec.ha.config.crm.setpwd">
  <title>Setting Passwords Independent of <filename>cib.xml</filename></title>

  <para>
   In case your cluster configuration contains sensitive information, such
   as passwords, it should be stored in local files. That way, these
   parameters will never be logged or leaked in support reports.
  </para>

  <para>
   Before using <command>secret</command>, better run the
   <command>show</command> command first to get an overview of all your
   resources:
  </para>

<screen>&prompt.root;<command>crm</command> configure show
primitive mydb ocf:heartbeat:mysql \
   params replication_user=admin ...</screen>

  <para>
   If you want to set a password for the above <literal>mydb</literal>
   resource, use the following commands:
  </para>

<screen>&prompt.root;<command>crm</command> resource secret mydb set passwd linux
INFO: syncing /var/lib/heartbeat/lrm/secrets/mydb/passwd to [your node list]</screen>

  <para>
   You can get the saved password back with:
  </para>

<screen>&prompt.root;<command>crm</command> resource secret mydb show passwd
linux</screen>

  <para>
   Note that the parameters need to be synchronized between nodes; the
   <command>crm resource secret</command> command will take care of that. We
   highly recommend to only use this command to manage secret parameters.
  </para>
 </sect1>
<!-- FATE#310358, #310174, #310172 -->
 <sect1 xml:id="sec.ha.config.crm.history">
  <title>Retrieving History Information</title>

  <para>
   Investigating the cluster history is a complex task. To simplify this
   task, &crmsh; contains the <command>history</command> command with its
   subcommands. It is assumed SSH is configured correctly.
  </para>

  <para>
   Each cluster moves states, migrates resources, or starts important
   processes. All these actions can be retrieved by subcommands of
   <command>history</command>. Alternatively, use &hawk; as explained in
   <xref linkend="pro.ha.config.hawk.history.explorer"/>.
  </para>

  <para>
   By default, all <command>history</command> commands look at the events of
   the last hour. To change this time frame, use the
   <command>limit</command> subcommand. The syntax is:
  </para>

<screen>&prompt.root;<command>crm</command> history
&prompt.crm.hist;<command>limit</command> <replaceable>FROM_TIME</replaceable> [<replaceable>TO_TIME</replaceable>]</screen>

  <para>
   Some valid examples include:
  </para>

  <variablelist>
   <varlistentry>
    <term><command>limit</command><literal>4:00pm</literal>
    </term>
    <term><command>limit</command><literal>16:00</literal>
    </term>
    <listitem>
     <para>
      Both commands mean the same, today at 4pm.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>limit</command><literal>2012/01/12 6pm</literal>
    </term>
    <listitem>
     <para>
      January 12th 2012 at 6pm
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>limit</command><literal>"Sun 5 20:46"</literal>
    </term>
    <listitem>
     <para>
      In the current year of the current month at Sunday the 5th at 8:46pm
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

<!-- On SLE 11 HA SP2: 
    # rpm -q python-dateutil
python-dateutil-1.4.1-1.20
  -->

  <para>
   Find more examples and how to create time frames at
   <link xlink:href="http://labix.org/python-dateutil"/>.
  </para>

  <para>
   The <command>info</command> subcommand shows all the parameters which are
   covered by the <command>crm report</command>:
  </para>

<screen>&prompt.crm.hist;<command>info</command>
Source: live
Period: 2012-01-12 14:10:56 - end
Nodes: &node1;
Groups: 
Resources:</screen>

  <para>
   To limit <command>crm report</command> to certain parameters view the
   available options with the subcommand <command>help</command>.
  </para>

  <para>
   To narrow down the level of detail, use the subcommand
   <command>detail</command> with a level:
  </para>

<screen>&prompt.crm.hist;<command>detail</command> 2</screen>

  <para>
   The higher the number, the more detailed your report will be. Default is
   <literal>0</literal> (zero).
  </para>

  <para>
   After you have set above parameters, use <command>log</command> to show
   the log messages.
  </para>

  <para>
   To display the last transition, use the following command:
  </para>

<screen>&prompt.crm.hist;<command>transition</command> -1
INFO: fetching new logs, please wait ...</screen>

  <para>
   This command fetches the logs and runs <command>dotty</command> (from the
   <systemitem class="resource">graphviz</systemitem> package) to show the
   transition graph. The shell opens the log file which you can browse with
   the <keycap function="down"/> and <keycap function="up"/> cursor keys.
  </para>

  <para>
   If you do not want to open the transition graph, use the
   <option>nograph</option> option:
  </para>

<screen>&prompt.crm.hist;<command>transition</command> -1 nograph</screen>
 </sect1>
 <sect1 xml:id="sec.ha.config.crm.more">
  <title>For More Information</title>

  <itemizedlist>
   <listitem>
    <para>
     The crm man page.
    </para>
   </listitem>
   <listitem>
    <para>
     Visit the upstream project documentation at
     <link xlink:href="http://crmsh.github.io/documentation"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     See <xref linkend="art_ha_quick_nfs"/> for an exhaustive example.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
