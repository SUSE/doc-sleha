<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<!--
 This Quick Start should link to the "Installation and Setup Quick Start";
 this would cover all installation and setup of a two-node cluster.
 The rest (DRBD, LVM & NFS) would be covered by this guide.
-->
<?provo dirname="nfs_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art-ha-quick-nfs"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:its="http://www.w3.org/2005/11/its"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&nfsquick;</title>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp format="B d, Y"?></date>
  <xi:include href="common_copyright_gfdl.xml"/>
  <abstract>
   <para>
    &abstract-nfsquick;
   </para>
  </abstract>
  <meta name="title" its:translate="yes">&nfsquick;</meta>
  <meta name="series" its:translate="no">Products &amp; Solutions</meta>
  <meta name="description" its:translate="yes">How to set up highly available NFS storage using DRBD, LVM and &pace; in a two-node cluster</meta>
  <meta name="social-descr" its:translate="yes">Set up highly available NFS storage</meta>
  <meta name="task" its:translate="no">
   <phrase>Installation</phrase>
   <phrase>Administration</phrase>
   <phrase>Clustering</phrase>
  </meta>
  <revhistory xml:id="rh-art-ha-nfs-quick">
   <revision>
    <date>2019-12-09</date>
    <revdescription>
     <para>
      Updated for the initial release of &productname; &productnumber;.
     </para>
    </revdescription>
   </revision>
  </revhistory>
 </info>
 <sect1 xml:id="sec-ha-quick-nfs-usagescenario">
  <title>Usage Scenario</title>
  <para>
   This document helps you set up a highly available NFS server.
   The cluster used for the highly available NFS storage has the
   following properties:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Two nodes: <systemitem class="server">&node1;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.1</systemitem>)
     and <systemitem class="server">&node2;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.2</systemitem>),
     connected to each other via network.
    </para>
   </listitem>
   <listitem>
    <para>
     Two floating, virtual IP addresses (<systemitem class="ipaddress"
      >&nfs-vip-hawk;</systemitem> and <systemitem class="ipaddress"
      >&nfs-vip-exports;</systemitem>), allowing clients to connect to
     a service no matter which physical node it is running on.
     One IP address is used for cluster administration with &hawk2;, and the other
     IP address is used exclusively for the NFS exports.
    </para>
   </listitem>
   <listitem>
    <para>A shared storage device, used as an SBD fencing mechanism.
     This avoids split brain scenarios.
    </para>
   </listitem>
   <listitem>
    <para>
     Failover of resources from one node to the other if the active host breaks
     down (<emphasis>active/passive</emphasis> setup).
    </para>
   </listitem>
   <listitem>
    <para>
     Local storage on each node. The data is synchronized between the
     nodes using DRBD on top of LVM.
    </para>
   </listitem>
   <listitem>
    <para>
      A file system exported through NFS and a separate file system used to track
      the NFS client states.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   After installing and setting up the basic two-node cluster, and extending it
   with storage and cluster resources for NFS, you will have a highly
   available NFS storage server.
  </para>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-installation">
  <title>Preparing a Two-node Cluster</title>
  <para>
   Before you can set up highly available NFS storage, you must prepare a &ha; cluster:
  </para>
  <procedure xml:id="pro-ha-nfs-prepare-cluster">
   <title>Preparing a Two-node Cluster for NFS Storage</title>
   <step>
    <para>
     Install and set up a basic two-node cluster as described in
     <link xlink:href="&doc-url;/html/SLE-HA-all/article-installation.html">&haquick;</link>.
<!--trichardson 2023-05-17: using doc-url because this link needs to be version-specific, not generic-->
    </para>
   </step>
   <step>
    <para>
     On <emphasis>both</emphasis> nodes, install the package <package>nfs-kernel-server</package>:
    </para>
<screen>&prompt.root;<command>zypper install nfs-kernel-server</command></screen>
   </step>
   <step>
    <para>
     On <emphasis>both</emphasis> nodes, set the NFS server scope:
    </para>
    <substeps>
     <step>
      <para>
       Create a new directory named <filename>nfs-server.service.d</filename>:
      </para>
<screen>&prompt.root;<command>mkdir -p /etc/systemd/system/nfs-server.service.d</command></screen>
     </step>
     <step>
      <para>
       Create the file <filename>/etc/systemd/system/nfs-server.service.d/scope.conf</filename>
       and add the following content:
      </para>
<screen>[Service]
ExecStart= <co xml:id="co-nfs-server-empty-exec"/>
ExecStart=/usr/sbin/rpc.nfsd --scope SUSE $RPCNFSDARGS <co xml:id="co-nfs-server-scope"/></screen>
      <calloutlist>
       <callout arearefs="co-nfs-server-empty-exec">
        <para>
         A service can only have one <literal>ExecStart</literal> setting, so the empty
         <literal>ExecStart</literal> line in this override file is used to undo
         any existing <literal>ExecStart</literal> setting in the NFS service file.
        </para>
       </callout>
       <callout arearefs="co-nfs-server-scope">
        <para>
         The scope must be the same on all nodes in the cluster that run the NFS server.
         All clusters using &suse; software can use the same scope, so we recommend setting
         the value to <literal>SUSE</literal>.
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Reload the &systemd; files:
      </para>
<screen>&prompt.root;<command>systemctl daemon-reload</command></screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-lvm">
   <title>Creating LVM Devices</title>
   <para>
    LVM (Logical Volume Manager) enables flexible distribution of storage space
    across several file systems.
   </para>
   <para>
    Use <command>crm cluster run</command> to run these commands on both nodes at once.
   </para>
   <procedure xml:id="pro-ha-nfs-create-lvm-devices">
    <title>Creating LVM Devices for DRBD</title>
    <step>
     <para>
      Check if the locking type of LVM2 is cluster-aware. The keyword
     <literal>locking_type</literal> in
     <filename>/etc/lvm/lvm.conf</filename> must contain the value
     <literal>3</literal> (the default is <literal>1</literal>).
      Copy the configuration to all nodes, if necessary.
     </para>
    </step>
    <step>
     <para>
      Create an LVM physical volume, replacing
      <filename>/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></filename>
      with your corresponding device for LVM:</para>
     <screen>&prompt.root;<command>crm cluster run "pvcreate /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>"</command></screen>
    </step>
    <step>
     <para>Create an LVM volume group <systemitem>nfs</systemitem>
            that includes this physical volume: </para>
     <screen>&prompt.root;<command>crm cluster run "vgcreate nfs /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable>"</command></screen>
    </step>
    <step>
      <para>
       Create a logical volume named <systemitem>share</systemitem> in the
       volume group <systemitem>nfs</systemitem>:
      </para>
      <screen>&prompt.root;<command>crm cluster run "lvcreate -n share -L 20G nfs"</command></screen>
      <para>
       This volume is for the NFS exports.
      </para>
     </step>
     <step>
      <para>
       Create a logical volume named <systemitem>state</systemitem> in the
       volume group <systemitem>nfs</systemitem>:
      </para>
      <screen>&prompt.root;<command>crm cluster run "lvcreate -n state -L 8G nfs"</command></screen>
      <para>
       This volume is for the NFS client states. The 8 GB volume size
       used in this example should support several thousand concurrent NFS clients.
      </para>
     </step>
     <step>
      <para>
       Activate the volume group: </para>
<screen>&prompt.root;<command>crm cluster run "vgchange -ay nfs"</command></screen>
     </step>
   </procedure>
   <para>
    You should now see the following devices on the system:
    <filename>/dev/nfs/share</filename> and <filename>/dev/nfs/state</filename>.
   </para>
  </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-drbd-device">
   <title>Creating DRBD Devices</title>
   <para>
    This section describes how to set up DRBD devices on top of LVM.
    Using LVM as a back-end of DRBD has the following benefits:
   </para>
  <itemizedlist>
   <listitem>
    <para>Easier setup than with LVM on top of DRBD.</para>
   </listitem>
   <listitem>
    <para>Easier administration in case the LVM disks need to be resized or
     more disks are added to the volume group.
    </para>
   </listitem>
  </itemizedlist>
  <para>
    The following procedures result in two DRBD devices: one device for the
    NFS exports, and a second device to track the NFS client states.
   </para>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-config">
    <title>Creating the DRBD Configuration</title>
    <para>
     DRBD configuration files are kept in the <filename>/etc/drbd.d/</filename>
     directory and must end with a <filename class="extension">.res</filename>
     extension. In this procedure, the configuration file is named
     <filename>/etc/drbd.d/nfs.res</filename>.
    </para>
    <procedure xml:id="pro-ha-nfs-create-drbd-config">
     <title>Creating a DRBD Configuration</title>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/nfs.res</filename> with the
        following contents:
      </para>
<screen>resource nfs {
   volume 0 { <co xml:id="co-ha-quick-nfs-drbd-volume"/>
      device           /dev/drbd0; <co xml:id="co-ha-quick-nfs-drbd-device"/>
      disk             /dev/nfs/state; <co xml:id="co-ha-quick-nfs-drbd-disk"/>
      meta-disk        internal; <co xml:id="co-ha-quick-nfs-drbd-metadisk"/>
   }
   volume 1 {
      device           /dev/drbd1;
      disk             /dev/nfs/share;
      meta-disk        internal;
   }

   net {
      protocol  C; <co xml:id="co-ha-quick-nfs-drbd-protocol"/>
      fencing: resource-and-stonith; <co xml:id="co-ha-quick-nfs-fencing-policy"/>
   }

   handlers { <co xml:id="co-ha-quick-nfs-fencing-handlers"/>
      fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
      after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
   }

   connection-mesh { <co xml:id="co-ha-quick-nfs-connectionmesh"/>
      hosts     &node1; &node2;;
   }
   on &node1; { <co xml:id="co-ha-quick-nfs-drbd-on"/>
      address   &subnetI;.1:&drbd.port;;
      node-id   0;
   }
   on &node2; { <xref linkend="co-ha-quick-nfs-drbd-on"/>
      address   &subnetI;.2:&drbd.port;;
      node-id   1;
   }
}</screen>
      <calloutlist>
       <callout arearefs="co-ha-quick-nfs-drbd-volume">
        <para>The volume number for each DRBD device you want to create.</para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-device">
        <para>The DRBD device that applications will access.</para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-disk">
        <para>The lower-level block device used by DRBD to store the actual
         data. This is the LVM device that was created in <xref
          linkend="sec-ha-quick-nfs-lvm"/>.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-metadisk">
        <para>Where the metadata is stored. Using
         <literal>internal</literal>, the metadata is stored together with
         the user data on the same device. See the man page for further
         information.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-protocol">
        <para>The protocol to use for this connection. Protocol <literal>C</literal>
         is the default option. It provides better data availability and does not consider
         a write to be complete until it has reached all local and remote disks.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-fencing-policy">
        <para>
         Specifies the fencing policy <literal>resource-and-stonith</literal> at the DRBD level.
         This policy immediately suspends active I/O operations until &stonith; completes.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-fencing-handlers">
        <para>
         Enables resource-level fencing to prevent &pace; from starting a service
         with outdated data. If the DRBD replication link
         becomes disconnected, the <command>crm-fence-peer.9.sh</command> script
         stops the DRBD resource from being promoted to another node until the replication link
         becomes connected again and DRBD completes its synchronization process.
        </para>
       </callout>
       <callout arearefs="co-ha-quick-nfs-connectionmesh">
        &drbd-connection-mesh;
       </callout>
       <callout arearefs="co-ha-quick-nfs-drbd-on">
        <para>Contains the IP address and a unique identifier for each node.</para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Open <filename>/etc/csync2/csync2.cfg</filename> and check whether the
       following two lines exist:
      </para>
<screen>include /etc/drbd.conf;
include /etc/drbd.d;</screen>
      <para>
       If not, add them to the file.
      </para>
     </step>
     <step>
      <para>
       Copy the file to the other nodes:
      </para>
<screen>&prompt.root;<command>csync2</command> -xv</screen>
      <para>
       For information about &csync;, see
       <xref linkend="sec-ha-installation-setup-csync2"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-activate">
    <title>Activating the DRBD Devices</title>
    <para>
     After preparing the DRBD configuration, activate the devices:
    </para>
    <procedure xml:id="pro-ha-nfs-activate-drbd-devices">
     <title>Activating DRBD Devices</title>
     <step>
      <para>
       If you use a firewall in the cluster, open port
       <systemitem>&drbd.port;</systemitem> in the firewall configuration.
      </para>
     </step>
     <step>
      <para>
       Initialize the metadata storage:
      </para>
<screen>&prompt.root;<command>crm cluster run "drbdadm create-md nfs"</command></screen>
     </step>
     <step>
      <para>
       Create the DRBD devices:
      </para>
<screen>&prompt.root;<command>crm cluster run "drbdadm up nfs"</command></screen>
     </step>
     <step>
      <para>
       The devices do not have data yet, so you can run these commands to
       skip the initial synchronization:
      </para>
<screen>&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap nfs/0</command>
&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap nfs/1</command></screen>
     </step>
     <step>
       <para>Make <systemitem>&node1;</systemitem> primary:</para>
<screen>&prompt.root;<command>drbdadm primary --force nfs</command></screen>
     </step>
      <step>
       <para>Check the DRBD status of <literal>nfs</literal>:</para>
<screen>&prompt.root;<command>drbdadm status nfs</command></screen>
       <para>This returns the following message:</para>
<screen>nfs role:Primary
  volume:0 disk:UpToDate
  volume:1 disk:UpToDate
  &node2; role:Secondary
    volume:0 peer-disk:UpToDate
    volume:1 peer-disk:UpToDate</screen>
     </step>
    </procedure>
    <para>
     You can access the DRBD resources on the block devices
     <filename>/dev/drbd0</filename> and <filename>/dev/drbd1</filename>.
     </para>
   </sect2>

   <sect2 xml:id="sec-ha-quick-nfs-drbd-createfs">
    <title>Creating the File Systems</title>
    <para>
     After activating the DRBD devices, create file systems on them:

    </para>
    <procedure xml:id="pro-ha-nfs-create-drbd-file-systems">
     <title>Creating File Systems for DRBD</title>
     <step>
      <para>
       Create an <literal>ext4</literal> file system on <filename>/dev/drbd0</filename>:
      </para>
      <screen>&prompt.root;<command>mkfs.ext4 /dev/drbd0</command></screen>
     </step>
     <step>
      <para>
       Create an <literal>ext4</literal> file system on <filename>/dev/drbd1</filename>:
      </para>
      <screen>&prompt.root;<command>mkfs.ext4 /dev/drbd1</command></screen>
     </step>
    </procedure>
   </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-resources">
  <title>Creating Cluster Resources</title>
  <para>
   The following procedures describe how to configure the resources required
   for a highly available NFS cluster.
  </para>
  <variablelist>
   <title>Overview of Cluster Resources</title>
   <varlistentry>
    <term>DRBD Primitive and Multi-state Resources</term>
    <listitem>
     <para>
      These resources are used to replicate data. The multi-state resource
      is switched to and from the primary and secondary roles as deemed necessary
      by the cluster resource manager.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>File System Resources</term>
    <listitem>
     <para>
      These resources manage the file system that will be exported, and the
      file system that will track NFS client states.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Kernel Server Resource</term>
    <listitem>
     <para>
      This resource manages the NFS server daemon.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Exports</term>
    <listitem>
     <para>
      This resource is used to export the directory <filename>/srv/nfs/share</filename>
      to clients.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual IP Address</term>
    <listitem>
     <para>
      The initial installation creates an administrative virtual IP address for &hawk2;.
      Create another virtual IP address exclusively for NFS exports. This makes it
      easier to apply security restrictions later.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <itemizedlist>
    <title>Example NFS Scenario</title>
    <listitem>
        <para>The following configuration examples assume that
         <systemitem class="ipaddress">&nfs-vip-exports;</systemitem> is the virtual
         IP address to use for an NFS server which serves clients in the
         <systemitem class="ipaddress">&subnetI;.x/24</systemitem> subnet.</para>
    </listitem>
    <listitem>
        <para>The service exports data served from
         <literal>/srv/nfs/share</literal>. </para>
    </listitem>
    <listitem>
        <para>Into this export directory, the cluster mounts an
            <literal>ext4</literal> file system from the DRBD device
         <filename>/dev/drbd1</filename>.
         This DRBD device sits on top of an LVM logical volume named
         <literal>/dev/nfs/share</literal>.
        </para>
    </listitem>
    <listitem>
     <para>
      The DRBD device <literal>/dev/drbd0</literal> is used to share the
      NFS client states from <filename>/var/lib/nfs</filename>. This DRBD device
      sits on top of an LVM logical volume named <literal>/dev/nfs/state</literal>.
     </para>
    </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-ha-quick-nfs-resources-drbd">
   <title>Creating DRBD Primitive and Multi-state Resources</title>
   <para>
    Create a cluster resource to manage the DRBD devices, and a multi-state
    resource to allow the DRBD resource to run on both nodes:
   </para>
   <procedure xml:id="pro-ha-nfs-create-drbd-resource">
    <title>Creating a DRBD Resource for NFS</title>
    <step>
     <para>
      Start the <command>crm</command> interactive shell:
     </para>
<screen>&prompt.root;<command>crm configure</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the DRBD configuration <literal>nfs</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive drbd-nfs ocf:linbit:drbd \
  params drbd_resource="nfs" \
  op monitor interval=15 role=Master \
  op monitor interval=30 role=Slave</command></screen>
    </step>
    <step>
     <para>
      Create a multi-state resource for the <literal>drbd-nfs</literal> primitive:
     </para>
<screen>&prompt.crm.conf;<command>ms ms-drbd-nfs drbd-nfs \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    Pacemaker activates the DRBD resources on both nodes and promotes
    them to the primary role on one of the nodes. Check the state of the
    cluster with the <command>crm status</command> command, or run
    <command>drbdadm status</command>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-lvm">
   <title>Creating File System Resources</title>
   <para>
    Create cluster resources to manage the file systems for export and state tracking:
   </para>
   <procedure xml:id="pro-ha-nfs-create-fs-resource">
    <title>Creating File System Resources for NFS</title>
    <step>
     <para>
      Create a primitive for the NFS client states on <literal>/dev/drbd0</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive fs-nfs-state Filesystem \
  params device=/dev/drbd0 directory=/var/lib/nfs fstype=ext4</command></screen>
    </step>
    <step>
     <para>
      Create a primitive for the file system to be exported on
      <literal>/dev/drbd1</literal>:
     </para>
<screen>&prompt.crm.conf;<command>primitive fs-nfs-share Filesystem \
  params device=/dev/drbd1 directory=/srv/nfs/share fstype=ext4</command></screen>
     <para>
      <emphasis>Do not</emphasis> commit this configuration until
    after you add the colocation and order constraints.
     </para>
    </step>
    <step>
     <para>
      Add both of these resources to a resource group named <literal>g-nfs</literal>:
     </para>
<screen>&prompt.crm.conf;<command>group g-nfs fs-nfs-state fs-nfs-share</command></screen>
     <para>
      Resources start in the order they are added to the group and stop in reverse order.
     </para>
    </step>
    <step>
     <para>
      Add a colocation constraint to make sure that the resource group always
      starts on the node where the DRBD multi-state resource is in the primary role:
     </para>
<screen>&prompt.crm.conf;<command>colocation col-nfs-on-drbd inf: g-nfs ms-drbd-nfs:Master</command></screen>
    </step>
    <step>
     <para>
      Add an order constraint to make sure the DRBD multi-state resource always
      starts before the resource group:
     </para>
<screen>&prompt.crm.conf;<command>order o-drbd-before-nfs Mandatory: ms-drbd-nfs:promote g-nfs:start</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    Pacemaker mounts <literal>/dev/drbd0</literal> to <filename>/var/lib/nfs</filename>,
    and <literal>/dev/drbd1</literal> to <filename>srv/nfs/share</filename>. Confirm this
    with <command>mount</command>, or by looking at <filename >/proc/mounts</filename>.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-nfsserver">
   <title>Creating an NFS Kernel Server Resource</title>
   <para>
    Create a cluster resource to manage the NFS server daemon:
   </para>
   <procedure xml:id="pro-ha-nfs-create-nfs-server-resource">
    <title>Creating an NFS Kernel Server Resource</title>
    <step>
     <para>
      Create a primitive to manage the NFS server daemon:
     </para>
<screen>&prompt.crm.conf;<command>primitive nfsserver nfsserver \
  params nfs_shared_infodir="/var/lib/nfs"</command></screen>
     <warning>
      <title>Low lease time can cause loss of file state</title>
      <para>
       NFS clients regularly renew their state with the NFS server. If the lease time
       is too low, system or network delays can cause the timer to expire before the
       renewal is complete. This can lead to I/O errors and loss of file state.
      </para>
      <para>
       <literal>NFSV4LEASETIME</literal> is set on the NFS server in the file
       <filename>/etc/sysconfig/nfs</filename>. The default is 90 seconds.
       If lowering the lease time is necessary, we recommend a value of 60 or
       higher. We strongly discourage values lower than 30.
      </para>
     </warning>
    </step>
    <step>
     <para>
      Append this resource to the existing <literal>g-nfs</literal> resource group:
     </para>
<screen>&prompt.crm.conf;<command>modgroup g-nfs add nfsserver</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-nfsexport">
   <title>Creating an NFS Export Resource</title>
   <para>
    Create a cluster resource to manage the NFS exports:
   </para>
   <procedure xml:id="pro-ha-nfs-create-nfs-export-resource">
    <title>Creating an NFS Export Resource</title>
     <step>
      <para>
       Create a primitive for the NFS exports:
      </para>
<screen>&prompt.crm.conf;<command>primitive exportfs-nfs exportfs \
  params directory="/srv/nfs/share" \
  options="rw,mountpoint" clientspec="192.168.1.0/24" fsid=101 \</command><co xml:id="co-exportfs-fsid"/>
  <command>op monitor interval=30s timeout=90s</command><co xml:id="co-exportfs-monitor-timeout"/></screen>
      <calloutlist>
       <callout arearefs="co-exportfs-fsid">
        <para>
         The <literal>fsid</literal> must be unique for each NFS export resource.
        </para>
       </callout>
       <callout arearefs="co-exportfs-monitor-timeout">
        <para>
         The value of <literal>op monitor timeout</literal> must be higher
         than the value of <literal>stonith-timeout</literal>. To find the
         <literal>stonith-timeout</literal> value, run <command>crm configure show</command>
         and look under the <literal>property</literal> section.
        </para>
       </callout>
      </calloutlist>
      <important>
       <title>Do not set <literal>wait_for_leasetime_on_stop=true</literal></title>
       <para>
        Setting this option to <literal>true</literal> in a highly available
        NFS setup can cause unnecessary delays and loss of locks.
       </para>
       <para>
        The default value for <literal>wait_for_leasetime_on_stop</literal> is
        <literal>false</literal>. There is no need to set it to <literal>true</literal>
        when <filename>/var/lib/nfs</filename> and <literal>nfsserver</literal>
        are configured as described in this guide.
       </para>
      </important>
     </step>
     <step>
      <para>
       Append this resource to the existing <literal>g-nfs</literal> resource group:
      </para>
 <screen>&prompt.crm.conf;<command>modgroup g-nfs add exportfs-nfs</command></screen>
     </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
    <step>
     <para>
      Confirm that the NFS exports are set up properly:
     </para>
<screen>&prompt.root;<command>exportfs -v</command>
/srv/nfs/share   <replaceable>IP_ADDRESS_OF_CLIENT</replaceable>(<replaceable>OPTIONS</replaceable>)</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-quick-nfs-resources-vip">
   <title>Creating a Virtual IP Address for NFS Exports</title>
   <para>
    Create a cluster resource to manage the virtual IP address for the NFS exports:
   </para>
   <procedure xml:id="pro-ha-nfs-create-vip-resource">
    <title>Creating a Virtual IP Address for NFS Exports</title>
    <step>
     <para>
      Create a primitive for the virtual IP address:
     </para>
<screen>&prompt.crm.conf;<command>primitive vip-nfs IPaddr2 params ip=&nfs-vip-exports;</command></screen>
    </step>
    <step>
     <para>
      Append this resource to the existing <literal>g-nfs</literal> resource group:
     </para>
<screen>&prompt.crm.conf;<command>modgroup g-nfs add vip-nfs</command></screen>
    </step>
    <step>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
    <step>
     <para>
      Leave the <command>crm</command> interactive shell:
     </para>
  <screen>&prompt.crm.conf;<command>quit</command></screen>
    </step>
    <step>
     <para>
      Check the status of the cluster. The resources in the <literal>g-nfs</literal>
      group should appear in the following order:
     </para>
  <screen>&prompt.root;<command>crm status</command>
  [...]
  Full List of Resources
    [...]
    * Resource Group: g-nfs:
      * fs-nfs-state    (ocf:heartbeat:Filesystem):   Started &node1;
      * fs-nfs-share    (ocf:heartbeat:Filesystem):   Started &node1;
      * nfsserver       (ocf:heartbeat:nfsserver):    Started &node1;
      * exportfs-nfs    (ocf:heartbeat:exportfs):     Started &node1;
      * vip-nfs         (ocf:heartbeat:IPaddr2):      Started &node1;</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-quick-nfs-use">
  <title>Using the NFS Service</title>
  <para>
   This section outlines how to use the highly available NFS service from an
   NFS client.
  </para>
  <para>
   To connect to the NFS service, make sure to use the <emphasis>virtual IP
   address</emphasis> to connect to the cluster rather than a physical IP
   configured on one of the cluster nodes' network interfaces. For compatibility
   reasons, use the <emphasis>full</emphasis> path of the NFS export on the server.
  </para>
  <para>
   The command to mount the NFS export looks like this:
  </para>
<screen>&prompt.root;<command>mount &nfs-vip-exports;:/srv/nfs/share /home/share</command></screen>
  <para>
   If you need to configure other mount options, such as a specific transport protocol
   (<option>proto</option>), maximum read and write request sizes (<option>rsize</option>
   and <option>wsize</option>), or a specific NFS version (<option>vers</option>),
   use the <option>-o</option> option. For example:
  </para>
<screen>&prompt.root;<command>mount -o proto=tcp,rsize=32768,wsize=32768,vers=3 \
&nfs-vip-exports;:/srv/nfs/share /home/share</command></screen>
  <para>
   For further NFS mount options, see the <command>nfs</command> man page.
  </para>
  <note>
   <title>Loopback mounts</title>
   <para>
    Loopback mounts are only supported for NFS version 3, <emphasis>not</emphasis>
    NFS version 4. For more information, see
    <link xlink:href="https://www.suse.com/support/kb/doc/?id=000018709"/>.
   </para>
  </note>
 </sect1>

<sect1 xml:id="sec-ha-quick-nfs-add-filesystems">
 <title>Adding More NFS Shares to the Cluster</title>
 <para>
  If you need to increase the available storage, you can add more NFS shares
  to the cluster.
 </para>
 <para>
  In this example, a new DRBD device named <literal>/dev/drbd2</literal> sits
  on top of an LVM logical volume named <literal>/dev/nfs/share2</literal>.
 </para>
 <procedure xml:id="pro-ha-nfs-add-nfs-shares">
  <title>Adding More NFS Shares to the Cluster</title>
  <step>
   <para>
    Create an LVM logical volume for the new share:
   </para>
<screen>&prompt.root;<command>crm cluster run "lvcreate -n share2 -L 20G nfs"</command></screen>
  </step>
  <step>
   <para>
    Update the file <filename>/etc/drbd.d/nfs.res</filename> to add the new volume
    under the existing volumes:
   </para>
<screen>   volume 2 {
      device           /dev/drbd2;
      disk             /dev/nfs/share2;
      meta-disk        internal;
   }</screen>
  </step>
  <step>
   <para>
    Copy the updated file to the other nodes:
   </para>
<screen>&prompt.root;<command>csync2 -xv</command></screen>
  </step>
  <step>
   <para>
    Initialize the metadata storage for the new volume:
   </para>
<screen>&prompt.root;<command>crm cluster run "drbdadm create-md nfs/2 --force"</command></screen>
  </step>
  <step>
   <para>
    Update the <literal>nfs</literal> configuration to create the new device:
   </para>
<screen>&prompt.root;<command>crm cluster run "drbdadm adjust nfs"</command></screen>
  </step>
  <step>
   <para>
    Skip the initial synchronization for the new device:
   </para>
<screen>&prompt.root;<command>drbdadm new-current-uuid --clear-bitmap nfs/2</command></screen>
  </step>
  <step>
   <para>
    The NFS cluster resources might have moved to another node since they were created.
    Check the DRBD status with <command>drbdadm status nfs</command>, and make a
    note of which node is in the <literal>Primary</literal> role.
   </para>
  </step>
  <step>
   <para>
    On the node that is in the <literal>Primary</literal> role, create an
    <literal>ext4</literal> file system on <literal>/dev/drbd2</literal>:
   </para>
<screen>&prompt.root;<command>mkfs.ext4 /dev/drbd2</command></screen>
  </step>
  <step>
   <para>
    Start the <command>crm</command> interactive shell:
   </para>
<screen>&prompt.root;<command>crm configure</command></screen>
  </step>
  <step>
   <para>
    Create a primitive for the file system to be exported on <literal>/dev/drbd2</literal>:
   </para>
<screen>&prompt.crm.conf;<command>primitive fs-nfs-share2 Filesystem \
  params device="/dev/drbd2" directory="/srv/nfs/share2" fstype=ext4</command></screen>
  </step>
  <step>
   <para>
    Add the new file system resource to the <literal>g-nfs</literal> group
    <emphasis>before</emphasis> the <literal>nfsserver</literal> resource:
   </para>
<screen>&prompt.crm.conf;<command>modgroup g-nfs add fs-nfs-share2 before nfsserver</command></screen>
  </step>
  <step>
   <para>
    Create a primitive for NFS exports from the new share:
   </para>
<screen>&prompt.crm.conf;<command>primitive exportfs-nfs2 exportfs \
  params directory="/srv/nfs/share2" \
  options="rw,mountpoint" clientspec="192.168.1.0/24" fsid=102 \
  op monitor interval=30s timeout=90s</command></screen>
  </step>
  <step>
   <para>
    Add the new NFS export resource to the <literal>g-nfs</literal> group
    <emphasis>before</emphasis> the <literal>vip-nfs</literal> resource:
   </para>
<screen>&prompt.crm.conf;<command>modgroup g-nfs add exportfs-nfs2 before vip-nfs</command></screen>
  </step>
  <step>
   <para>
    Commit this configuration:
   </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
  </step>
  <step>
   <para>
    Leave the <command>crm</command> interactive shell:
   </para>
<screen>&prompt.crm.conf;<command>quit</command></screen>
  </step>
  <step>
   <para>
    Check the status of the cluster. The resources in the <literal>g-nfs</literal>
    group should appear in the following order:
   </para>
<screen>&prompt.root;<command>crm status</command>
[...]
Full List of Resources
  [...]
  * Resource Group: g-nfs:
    * fs-nfs-state    (ocf:heartbeat:Filesystem):   Started &node1;
    * fs-nfs-share    (ocf:heartbeat:Filesystem):   Started &node1;
    * fs-nfs-share2   (ocf:heartbeat:Filesystem):   Started &node1;
    * nfsserver       (ocf:heartbeat:nfsserver):    Started &node1;
    * exportfs-nfs    (ocf:heartbeat:exportfs):     Started &node1;
    * exportfs-nfs2   (ocf:heartbeat:exportfs):     Started &node1;
    * vip-nfs         (ocf:heartbeat:IPaddr2):      Started &node1;</screen>
  </step>
  <step>
   <para>
    Confirm that the NFS exports are set up properly:
   </para>
<screen>&prompt.root;<command>exportfs -v</command>
/srv/nfs/share   <replaceable>IP_ADDRESS_OF_CLIENT</replaceable>(<replaceable>OPTIONS</replaceable>)
/srv/nfs/share2  <replaceable>IP_ADDRESS_OF_CLIENT</replaceable>(<replaceable>OPTIONS</replaceable>)</screen>
  </step>
 </procedure>
</sect1>

 <sect1 xml:id="sec-ha-quick-nfs-more-info">
  <title>For More Information</title>
  <itemizedlist>
   <listitem>
    <para>
     For more details about the steps in this guide, see
     <link xlink:href="https://www.suse.com/support/kb/doc/?id=000020396"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more information about NFS and LVM, see
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html">
     &storage_guide; for &sles;</link>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more information about DRBD, see <xref linkend="cha-ha-drbd"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more information about cluster resources, see
     <xref linkend="sec-ha-config-basics-resources"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <xi:include href="common_legal.xml"/>
</article>
