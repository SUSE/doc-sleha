<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!--taroth 2014-08-11: https://fate.suse.com/316223: [docu] sync and change config files (prio: mandatory)-->
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-geo-sync">
 <title>Synchronizing Configuration Files Across All Sites and Arbitrators</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer/>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release/>
   <dm:repository/>
  </dm:docmanager>
 </info>
 <para>
  To replicate important configuration files across all nodes in the cluster
  and across &geo; clusters, use &csync;.
<!--taroth 2014-09-25: the following is copied from ha_installation.xml, 
     varlistentry vle.csync2-->
  &csync; can handle any number of hosts, sorted into synchronization groups.
  Each synchronization group has its own list of member hosts and its
  include/exclude patterns that define which Ô¨Åles should be synchronized in
  the synchronization group. The groups, the host names belonging to each
  group, and the include/exclude rules for each group are specified in the
  &csync; configuration file, <filename>/etc/csync2/csync2.cfg</filename>.
 </para>
 <para>
  For authentication, &csync; uses the IP addresses and pre-shared keys within
  a synchronization group. You need to generate one key file for each
  synchronization group and copy it to all group members.
 </para>
 <para>
  &csync; will contact other servers via a TCP port (by default
  <literal>6556</literal>), and start remote &csync; instances. For detailed
  information about &csync;, refer to
  <link xlink:href="http://oss.linbit.com/csync2/paper.pdf"/>
 </para>
 <sect1 xml:id="sec-ha-geo-booth-sync-csync2-setup">
  <title>&csync; Setup for &geo; Clusters</title>

  <para>
   How to set up &csync; for individual clusters with &yast; is explained in
   <xref linkend="sec-ha-installation-setup-csync2"/>. However,
   &yast; cannot handle more complex &csync; setups, like those that are needed
   for &geo; clusters. For the following setup, as shown in
   <xref linkend="fig-ha-geo-csync-config"/>, configure &csync; manually by
   editing the configuration files.
  </para>

  <para>
   To adjust &csync; for synchronizing files not only within local clusters but
   also across &geo.dispersed; sites, you need to define two synchronization
   groups in the &csync; configuration:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A global group <literal>ha_global</literal> (for the files that need to be
     synchronized globally, across all sites and arbitrators belonging to a
     &geo; cluster).
    </para>
   </listitem>
   <listitem>
    <para>
     A group for the local cluster site <literal>ha_local</literal> (for the
     files that need to be synchronized within the local cluster).
    </para>
   </listitem>
  </itemizedlist>

  <para>
   For an overview of the multiple &csync; configuration files for the two
   synchronization groups, see <xref linkend="fig-ha-geo-csync-config"/>.
  </para>

  <figure xml:id="fig-ha-geo-csync-config">
   <title>Example &csync; Setup for &geo; Clusters</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="multi-site-csync2.svg" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="multi-site-csync2.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Authentication key files and their references are displayed in red. The
   names of &csync; configuration files are displayed in blue, and their
   references are displayed in green. For details, refer to
   <xref linkend="vl-fig-ha-geo-csync-config-files"/>.
  </para>

  <variablelist xml:id="vl-fig-ha-geo-csync-config-files">
   <title>Example &csync; Setup: Configuration Files</title>
   <varlistentry xml:id="vle-ha-geo-csync-csync2-cfg">
    <term><filename>/etc/csync2/csync2.cfg</filename>
    </term>
    <listitem>
     <para>
      The main &csync; configuration file. It is kept short and simple on
      purpose and only contains the following:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        The definition of the synchronization group
        <literal>ha_local</literal>. The group consists of two nodes
        (<literal>this-site-host-1</literal> and
        <literal>this-site-host-2</literal>) and uses
        <filename>/etc/csync2/ha_local.key</filename> for authentication. A
        list of files to be synchronized for this group only is defined in
        another &csync; configuration file,
        <filename>/etc/csync2/ha_local.cfg</filename>. It is included with the
        <literal>config</literal> statement.
       </para>
      </listitem>
      <listitem>
       <para>
        A reference to another &csync; configuration file,
        <filename>/etc/csync2.cfg/ha_global.cfg</filename>, included with the
        <literal>config</literal> statement.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-geo-csync-ha-local-cfg">
    <term><filename>/etc/csync2/ha_local.cfg</filename>
    </term>
    <listitem>
     <para>
      This file concerns only the local cluster. It specifies a list of files
      to be synchronized only within the <literal>ha_local</literal>
      synchronization group, as these files are specific per cluster. The most
      important ones are the following:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <filename>/etc/csync2/csync2.cfg</filename>, as this file contains the
        list of the local cluster nodes.
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/csync2/ha_local.key</filename>, the authentication key
        to be used for &csync; synchronization within the local cluster.
       </para>
      </listitem>
      <listitem>
       <para>
        &corosync.conf;, as this file defines the communication channels
        between the local cluster nodes.
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/corosync/authkey</filename>, the &corosync;
        authentication key.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The rest of the file list depends on your specific cluster setup. The
      files listed in <xref linkend="fig-ha-geo-csync-config"/> are only
      examples. If you also want to synchronize files for any site-specific
      applications, include them in <filename>ha_local.cfg</filename>, too.
      Even though <filename>ha_local.cfg</filename> is targeted at the nodes
      belonging to one site of your &geo; cluster, the content may be identical
      on all sites. If you need different sets of hosts or different keys,
      adding extra groups may be necessary.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-geo-csync-ha-global-cfg">
    <term><filename>/etc/csync2.cfg/ha_global.cfg</filename>
    </term>
    <listitem>
     <para>
      This file defines the &csync; synchronization group
      <literal>ha_global</literal>. The group spans <emphasis>all</emphasis>
      cluster nodes across multiple sites, including the arbitrator. As it is
      recommended to use a separate key for each &csync; synchronization group,
      this group uses <filename>/etc/csync2/ha_global.key</filename> for
      authentication. The <literal>include</literal> statements define the list
      of files to be synchronized within the <literal>ha_global</literal>
      synchronization group. The most important ones are the following:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <filename>/etc/csync2/ha_global.cfg</filename> and
        <filename>/etc/csync2/ha_global.key</filename> (the configuration file
        for the <literal>ha_global</literal>synchronization group and the
        authentication key used for synchronization within the group).
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/booth/</filename>, the default directory holding the
        booth configuration. In case you are using a booth setup for multiple
        tenants, it contains more than one booth configuration file. If you use
        authentication for booth, it is useful to place the key file in this
        directory, too.
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/drbd.conf</filename> and
        <filename>/etc/drbd.d</filename> (if you are using DRBD within your
        cluster setup). The DRBD configuration can be globally synchronized, as
        it derives the configuration from the host names contained in the
        resource configuration file.
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/zypp/repos.de</filename>. The package repositories are
        likely to be the same on all cluster nodes.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The other files shown
      (<filename>/etc/root/<replaceable>*</replaceable></filename>) are
      examples that may be included for reasons of convenience (to make a
      cluster administrator's life easier).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <para>
    The files <filename>csync2.cfg</filename> and
    <filename>ha_local.key</filename> are site-specific, which means you need
    to create different ones for each cluster site. The files are identical on
    the nodes belonging to the same cluster but different on another cluster.
    Each <filename>csync2.cfg</filename> file needs to contain a lists of hosts
    (cluster nodes) belonging to the site, plus a site-specific authentication
    key.
   </para>
   <para>
    The arbitrator needs a <filename>csync2.cfg</filename> file, too. It only
    needs to reference <filename>ha_global.cfg</filename> though.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-geo-booth-sync-csync2-start">
  <title>Synchronizing Changes with &csync;</title>

  <para>
   To successfully synchronize the files with &csync;, the following
   prerequisites must be met:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The same &csync; configuration is available on all machines that belong to
     the same synchronization group.
    </para>
   </listitem>
   <listitem>
    <para>
     The &csync; authentication key for each synchronization group must be
     available on all members of that group.
    </para>
   </listitem>
   <listitem>
    <para>
     &csync; must be running on <emphasis>all</emphasis> nodes and the
     arbitrator.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Before the first &csync; run, you therefore need to make the following
   preparations:
  </para>

  <procedure>
   <step>
    <para>
     Log in to one machine per synchronization group and generate an
     authentication key for the respective group:
    </para>
<screen>&prompt.root;<command>csync2</command> -k <replaceable>NAME_OF_KEYFILE</replaceable></screen>
    <para>
     However, do <emphasis>not</emphasis> regenerate the key file on any other
     member of the same group.
    </para>
    <para>
     With regard to <xref linkend="fig-ha-geo-csync-config"/>, this would
     result in the following key files:
     <filename>/etc/csync2/ha_global.key</filename> and one local key
     (<filename>/etc/csync2/ha_local.key</filename>) per site.
    </para>
   </step>
   <step>
    <para>
     Copy each key file to <emphasis>all</emphasis> members of the respective
     synchronization group. With regard to
     <xref linkend="fig-ha-geo-csync-config"/>:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Copy <filename>/etc/csync2/ha_global.key</filename> to
       <emphasis>all</emphasis> parties (the arbitrator and all cluster nodes
       on all sites of your &geo; cluster). The key file needs to be available
       on all hosts listed within the <literal>ha_global</literal> group that
       is defined in <filename>ha_global.cfg</filename>.
      </para>
     </step>
     <step>
      <para>
       Copy the local key file for each site
       (<filename>/etc/csync2/ha_local.key</filename>) to all cluster nodes
       belonging to the respective site of your &geo; cluster.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Copy the site-specific <filename>/etc/csync2/csync2.cfg</filename>
     configuration file to all cluster nodes belonging to the respective site
     of your &geo; cluster and to the arbitrator.
    </para>
   </step>
   <step>
    <para>
     Execute the following command on all nodes and the arbitrator to make the
     csync2 service start automatically at boot time:
    </para>
<screen>&prompt.root;<command>systemctl</command> enable csync2.socket</screen>
   </step>
   <step>
    <para>
     Execute the following command on all nodes and the arbitrator to start the
     service now:
    </para>
<screen>&prompt.root;<command>systemctl</command> start csync2.socket</screen>
   </step>
  </procedure>

  <procedure xml:id="pro-ha-geo-setup-csync2-start">
   <title>Synchronizing Files with &csync;</title>
   <step>
    <para>
     To initially synchronize all files once, execute the following command on
     the machine that you want to copy the configuration
     <emphasis>from</emphasis>:
    </para>
<screen>&prompt.root;<command>csync2</command> <option>-xv</option></screen>
    <para>
     This will synchronize all the files once by pushing them to the other
     members of the synchronization groups. If all files are synchronized
     successfully, &csync; will finish with no errors.
    </para>
    <para>
     If one or several files that are to be synchronized have been modified on
     other machines (not only on the current one), &csync; will report a
     conflict. You will get an output similar to the one below:
    </para>
<screen>While syncing file /etc/corosync/corosync.conf:
ERROR from peer site-2-host-1: File is also marked dirty here!
Finished with 1 errors.</screen>
   </step>
   <step>
    <para>
     If you are sure that the file version on the current machine is the
     <quote>best</quote> one, you can resolve the conflict by forcing this file
     and re-synchronizing:
    </para>
<screen>&prompt.root;<command>csync2</command> <option>-f</option> /etc/corosync/corosync.conf
&prompt.root;<command>csync2</command> <option>-x</option></screen>
   </step>
  </procedure>

  <para>
   For more information on the &csync; options, run
   <command>csync2&nbsp;</command> <option>-help</option>.
  </para>

  <note>
   <title>Pushing Synchronization After Any Changes</title>
   <para>
    &csync; only pushes changes. It does <emphasis>not</emphasis> continuously
    synchronize files between the machines.
   </para>
   <para>
    Each time you update files that need to be synchronized, you need to push
    the changes to the other machines of the same synchronization group: Run
    <command>csync2&nbsp;</command> <option>-xv</option> on the machine where
    you did the changes. If you run the command on any of the other machines
    with unchanged files, nothing will happen.
   </para>
  </note>
 </sect1>
</chapter>
