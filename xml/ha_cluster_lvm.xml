<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ha-clvm">
 <title>Cluster logical volume manager (&clvm;)</title>
 <info>
      <abstract>
        <para>
    When managing shared storage on a cluster, every node must be informed
    about changes that are done to the storage subsystem. The Logical Volume
    Manager&nbsp;2 (LVM2), which is widely used to manage local storage,
    has been extended to support transparent management of volume groups
    across the whole cluster. Volume groups shared among multiple hosts
    can be managed using the same commands as local storage.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-clvm-overview">
  <title>Conceptual overview</title>

  <para>
   &clvm; is coordinated with different tools:
  </para>

  <variablelist>
   <varlistentry>
    <term>Distributed lock manager (DLM)</term>
    <listitem>
     <para> Coordinates access to shared resources among multiple hosts through
      cluster-wide locking.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Logical volume manager (LVM2)</term>
    <listitem>
     <para>
      LVM2 provides a virtual pool of disk space and enables flexible distribution of
      one logical volume over several disks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster logical volume manager (&clvm;)</term>
    <listitem>
     <para>
      The term <literal>&clvm;</literal> indicates that LVM2 is being used
      in a cluster environment. This needs some configuration adjustments
      to protect the LVM2 metadata on shared storage. From &sle; 15 onward, the
      cluster extension uses lvmlockd, which replaces the well-known
      clvmd. For more information about lvmlockd, see the man page of the
      <command>lvmlockd</command> command (<command>man 8
      lvmlockd</command>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Volume group and logical volume</term>
    <listitem>
     <para>
      Volume groups (VGs) and logical volumes (LVs) are basic concepts of LVM2.
      A volume group is a storage pool of multiple physical
      disks. A logical volume belongs to a volume group, and can be seen as an
      elastic volume on which you can create a file system. In a cluster environment,
      there is a concept of shared VGs, which consist of shared storage and can
      be used concurrently by multiple hosts.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ha-clvm-config">
  <title>Configuration of &clvm;</title>

  <para>
   Make sure the following requirements are fulfilled:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A shared storage device is available, provided by a Fibre
     Channel, FCoE, SCSI, iSCSI SAN, or DRBD*, for example.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure the following packages have been installed: <systemitem
     class="resource">lvm2</systemitem> and <systemitem
     class="resource">lvm2-lockd</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     From &sle; 15 onward, we use lvmlockd as the LVM2 cluster extension,
     rather than clvmd. Make sure the clvmd daemon is not running,
     otherwise lvmlockd will fail to start.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-ha-clvm-config-resources">
   <title>Creating the cluster resources</title>
   <para>
    Perform the following basic steps on one node to configure a shared VG in
    the cluster:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="pro-ha-clvm-rsc-dlm" xrefstyle="select:title"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="pro-ha-clvm-rsc-lvmlockd" xrefstyle="select:title"/></para>
    </listitem>
    <listitem>
    <para>
      <xref linkend="pro-ha-clvm-rsc-vg-lv" xrefstyle="select:title"/></para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="pro-ha-clvm-rsc-lvm-activate" xrefstyle="select:title"/>
     </para>
    </listitem>
   </itemizedlist>

   <procedure xml:id="pro-ha-clvm-rsc-dlm">
    <title>Creating a DLM resource</title>
    <step>
     <para>
      Start a shell and log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Check the current configuration of the cluster resources:
     </para>
     <screen>&prompt.root;crm configure show</screen>
    </step>
    <step>
     <para>
      If you have already configured a DLM resource (and a corresponding
      base group and base clone), continue with <xref linkend="pro-ha-clvm-rsc-lvmlockd"/>.
     </para>
     <para>
      Otherwise, configure a DLM resource and a corresponding base group and
      base clone as described in <xref linkend="pro-dlm-resources"/>.
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-clvm-rsc-lvmlockd">
    <title>Creating an lvmlockd resource</title>
    <step>
     <para>
      Start a shell and log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Run the following command to see the usage of this resource:
     </para>
     <screen>&prompt.root;crm configure ra info lvmlockd</screen>
    </step>
    <step>
     <para>
      Configure a <systemitem>lvmlockd</systemitem> resource as follows:
     </para>
     <screen>&prompt.root;crm configure primitive lvmlockd lvmlockd \
  op start timeout="90" \
  op stop timeout="100" \
  op monitor interval="30" timeout="90"</screen>
    </step>
    <step>
     <para>
      To ensure the <systemitem>lvmlockd</systemitem> resource is started on every node, add the primitive resource
      to the base group for storage you have created in <xref linkend="pro-ha-clvm-rsc-dlm"/>:
     </para>
     <screen>&prompt.root;crm configure modgroup g-storage add lvmlockd</screen>
    </step>
    <step>
     <para>
      Review your changes:</para>
     <screen>&prompt.root;crm configure show</screen>
    </step>
    <step>
     <para>Check if the resources are running well:
     </para>
     <screen>&prompt.root;crm status full</screen>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-clvm-rsc-vg-lv">
    <title>Creating a shared VG and LV</title>
    <step>
     <para>
      Start a shell and log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
     Assuming you already have two shared disks, create a shared VG with them:
     </para>
     <screen>&prompt.root;vgcreate --shared vg1 /dev/sda /dev/sdb</screen>
    </step>
    <step>
     <para>
      Create an LV and do not activate it initially:
     </para>
     <screen>&prompt.root;lvcreate -an -L10G -n lv1 vg1</screen>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-clvm-rsc-lvm-activate">
    <title>Creating an LVM-activate resource</title>
    <step>
     <para>
      Start a shell and log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Run the following command to see the usage of this resource:
     </para>
     <screen>&prompt.root;crm configure ra info LVM-activate</screen>
     <para>
      This resource manages the activation of a VG. In a shared VG, LV activation
      has two different modes: exclusive and shared mode. The exclusive mode is
      the default and should be used normally, when a local file system like <systemitem>ext4</systemitem>
      uses the LV. The shared mode should only be used for cluster file systems
      like &ocfs;.
     </para>
    </step>
    <step>
     <para>
      Configure a resource to manage the activation of your VG. Choose one of the
      following options according to your scenario:
     </para>
     <itemizedlist>
      <listitem>
       <para>Use exclusive activation mode for local file system usage:</para>
<screen>&prompt.root;crm configure primitive vg1 LVM-activate \
  params vgname=vg1 vg_access_mode=lvmlockd \
  op start timeout=90s interval=0 \
  op stop timeout=90s interval=0 \
  op monitor interval=30s timeout=90s</screen>
      </listitem>
      <listitem>
       <para>
        Use shared activation mode for &ocfs; and add it to the cloned
        <literal>g-storage</literal> group:
       </para>
<screen>&prompt.root;crm configure primitive vg1 LVM-activate \
  params vgname=vg1 vg_access_mode=lvmlockd activation_mode=shared \
  op start timeout=90s interval=0 \
  op stop timeout=90s interval=0 \
  op monitor interval=30s timeout=90s
&prompt.root;crm configure modgroup g-storage add vg1</screen>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      Check if the resources are running well:
     </para>
     <screen>&prompt.root;crm status full</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-clvm-scenario-iscsi">
   <title>Scenario: &clvm; with iSCSI on SANs</title>
   <para>
    The following scenario uses two SAN boxes which export their iSCSI
    targets to several clients. The general idea is displayed in
    <xref linkend="fig-ha-clvm-scenario-iscsi"/>.
   </para>
   <figure xml:id="fig-ha-clvm-scenario-iscsi">
    <title>Setup of a shared disk with &clvm;</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ha_clvm.svg" width="80%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ha_clvm.png" width="45%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <warning>
    <title>Data loss</title>
    <para>
     The following procedures will destroy any data on your disks!
    </para>
   </warning>
   <para>
    Configure only one SAN box first. Each SAN box needs to export its own
    iSCSI target. Proceed as follows:
   </para>
<!-- 
    Another setup:
    1. Configure iSCSI daemon on file /etc/ietd.conf
    
    Target iqn.2010-03.de.&wsI;disks
    Lun 0 Sectors=x,Type=nullio
    Lun 1 Path=/dev/sda1,Type=blockio,ScsiId=0
    Lun 2 Path=/dev/sda2,Type=blockio,ScsiId=1
    
    Lun 0 are used only for testing and tuning purpose.
    
    2. Start the Daemon:
    # rciscsitarget start
    
    3. Check:
    # netstat -nltp | grep iet
    tcp 0 0 0.0.0.0:3260 0.0.0.0:* LISTEN 12586/ietd
    tcp 0 0 :::3260 :::* LISTEN 12586/ietd
    
    # tail /var/log/messages
    <ADD LOG MESSAGE>
    
    => Disks are shared over Ethernet
    
    
    Configuring Node Machines:
    1. /etc/iscsi/iscsi.conf:
    node.start = automatic
    
    2. rcopen-iscsi start
    
    3. Detect remote disks:
    # iscsi_discovery <SERVER_IP>
    192.168.1.2:3260,1 iqn.2010-03.&wsI;disks
    192.168.1.2:3260,1 iqn.2010-03.&wsI;disks
    
    
   -->
   <procedure xml:id="pro-ha-clvm-scenario-iscsi-targets">
    <title>Configuring iSCSI targets (SAN)</title>
    <step>
     <para>
      Run &yast; and click <menuchoice> <guimenu>Network
      Services</guimenu> <guimenu>iSCSI LIO Target</guimenu> </menuchoice> to
      start the iSCSI Server module.
     </para>
    </step>
    <step>
     <para>
      If you want to start the iSCSI target whenever your computer is
      booted, choose <guimenu>When Booting</guimenu>, otherwise choose
      <guimenu>Manually</guimenu>.
     </para>
    </step>
    <step>
     <para>
      If you have a firewall running, enable <guimenu>Open Port in
      Firewall</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Switch to the <guimenu>Global</guimenu> tab. If you need
      authentication, enable incoming or outgoing authentication or both. In
      this example, we select <guimenu>No Authentication</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Add a new iSCSI target:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Switch to the <guimenu>Targets</guimenu> tab.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Add</guimenu>.
       </para>
      </step>
      <step xml:id="st-ha-clvm-iscsi-iqn">
       <para>
        Enter a target name. The name needs to be formatted like this:
       </para>
<screen>iqn.<replaceable>DATE</replaceable>.<replaceable>DOMAIN</replaceable></screen>
       <para>
        For more information about the format, refer to <citetitle>Section
        3.2.6.3.1. Type "iqn." (iSCSI Qualified Name) </citetitle> at
        <link xlink:href="http://www.ietf.org/rfc/rfc3720.txt"/>.
       </para>
      </step>
      <step>
       <para>
        If you want a more descriptive name, you can change it as long as
        your identifier is unique for your different targets.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Add</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Enter the device name in <guimenu>Path</guimenu> and use a
        <guimenu>Scsiid</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Next</guimenu> twice.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Confirm the warning box with <guimenu>Yes</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Open the configuration file <filename>/etc/iscsi/iscsid.conf</filename>
      and change the parameter <literal>node.startup</literal> to
      <literal>automatic</literal>.
     </para>
    </step>
   </procedure>
   <para>
    Now set up your iSCSI initiators as follows:
   </para>
   <procedure xml:id="pro-ha-clvm-scenarios-iscsi-initiator">
    <title>Configuring iSCSI initiators</title>
    <step>
     <para>
      Run &yast; and click <menuchoice> <guimenu>Network
      Services</guimenu> <guimenu>iSCSI Initiator</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      If you want to start the iSCSI initiator whenever your computer is
      booted, choose <guimenu>When Booting</guimenu>, otherwise set
      <guimenu>Manually</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Change to the <guimenu>Discovery</guimenu> tab and click the
      <guimenu>Discovery</guimenu> button.
     </para>
    </step>
    <step>
     <para>
      Add the IP address and the port of your iSCSI target (see
      <xref linkend="pro-ha-clvm-scenario-iscsi-targets"/>). Normally, you
      can leave the port as it is and use the default value.
     </para>
    </step>
    <step>
     <para>
      If you use authentication, insert the incoming and outgoing user name
      and password, otherwise activate <guimenu>No Authentication</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Select <guimenu>Next</guimenu>. The found connections are displayed in
      the list.
     </para>
    </step>
    <step>
     <para>
      Proceed with <guimenu>Finish</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Open a shell, log in as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Test if the iSCSI initiator has been started successfully:
     </para>
<screen>&prompt.root;<command>iscsiadm</command> -m discovery -t st -p &wwwip;
&wwwip;:3260,1 iqn.2010-03.de.&wsI;:san1</screen>
    </step>
    <step>
     <para>
      Establish a session:
     </para>
<screen>&prompt.root;<command>iscsiadm</command> -m node -l -p &wwwip; -T iqn.2010-03.de.&wsI;:san1
Logging in to [iface: default, target: iqn.2010-03.de.&wsI;:san1, portal: &wwwip;,3260]
Login to [iface: default, target: iqn.2010-03.de.&wsI;:san1, portal: &wwwip;,3260]: successful</screen>
     <para>
      See the device names with <command>lsscsi</command>:
     </para>
<screen>...
[4:0:0:2]    disk    IET      ...     0     /dev/sdd
[5:0:0:1]    disk    IET      ...     0     /dev/sde</screen>
     <para>
      Look for entries with <literal>IET</literal> in their third column. In
      this case, the devices are <filename>/dev/sdd</filename> and
      <filename>/dev/sde</filename>.
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-clvm-scenarios-iscsi-lvm">
    <title>Creating the shared volume groups</title>
    <step>
     <para>
      Open a &rootuser; shell on one of the nodes you have run the iSCSI
      initiator from
      <xref linkend="pro-ha-clvm-scenarios-iscsi-initiator"/>.
     </para>
    </step>
    <step>
     <para>
     Create the shared volume group on disks <filename>/dev/sdd</filename> and <filename>/dev/sde</filename>:
     </para>
<screen>&prompt.root;vgcreate --shared testvg /dev/sdd /dev/sde</screen>
    </step>
    <step>
     <para>
      Create logical volumes as needed:
     </para>
<screen>&prompt.root;<command>lvcreate</command> --name lv1 --size 500M testvg</screen>
    </step>
    <step>
     <para>
      Check the volume group with <command>vgdisplay</command>:
     </para>
<screen>  --- Volume group ---
      VG Name               testvg
      System ID
      Format                lvm2
      Metadata Areas        2
      Metadata Sequence No  1
      VG Access             read/write
      VG Status             resizable
      MAX LV                0
      Cur LV                0
      Open LV               0
      Max PV                0
      Cur PV                2
      Act PV                2
      VG Size               1016,00 MB
      PE Size               4,00 MB
      Total PE              254
      Alloc PE / Size       0 / 0
      Free  PE / Size       254 / 1016,00 MB
      VG UUID               UCyWw8-2jqV-enuT-KH4d-NXQI-JhH3-J24anD</screen>
    </step>
    <step>
     <para>
      Check the shared state of the volume group with the command <command>vgs</command>:
     </para>
     <screen>&prompt.root;<command>vgs</command>
  VG       #PV #LV #SN Attr   VSize     VFree  
  vgshared   1   1   0 wz--ns 1016.00m  1016.00m</screen>
     <para>
      The <literal>Attr</literal> column shows the volume attributes. In this example,
      the volume group is writable (<literal>w</literal>),
      resizeable (<literal>z</literal>), the allocation policy is normal (<literal>n</literal>),
      and it is a shared resource (<literal>s</literal>).
      See the man page of <command>vgs</command> for details.</para>
    </step>
   </procedure>
   <para>
    After you have created the volumes and started your resources you should have new device
    names under <filename>/dev/testvg</filename>, for example <filename>/dev/testvg/lv1</filename>.
    This indicates the LV has been activated for use.
   </para>
  </sect2>

  <sect2 xml:id="sec-ha-clvm-scenario-drbd">
   <title>Scenario: &clvm; with DRBD</title>
   <para>
    The following scenarios can be used if you have data centers located in
    different parts of your city, country, or continent.
   </para>
   <procedure xml:id="pro-ha-clvm-withdrbd">
    <title>Creating a cluster-aware volume group with DRBD</title>
    <step>
     <para>
      Create a primary/primary DRBD resource:
     </para>
     <substeps performance="required">
      <step>
       <para>
        First, set up a DRBD device as primary/secondary as described in
        <xref linkend="pro-drbd-configure"/>. Make sure the disk state is
        <literal>up-to-date</literal> on both nodes. Check this with
        <command>drbdadm status</command>.
       </para>
      </step>
      <step>
       <para>
        Add the following options to your configuration file (usually
        something like <filename>/etc/drbd.d/r0.res</filename>):
       </para>
<screen>resource r0 {
  net {
     allow-two-primaries;
  }
  ...
}</screen>
      </step>
      <step>
       <para>
        Copy the changed configuration file to the other node, for example:
       </para>
<screen>&prompt.root;<command>scp</command> /etc/drbd.d/r0.res &wsII;:/etc/drbd.d/</screen>
      </step>
      <step>
       <para>
        Run the following commands on <emphasis>both</emphasis> nodes:
       </para>
<screen>&prompt.root;<command>drbdadm</command> disconnect r0
&prompt.root;<command>drbdadm</command> connect r0
&prompt.root;<command>drbdadm</command> primary r0</screen>
      </step>
      <step>
       <para>
        Check the status of your nodes:
    </para>
    <screen>&prompt.root;<command>drbdadm</command> status r0</screen>
<!--
<screen>&prompt.root;<command>cat</command> /proc/drbd
...
0: cs:Connected ro:Primary/Primary ds:UpToDate/UpToDate C r&#45;&#45;&#45;&#45;</screen>
-->
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Include the lvmlockd resource as a clone in the pacemaker configuration,
      and make it depend on the DLM clone resource. See
      <xref linkend="pro-ha-clvm-rsc-dlm"/> for detailed instructions.
      Before proceeding, confirm that these resources have started
      successfully on your cluster. Use <command>crm status</command>
      or the Web interface to check the running services.
     </para>
    </step>
    <step>
     <para>
      Prepare the physical volume for LVM with the command
      <command>pvcreate</command>. For example, on the device
      <filename>/dev/drbd_r0</filename> the command would look like this:
     </para>
<screen>&prompt.root;<command>pvcreate</command> /dev/drbd_r0</screen>
    </step>
    <step>
     <para>
      Create a shared volume group:
     </para>
<screen>&prompt.root;<command>vgcreate</command> --shared testvg /dev/drbd_r0</screen>
    </step>
    <step>
     <para>
      Create logical volumes as needed. You probably want to change the
      size of the logical volume. For example, create a 4&nbsp;GB logical
      volume with the following command:
     </para>
<screen>&prompt.root;<command>lvcreate</command> --name lv1 -L 4G testvg</screen>
    </step>
    <step>
     <para>
      <remark role="grammar">taroth 2011-10-24: comment by bwiedemann: as file system
      mounts or raw usage - *as* raw usage passt nicht - for?</remark>
      The logical volumes within the VG are now available as file system
      mounts or raw usage. Ensure that services using them have proper
      dependencies to collocate them with and order them after the VG has
      been activated.
     </para>
    </step>
   </procedure>
   <para>
    After finishing these configuration steps, the LVM2 configuration can be
    done like on any stand-alone workstation.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ha-clvm-drbd">
  <title>Configuring eligible LVM2 devices explicitly</title>

  <para>
   When several devices seemingly share the same physical volume signature
   (as can be the case for multipath devices or DRBD), we recommend to
   explicitly configure the devices which LVM2 scans for PVs.
  </para>

  <para>
   For example, if the command <command>vgcreate</command> uses the physical
   device instead of using the mirrored block device, DRBD will be confused.
   This may result in a split brain condition for DRBD.
  </para>

  <para>
   To deactivate a single device for LVM2, do the following:
  </para>

  <procedure>
   <step>
    <para>
     Edit the file <filename>/etc/lvm/lvm.conf</filename> and search for the
     line starting with <literal>filter</literal>.
    </para>
   </step>
   <step>
    <para>
     The patterns there are handled as regular expressions. A leading
     <quote>a</quote> means to accept a device pattern to the scan, a
     leading <quote>r</quote> rejects the devices that follow the device
     pattern.
    </para>
   </step>
   <step>
    <para>
     To remove a device named <filename>/dev/sdb1</filename>, add the
     following expression to the filter rule:
    </para>
<screen>"r|^/dev/sdb1$|"</screen>
    <para>
     The complete filter line will look like the following:
    </para>
<screen>filter = [ "r|^/dev/sdb1$|", "r|/dev/.*/by-path/.*|",<!--
 --> "r|/dev/.*/by-id/.*|", "a/.*/" ]</screen>
    <para>
     A filter line that accepts DRBD and MPIO devices but rejects all other
     devices would look like this:
    </para>
<screen>filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|",<!--
--> "r/.*/" ]</screen>
   </step>
<!--   
   <step>
    <para>
     Explicitly accept all the eligible devices, and reject all others.
     For example, a filter line which accepts all MPIO and drbd devices would
     look like this:
    </para>
    <screen>filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|", "r/.*/"]</screen>
   </step> -->
   <step>
    <para>
     Write the configuration file and copy it to all cluster nodes.
    </para>
   </step>
  </procedure>
 </sect1>

<sect1 xml:id="sec-ha-clvm-migrate">
  <title>Online migration from mirror LV to cluster MD</title>
  <para>
   Starting with &productname; 15, <systemitem class="daemon">cmirrord</systemitem> in cluster LVM is deprecated. We highly
   recommend to migrate the mirror logical volumes in your cluster to cluster MD.
   Cluster MD stands for cluster multi-device and is a software-based
   RAID storage solution for a cluster.
  </para>

 <sect2 xml:id="sec-ha-clvm-migrate-setup-before">
  <title>Example setup before migration</title>
    <para>
     Let us assume you have the following example setup:
    </para>
 <itemizedlist>
  <listitem>
   <para>
    You have a two-node cluster consisting of the nodes <literal>&node1;</literal>
    and <literal>&node2;</literal>.
   </para>
  </listitem>
  <listitem>
    <para>
     A mirror logical volume named <literal>test-lv</literal> was
     created from a volume group named <literal>cluster-vg2</literal>.
    </para>
  </listitem>
  <listitem>
   <para>
     The volume group <literal>cluster-vg2</literal> is composed of the
     disks <filename>/dev/vdb</filename> and <filename>/dev/vdc</filename>.
   </para>
  </listitem>
 </itemizedlist>
 <screen>&prompt.root;<command>lsblk</command>
NAME                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda                                   253:0    0   40G  0 disk
├─vda1                                253:1    0    4G  0 part [SWAP]
└─vda2                                253:2    0   36G  0 part /
vdb                                   253:16   0   20G  0 disk
├─cluster--vg2-test--lv_mlog_mimage_0 254:0    0    4M  0 lvm
│ └─cluster--vg2-test--lv_mlog        254:2    0    4M  0 lvm
│   └─cluster--vg2-test--lv           254:5    0   12G  0 lvm
└─cluster--vg2-test--lv_mimage_0      254:3    0   12G  0 lvm
  └─cluster--vg2-test--lv             254:5    0   12G  0 lvm
vdc                                   253:32   0   20G  0 disk
├─cluster--vg2-test--lv_mlog_mimage_1 254:1    0    4M  0 lvm
│ └─cluster--vg2-test--lv_mlog        254:2    0    4M  0 lvm
│   └─cluster--vg2-test--lv           254:5    0   12G  0 lvm
└─cluster--vg2-test--lv_mimage_1      254:4    0   12G  0 lvm
  └─cluster--vg2-test--lv             254:5    0   12G  0 lvm</screen>

  <important xml:id="adm-migration-fail">
  <title>Avoiding migration failures</title>
  <para>
   Before you start the migration procedure, check the capacity and degree
   of utilization of your logical and physical volumes. If the logical volume
   uses 100% of the physical volume capacity, the migration might fail with an
   <literal>insufficient free space</literal> error on the target volume.
   How to prevent this migration failure depends on the options used for
   mirror log:
  </para>
  <itemizedlist>
    <listitem>
     <formalpara>
      <title>Is the mirror log itself mirrored (<option>mirrored</option>
       option) and allocated on the same device as the mirror leg?</title>
      <para> (For example, this might be the case if you have created the
       logical volume for a <systemitem class="daemon">cmirrord</systemitem> setup on &productname; 11 or 12 as
       described in the <link
        xlink:href="https://documentation.suse.com/sle-ha/12-SP5/html/SLE-HA-all/cha-ha-clvm.html#sec-ha-clvm-config-cmirrord">
        <citetitle>&admin;</citetitle> for those versions</link>.)</para>
     </formalpara>
     <para>
      By default, <command>mdadm</command> reserves a certain amount of space
      between the start of a device and the start of array data. During migration,
      you can check for the unused padding space and reduce it with the
      <option>data-offset</option> option as shown in <xref linkend="step-data-offset"/>
      and following.
     </para>
     <para>
      The <option>data-offset</option> must leave enough space on the device
      for cluster MD to write its metadata to it. On the other hand, the offset
      must be small enough for the remaining capacity of the device to accommodate
      all physical volume extents of the migrated volume. Because the volume may
      have spanned the complete device minus the mirror log, the offset must be
      smaller than the size of the mirror log.
     </para>
     <para>
      We recommend to set the <option>data-offset</option> to 128&nbsp;kB.
      If no value is specified for the offset, its default value is 1&nbsp;kB
      (1024&nbsp;bytes).
     </para>
    </listitem>
   <listitem>
    <formalpara>
     <title>
      Is the mirror log written to a different device (<option>disk</option>
      option) or kept in memory (<option>core</option> option)?
     </title>
     <para>
      Before starting the migration, either enlarge the size of the physical
      volume or reduce the size of the logical volume (to free more space for
      the physical volume).
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>
 </important>
</sect2>
<sect2 xml:id="sec-ha-clvm-migrate-lv2clustermd">
 <title>Migrating a mirror LV to cluster MD</title>
  <para>
    The following procedure is based on <xref linkend="sec-ha-clvm-migrate-setup-before"/>.
    Adjust the instructions to match your setup and replace the names for the
    LVs, VGs, disks and the cluster MD device accordingly.
  </para>
  <para>
  The migration does not involve any downtime. The file system can
  still be mounted during the migration procedure.
 </para>
 <procedure>
   <step>
    <para>
     On node <literal>&node1;</literal>, execute the following steps:
    </para>
   <substeps>
   <step>
    <para>
     Convert the mirror logical volume <literal>test-lv</literal>
     to a linear logical volume:
    </para>
    <screen>&prompt.root;lvconvert -m0 cluster-vg2/test-lv /dev/vdc</screen>
   </step>
   <step>
    <para>
      Remove the physical volume <filename>/dev/vdc</filename> from the volume
      group <literal>cluster-vg2</literal>:
    </para>
    <screen>&prompt.root;vgreduce cluster-vg2 /dev/vdc</screen>
   </step>
   <step>
    <para>
      Remove this physical volume from LVM:
    </para>
    <screen>&prompt.root;pvremove /dev/vdc</screen>
    <para>When you run <command>lsblk</command> now, you get:</para>
   <screen>NAME                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda                     253:0    0   40G  0 disk
├─vda1                  253:1    0    4G  0 part [SWAP]
└─vda2                  253:2    0   36G  0 part /
vdb                     253:16   0   20G  0 disk
└─cluster--vg2-test--lv 254:5    0   12G  0 lvm
vdc                     253:32   0   20G  0 disk</screen>
   </step>
   <step xml:id="step-data-offset">
    <para>
      Create a cluster MD device <filename>/dev/md0</filename> with the disk
     <filename>/dev/vdc</filename>:
    </para>
    <screen>&prompt.root;mdadm --create /dev/md0 --bitmap=clustered \
--metadata=1.2 --raid-devices=1 --force --level=mirror \
/dev/vdc --data-offset=128</screen>
    <!--
     https://bugzilla.suse.com/show_bug.cgi?id=1123496
    -->
    <para>
     For details on why to use the <option>data-offset</option> option,
     see <xref linkend="adm-migration-fail"/>.
    </para>
   </step>
  </substeps>
  </step>
   <step>
    <para>
     On node <literal>&node2;</literal>, assemble this MD device:
    </para>
    <screen>&prompt.root;mdadm --assemble md0 /dev/vdc</screen>
    <para>
     If your cluster consists of more than two nodes, execute this step on all
     remaining nodes in your cluster.
    </para>
   </step>
   <step>
    <para>Back on node <literal>&node1;</literal>:
   </para>
   <substeps>
   <step>
    <para>
     Initialize the MD device <filename>/dev/md0</filename> as physical volume
     for use with LVM:
    </para>
    <screen>&prompt.root;pvcreate /dev/md0</screen>
   </step>
   <step>
    <para>
     Add the MD device <filename>/dev/md0</filename> to the volume group
     <literal>cluster-vg2</literal>:
    </para>
    <screen>&prompt.root;vgextend cluster-vg2 /dev/md0</screen>
   </step>
   <step>
    <para>
     Move the data from the disk <filename>/dev/vdb</filename> to the
     <filename>/dev/md0</filename> device:
    </para>
    <screen>&prompt.root;pvmove /dev/vdb /dev/md0</screen>
   </step>
   <step>
    <para>
     Remove the physical volume <filename>/dev/vdb</filename> from the volume
     <literal>group cluster-vg2</literal>:
    </para>
    <screen>&prompt.root;vgreduce cluster-vg2 /dev/vdb</screen>
   </step>
   <step>
    <para>
     Remove the label from the device so that LVM no longer recognizes it as
     physical volume:
    </para>
    <screen>&prompt.root;pvremove /dev/vdb</screen>
   </step>
   <step>
    <para>
     Add <filename>/dev/vdb</filename> to the MD device <filename>/dev/md0</filename>:
    </para>
    <screen>&prompt.root;mdadm --grow /dev/md0 --raid-devices=2 --add /dev/vdb</screen>
   </step>
  </substeps>
  </step>
 </procedure>
</sect2>

<sect2 xml:id="ex-ha-clvm-migrate-setup-after">
 <title>Example setup after migration</title>
  <para>
   When you run <command>lsblk</command> now, you get:
  </para>
  <screen>NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
vda                       253:0    0   40G  0 disk
├─vda1                    253:1    0    4G  0 part  [SWAP]
└─vda2                    253:2    0   36G  0 part  /
vdb                       253:16   0   20G  0 disk
└─md0                       9:0    0   20G  0 raid1
  └─cluster--vg2-test--lv 254:5    0   12G  0 lvm
vdc                       253:32   0   20G  0 disk
└─md0                       9:0    0   20G  0 raid1
  └─cluster--vg2-test--lv 254:5    0   12G  0 lvm</screen>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-clvm-more">
  <title>For more information</title>

  <para>
   For more information about lvmlockd, see the man page of the
   <command>lvmlockd</command> command (<command>man 8 lvmlockd</command>).
  </para>
  <para>
   Thorough information is available from the pacemaker mailing list,
   available at
   <link xlink:href="http://www.clusterlabs.org/wiki/Help:Contents"/>.
  </para>  
 </sect1>
</chapter>
