<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:novdoc-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<!DOCTYPE sect1 PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
                      "novdocx.dtd"
[
<!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<sect1 id="sec_ha_quick_nfs_resources">
 <title>Cluster Resources for an HA NFS Server</title>

 <para>
  A highly available NFS service consists of the following cluster
  resources:
 </para>

 <variablelist>
  <varlistentry>
   <term><xref linkend="sec_ha_quick_nfs_resources_drbd" xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     These resources are used to replicate data: The master/slave resource
     is switched from and to the Primary and Secondary roles as deemed
     necessary by the cluster resource manager.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec_ha_quick_nfs_resources_nfsserver" xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     With this resource, Pacemaker ensures that the NFS server daemons are
     always available.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec_ha_quick_nfs_resources_lvm"  xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     The LVM Volume Group is made available on whichever node currently
     holds the DRBD resource in the Primary role. Apart from that, you need
     resources for one or more file systems residing on any of the Logical
     Volumes in the Volume Group. They are mounted by the cluster manager
     wherever the Volume Group is active.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec_ha_quick_nfs_resources_nfsexport_nfsv4root"  xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     A virtual NFS root export. (Only needed for NFSv4 clients).
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec_ha_quick_nfs_resources_nfsexport_nonroot"  xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     One or more NFS exports, typically corresponding to the file system
     mounted from LVM Logical Volumes.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec_ha_quick_nfs_resources_ipaddr" xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     A virtual, floating cluster IP address, allowing NFS clients to connect
     to the service no matter which physical node it is running on.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>

 <para>
  How to configure these resources (using the <command>crm</command> shell)
  is covered in detail in the following sections.
 </para>

 <example>
  <title>NFS Scenario</title>
  <para>
   The following configuration examples assume that
   <literal>10.9.9.180</literal> is the virtual IP address to use for an NFS
   server which serves clients in the <literal>10.9.9.0/24</literal> subnet.
  </para>
  <para>
   The service is to host an NFSv4 virtual file system root hosted from
   <literal>/srv/nfs</literal>, with exports served from <literal>/srv/nfs/sales</literal> and
   <literal>/srv/nfs/engineering</literal>.
  </para>
  <para>
   Into these export directories, the cluster will mount
   <literal>ext3</literal> file systems from Logical Volumes named
   <literal>sales</literal> and <literal>engineering</literal>,
   respectively. Both of these Logical Volumes will be part of a highly
   available Volume Group, named <literal>nfs</literal>, which is hosted on
   a DRBD device.
  </para>
 </example>

 <sect2 id="sec_ha_quick_nfs_resources_drbd">
  <title>DRBD Primitive, and Master/Slave Resources</title>
  <para>
   To configure these resources, issue the following commands from the
   <command>crm</command> shell:
  </para>
<screen>crm(live)# configure
crm(live)configure# primitive p_drbd_nfs \
  ocf:linbit:drbd \
    params drbd_resource="nfs" \
  op monitor interval="15" role="Master" \
  op monitor interval="30" role="Slave"
crm(live)configure# ms ms_drbd_nfs p_drbd_nfs \
  meta master-max="1" master-node-max="1" clone-max="2" \
  clone-node-max="1" notify="true"
crm(live)configure# commit</screen>
  <para>
   This will create a Pacemaker Master/Slave resource corresponding to the
   DRBD resource <literal>nfs</literal>. Pacemaker should now activate your
   DRBD resource on both nodes, and promote it to the Master role on one of
   them.
  </para>
  <para>
   Check this with the <command>crm_mon</command> command, or by looking at
   the contents of <filename>/proc/drbd</filename>.
  </para>
 </sect2>

 <sect2 id="sec_ha_quick_nfs_resources_nfsserver">
  <title>NFS Kernel Server Resource</title>
  <para>
   In the <literal>crm</literal> shell, the resource for the NFS server
   daemons must be configured as a <emphasis>clone</emphasis> of an
   <literal>lsb</literal> resource type, as follows:
  </para>
<screen>crm(live)configure# primitive p_lsb_nfsserver \
  lsb:nfsserver \
  op monitor interval="30s"
crm(live)configure# clone cl_lsb_nfsserver p_lsb_nfsserver 
crm(live)configure# commit</screen>
  <note>
   <title>Resource Type Name and NFS Server init Script</title>
   <para os="sles">
    The name of the <literal>lsb</literal> resource type must be
    <emphasis>exactly</emphasis> identical to the filename of the NFS server
    init script, installed under <filename>/etc/init.d</filename>. &sls;
    ships the init script as <literal>/etc/init.d/nfsserver</literal>
    (package <systemitem class="resource">nfs-kernel-server</systemitem>).
    Hence the resource must be of type <literal>lsb:nfsserver</literal>.
   </para>
   <para os="linbit">
    The name of the <literal>lsb</literal> resource type must be
    <emphasis>exactly</emphasis> identical to the filename of the NFS server
    init script, installed under <filename>/etc/init.d</filename>. As &sle;
    ships the init script as <literal>/etc/init.d/nfs</literal>, the init
    script is installed under <literal>/etc/init.d</literal>. For example,
    if your distribution ships this script as
    <literal>/etc/init.d/nfs</literal>, then the resource must use the
    <literal>lsb:nfs</literal> resource type.
   </para>
  </note>
  <para>
   After you have committed this configuration, Pacemaker should start the
   NFS Kernel server processes on both nodes.
  </para>
 </sect2>

 <sect2 id="sec_ha_quick_nfs_resources_lvm">
  <title>LVM and File System Resources</title>
  <orderedlist>
   <listitem>
    <para>
     Configure LVM and Filesystem type resources as follows (but do
     <emphasis>not commit</emphasis> this configuration yet):
    </para>
<screen>crm(live)configure# primitive p_lvm_nfs \
  ocf:heartbeat:LVM \
    params volgrpname="nfs" \
  op monitor interval="30s"
crm(live)configure# primitive p_fs_engineering \
  ocf:heartbeat:Filesystem \
  params device=/dev/nfs/engineering \
    directory=/srv/nfs/engineering \
    fstype=ext3 \
  op monitor interval="10s"
crm(live)configure# primitive p_fs_sales \
  ocf:heartbeat:Filesystem \
  params device=/dev/nfs/sales \
    directory=/srv/nfs/sales \
    fstype=ext3 \
 op monitor interval="10s"</screen>
   </listitem>
   <listitem>
    <para>
     Combine these resources into a Pacemaker resource
     <emphasis>group</emphasis>:
    </para>
<screen>crm(live)configure# group g_nfs \
  p_lvm_nfs p_fs_engineering p_fs_sales</screen>
   </listitem>
   <listitem>
    <para>
     Add the following constraints to make sure that the group is started on
     the same node where the DRBD Master/Slave resource is in the Master
     role:
    </para>
<screen>crm(live)configure# order o_drbd_before_nfs inf: \
  ms_drbd_nfs:promote g_nfs:start
crm(live)configure# colocation c_nfs_on_drbd inf: \
  g_nfs ms_drbd_nfs:Master</screen>
   </listitem>
   <listitem>
    <para>
     Commit this configuration:
    </para>
<screen>crm(live)configure# commit</screen>
   </listitem>
  </orderedlist>
  <para>
   After these changes have been committed, Pacemaker does the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     It activates all Logical Volumes of the <literal>nfs</literal> LVM
     Volume Group on the same node where DRBD is in the Primary role.
     Confirm this with <command>vgdisplay</command> or
     <command>lvs</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     It mounts the two Logical Volumes to
     <filename>/srv/nfs/sales</filename> and
     <filename>/srv/nfs/engineering</filename> on the same node. Confirm
     this with <command>mount</command> (or by looking at
     <filename>/proc/mounts</filename>).
    </para>
   </listitem>
  </itemizedlist>
 </sect2>

 <sect2 id="sec_ha_quick_nfs_resources_nfsexport">
  <title>NFS Export Resources</title>
  <para>
   Once your DRBD, LVM, and file system resources are working properly,
   continue with the resources managing your NFS exports. To create highly
   available NFS export resources, use the <literal>exportfs</literal>
   resource type.
  </para>
  <sect3 id="sec_ha_quick_nfs_resources_nfsexport_nfsv4root">
   <title>NFSv4 Virtual File System Root</title>
   <para>
    If clients exclusively use NFSv3 to connect to the server, you do not
    need this resource. In this case, continue with
    <xref
            linkend="sec_ha_quick_nfs_resources_nfsexport_nonroot"/>.
   </para>
   <orderedlist>
    <listitem>
     <para>
      To enable NFSv4 support, configure one&mdash;and only one&mdash;NFS
      export whose <literal>fsid</literal> option is either
      <literal>0</literal> (as used in the example below) or the string
      <literal>root</literal>. This is the root of the virtual NFSv4 file
      system.
     </para>
<screen>crm(live)configure# primitive p_exportfs_root \
  ocf:heartbeat:exportfs \
  params fsid=0 \
    directory="/srv/nfs" \
    options="rw,crossmnt" \
    clientspec="10.9.9.0/255.255.255.0" \
  op monitor interval="30s"
crm(live)configure# clone cl_exportfs_root p_exportfs_root</screen>
     <para>
      This resource does not hold any actual NFS-exported data, merely the
      empty directory (<filename>/srv/nfs</filename>) that the other NFS
      exports are mounted into. Since there is no shared data involved here,
      we can safely <emphasis>clone</emphasis> this resource.
     </para>
    </listitem>
    <listitem>
     <para>
      Since any data should be exported only on nodes where this clone has
      been properly started, add the following constraints to the
      configuration:
     </para>
<screen>crm(live)configure# order o_root_before_nfs inf: \
  cl_exportfs_root g_nfs:start
crm(live)configure# colocation c_nfs_on_root inf: \
  g_nfs cl_exportfs_root
crm(live)configure# commit</screen>
     <para>
      After this, Pacemaker should start the NFSv4 virtual file system root
      on both nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the output of the <command>exportfs -v</command> command to
      verify this.
     </para>
    </listitem>
   </orderedlist>
  </sect3>
  <sect3 id="sec_ha_quick_nfs_resources_nfsexport_nonroot">
   <title>Non-root NFS Exports</title>
   <para>
    All NFS exports that do <emphasis>not</emphasis> represent an NFSv4
    virtual file system root must set the <literal>fsid</literal> option to
    either a unique positive integer (as used in the example), or a UUID
    string (32 hex digits with arbitrary punctuation).
   </para>
   <orderedlist>
    <listitem>
     <para>
      Create NFS exports with the following commands:
     </para>
<screen>crm(live)configure# primitive p_exportfs_sales \
  ocf:heartbeat:exportfs \
    params fsid=1 \
      directory="/srv/nfs/sales" \
      options="rw,mountpoint" \
      clientspec="10.9.9.0/255.255.255.0" \
      wait_for_leasetime_on_stop=true \
  op monitor interval="30s"
crm(live)configure# primitive p_exportfs_engineering \
  ocf:heartbeat:exportfs \
    params fsid=2 \
      directory="/srv/nfs/engineering" \
      options="rw,mountpoint" \
      clientspec="10.9.9.0/255.255.255.0" \
      wait_for_leasetime_on_stop=true \
  op monitor interval="30s"</screen>
    </listitem>
    <listitem>
     <para>
      After you have created these resources, add them to the existing
      <literal>g_nfs</literal> resource group:
     </para>
<screen>crm(live)configure# edit g_nfs</screen>
    </listitem>
    <listitem>
     <para>
      Edit the group configuration so it looks like this:
     </para>
<screen>group g_nfs \
  p_lvm_nfs p_fs_engineering p_fs_sales \
  p_exportfs_engineering p_exportfs_sales</screen>
    </listitem>
    <listitem>
     <para>
      Commit this configuration:
     </para>
<screen>crm(live)configure# commit</screen>
     <para>
      Pacemaker will export the NFS virtual file system root and the two
      other exports.
     </para>
    </listitem>
    <listitem>
     <para>
      Confirm that the NFS exports are set up properly:
     </para>
<screen>exportfs -v</screen>
    </listitem>
   </orderedlist>
  </sect3>
 </sect2>

 <sect2 id="sec_ha_quick_nfs_resources_ipaddr">
  <title>Resource for Floating IP Address</title>
  <para>
   To enable smooth and seamless failover, your NFS clients will be
   connecting to the NFS service via a floating cluster IP address, rather
   than via any of the hosts' physical IP addresses.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Add the following resource to the cluster configuration:
    </para>
<screen>crm(live)configure# primitive p_ip_nfs \
  ocf:heartbeat:IPaddr2 \
    params ip=10.9.9.180 \
      cidr_netmask=24 \
    op monitor interval="30s"</screen>
   </listitem>
   <listitem>
    <para>
     Add the IP address to the resource group (like you did with the
     <literal>exportfs</literal> resources):
    </para>
<screen>crm(live)configure# edit g_nfs</screen>
    <para>
     This is the final setup of the resource group:
    </para>
<screen>group g_nfs \
  p_lvm_nfs p_fs_engineering p_fs_sales \
  p_exportfs_engineering p_exportfs_sales \
  p_ip_nfs</screen>
   </listitem>
   <listitem>
    <para>
     Complete the cluster configuration:
    </para>
<screen>crm(live)configure# commit</screen>
    <para>
     At this point Pacemaker will set up the floating cluster IP address.
    </para>
   </listitem>
   <listitem>
    <para>
     Confirm that the cluster IP is running correctly:
    </para>
<screen>ip address show</screen>
    <para>
     The cluster IP should be added as a <literal>secondary</literal>
     address to whatever interface is connected to the
     <literal>10.9.9.0/24</literal> subnet.
    </para>
   </listitem>
  </orderedlist>
  <note>
   <title>Connection of Clients</title>
   <para>
    There is no way to make your NFS exports bind to
    <emphasis>just</emphasis> this cluster IP address. The Kernel NFS server
    always binds to the wildcard address (<literal>0.0.0.0</literal> for
    IPv4). However, your clients must connect to the NFS exports through the
    floating IP address <emphasis>only,</emphasis> otherwise the clients
    will suffer service interruptions on cluster failover.
    <remark>pmarek
    2013-11-28: you can use_iptables_ to make sure that the management address
    isn't used</remark>
   </para>
  </note>
 </sect2>
</sect1>
