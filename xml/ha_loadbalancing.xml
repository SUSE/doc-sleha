<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!--taroth 2011-11-25: maintainer of ipvsadmin: marius tomaschewski-->

<!-- 
toms 2014-05-27:
* Restructured file to make it easier to describe both LVS and HAProxy
* Look for "TODO" to see the special topics which needs some help.
* Let Darix/Lars proofread that file.
-->

<chapter id="cha.ha.lvs">
 <title>Load Balancing</title>
<!--taroth 090519: see http://www.linuxvirtualserver.org/-->
 <para>
  The goal of &lvs; (LVS) is to provide a basic framework that directs
  network connections to multiple servers that share their workload. &lvs;
  is a cluster of servers (one or more load balancers and several real
  servers for running services) which appears to be one large, fast server
  to an outside client. This apparent single server is called a
  <emphasis>virtual server</emphasis>. The &lvs; can be used to build highly
  scalable and highly available network services, such as Web, cache, mail,
  FTP, media and VoIP services.
 </para>
 <para>
  The real servers and the load balancers may be interconnected by either
  high-speed LAN or by geographically dispersed WAN. The load balancers can
  dispatch requests to the different servers. They make parallel services of
  the cluster appear as a virtual service on a single IP address (the
  virtual IP address or VIP). Request dispatching can use IP load balancing
  technologies or application-level load balancing technologies. Scalability
  of the system is achieved by transparently adding or removing nodes in the
  cluster. High availability is provided by detecting node or daemon
  failures and reconfiguring the system appropriately.
 </para>
 <sect1 id="sec.ha.lb.overview">
  <title>Conceptual Overview</title>
   <remark>toms 2014-05-27: TODO: I've inserted the following para's and 
     itemizedlist's today. Not sure if this is correct as this is dedicated
     to both (LVS _and_ HAProxy).
   </remark>
   <para>As the capability of handling the workload of any server is finite, 
     the site's maintainer has to find ways to spread the load on 
     several servers. This can be achieved with different methods:</para>
   <itemizedlist>
     <listitem>
       <para>via internal mechanisms included in the application server,</para>
     </listitem>
     <listitem>
       <para>via external components,</para>
     </listitem>
     <listitem>
       <para>via architectural redesign.</para>
     </listitem>
   </itemizedlist>
   
   <para>Load balancing is the ability to make several servers participate 
     in the same service and do the same work.
   </para>
   
   <para>There are several load balancing techniques:</para>
   
   <itemizedlist>
     <listitem>
       <formalpara>
         <title>Via DNS</title>
         <para>A DNS server can have several entries for a given hostname.
           With DNS roundrobin, the DNS server will return all of them in a 
           rotating order. Different users will see different addresses.
         </para>
       </formalpara>
     </listitem>
     <listitem>
       <formalpara>
         <title>Selecting the <quote>best</quote> server</title>
         <para>Although it has several drawbacks, but balancing could
           be implemented with an <quote>the first server who responds</quote>
           or <quote>the least loaded server</quote> approach.</para>
       </formalpara>
     </listitem>
     <listitem>
       <formalpara>
         <title>Reduce number of users per server</title>
         <para>A load balancer between users and servers can divide the
           number of users across multiple servers.
         </para>
       </formalpara>
     </listitem>
   </itemizedlist>
   
   <remark>toms 2014-05-27: TODO: Should insert a connection between the 
     general issues above to the specific issues below.</remark>
   
  <para>
   The following sections give an overview of the main LVS components and
   concepts.
  </para>

  <remark>toms 2014-05-27: TODO: The following sect2's needs to be reworked
   to take HAProxy into account too. 
  </remark>

  <sect2 id="sec.ha.lvs.overview.director">
   <title>Director</title>
   <para>
    The main component of LVS is the ip_vs (or IPVS) Kernel code. It
    implements transport-layer load balancing inside the Linux Kernel
    (layer-4 switching). The node that runs a Linux Kernel including the
    IPVS code is called <emphasis>director</emphasis>. The IPVS code running
    on the director is the essential feature of LVS.
   </para>
   <para>
    When clients connect to the director, the incoming requests are
    load-balanced across all cluster nodes: The director forwards packets to
    the real servers, using a modified set of routing rules that make the
    LVS work. For example, connections do not originate or terminate on the
    director, it does not send acknowledgments. The director acts as a
    specialized router that forwards packets from end-users to real servers
    (the hosts that run the applications that process the requests).
   </para>
   <para>
    By default, the Kernel does not have the IPVS module installed. The IPVS
    Kernel module is included in the
    <systemitem class="resource">cluster-network-kmp-default</systemitem>
    package.
   </para>
  </sect2>

  <sect2 id="sec.ha.lvs.overview.userspace">
   <title>User Space Controller and Daemons</title>
   <para>
    The <systemitem class="daemon">ldirectord</systemitem> daemon is a
    user-space daemon for managing &lvs; and monitoring the real servers in
    an LVS cluster of load balanced virtual servers. A configuration file,
    <filename>/etc/ha.d/ldirectord.cf</filename>, specifies the virtual
    services and their associated real servers and tells
    <systemitem class="daemon"
     >ldirectord</systemitem> how to
    configure the server as a LVS redirector. When the daemon is
    initialized, it creates the virtual services for the cluster.
   </para>
   <para>
    By periodically requesting a known URL and checking the responses, the
    <systemitem
     class="daemon">ldirectord</systemitem> daemon monitors
    the health of the real servers. If a real server fails, it will be
    removed from the list of available servers at the load balancer. When
    the service monitor detects that the dead server has recovered and is
    working again, it will add the server back to the list of available
    servers. In case that all real servers should be down, a fall-back
    server can be specified to which to redirect a Web service. Typically
    the fall-back server is localhost, presenting an emergency page about
    the Web service being temporarily unavailable.
   </para>
   <para>
    The <systemitem class="daemon">ldirectord</systemitem> uses the
    <systemitem>ipvsadm</systemitem> tool (package
    <systemitem class="resource"
     >ipvsadm</systemitem>) to manipulate
    the virtual server table in the Linux Kernel.
   </para>
  </sect2>

  <sect2 id="sec.ha.lvs.overview.forwarding">
   <title>Packet Forwarding</title>
   <para>
    There are three different methods of how the director can send packets
    from the client to the real servers:
   </para>
   <variablelist>
    <varlistentry>
     <term>Network Address Translation (NAT)</term>
     <listitem>
      <para>
       Incoming requests arrive at the virtual IP and are forwarded to the
       real servers by changing the destination IP address and port to that
       of the chosen real server. The real server sends the response to the
       load balancer which in turn changes the destination IP address and
       forwards the response back to the client, so that the end user
       receives the replies from the expected source. As all traffic goes
       through the load balancer, it usually becomes a bottleneck for the
       cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>IP Tunneling (IP-IP Encapsulation)</term>
     <listitem>
      <para>
       IP tunneling enables packets addressed to an IP address to be
       redirected to another address, possibly on a different network. The
       LVS sends requests to real servers through an IP tunnel (redirecting
       to a different IP address) and the real servers reply directly to the
       client using their own routing tables. Cluster members can be in
       different subnets.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Direct Routing</term>
     <listitem>
      <para>
       Packets from end users are forwarded directly to the real server. The
       IP packet is not modified, so the real servers must be configured to
       accept traffic for the virtual server's IP address. The response from
       the real server is sent directly to the client. The real servers and
       load balancers have to be in the same physical network segment.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 id="sec.ha.lvs.overview.schedulers">
   <title>Scheduling Algorithms</title>
   <para>
    Deciding which real server to use for a new connection requested by a
    client is implemented using different algorithms. They are available as
    modules and can be adapted to specific needs. For an overview of
    available modules, refer to the <command>ipvsadm(8)</command> man page.
    Upon receiving a connect request from a client, the director assigns a
    real server to the client based on a <emphasis>schedule</emphasis>. The
    scheduler is the part of the IPVS Kernel code which decides which real
    server will get the next new connection.
   </para>
  </sect2>
 
  <!--  toms 2014-05-27: TODO: Insert comparison table between 
        LVS and HAProxy
  -->
 </sect1>
 
  <sect1 id="sec.ha.lb.lvs">
   <title>Configuring Load Balancing with &lvs;</title>
   <remark>toms 2014-05-27: TODO: add an introducery para.</remark>
   
   <remark>toms 2014-05-27: TODO: Do we need an additional section about
    configuring LVS manually (e.g. without YaST)?</remark>
    <para></para>
   <sect2 id="sec.ha.lvs.ldirectord">
     <title>Setting Up IP Load Balancing with &yast;</title>

  <para>
   You can configure Kernel-based IP load balancing with the &yast; IP Load
   Balancing module. It is a front-end for
   <systemitem class="daemon">ldirectord</systemitem>.
  </para>

  <para>
   To access the IP Load Balancing dialog, start &yast; as &rootuser; and
   select <menuchoice> <guimenu>&ha;</guimenu> <guimenu>IP Load
   Balancing</guimenu> </menuchoice>. Alternatively, start the &yast;
   cluster module as &rootuser; on a command line with
   <command>yast2&nbsp;iplb</command>.
  </para>

  <para>
   The &yast; module writes its configuration to
   <filename>/etc/ha.d/ldirectord.cf</filename>. The tabs available in the
   &yast; module correspond to the structure of the
   <filename>/etc/ha.d/ldirectord.cf</filename> configuration file, defining
   global options and defining the options for the virtual services.
  </para>

  <para>
   For an example configuration and the resulting processes between load
   balancers and real servers, refer to
   <xref linkend="ex.ha.lvs.ldirectord"/>.
  </para>

  <note>
   <title>Global Parameters and Virtual Server Parameters</title>
   <para>
    If a certain parameter is specified in both the virtual server section
    and in the global section, the value defined in the virtual server
    section overrides the value defined in the global section.
   </para>
  </note>

  <procedure id="sec.ha.lvs.ldirectord.global">
   <title>Configuring Global Parameters</title>
   <para>
    The following procedure describes how to configure the most important
    global parameters. For more details about the individual parameters (and
    the parameters not covered here), click <guimenu>Help</guimenu> or refer
    to the <systemitem class="daemon">ldirectord</systemitem> man page.
   </para>
   <step>
    <para>
     With <guimenu>Check Interval</guimenu>, define the interval in which
     <systemitem
      class="daemon">ldirectord</systemitem> will connect
     to each of the real servers to check if they are still online.
    </para>
   </step>
   <step>
    <para>
     With <guimenu>Check Timeout</guimenu>, set the time in which the real
     server should have responded after the last check.
    </para>
   </step>
   <step>
    <para>
     With <guimenu>Failure Count </guimenu> you can define how many times
     <systemitem
      class="daemon">ldirectord</systemitem> will attempt
     to request the real servers until the check is considered failed.
    </para>
   </step>
   <step>
    <para>
     With <guimenu>Negotiate Timeout</guimenu> define a timeout in seconds
     for negotiate checks.
    </para>
   </step>
   <step>
    <para>
     In <guimenu>Fallback</guimenu>, enter the hostname or IP address of the
     Web server onto which to redirect a Web service in case all real
     servers are down.
    </para>
   </step>
   <step>
    <para>
     If you want the system to send alerts in case the connection status to
     any real server changes, enter a valid e-mail address in <guimenu>Email
     Alert</guimenu>.
    </para>
   </step>
   <step>
    <para>
     With <guimenu>Email Alert Frequency</guimenu>, define after how many
     seconds the e-mail alert should be repeated if any of the real servers
     remains inaccessible.
    </para>
   </step>
   <step>
    <para>
     In <guimenu>Email Alert Status</guimenu> specify the server states for
     which email alerts should be sent. If you want to define more than one
     state, use a comma-separated list.
    </para>
   </step>
   <step>
    <para>
     With <guimenu>Auto Reload</guimenu> define, if
     <systemitem class="daemon"
      >ldirectord</systemitem> should
     continuously monitor the configuration file for modification. If set to
     <literal>yes</literal>, the configuration is automatically reloaded
     upon changes.
    </para>
   </step>
   <step>
    <para>
     With the <guimenu>Quiescent</guimenu> switch, define if to remove
     failed real servers from the Kernel's LVS table or not. If set to
     <guimenu>Yes</guimenu>, failed servers are not removed. Instead their
     weight is set to <literal>0</literal> which means that no new
     connections will be accepted. Already established connections will
     persist until they time out.
    </para>
   </step>
   <step>
    <para>
     If you want to use an alternative path for logging, specify a path for
     the logs in <guimenu>Log File</guimenu>. By default,
     <systemitem class="daemon">ldirectord</systemitem> writes its logs to
     <filename>/var/log/ldirectord.log</filename>.
    </para>
   </step>
  </procedure>

  <figure id="fig.ha.lvs.yast.global">
   <title>&yast; IP Load Balancing&mdash;Global Parameters</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="yast2_iplb_global.png" width="95%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="yast2_iplb_global.png" width="65%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <procedure id="sec.ha.lvs.ldirectord.virtual">
   <title>Configuring Virtual Services</title>
   <para>
    You can configure one or more virtual services by defining a couple of
    parameters for each. The following procedure describes how to configure
    the most important parameters for a virtual service. For more details
    about the individual parameters (and the parameters not covered here),
    click <guimenu>Help</guimenu> or refer to the
    <systemitem class="daemon">ldirectord</systemitem> man page.
   </para>
   <step>
    <para>
     In the &yast; IP Load Balancing module, switch to the <guimenu>Virtual
     Server Configuration</guimenu> tab.
    </para>
   </step>
   <step>
    <para>
     <guimenu>Add</guimenu> a new virtual server or <guimenu>Edit</guimenu>
     an existing virtual server. A new dialog shows the available options.
    </para>
   </step>
   <step>
    <para>
     In <guimenu>Virtual Server</guimenu> enter the shared virtual IP
     address (IPv4 or IPv6) and port under which the load balancers and the
     real servers are accessible as LVS. Instead of IP address and port
     number you can also specify a hostname and a service. Alternatively,
     you can also use a firewall mark. A firewall mark is a way of
     aggregating an arbitrary collection of <literal>VIP:port</literal>
     services into one virtual service.
    </para>
   </step>
   <step>
    <para>
     To specify the <guimenu>Real Servers</guimenu>, you need to enter the
     IP addresses (IPv4, IPv6, or hostnames) of the servers, the ports (or
     service names) and the forwarding method. The forwarding method must
     either be <literal>gate</literal>, <literal>ipip</literal> or
     <literal>masq</literal>, see
     <xref linkend="sec.ha.lvs.overview.forwarding"/>.
    </para>
    <para>
     Click the <guimenu>Add</guimenu> button and enter the required
     arguments for each real server.
    </para>
   </step>
   <step>
    <para>
     As <guimenu>Check Type</guimenu>, select the type of check that should
     be performed to test if the real servers are still alive. For example,
     to send a request and check if the response contains an expected
     string, select <literal>Negotiate</literal>.
    </para>
   </step>
   <step id="step.ha.lvs.ldirectord.service">
    <para>
     If you have set the <guimenu>Check Type</guimenu> to
     <literal>Negotiate</literal>, you also need to define the type of
     service to monitor. Select it from the <guimenu>Service</guimenu>
     drop-down list.
    </para>
   </step>
   <step>
    <para>
     In <guimenu>Request</guimenu>, enter the URI to the object that is
     requested on each real server during the check intervals.
    </para>
   </step>
   <step>
    <para>
     If you want to check if the response from the real servers contains a
     certain string (<quote>I'm alive</quote> message), define a regular
     expression that needs to be matched. Enter the regular expression into
     <guimenu>Receive</guimenu>. If the response from a real server contains
     this expression, the real server is considered to be alive.
    </para>
   </step>
   <step>
    <para>
     Depending on the type of <guimenu>Service</guimenu> you have selected
     in <xref linkend="step.ha.lvs.ldirectord.service"/>, you also need to
     specify further parameters for authentication. Switch to the
     <guimenu>Auth type</guimenu> tab and enter the details like
     <guimenu>Login</guimenu>, <guimenu>Password</guimenu>,
     <guimenu>Database</guimenu>, or <guimenu>Secret</guimenu>. For more
     information, refer to the &yast; help text or to the
     <systemitem
      class="daemon">ldirectord</systemitem> man page.
    </para>
   </step>
   <step>
    <para>
     Switch to the <guimenu>Others</guimenu> tab.
    </para>
   </step>
   <step>
    <para>
     Select the <guimenu>Scheduler</guimenu> to be used for load balancing.
     For information on the available schedulers, refer to the
     <command>ipvsadm(8)</command> man page.
    </para>
   </step>
   <step>
    <para>
     Select the <guimenu>Protocol</guimenu> to be used.
<!--<remark>taroth 2010-03-16: DEVs, to be
      used for what? for forwarding the requests arriving at the VIP to the real servers? or for
      which purpose?</remark> taroth 2013-04-08: no answer to this question yet-->
     If the virtual service is specified as an IP address and port, it must
     be either <literal>tcp</literal> or <literal>udp</literal>. If the
     virtual service is specified as a firewall mark, the protocol must be
     <literal>fwm</literal>.
    </para>
   </step>
   <step>
    <para>
     Define further parameters, if needed. Confirm your configuration with
     <guimenu>OK</guimenu>. &yast; writes the configuration to
     <filename>/etc/ha.d/ldirectord.cf</filename>.
    </para>
   </step>
  </procedure>

<!--taroth 2011-05-04: filed https://bugzilla.novell.com/show_bug.cgi?id=691671, based on 
   http://doccomments.provo.novell.com/admin/viewcomment/16314 - FIXED-->

  <figure id="fig.ha.lvs.yast.virtual">
   <title>&yast; IP Load Balancing&mdash;Virtual Services</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="yast2_iplb_virtual.png" width="95%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="yast2_iplb_virtual.png" width="65%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <example id="ex.ha.lvs.ldirectord">
   <title>Simple ldirectord Configuration</title>
   <para>
    The values shown in <xref linkend="fig.ha.lvs.yast.global"/> and
    <xref
     linkend="fig.ha.lvs.yast.virtual"/>, would lead to the
    following configuration, defined in
    <filename>/etc/ha.d/ldirectord.cf</filename>:
   </para>
<screen>autoreload = yes <co id="co.ha.ldirectord.autoreload"/>
checkinterval = 5 <co id="co.ha.ldirectord.checkintervall"/>
checktimeout = 3 <co id="co.ha.ldirectord.checktimeout"/>
quiescent = yes <co id="co.ha.ldirectord.quiescent"/>
    virtual = 192.168.0.200:80 <co id="co.ha.ldirectord.virtual"/>
    checktype = negotiate <co id="co.ha.ldirectord.checktype"/>
    fallback = 127.0.0.1:80 <co id="co.ha.ldirectord.fallback"/>
    protocol = tcp <co id="co.ha.ldirectord.protocol"/>
    real = 192.168.0.110:80 gate <co id="co.ha.ldirectord.real"/>
    real = 192.168.0.120:80 gate <xref linkend="co.ha.ldirectord.real" xrefstyle="select:label nopage"/>
    receive = "still alive" <co id="co.ha.ldirectord.receive"/>
    request = "test.html" <co id="co.ha.ldirectord.request"/>
    scheduler = wlc <co id="co.ha.ldirectord.scheduler"/>
    service = http <co id="co.ha.ldirectord.service"/></screen>
   <calloutlist>
    <callout arearefs="co.ha.ldirectord.autoreload">
     <para>
      Defines that <systemitem class="daemon">ldirectord</systemitem> should
      continuously check the configuration file for modification.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.checkintervall">
     <para>
      Interval in which
      <systemitem
      class="daemon">ldirectord</systemitem> will connect
      to each of the real servers to check if they are still online.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.checktimeout">
     <para>
      Time in which the real server should have responded after the last
      check.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.quiescent">
     <para>
      Defines not to remove failed real servers from the Kernel's LVS table,
      but to set their weight to <literal>0</literal> instead.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.virtual">
     <para>
      Virtual IP address (VIP) of the LVS. The LVS is available at port
      <literal>80</literal>.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.checktype">
     <para>
      Type of check that should be performed to test if the real servers are
      still alive.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.fallback">
     <para>
      Server onto which to redirect a Web service all real servers for this
      service are down.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.protocol">
     <para>
      Protocol to be used.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.real">
     <para>
      Two real servers defined, both available at port
      <literal>80</literal>. The packet forwarding method is
      <literal>gate</literal>, meaning that direct routing is used.
     </para>
    </callout>
<!-- <callout arearefs="co.ha.ldirectord.10">
     <para>
      One of two real servers defined, available at port
      <literal>80</literal>. The packet forwarding method is
      <literal>gate</literal>, meaning that direct routing is used.
     </para>
    </callout>-->
    <callout arearefs="co.ha.ldirectord.receive">
     <para>
      Regular expression that needs to be matched in the response string
      from the real server.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.request">
     <para>
      URI to the object that is requested on each real server during the
      check intervals.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.scheduler">
     <para>
      Selected scheduler to be used for load balancing.
     </para>
    </callout>
    <callout arearefs="co.ha.ldirectord.service">
     <para>
      Type of service to monitor.
     </para>
    </callout>
   </calloutlist>
   <para>
    This configuration would lead to the following process flow: The
    <systemitem class="daemon">ldirectord</systemitem> will connect to each
    real server once every 5 seconds
    (<xref
     linkend="co.ha.ldirectord.checkintervall" xrefstyle="select:label nopage"/>)
    and request <literal>192.168.0.110:80/test.html</literal> or
    <literal>192.168.0.120:80/test.html</literal> as specified in
    <xref
     linkend="co.ha.ldirectord.real" xrefstyle="select:label nopage"/>
    and
    <xref
     linkend="co.ha.ldirectord.request" xrefstyle="select:label nopage"/>.
    If it does not receive the expected <literal>still alive</literal>
    string
    (<xref
     linkend="co.ha.ldirectord.receive" xrefstyle="select:label nopage"/>)
    from a real server within 3 seconds
    (<xref linkend="co.ha.ldirectord.checktimeout"
     xrefstyle="select:label nopage"/>)
    of the last check, it will remove the real server from the available
    pool. However, because of the <literal>quiescent=yes</literal> setting
    (<xref
     linkend="co.ha.ldirectord.quiescent" xrefstyle="select:label nopage"/>),
    the real server will not be removed from the LVS table, but its weight
    will be set to <literal>0</literal> so that no new connections to this
    real server will be accepted. Already established connections will be
    persistent until they time out.
   </para>
  </example>
 </sect2>

   <sect2 id="sec.ha.lvs.further">
     <title>Further Setup</title>
     
     <para>
       Apart from the configuration of
       <systemitem class="daemon">ldirectord</systemitem> with &yast;, you need
       to make sure the following conditions are fulfilled to complete the LVS
       setup:
     </para>
     
     <itemizedlist>
       <listitem>
         <para>
           The real servers are set up correctly to provide the needed services.
         </para>
       </listitem>
       <listitem>
         <para>
           The load balancing server (or servers) must be able to route traffic to
           the real servers using IP forwarding. The network configuration of the
           real servers depends on which packet forwarding method you have chosen.
         </para>
       </listitem>
       <listitem>
         <para>
           To prevent the load balancing server (or servers) from becoming a
           single point of failure for the whole system, you need to set up one or
           several backups of the load balancer. In the cluster configuration,
           configure a primitive resource for
           <systemitem class="daemon"
             >ldirectord</systemitem>, so that
           <systemitem class="daemon">ldirectord</systemitem> can fail over to
           other servers in case of hardware failure.
         </para>
       </listitem>
       <listitem>
         <para>
           As the backup of the load balancer also needs the
           <systemitem class="daemon"
             >ldirectord</systemitem> configuration
           file to fulfill its task, make sure the
           <filename>/etc/ha.d/ldirectord.cf</filename> is available on all
           servers that you want to use as backup for the load balancer. You can
           synchronize the configuration file with &csync; as described in
           <xref linkend="sec.ha.installation.setup.csync2"/>.
         </para>
       </listitem>
     </itemizedlist>
   </sect2>
   
 </sect1>

  <sect1 id="sec.ha.lb.haproxy">
    <title>Configuring Load Balancing with &haproxy;</title>    
    <remark>toms 2014-05-27: Most of the following items are taken from 
      FATE#316459. Not sure if this is really correct. Needs technical 
      proofreading by an expert.</remark>
    <para>Although there is some overlap, &haproxy; can be used in scenarios 
      where LVS/<command>ipvsadm</command> are not adequate and vice versa:      
    </para>
    <itemizedlist>
      <listitem>
        <para>SSL termination: the front-end load-balancers can handle the 
          SSL layer and thus the cloud nodes do not need to have access to 
          the SSL keys, or could take advantage of SSL accelerators in the
          load balancers.</para>
      </listitem>
      <listitem>
        <para>&haproxy; operates at the application level, allowing the load 
          balancing decisions to be influenced by the content stream. This 
          allows for persistence based on cookies and other such filters.</para>
      </listitem>
    </itemizedlist>
    
   <para>On the other hand, LVS/<command>ipvsadm</command> cannot be fully 
     replaced by &haproxy;:</para>
   
   <itemizedlist>
     <listitem>
       <para>LVS supports <quote>direct routing</quote>, where the load balancer is only 
         in the inbound stream, whereas the outbound traffic is routed to the 
         clients directly. This allows for potentially much higher throughput 
         in asymmetric environments.</para>
     </listitem>
     <listitem>
       <para>LVS supports stateful connection table replication (via conntrackd). 
         This allows for load-balancer fail-over that is transparent to the 
         client and server.</para>
     </listitem>
   </itemizedlist>
    
    <sect2 id="sec.ha.lb.haproxy.conf">
      <title>Setting Up  &haproxy;</title>
      <para><remark>toms 2014-05-27: TODO Maybe add something like this</remark></para>
    </sect2>
  </sect1>

  <sect1 id="sec.ha.lb.more">
  <title>For More Information</title>
  <remark>toms 2014-05-27: TODO: Add some links for HAProxy as well. Using
    all the links as a list is enough IMHO</remark>
  <para>
   To learn more about &lvs;, refer to the project home page available at
   <ulink
  url="http://www.linuxvirtualserver.org/"/>.
  </para>

  <para>
   For more information about
   <systemitem class="daemon">ldirectord</systemitem>, refer to its
   comprehensive man page.
  </para>
 </sect1>
</chapter>
