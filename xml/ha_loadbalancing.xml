<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<!--taroth 2011-11-25: maintainer of ipvsadmin: marius tomaschewski-->
<!-- 
toms 2014-05-27:
* Restructured file to make it easier to describe both LVS and HAProxy
* Let Darix/Lars proofread that file.
-->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ha.lb">
<!--taroth 090519: see http://www.linuxvirtualserver.org/-->
 <title>Load Balancing</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer></dm:maintainer>
        <dm:status>editing</dm:status>
        <dm:deadline></dm:deadline>
        <dm:priority></dm:priority>
        <dm:translation></dm:translation>
        <dm:languages></dm:languages>
        <dm:release></dm:release>
        <dm:repository></dm:repository>
      </dm:docmanager>
    </info>
    <!--taroth 090519: see http://www.linuxvirtualserver.org/-->
 <para>
  <emphasis>Load Balancing</emphasis> makes a cluster of servers appear as
  one large, fast server to outside clients. This apparent single server is
  called a <emphasis>virtual server</emphasis>. It consists of one or more
  load balancers dispatching incoming requests and several real servers
  running the actual services. With a load balancing setup of &hasi;, you
  can build highly scalable and highly available network services, such as
  Web, cache, mail, FTP, media and VoIP services.
 </para>
 <para>
  &hasi; supports two technologies for load balancing: &lvs; (LVS) and
  &haproxy;. The key difference is &lvs; operates at OSI layer 4
  (Transport), configuring the network layer of kernel, while &haproxy;
  operates at layer 7 (Application), running in user space. Thus &lvs;
  needs fewer resources and can handle higher loads, while &haproxy; can
  inspect the traffic, do SSL termination and make dispatching decisions
  based on the content of the traffic.
 </para>
 <para>On the other hand, &lvs; includes two different software:
   IPVS (IP Virtual Server) and KTCPVS (Kernel TCP Virtual Server).
   IPVS provides layer 4 load balancing whereas KTCPVS provides layer 7
   load balancing.
 </para>

 <para>
  This section gives you a conceptual overview of load balancing in
  combination with high availability, then briefly introduces you to
  &lvs; and &haproxy;. At last, it points you to further reading.
 </para>
 <sect1 xml:id="sec.ha.lb.overview">
  <title>Conceptual Overview</title>

  <para>
   The real servers and the load balancers may be interconnected by either
   high-speed LAN or by geographically dispersed WAN. The load balancers
   dispatch requests to the different servers. They make parallel services
   of the cluster appear as one virtual service on a single IP address (the
   virtual IP address or VIP). Request dispatching can use IP load balancing
   technologies or application-level load balancing technologies.
   Scalability of the system is achieved by transparently adding or removing
   nodes in the cluster.
  </para>

  <para>
   High availability is provided by detecting node or service failures and
   reconfiguring the whole virtual server system appropriately, as usual.
  </para>

  <remark>Most of the following items are taken from FATE#316459. Not
  sure if this is really correct. Needs technical proofreading by an
  expert.</remark>

  <para>
   There are several load balancing strategies. Here are some Layer 4
   strategies, suitable for &lvs;:
  </para>

  <itemizedlist>
   <listitem>
    <formalpara>
     <title>Round Robin</title>
     <para>
      The simplest strategy is to direct each connection to a different
      address, taking turns. For example, a DNS server can have several
      entries for a given host name. With DNS roundrobin, the DNS server
      will return all of them in a rotating order. Thus different clients
      will see different addresses.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Selecting the <quote>best</quote> server</title>
     <para>
      Although this has several drawbacks, balancing could be implemented
      with an <quote>the first server who responds</quote> or <quote>the
      least loaded server</quote> approach.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Balance number of connections per server</title>
     <para>
      A load balancer between users and servers can divide the number of
      users across multiple servers.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Geolocation</title>
     <para>
      It is possible to direct clients to a server nearby.
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>

  <para>
   Here are some Layer 7 strategies, suitable for &haproxy;:
  </para>

  <itemizedlist>
   <listitem>
    <formalpara>
     <title>URI</title>
     <para>
      Inspect the HTTP content and dispatch to a server most suitable for
      this specific URI.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>URL parameter, RDP cookie</title>
     <para>
      Inspect the HTTP content for a session parameter, possibly in post
      parameters, or the RDP (remote desktop protocol) session cookie, and
      dispatch to the server serving this session.
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>

  <para>
   Although there is some overlap, &haproxy; can be used in scenarios
   where LVS/<command>ipvsadm</command> is not adequate and vice versa:
  </para>

  <itemizedlist>
   <listitem>
    <formalpara>
     <title>SSL termination</title>
     <para>
      The front-end load balancers can handle the SSL layer. Thus the cloud
      nodes do not need to have access to the SSL keys, or could take
      advantage of SSL accelerators in the load balancers.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Application level</title>
     <para>
      &haproxy; operates at the application level, allowing the load
      balancing decisions to be influenced by the content stream. This
      allows for persistence based on cookies and other such filters.
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>

  <para>
   On the other hand, LVS/<command>ipvsadm</command> cannot be fully
   replaced by &haproxy;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     LVS supports <quote>direct routing</quote>, where the load balancer is
     only in the inbound stream, whereas the outbound traffic is routed to
     the clients directly. This allows for potentially much higher
     throughput in asymmetric environments.
    </para>
   </listitem>
   <listitem>
    <para>
     LVS supports stateful connection table replication (via
     <systemitem class="daemon">conntrackd</systemitem>). This allows for
     load balancer failover that is transparent to the client and server.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.ha.lb.lvs">
  <title>Configuring Load Balancing with &lvs;</title>

  <para>
   The following sections give an overview of the main LVS components and
   concepts. Then we explain how to set up &lvs; on &hasi;.
  </para>

  <sect2 xml:id="sec.ha.lvs.overview.director">
   <title>Director</title>
   <para>
    The main component of LVS is the ip_vs (or IPVS) Kernel code. It
    implements transport-layer load balancing inside the Linux Kernel
    (layer-4 switching). The node that runs a Linux Kernel including the
    IPVS code is called <emphasis>director</emphasis>. The IPVS code running
    on the director is the essential feature of LVS.
   </para>
   <para>
    When clients connect to the director, the incoming requests are
    load-balanced across all cluster nodes: The director forwards packets to
    the real servers, using a modified set of routing rules that make the
    LVS work. For example, connections do not originate or terminate on the
    director, it does not send acknowledgments. The director acts as a
    specialized router that forwards packets from end users to real servers
    (the hosts that run the applications that process the requests).
   </para>
   <para>
    By default, the Kernel does not need the IPVS module installed. The IPVS
    Kernel module is included in the
    <systemitem class="resource">cluster-network-kmp-default</systemitem>
    package.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.lvs.overview.userspace">
   <title>User Space Controller and Daemons</title>
   <para>
    The <systemitem class="daemon">ldirectord</systemitem> daemon is a
    user-space daemon for managing &lvs; and monitoring the real servers
    in an LVS cluster of load balanced virtual servers. A configuration
    file, <filename>/etc/ha.d/ldirectord.cf</filename>, specifies the
    virtual services and their associated real servers and tells
    <systemitem class="daemon">ldirectord</systemitem> how to configure the
    server as an LVS redirector. When the daemon is initialized, it creates
    the virtual services for the cluster.
   </para>
   <para>
    By periodically requesting a known URL and checking the responses, the
    <systemitem class="daemon">ldirectord</systemitem> daemon monitors the
    health of the real servers. If a real server fails, it will be removed
    from the list of available servers at the load balancer. When the
    service monitor detects that the dead server has recovered and is
    working again, it will add the server back to the list of available
    servers. In case that all real servers should be down, a fall-back
    server can be specified to which to redirect a Web service. Typically
    the fall-back server is localhost, presenting an emergency page about
    the Web service being temporarily unavailable.
   </para>
   <para>
    The <systemitem class="daemon">ldirectord</systemitem> uses the
    <systemitem>ipvsadm</systemitem> tool (package
    <systemitem class="resource">ipvsadm</systemitem>) to manipulate the
    virtual server table in the Linux Kernel.
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.lvs.overview.forwarding">
   <title>Packet Forwarding</title>
   <para>
    There are three different methods of how the director can send packets
    from the client to the real servers:
   </para>
   <variablelist>
    <varlistentry>
     <term>Network Address Translation (NAT)</term>
     <listitem>
      <para>
       Incoming requests arrive at the virtual IP. They are forwarded to the
       real servers by changing the destination IP address and port to that
       of the chosen real server. The real server sends the response to the
       load balancer which in turn changes the destination IP address and
       forwards the response back to the client. Thus, the end user receives
       the replies from the expected source. As all traffic goes through the
       load balancer, it usually becomes a bottleneck for the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>IP Tunneling (IP-IP Encapsulation)</term>
     <listitem>
      <para>
       IP tunneling enables packets addressed to an IP address to be
       redirected to another address, possibly on a different network. The
       LVS sends requests to real servers through an IP tunnel (redirecting
       to a different IP address) and the real servers reply directly to the
       client using their own routing tables. Cluster members can be in
       different subnets.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Direct Routing</term>
     <listitem>
      <para>
       Packets from end users are forwarded directly to the real server. The
       IP packet is not modified, so the real servers must be configured to
       accept traffic for the virtual server's IP address. The response from
       the real server is sent directly to the client. The real servers and
       load balancers need to be in the same physical network segment.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.ha.lvs.overview.schedulers">
   <title>Scheduling Algorithms</title>
   <para>
    Deciding which real server to use for a new connection requested by a
    client is implemented using different algorithms. They are available as
    modules and can be adapted to specific needs. For an overview of
    available modules, refer to the <command>ipvsadm(8)</command> man page.
    Upon receiving a connect request from a client, the director assigns a
    real server to the client based on a <emphasis>schedule</emphasis>. The
    scheduler is the part of the IPVS Kernel code which decides which real
    server will get the next new connection.
   </para>
    <para>More detailed description about &lvs; scheduling algorithms can be
      found at <link xlink:href="http://kb.linuxvirtualserver.org/wiki/IPVS"/>.
      Furthermore, search for <option>--scheduler</option> in the
      <command>ipvsadm</command> manpage.
    </para>
    <para>Related load balancing strategies for &haproxy; can be found at
      <link
      xlink:href="http://www.haproxy.org/download/1.6/doc/configuration.txt"/>.
    </para>
  </sect2>

  <sect2 xml:id="sec.ha.lvs.ldirectord">
   <title>Setting Up IP Load Balancing with &yast;</title>
   <para>
    You can configure Kernel-based IP load balancing with the &yast; IP
    Load Balancing module. It is a front-end for
    <systemitem class="daemon">ldirectord</systemitem>.
   </para>
   <para>
    To access the IP Load Balancing dialog, start &yast; as &rootuser;
    and select <menuchoice> <guimenu>&ha;</guimenu> <guimenu>IP Load
    Balancing</guimenu> </menuchoice>. Alternatively, start the &yast;
    cluster module as &rootuser; on a command line with
    <command>yast2&nbsp;iplb</command>.
   </para>
   <para>
    The &yast; module writes its configuration to
    <filename>/etc/ha.d/ldirectord.cf</filename>. The tabs available in the
    &yast; module correspond to the structure of the
    <filename>/etc/ha.d/ldirectord.cf</filename> configuration file,
    defining global options and defining the options for the virtual
    services.
   </para>
   <para>
    For an example configuration and the resulting processes between load
    balancers and real servers, refer to
    <xref linkend="ex.ha.lvs.ldirectord"/>.
   </para>
   <note>
    <title>Global Parameters and Virtual Server Parameters</title>
    <para>
     If a certain parameter is specified in both the virtual server section
     and in the global section, the value defined in the virtual server
     section overrides the value defined in the global section.
    </para>
   </note>
   <procedure xml:id="sec.ha.lvs.ldirectord.global">
    <title>Configuring Global Parameters</title>
    <para>
     The following procedure describes how to configure the most important
     global parameters. For more details about the individual parameters
     (and the parameters not covered here), click <guimenu>Help</guimenu> or
     refer to the <systemitem class="daemon">ldirectord</systemitem> man
     page.
    </para>
    <step>
     <para>
      With <guimenu>Check Interval</guimenu>, define the interval in which
      <systemitem class="daemon">ldirectord</systemitem> will connect to
      each of the real servers to check if they are still online.
     </para>
    </step>
    <step>
     <para>
      With <guimenu>Check Timeout</guimenu>, set the time in which the real
      server should have responded after the last check.
     </para>
    </step>
    <step>
     <para>
      With <guimenu>Failure Count </guimenu> you can define how many times
      <systemitem class="daemon">ldirectord</systemitem> will attempt to
      request the real servers until the check is considered failed.
     </para>
    </step>
    <step>
     <para>
      With <guimenu>Negotiate Timeout</guimenu> define a timeout in seconds
      for negotiate checks.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Fallback</guimenu>, enter the host name or IP address of
      the Web server onto which to redirect a Web service in case all real
      servers are down.
     </para>
    </step>
    <step>
     <para>
      If you want the system to send alerts in case the connection status to
      any real server changes, enter a valid e-mail address in
      <guimenu>Email Alert</guimenu>.
     </para>
    </step>
    <step>
     <para>
      With <guimenu>Email Alert Frequency</guimenu>, define after how many
      seconds the e-mail alert should be repeated if any of the real servers
      remains inaccessible.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Email Alert Status</guimenu> specify the server states for
      which e-mail alerts should be sent. If you want to define more than
      one state, use a comma-separated list.
     </para>
    </step>
    <step>
     <para>
      With <guimenu>Auto Reload</guimenu> define, if
      <systemitem class="daemon">ldirectord</systemitem> should continuously
      monitor the configuration file for modification. If set to
      <literal>yes</literal>, the configuration is automatically reloaded
      upon changes.
     </para>
    </step>
    <step>
     <para>
      With the <guimenu>Quiescent</guimenu> switch, define if to remove
      failed real servers from the Kernel's LVS table or not. If set to
      <guimenu>Yes</guimenu>, failed servers are not removed. Instead their
      weight is set to <literal>0</literal> which means that no new
      connections will be accepted. Already established connections will
      persist until they time out.
     </para>
    </step>
    <step>
     <para>
      If you want to use an alternative path for logging, specify a path for
      the log files in <guimenu>Log File</guimenu>. By default,
      <systemitem class="daemon">ldirectord</systemitem> writes its log
      files to <filename>/var/log/ldirectord.log</filename>.
     </para>
    </step>
   </procedure>
   <figure xml:id="fig.ha.lvs.yast.global">
    <title>&yast; IP Load Balancing&mdash;Global Parameters</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_iplb_global.png" width="95%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_iplb_global.png" width="65%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <procedure xml:id="sec.ha.lvs.ldirectord.virtual">
    <title>Configuring Virtual Services</title>
    <para>
     You can configure one or more virtual services by defining a couple of
     parameters for each. The following procedure describes how to configure
     the most important parameters for a virtual service. For more details
     about the individual parameters (and the parameters not covered here),
     click <guimenu>Help</guimenu> or refer to the
     <systemitem class="daemon">ldirectord</systemitem> man page.
    </para>
    <step>
     <para>
      In the &yast; IP Load Balancing module, switch to the
      <guimenu>Virtual Server Configuration</guimenu> tab.
     </para>
    </step>
    <step>
     <para>
      <guimenu>Add</guimenu> a new virtual server or <guimenu>Edit</guimenu>
      an existing virtual server. A new dialog shows the available options.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Virtual Server</guimenu> enter the shared virtual IP
      address (IPv4 or IPv6) and port under which the load balancers and the
      real servers are accessible as LVS. Instead of IP address and port
      number you can also specify a host name and a service. Alternatively,
      you can also use a firewall mark. A firewall mark is a way of
      aggregating an arbitrary collection of <literal>VIP:port</literal>
      services into one virtual service.
     </para>
    </step>
    <step>
     <para>
      To specify the <guimenu>Real Servers</guimenu>, you need to enter the
      IP addresses (IPv4, IPv6, or host names) of the servers, the ports (or
      service names) and the forwarding method. The forwarding method must
      either be <literal>gate</literal>, <literal>ipip</literal> or
      <literal>masq</literal>, see
      <xref linkend="sec.ha.lvs.overview.forwarding"/>.
     </para>
     <para>
      Click the <guimenu>Add</guimenu> button and enter the required
      arguments for each real server.
     </para>
    </step>
    <step>
     <para>
      As <guimenu>Check Type</guimenu>, select the type of check that should
      be performed to test if the real servers are still alive. For example,
      to send a request and check if the response contains an expected
      string, select <literal>Negotiate</literal>.
     </para>
    </step>
    <step xml:id="step.ha.lvs.ldirectord.service">
     <para>
      If you have set the <guimenu>Check Type</guimenu> to
      <literal>Negotiate</literal>, you also need to define the type of
      service to monitor. Select it from the <guimenu>Service</guimenu>
      drop-down box.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Request</guimenu>, enter the URI to the object that is
      requested on each real server during the check intervals.
     </para>
    </step>
    <step>
     <para>
      If you want to check if the response from the real servers contains a
      certain string (<quote>I'm alive</quote> message), define a regular
      expression that needs to be matched. Enter the regular expression into
      <guimenu>Receive</guimenu>. If the response from a real server
      contains this expression, the real server is considered to be alive.
     </para>
    </step>
    <step>
     <para>
      Depending on the type of <guimenu>Service</guimenu> you have selected
      in <xref linkend="step.ha.lvs.ldirectord.service"/>, you also need to
      specify further parameters for authentication. Switch to the
      <guimenu>Auth type</guimenu> tab and enter the details like
      <guimenu>Login</guimenu>, <guimenu>Password</guimenu>,
      <guimenu>Database</guimenu>, or <guimenu>Secret</guimenu>. For more
      information, refer to the &yast; help text or to the
      <systemitem class="daemon">ldirectord</systemitem> man page.
     </para>
    </step>
    <step>
     <para>
      Switch to the <guimenu>Others</guimenu> tab.
     </para>
    </step>
    <step>
     <para>
      Select the <guimenu>Scheduler</guimenu> to be used for load balancing.
      For information on the available schedulers, refer to the
      <command>ipvsadm(8)</command> man page.
     </para>
    </step>
    <step>
     <para>
      Select the <guimenu>Protocol</guimenu> to be used.
<!--<remark>taroth 2010-03-16: DEVs, to be
	  used for what? for forwarding the requests arriving at the VIP to the real servers? or for
	  which purpose?</remark> taroth 2013-04-08: no answer to this question yet-->
      If the virtual service is specified as an IP address and port, it must
      be either <literal>tcp</literal> or <literal>udp</literal>. If the
      virtual service is specified as a firewall mark, the protocol must be
      <literal>fwm</literal>.
     </para>
    </step>
    <step>
     <para>
      Define further parameters, if needed. Confirm your configuration with
      <guimenu>OK</guimenu>. &yast; writes the configuration to
      <filename>/etc/ha.d/ldirectord.cf</filename>.
     </para>
    </step>
   </procedure>
<!--taroth 2011-05-04: filed https://bugzilla.novell.com/show_bug.cgi?id=691671, based on 
       http://doccomments.provo.novell.com/admin/viewcomment/16314 - FIXED-->
   <figure xml:id="fig.ha.lvs.yast.virtual">
    <title>&yast; IP Load Balancing&mdash;Virtual Services</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_iplb_virtual.png" width="95%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_iplb_virtual.png" width="65%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <example xml:id="ex.ha.lvs.ldirectord">
    <title>Simple ldirectord Configuration</title>
    <para>
     The values shown in <xref linkend="fig.ha.lvs.yast.global"/> and
     <xref linkend="fig.ha.lvs.yast.virtual"/>, would lead to the following
     configuration, defined in <filename>/etc/ha.d/ldirectord.cf</filename>:
    </para>
<screen>autoreload = yes <co xml:id="co.ha.ldirectord.autoreload"/>
    checkinterval = 5 <co xml:id="co.ha.ldirectord.checkintervall"/>
    checktimeout = 3 <co xml:id="co.ha.ldirectord.checktimeout"/>
    quiescent = yes <co xml:id="co.ha.ldirectord.quiescent"/>
    virtual = 192.168.0.200:80 <co xml:id="co.ha.ldirectord.virtual"/>
    checktype = negotiate <co xml:id="co.ha.ldirectord.checktype"/>
    fallback = 127.0.0.1:80 <co xml:id="co.ha.ldirectord.fallback"/>
    protocol = tcp <co xml:id="co.ha.ldirectord.protocol"/>
    real = 192.168.0.110:80 gate <co xml:id="co.ha.ldirectord.real"/>
    real = 192.168.0.120:80 gate <xref linkend="co.ha.ldirectord.real" xrefstyle="select:label nopage"/>
    receive = "still alive" <co xml:id="co.ha.ldirectord.receive"/>
    request = "test.html" <co xml:id="co.ha.ldirectord.request"/>
    scheduler = wlc <co xml:id="co.ha.ldirectord.scheduler"/>
    service = http <co xml:id="co.ha.ldirectord.service"/></screen>
    <calloutlist>
     <callout arearefs="co.ha.ldirectord.autoreload">
      <para>
       Defines that <systemitem class="daemon">ldirectord</systemitem>
       should continuously check the configuration file for modification.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.checkintervall">
      <para>
       Interval in which <systemitem class="daemon">ldirectord</systemitem>
       will connect to each of the real servers to check if they are still
       online.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.checktimeout">
      <para>
       Time in which the real server should have responded after the last
       check.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.quiescent">
      <para>
       Defines not to remove failed real servers from the Kernel's LVS
       table, but to set their weight to <literal>0</literal> instead.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.virtual">
      <para>
       Virtual IP address (VIP) of the LVS. The LVS is available at port
       <literal>80</literal>.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.checktype">
      <para>
       Type of check that should be performed to test if the real servers
       are still alive.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.fallback">
      <para>
       Server onto which to redirect a Web service all real servers for this
       service are down.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.protocol">
      <para>
       Protocol to be used.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.real">
      <para>
       Two real servers defined, both available at port
       <literal>80</literal>. The packet forwarding method is
       <literal>gate</literal>, meaning that direct routing is used.
      </para>
     </callout>
<!-- <callout arearefs="co.ha.ldirectord.10">
	  <para>
	  One of two real servers defined, available at port
	  <literal>80</literal>. The packet forwarding method is
	  <literal>gate</literal>, meaning that direct routing is used.
	  </para>
	  </callout>-->
     <callout arearefs="co.ha.ldirectord.receive">
      <para>
       Regular expression that needs to be matched in the response string
       from the real server.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.request">
      <para>
       URI to the object that is requested on each real server during the
       check intervals.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.scheduler">
      <para>
       Selected scheduler to be used for load balancing.
      </para>
     </callout>
     <callout arearefs="co.ha.ldirectord.service">
      <para>
       Type of service to monitor.
      </para>
     </callout>
    </calloutlist>
    <para>
     This configuration would lead to the following process flow: The
     <systemitem class="daemon">ldirectord</systemitem> will connect to each
     real server once every 5 seconds
     (<xref linkend="co.ha.ldirectord.checkintervall" xrefstyle="select:label nopage"/>)
     and request <literal>192.168.0.110:80/test.html</literal> or
     <literal>192.168.0.120:80/test.html</literal> as specified in
     <xref linkend="co.ha.ldirectord.real" xrefstyle="select:label nopage"/>
     and
     <xref linkend="co.ha.ldirectord.request" xrefstyle="select:label nopage"/>.
     If it does not receive the expected <literal>still alive</literal>
     string
     (<xref linkend="co.ha.ldirectord.receive" xrefstyle="select:label nopage"/>)
     from a real server within 3 seconds
     (<xref linkend="co.ha.ldirectord.checktimeout" xrefstyle="select:label nopage"/>)
     of the last check, it will remove the real server from the available
     pool. However, because of the <literal>quiescent=yes</literal> setting
     (<xref linkend="co.ha.ldirectord.quiescent" xrefstyle="select:label nopage"/>),
     the real server will not be removed from the LVS table. Instead its
     weight will be set to <literal>0</literal> so that no new connections
     to this real server will be accepted. Already established connections
     will be persistent until they time out.
    </para>
   </example>
  </sect2>

  <sect2 xml:id="sec.ha.lvs.further">
   <title>Further Setup</title>
   <para>
    Apart from the configuration of
    <systemitem class="daemon">ldirectord</systemitem> with &yast;, you
    need to make sure the following conditions are fulfilled to complete the
    LVS setup:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The real servers are set up correctly to provide the needed services.
     </para>
    </listitem>
    <listitem>
     <para>
      The load balancing server (or servers) must be able to route traffic
      to the real servers using IP forwarding. The network configuration of
      the real servers depends on which packet forwarding method you have
      chosen.
     </para>
    </listitem>
    <listitem>
     <para>
      To prevent the load balancing server (or servers) from becoming a
      single point of failure for the whole system, you need to set up one
      or several backups of the load balancer. In the cluster configuration,
      configure a primitive resource for
      <systemitem class="daemon">ldirectord</systemitem>, so that
      <systemitem class="daemon">ldirectord</systemitem> can fail over to
      other servers in case of hardware failure.
     </para>
    </listitem>
    <listitem>
     <para>
      As the backup of the load balancer also needs the
      <systemitem class="daemon">ldirectord</systemitem> configuration file
      to fulfill its task, make sure the
      <filename>/etc/ha.d/ldirectord.cf</filename> is available on all
      servers that you want to use as backup for the load balancer. You can
      synchronize the configuration file with &csync; as described in
      <xref linkend="sec.ha.installation.setup.csync2"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ha.lb.haproxy">
  <title>Configuring Load Balancing with &haproxy;</title>

  <remark>
   This is a straightforward explanation how to set up a HA openSUSE
   13.1 http load balancer, but not using SLE HA tools, but plain
   vanilla openSUSE with vi.
  
  </remark>

  <para>
   The following section gives an overview of the &haproxy; and how to
   set up on &ha;. The load balancer distributes all requests to its
   back-end servers. It is configured as active/passive, meaning if one
   master fails, the slave becomes the master. In such a scenario, the user
   will not notice any interruption.
  </para>

  <para>
   In this section, we will use the following setup:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Two load balancers, with the IP addresses &node1; (IP:
     <systemitem class="ipaddress">&subnetI;.100</systemitem>) and
     &node2; (IP:
     <systemitem class="ipaddress">&subnetI;.101</systemitem>)
    </para>
   </listitem>
   <listitem>
    <para>
     A virtual, floating IP address
     <systemitem class="ipaddress">&subnetI;.99</systemitem>
    </para>
   </listitem>
   <listitem>
    <para>
     Our servers (usually for Web content)
     <systemitem class="server">www1.&exampledomain;</systemitem> (IP:
     <systemitem class="ipaddress">&subnetI;.200</systemitem>) and
     <systemitem class="server">www2.&exampledomain;</systemitem> (IP:
     <systemitem class="ipaddress">&subnetI;.201</systemitem>)
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To configure &haproxy;, use the following procedure:
  </para>

  <procedure>
   <step>
    <para>
     Install the <systemitem class="resource">haproxy</systemitem> package.
    </para>
   </step>
   <step>
    <para>
     Create the file <filename>/etc/haproxy/haproxy.cfg</filename> with the
     following contents:
    </para>
<!--<screen>global
  maxconn 256
  daemon
defaults
  mode    http
  options redispatch
  retries 3
  maxconn 10000
  timeout connect     5000
  timeout client     50000
  timeout server    450000
listen stats
  bind :80
  server www01 www1.&exampledomain;
  server www02 www2.&exampledomain;
  server www03 www3.&exampledomain;</screen>-->
    <remark>toms 2014-09-17: Not sure about all the options.</remark>
<screen>global <co xml:id="co.ha.lb.global"/>
  maxconn 256
  daemon

defaults <co xml:id="co.ha.lb.defaults"/>
  log     global
  mode    http
  option  httplog
  option  dontlognull
  retries 3
  option redispatch
  maxconn 2000
  timeout connect   5000  <co xml:id="co.ha.lb.timeout.connect"/>
  timeout client    50s   <co xml:id="co.ha.lb.timeout.client"/>
  timeout server    50000 <co xml:id="co.ha.lb.timeout.server"/>

  bind &subnetI;.99:80 <co xml:id="co.ha.lb.listen"/>
  mode http
  stats enable
  stats uri /haproxy?stats
  stats auth someuser:somepassword
  balance leastconn
  cookie JSESSIONID prefix
  option httpclose
  option forwardfor
  option httpchk GET /robots.txt HTTP/1.0
  server webA &subnetI;.200:80 cookie A check
  server webB &subnetI;.201:80 cookie B check</screen>
    <calloutlist>
     <callout arearefs="co.ha.lb.global">
      <para>
       Section which contains process-wide and OS-specific options.
      </para>
      <variablelist>
       <varlistentry>
        <term><option>maxconn</option>
        </term>
        <listitem>
         <para>
          Maximum per-process number of concurrent connections.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><option>daemon</option>
        </term>
        <listitem>
         <para>
          Recommended mode, &haproxy; runs in the background.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
     </callout>
     <callout arearefs="co.ha.lb.defaults">
      <para>
       Section which sets default parameters for all other sections
       following its declaration. Some important lines:
      </para>
      <variablelist>
       <varlistentry>
        <term><option>redispatch</option>
        </term>
        <listitem>
         <para>
          Enables or disables session redistribution in case of connection
          failure.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><option>log</option>
        </term>
        <listitem>
         <para>
          Enables logging of events and traffic.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>mode http</literal>
        </term>
        <listitem>
         <para>
          Operates in HTTP mode (recommended mode for &haproxy;). In this
          mode, a request will be analyzed before a connection to any server
          is performed. Request that are not RFC-compliant will be rejected.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>option forwardfor</literal>
        </term>
        <listitem>
         <para>
          Adds the HTTP <option>X-Forwarded-For</option> header into the
          request. You need this option if you want to preserve the client's
          IP address.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
     </callout>
     <callout arearefs="co.ha.lb.timeout.connect">
       <para>The maximum time to wait for a connection attempt to a server
         to succeed.
       </para>
     </callout>
     <callout arearefs="co.ha.lb.timeout.client">
       <para>The maximum time of inactivity on the client side.</para>
     </callout>
     <callout arearefs="co.ha.lb.timeout.server">
       <para>The maximum time of inactivity on the server side.</para>
     </callout>
     <callout arearefs="co.ha.lb.listen">
      <para>
       Section which combines front-end and back-end sections in one.
      </para>
      <variablelist>
       <varlistentry>
        <term><literal>balance leastconn</literal>
        </term>
        <listitem>
         <para>
          Defines the load balancing algorithm, see
          <link xlink:href="http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#4-balance"/>.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>stats enable</literal>
        </term>
        <term><literal>stats auth</literal>
        </term>
        <listitem>
         <para>
          Enables statistics reporting (by <literal>stats enable</literal>).
          The <option>auth</option> option logs statistics with
          authentication to a specific account.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
     </callout>
    </calloutlist>
   </step>
   <step>
    <para>
     Test your configuration file:
    </para>
<screen>&prompt.root;<command>haproxy</command> -f /etc/haproxy/haproxy.cfg -c</screen>
   </step>
   <step>
    <para>
     Add the following line to &csync;'s configuration file
     <filename>/etc/csync2/csync2.cfg</filename> to make sure the
     &haproxy; configuration file is included:
    </para>
<screen>include /etc/haproxy/haproxy.cfg</screen>
   </step>
   <step>
    <para>
     Synchronize it:
    </para>
<screen>&prompt.root;<command>csync2</command> -f /etc/haproxy/haproxy.cfg
&prompt.root;<command>csync2</command> -xv</screen>
    <note>
     <para>
      The &csync; configuration part assumes that the HA nodes were
      configured using <command>ha-cluster-bootstrap</command>. For details,
      see <xref linkend="sec.ha.installation.setup.auto"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Make sure &haproxy; is disabled on both load balancers
     (<systemitem class="server">&node1;</systemitem> and
     <systemitem class="server">&node2;</systemitem>) as it is started by
     &pace;:
    </para>
<screen>&prompt.root;<command>systemctl</command> disable haproxy</screen>
   </step>
   <step>
    <para>
     Configure a new CIB:
    </para>
    <remark>toms 2014-09-16: According to Kristoffer: "This is the crmsh 
       configuration (edited using crm configure). It is possible to do a 
       similar configuration in hawk, but with the graphical interface.
       I think it can be improved, I will get back to you with more details 
       :)"</remark>
<screen>&prompt.root;<command>crm</command> configure
&prompt.crm;<command>cib</command> new haproxy-config
&prompt.crmhp;<command>primitive</command> haproxy systemd:haproxy op monitor interval=10s
&prompt.crmhp;<command>primitive</command> vip-www1 IPaddr2 params ip=&subnetI;.100
&prompt.crmhp;<command>primitive</command> vip-www2 IPaddr2 params ip=&subnetI;.101
&prompt.crmhp;<command>group</command> g-haproxy vip-www1 vip-www2 haproxy</screen>
   </step>
   <step>
    <para>
     Verify the new CIB and correct any errors:
    </para>
<screen>&prompt.crmhp;<command>verify</command></screen>
   </step>
   <step>
    <para>
     Commit the new CIB:
    </para>
<screen>&prompt.crmhp;<command>cib</command> use live
&prompt.crm;<command>cib</command> commit haproxy-config</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ha.lb.more">
  <title>For More Information</title>

  <itemizedlist>
    <listitem>
      <para><link xlink:href="http://www.haproxy.org"/></para>
    </listitem>
    <listitem>
      <para>Project home page at <link xlink:href="http://www.linuxvirtualserver.org/"/>.
  </para>
    </listitem>
    <listitem>
      <para>For more information about <systemitem
        class="daemon">ldirectord</systemitem>, refer to its
        comprehensive man page.</para>
    </listitem>
    <listitem>
      <para>LVS Knowledge Base: <link xlink:href="http://kb.linuxvirtualserver.org/wiki/Main_Page"/></para>
    </listitem>
  </itemizedlist>
 </sect1>
</chapter><!--taroth 2014-08-13: from fate#316459: Provide haproxy package:
  
 This is basically a revival of the following feature request: #310633: haproxy on SLES
 
 HA-Proxy is a fast and reliable proxy application for TCP and HTTP-based 
 connections, with a focus on high availability and load balancing. This proxy 
 software shares a lot of common features with LVS/ipvsadm, but has significant 
 differences in the way it operates and what's possible to do with it. 
 Please see the original request for a lot more details.
 
 Usecase 
 While there is significant overlap, haproxy can be used in scenarios where LVS/ipvsadm
 are not adequate and vice versa. 
 * SSL termination: the front-end load-balancers can handle the SSL layer and thus the cloud 
 nodes do not need to have access to the SSL keys, or could take advantageof SSL accelerators 
 in the load balancers. 
 * haproxy operates at the application level, allowing the load balancing decisions to be influenced 
 by the content stream. This allows for persistence based on cookies and other such filters. 
 
 On the other hand, LVS/ipvsadm cannot be fully replaced: 
 * LVS supports "Direct Routing", where the load balancer is only in the inbound stream, whereas the
 outbound traffic is routed to the clients directly. This allows for potentially much higher
 throughput in asymmetric environments. 
 * LVS supports stateful connection table replication (via conntrackd). This allows for load-balancer 
 failover that is transparent to the client and server.
 
 Because of their different architectures, it is not feasible to incorporate the functionality of one in
 the other easily.

 HAProxy documentation: http://cbonte.github.io/haproxy-dconv/configuration-1.4.html
 
 HAProxy is managed with systemd scripts
 
 ********************************
 other docs that migth be helpful (?): 
 https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/index.html
-->
