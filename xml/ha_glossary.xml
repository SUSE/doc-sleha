<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE glossary
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<!--taroth 2012-01-13: more terms to add for the future:
 * constraint (+ types of constraints)
 * types of resources: primitives, groups, etc.
 * meta attributes, instance attributes, operations, utilization attributes
 * transition graph
 * list all pacemaker daemons
 * tools: hawk, crmsh, yast
 * resource templates, configuration templates
 * resource set
 * pacemaker
 *corosync
 * OCFS2
 * bonding
 * LVS
 * Cluster LVM2
 * SAN
 * UDP-->
<!--taroth 2014-07-04: found this somewhere in my notes, check against current
 definition for quorum:
 Quorum is a mechanism that is used to avoid split-brain situations by selecting a
 subset of the cluster to represent the whole cluster when is forced to split into
 multiple sub-clusters because of communication issues. The selected cluster subset
 can run services that make the cluster available.
  (from IBM Redbooks)
-->
<!--taroth 2019-04-15: move some content to phrases-decl.ent (e.g. the definitions
   that are used in ha_concepts.xml, too)-->
<glossary xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="gl-heartb">

 <title>Glossary</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <glossentry><glossterm>active/active, active/passive</glossterm>
  <glossdef>
   <para>
    A concept of how services are running on nodes. An active-passive
    scenario means that one or more services are running on the active node
    and the passive node waits for the active node to fail. Active-active
    means that each node is active and passive at the same time. For
    example, it has <emphasis>some</emphasis> services running, but can take
    over other services from the other node. Compare with primary/secondary
    and dual-primary in DRBD speak.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>arbitrator</glossterm>
  <glossdef>
   <para>
    Additional instance in a &geo; cluster that helps to reach consensus
    about decisions such as failover of resources across sites. Arbitrators
    are single machines that run one or more booth instances in a special
    mode.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&ay;</glossterm>
  <glossdef>
   <para>
    &def-ay;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>bindnetaddr (bind network address)</glossterm>
  <glossdef>
   <para>
    &def-bindnetaddr;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>booth</glossterm>
  <glossdef>
   <para>
    The instance that manages the failover process between the sites of a
    &geo; cluster. It aims to get multi-site resources active on one and
    only one site. This is achieved by using so-called tickets that are
    treated as failover domain between cluster sites, in case a site should
    be down.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>boothd (booth
  daemon)</glossterm>
  <glossdef>
   <para>
    Each of the participating clusters and arbitrators in a &geo; cluster
    runs a service, the <systemitem class="daemon">boothd</systemitem>. It
    connects to the booth daemons running at the other sites and exchanges
    connectivity details.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>CCM (consensus cluster membership)</glossterm>
  <glossdef>
   <para>
    The CCM determines which nodes make up the cluster and shares this
    information across the cluster. Any new addition and any loss of nodes
    or quorum is delivered by the CCM. A CCM module runs on each node of the
    cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>CIB (cluster information base)</glossterm>
  <glossdef>
   <para>
    A representation of the whole cluster configuration and status (cluster
    options, nodes, resources, constraints and the relationship to each
    other). It is written in XML and resides in memory. A master CIB is kept
    and maintained on the
    <xref linkend="glos-dc" xrefstyle="select:nopage"/> and replicated to
    the other nodes. Normal read and write operations on the CIB are
    serialized through the master CIB.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cluster</glossterm>
  <glossdef>
   <para>
    A <emphasis>high-performance</emphasis> cluster is a group of computers
    (real or virtual) sharing the application load to achieve faster
    results. A <emphasis>high-availability</emphasis> cluster is designed
    primarily to secure the highest possible availability of services.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cluster partition</glossterm>
  <glossdef>
   <para>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs. The nodes of a cluster are
    split into partitions but still active. They can only communicate with
    nodes in the same partition and are unaware of the separated nodes. As
    the loss of the nodes on the other partition cannot be confirmed, a
    split brain scenario develops (see also
    <xref linkend="glos-splitbrain"/>).
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>cluster stack</glossterm>
  <glossdef>
   <para>
    The ensemble of software technologies and components that compose a cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>concurrency violation</glossterm>
  <glossdef>
   <para>
    A resource that should be running on only one node in the cluster is
    running on several nodes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>conntrack tools</glossterm>
  <glossdef>
   <para>
    &def-conntrack;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>CRM (cluster resource manager)</glossterm>
  <glossdef>
   <para>
    The management entity responsible for coordinating all non-local
    interactions in a &ha; cluster. The &hasi; uses Pacemaker as CRM.
    The CRM is implemented as <systemitem
    class="daemon">pacemaker-controld</systemitem>. It interacts with several
    components: local resource managers, both on its own node and on the other nodes,
    non-local CRMs, administrative commands, the fencing functionality, and the membership
    layer.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>crmsh</glossterm>
  <glossdef>
   <para>
    The command line utility &crmsh; manages your cluster, nodes, and
    resources.
   </para>
   <para>
    See <xref linkend="cha-ha-manual-config"/> for more information.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&csync;</glossterm>
  <glossdef>
   <para>
    &def-csync2;
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glos-dc"><glossterm>DC (designated coordinator)</glossterm>
  <glossdef>
   <para>
    The DC is elected from all nodes in the cluster. This happens if there
    is no DC yet or if the current DC leaves the cluster for any reason.
    The DC is the only entity
    in the cluster that can decide that a cluster-wide change needs to
    be performed, such as fencing a node or moving resources around. All
    other nodes get their configuration and resource allocation
    information from the current DC.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Disaster</glossterm>
  <glossdef>
   <para>
    Unexpected interruption of critical infrastructure induced by nature,
    humans, hardware failure, or software bugs.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Disaster Recovery</glossterm>
  <glossdef>
   <para>
    Disaster recovery is the process by which a business function is
    restored to the normal, steady state after a disaster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Disaster Recover Plan</glossterm>
  <glossdef>
   <para>
    A strategy to recover from a disaster with minimum impact on IT
    infrastructure.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>DLM (distributed lock manager)</glossterm>
  <glossdef>
   <para>
    DLM coordinates disk access for clustered file systems and administers
    file locking to increase performance and availability.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>DRBD</glossterm>
  <glossdef>
   <para>
    <trademark class="registered">DRBD</trademark> is a block device
    designed for building high availability clusters. The whole block device
    is mirrored via a dedicated network and is seen as a network RAID-1.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>existing cluster</glossterm>
  <glossdef>
   <para>
    &def-existing-cluster;
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glo-failover"><glossterm>failover</glossterm>
  <glossdef>
   <para>
    Occurs when a resource or node fails on one machine and the affected
    resources are started on another node.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>failover domain</glossterm>
  <glossdef>
   <para>
    A named subset of cluster nodes that are eligible to run a cluster
    service if a node fails.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>fencing</glossterm>
  <glossdef>
   <para>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. There are two classes of fencing:
    resource level fencing and node level fencing. Resource level fencing ensures
    exclusive access to a given resource. Node level fencing prevents a failed
    node from accessing shared resources entirely and prevents resources from running
    on a node whose status is uncertain. This is usually done in a simple and
    abrupt way: reset or power off the node.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Geo cluster (geographically dispersed cluster)</glossterm>
  <glossdef>
   <para>
    See <xref linkend="glos-geo"/>.
    <remark>toms 2014-02-12: (from Phil): refer to new, common chapter</remark>
   </para>
  </glossdef>
 </glossentry>
<!--taroth 2012-01-09: new term (merged from Fabian's version of ha_glossary.xml)-->
<!--<glossentry><glossterm>high availability</glossterm>
<glossdef>
<para>High availability is a system design approach and associated service
implementation that ensures a prearranged level of operational
performance will be met during a contractual measurement period.</para>
<para>Availability is a key aspect of service quality. Availability is usually calculated based on a model involving the Availability Ratio and techniques such as Fault Tree Analysis.</para>
<para>See also: <ulink url="http://en.wikipedia.org/wiki/High_availability/"/> <ulink url="http://www.itlibrary.org/index.php?page=Availability_Management"/></para>
</glossdef>
</glossentry>-->
 <glossentry xml:id="glos-lb"><glossterm>load balancing</glossterm>
  <glossdef>
   <para>
    The ability to make several servers participate in the same service and
    do the same work.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>local cluster</glossterm>
  <glossdef>
   <para>
    A single cluster in one location (for example, all nodes are located in
    one data center). Network latency can be neglected. Storage is typically
    accessed synchronously by all nodes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>LRM (local resource manager)</glossterm>
  <glossdef>
   <para>
    The local resource manager is located between the Pacemaker layer and the
    resources layer on each node. It is implemented as <systemitem
    class="daemon">pacemaker-execd</systemitem> daemon. Through this daemon,
    Pacemaker can start, stop, and monitor resources.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>mcastaddr (multicast address)</glossterm>
  <glossdef>
   <para>
    &def-mcastaddr;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>mcastport (multicast port)</glossterm>
  <glossdef>
   <para>
     &def-mcastport;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>metro cluster</glossterm>
  <glossdef>
   <para>
    A single cluster that can stretch over multiple buildings or data
    centers, with all sites connected by fibre channel. Network latency is
    usually low (&lt;5 ms for distances of approximately
    20&nbsp;miles). Storage is frequently replicated (mirroring or
    synchronous replication).
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>multicast</glossterm>
  <glossdef>
   <para>
    &def-multicast;
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glos-geo"><glossterm>&geo; cluster</glossterm>
  <glossdef>
   <para>
    Consists of multiple, &geo.dispersed; sites with a local cluster
    each. The sites communicate via IP. Failover across the sites is
    coordinated by a higher-level entity, the booth. &geo; clusters need
    to cope with limited network bandwidth and high latency. Storage is
    replicated asynchronously.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>node</glossterm>
  <glossdef>
   <para>
    Any computer (real or virtual) that is a member of a cluster and
    invisible to the user.
   </para>
  </glossdef>
 </glossentry>
<!-- pingd is deprecated!! the resource is named ping now
 <glossentry><glossterm>pingd</glossterm>
  <glossdef>
   <para>
    The ping daemon. It continuously contacts one or more servers outside
    the cluster with ICMP pings.
   </para>
  </glossdef>
 </glossentry>
-->
 <glossentry><glossterm>pacemaker-controld (cluster controller daemon)</glossterm>
  <glossdef>
   <para>
    The CRM is implemented as daemon, pacemaker-controld. It has an instance on each
    cluster node. All cluster decision-making is centralized by electing one
    of the pacemaker-controld instances to act as a master. If the elected pacemaker-controld process
    fails (or the node it ran on), a new one is established.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>PE (policy engine)</glossterm>
  <glossdef>
   <para>
    The policy engine is implemented as
    <systemitem class="daemon">pacemaker-schedulerd</systemitem> daemon.
    When a cluster transition is needed, based on the current state and
    configuration, <systemitem class="daemon">pacemaker-schedulerd</systemitem>
    calculates the expected next state of the cluster. It determines what
    actions need to be scheduled to achieve the next state.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-quorum"><glossterm>quorum</glossterm>
  <glossdef>
   <para>
    In a cluster, a cluster partition is defined to have quorum (be
    <quote>quorate</quote>) if it has the majority of nodes (or votes).
    Quorum distinguishes exactly one partition. It is part of the algorithm
    to prevent several disconnected partitions or nodes from proceeding and
    causing data and service corruption (split brain). Quorum is a
    prerequisite for fencing, which then ensures that quorum is indeed
    unique.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>RA (resource agent)</glossterm>
  <glossdef>
   <para>
    A script acting as a proxy to manage a resource (for example, to start,
    stop or monitor a resource). The &hasi; supports different
    kinds of resource agents: For details, see
    <xref linkend="sec-ha-config-basics-raclasses"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&rear; (Relax and Recover)</glossterm>
  <glossdef>
   <para>
    An administrator tool set for creating disaster recovery images.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>resource</glossterm>
  <glossdef>
   <para>
    Any type of service or application that is known to Pacemaker. Examples
    include an IP address, a file system, or a database.
   </para>
   <para>
    The term <quote>resource</quote> is also used for DRBD, where it names a
    set of block devices that are using a common connection for replication.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>RRP (redundant ring protocol)</glossterm>
  <glossdef>
   <para>
    &def-rrp;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>SBD (&stonith; Block Device)</glossterm>
  <glossdef>
   <para>
    Provides a node fencing mechanism through the exchange of messages via shared
    block storage (SAN, iSCSI, FCoE, etc.). Can also be used in diskless mode.
    Needs a hardware or software watchdog on each node to ensure that misbehaving
    nodes are really stopped.
   </para>
  </glossdef>
 </glossentry>
<!--taroth 2012-01-09: new term (merged from Fabian's version of ha_glossary.xml)-->
 <glossentry><glossterm>SFEX (shared disk file exclusiveness)</glossterm>
  <glossdef>
   <para>
    SFEX provides storage protection over SAN.
   </para>
  </glossdef>
 </glossentry>
<!--taroth 2012-01-09: new term (merged from Fabian's version of ha_glossary.xml)-->
 <glossentry xml:id="glos-splitbrain"><glossterm>split brain</glossterm>
  <glossdef>
   <para>
    A scenario in which the cluster nodes are divided into two or more
    groups that do not know of each other (either through a software or
    hardware failure). &stonith; prevents a split brain situation from badly
    affecting the entire cluster. Also known as a <quote>partitioned
    cluster</quote> scenario.
   </para>
   <para>
    The term split brain is also used in DRBD but means that the two nodes
    contain different data.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>SPOF (single point of failure)</glossterm>
  <glossdef>
   <para>
    Any component of a cluster that, should it fail, triggers the failure of
    the entire cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glo-stonith"><glossterm>&stonith;</glossterm>
  <glossdef>
   <para>
    The acronym for <quote>Shoot the other node in the head</quote>. It refers
    to the fencing mechanism that shuts down a misbehaving node to prevent it
    from causing trouble in a cluster. In a Pacemaker cluster, the implementation
    of node level fencing is &stonith;. For this, Pacemaker comes with a fencing
    subsystem, <systemitem class="daemon">pacemaker-fenced</systemitem>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>switchover</glossterm>
  <glossdef>
   <para>
    Planned, on-demand moving of services to other nodes in a cluster. See
    <xref linkend="glo-failover"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>ticket</glossterm>
  <glossdef>
   <para>
    A component used in &geo; clusters. A ticket grants the right to run
    certain resources on a specific cluster site. A ticket can only be owned
    by one site at a time. Resources can be bound to a certain ticket by
    dependencies. Only if the defined ticket is available at a site, the
    respective resources are started. Vice versa, if the ticket is removed,
    the resources depending on that ticket are automatically stopped.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>unicast</glossterm>
  <glossdef>
   <para>
    A technology for sending messages to a single network destination.
    &corosync; supports both multicast and unicast. In &corosync;,
    unicast is implemented as UDP-unicast (UDPU).
   </para>
  </glossdef>
 </glossentry>
</glossary>
