<?xml version="1.0"?>
<!DOCTYPE glossary [
  <!ENTITY % entities SYSTEM "generic-entities.ent">
  %entities;
]>

<glossary xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="gl-heartb">

 <title>Glossary</title>
 <info xmlns:d="http://docbook.org/ns/docbook">
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    <revhistory xml:id="rh-gl-heartb">
   <revision>
     <date>2025-04-14</date>
     <revdescription>
       <para/>
     </revdescription>
   </revision>
 </revhistory>
</info>
    <glossentry><glossterm>active/active, active/passive</glossterm>
  <glossdef>
   <para>
    A concept of how services are running on nodes. An active-passive
    scenario means that one or more services are running on the active node
    and the passive node waits for the active node to fail. Active-active
    means that each node is active and passive at the same time. For
    example, it has <emphasis>some</emphasis> services running, but can take
    over other services from the other node. Compare with primary/secondary
    and dual-primary in DRBD speak.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>arbitrator</glossterm>
  <glossdef>
   <para>
    Additional instance in a &geo; cluster that helps to reach consensus
    about decisions such as failover of resources across sites. Arbitrators
    are single machines that run one or more booth instances in a special
    mode.
   </para>
   <para>
    In regular clusters, <quote>arbitrator</quote> can also refer to &qnet;.
    &qnet; can be configured along with &qdevice; to participate in quorum decisions
    in clusters with an even number of nodes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&ay;</glossterm>
  <glossdef>
   <para>
    &def-ay;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>bindnetaddr (bind network address)</glossterm>
  <glossdef>
   <para>
    &def-bindnetaddr;
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glos-booth"><glossterm>booth</glossterm>
  <glossdef>
   <para>
    The instance that manages the failover process between the sites of a
    &geo; cluster. It aims to get multi-site resources active on one and
    only one site. This is achieved by using so-called tickets that are
    treated as failover domain between cluster sites, in case a site should
    be down.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>boothd (booth
  daemon)</glossterm>
  <glossdef>
   <para>
    Each of the participating clusters and arbitrators in a &geo; cluster
    runs a service, the <systemitem class="daemon">boothd</systemitem>. It
    connects to the booth daemons running at the other sites and exchanges
    connectivity details.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>CIB (cluster information base)</glossterm>
  <glossdef>
   <para>
    A representation of the whole cluster configuration and status (cluster
    options, nodes, resources, constraints and the relationship to each
    other). It is written in XML and resides in memory. A primary CIB is kept
    and maintained on the
    <xref linkend="glos-dc" xrefstyle="select:nopage"/> and replicated to
    the other nodes. Normal read and write operations on the CIB are
    serialized through the primary CIB.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-clone"><glossterm>clone</glossterm>
  <glossdef>
   <para>
    A <emphasis>clone</emphasis> can refer to an identical copy of an existing node.
    Cloning nodes can make deploying multiple nodes simpler.
   </para>
   <para>
    In the context of a cluster <xref linkend="gloss-resource"/>, a clone is a resource that
    can be active on multiple nodes. Any resource can be cloned, provided the respective
    resource agent supports it. A <xref linkend="gloss-prom-clone"/> (also known as a multi-state
    resource) is a special type of clone resource that can be promoted.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cluster</glossterm>
  <glossdef>
   <para>
    A <emphasis>high-performance</emphasis> cluster is a group of computers
    (real or virtual) sharing the application load to achieve faster
    results. A <emphasis>high-availability</emphasis> cluster is designed
    primarily to secure the highest possible availability of services.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Cluster logical volume manager (&clvm;)</glossterm>
  <glossdef>
   <para>
    The term <literal>&clvm;</literal> indicates that LVM is being used
    in a cluster environment. This needs some configuration adjustments
    to protect the LVM metadata on shared storage.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cluster partition</glossterm>
  <glossdef>
   <para>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs. The nodes of a cluster are
    split into partitions but still active. They can only communicate with
    nodes in the same partition and are unaware of the separated nodes. As
    the loss of the nodes on the other partition cannot be confirmed, a
    split-brain scenario develops (see also
    <xref linkend="glos-splitbrain"/>).
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>cluster site</glossterm>
  <glossdef>
   <para>
    In &geo; clustering, a cluster site (or just <quote>site</quote>) is a group of
    nodes in the same physical location, managed by <xref linkend="glos-booth"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>cluster stack</glossterm>
  <glossdef>
   <para>
    The ensemble of software technologies and components that compose a cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-col-con">
  <glossterm>colocation constraint</glossterm>
  <glossdef>
   <para>
    Colocation constraints tell the cluster which resources may or may not run together on a node.
    See also <xref linkend="gloss-resource-con"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>concurrency violation</glossterm>
  <glossdef>
   <para>
    A resource that should be running on only one node in the cluster is
    running on several nodes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>conntrack tools</glossterm>
  <glossdef>
   <para>
    &def-conntrack;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&corosync;</glossterm>
  <glossdef>
   <para>
    &corosync; provides reliable messaging, membership and quorum information about the
    cluster. This is handled by the &corosync; Cluster Engine, a group communication system.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>CRM (cluster resource manager)</glossterm>
  <glossdef>
   <para>
    The management entity responsible for coordinating all non-local
    interactions in a &ha; cluster. &productname; uses Pacemaker as CRM.
    The CRM is implemented as <systemitem class="daemon">pacemaker-controld</systemitem>. It interacts with several
    components: local resource managers, both on its own node and on the other nodes,
    non-local CRMs, administrative commands, the fencing functionality, and the membership
    layer.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm><command>crm</command> shell (crmsh)</glossterm>
  <glossdef>
   <para>
    The command-line utility &crmsh; manages your cluster, nodes and
    resources.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&csync;</glossterm>
  <glossdef>
   <para>
    &def-csync2;
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glos-dc"><glossterm>DC (designated coordinator)</glossterm>
  <glossdef>
   <para>
    The DC is elected from all nodes in the cluster. This happens if there
    is no DC yet or if the current DC leaves the cluster for any reason.
    The DC is the only entity
    in the cluster that can decide that a cluster-wide change needs to
    be performed, such as fencing a node or moving resources around. All
    other nodes get their configuration and resource allocation
    information from the current DC.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Disaster</glossterm>
  <glossdef>
   <para>
    Unexpected interruption of critical infrastructure induced by nature,
    humans, hardware failure, or software bugs.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Disaster Recovery</glossterm>
  <glossdef>
   <para>
    Disaster recovery is the process by which a business function is
    restored to the normal, steady state after a disaster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Disaster Recover Plan</glossterm>
  <glossdef>
   <para>
    A strategy to recover from a disaster with minimum impact on IT
    infrastructure.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>DLM (distributed lock manager)</glossterm>
  <glossdef>
   <para>
    DLM coordinates disk access for clustered file systems and administers
    file locking to increase performance and availability.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>DRBD</glossterm>
  <glossdef>
   <para>
    <trademark class="registered">DRBD</trademark> is a block device
    designed for building high availability clusters. The whole block device
    is mirrored via a dedicated network and is seen as a network RAID-1.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>existing cluster</glossterm>
  <glossdef>
   <para>
    &def-existing-cluster;
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glo-failover"><glossterm>failover</glossterm>
  <glossdef>
   <para>
    Occurs when a resource or node fails on one machine and the affected
    resources are started on another node.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>failover domain</glossterm>
  <glossdef>
   <para>
    A named subset of cluster nodes that are eligible to run a cluster
    service if a node fails.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>fencing</glossterm>
  <glossdef>
   <para>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. There are two classes of fencing:
    resource level fencing and node level fencing. Resource level fencing ensures
    exclusive access to a given resource. Node level fencing prevents a failed
    node from accessing shared resources entirely and prevents resources from running
    on a node whose status is uncertain. This is usually done in a simple and
    abrupt way: reset or power off the node.
   </para>
  </glossdef>
 </glossentry>
  <glossentry><glossterm>&geo; cluster (geographically dispersed cluster)</glossterm>
  <glossdef>
   <para>
    Consists of multiple, &geo.dispersed; sites with a local cluster
    each. The sites communicate via IP. Failover across the sites is
    coordinated by a higher-level entity, the booth. &geo; clusters need
    to cope with limited network bandwidth and high latency. Storage is
    replicated asynchronously.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>GFS2</glossterm>
  <glossdef>
   <para>
    Global File System 2 or GFS2 is a shared disk file system for Linux
    computer clusters. GFS2 allows all nodes to have direct concurrent
    access to the same shared block storage. GFS2 has no disconnected
    operating mode, and no client or server roles. All nodes in a GFS2
    cluster function as peers. GFS2 supports up to 32 cluster nodes. Using
    GFS2 in a cluster requires hardware to allow access to the shared
    storage, and a lock manager to control access to the storage.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>group</glossterm>
  <glossdef>
   <para>
    Resource groups contain multiple resources that need to be located together, started sequentially
    and stopped in the reverse order.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&hawk2;</glossterm>
  <glossdef>
   <para>
    A user-friendly Web-based interface with which you can monitor and
    administer your &ha; clusters from Linux or non-Linux machines alike.
    &hawk2; can be accessed from any machine inside or outside of the cluster
    by using a (graphical) Web browser.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-heuristics"><glossterm>heuristics</glossterm>
  <glossdef>
   <para>
    &qdevice; supports a set of commands (<quote>heuristics</quote>). The commands are
    executed locally on start-up of cluster services, cluster membership change, successful
    connection to the &qnet; server, or, optionally, at regular times. Only if all commands
    are executed successfully are the heuristics considered to have passed; otherwise, they
    failed. The heuristics' result is sent to the &qnet; server, where it is used in calculations
    to determine which partition should be quorate.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glos-lb"><glossterm>load balancing</glossterm>
  <glossdef>
   <para>
    The ability to make several servers participate in the same service and
    do the same work.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>local cluster</glossterm>
  <glossdef>
   <para>
    A single cluster in one location (for example, all nodes are located in
    one data center). Network latency can be neglected. Storage is typically
    accessed synchronously by all nodes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>location</glossterm>
  <glossdef>
   <para>
    In the context of a whole cluster, <quote>location</quote> can refer to the physical
    location of nodes (for example, all nodes might be located in the same data center).
   </para>
   <para>
    In the context of a <emphasis>location constraint</emphasis>, <quote>location</quote>
    refers to the nodes on which a resource can or cannot run.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-loc-con"><glossterm>location constraint</glossterm>
  <glossdef>
   <para>
    Location constraints define on which nodes a resource may be run, may not be run or
    is preferred to be run. See also <xref linkend="gloss-resource-con"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>LRM (local resource manager)</glossterm>
  <glossdef>
   <para>
    The local resource manager is located between the Pacemaker layer and the
    resources layer on each node. It is implemented as <systemitem class="daemon">pacemaker-execd</systemitem> daemon. Through this daemon,
    Pacemaker can start, stop, and monitor resources.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>mcastaddr (multicast address)</glossterm>
  <glossdef>
   <para>
    &def-mcastaddr;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>mcastport (multicast port)</glossterm>
  <glossdef>
   <para>
     &def-mcastport;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>meta attributes (resource options)</glossterm>
  <glossdef>
   <para>
    Parameters that tell the CRM how to treat a specific <xref linkend="gloss-resource"/>. For example,
    the priority or the target role.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>metro cluster</glossterm>
  <glossdef>
   <para>
    A single cluster that can stretch over multiple buildings or data
    centers, with all sites connected by fibre channel. Network latency is
    usually low (&lt;5&nbsp;ms for distances of approximately
    20&nbsp;miles). Storage is frequently replicated (mirroring or
    synchronous replication).
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>multicast</glossterm>
  <glossdef>
   <para>
    &def-multicast;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>network device bonding</glossterm>
  <glossdef>
   <para>
    Network device bonding combines two or more network interfaces into a single bonded
    device to increase bandwidth and/or provide redundancy.
    When using &corosync;, the bonding device is not managed by the cluster software.
    Therefore, the bonding device must be configured on every cluster node that might
    need to access the bonding device.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>node</glossterm>
  <glossdef>
   <para>
    Any computer (real or virtual) that is a member of a cluster and
    invisible to the user.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&ocfs;</glossterm>
  <glossdef>
   <para>
    Oracle Cluster File System 2 (&ocfs;) is a general-purpose journaling
    file system that allows you to store application binary files, data files and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with &corosync; and the Distributed
    Lock Manager (DLM).
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-ord-con"><glossterm>order constraint</glossterm>
  <glossdef>
   <para>
    Order constraints define the sequence of actions. See also <xref linkend="gloss-resource-con"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>pacemaker-controld (cluster controller daemon)</glossterm>
  <glossdef>
   <para>
    The CRM is implemented as the <systemitem class="daemon">pacemaker-controld</systemitem>
    daemon. It has an instance on each
    cluster node. All cluster decision-making is centralized by electing one
    of the pacemaker-controld instances to act as a primary. If the elected pacemaker-controld process
    fails (or the node it ran on), a new one is established.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>parameters (instance attributes)</glossterm>
  <glossdef>
   <para>
    Parameters determine which instance of a service the <xref linkend="gloss-resource"/> controls.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>PE (policy engine)</glossterm>
  <glossdef>
   <para>
    The policy engine is implemented as the
    <systemitem class="daemon">pacemaker-schedulerd</systemitem> daemon.
    When a cluster transition is needed, based on the current state and
    configuration, <systemitem class="daemon">pacemaker-schedulerd</systemitem>
    calculates the expected next state of the cluster. It determines what
    actions need to be scheduled to achieve the next state.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>primitive</glossterm>
  <glossdef>
   <para>
    A primitive resource is the most basic type of cluster <xref linkend="gloss-resource"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-prom-clone"><glossterm>promotable clone</glossterm>
  <glossdef>
   <para>
    Promotable clones (also known as multi-state resources) are a special type of
    <xref linkend="gloss-clone"/> resource that can be promoted. Active instances of
    these resources are divided into two states: active and passive. These are also
    sometimes called primary and secondary.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-qdevice"><glossterm>&qdevice; (<systemitem class="daemon">corosync-qdevice</systemitem>)</glossterm>
  <glossdef>
   <para>
    &qdevice; and &qnet; participate in quorum decisions. With assistance from
    the arbitrator <systemitem class="daemon">corosync-qnetd</systemitem>,
    <systemitem class="daemon">corosync-qdevice</systemitem> provides a configurable
    number of votes, allowing a cluster to sustain more node failures than the standard
    quorum rules allow.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-qnetd"><glossterm>&qnet; (<systemitem class="daemon">corosync-qnetd</systemitem>)</glossterm>
  <glossdef>
   <para>
    A systemd service (a daemon, the <quote>&qnet; server</quote>) which is not part of the cluster.
    &qnet; provides a vote to the <systemitem class="daemon">corosync-qdevice</systemitem> daemon
    to help it participate in quorum decisions.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-quorum"><glossterm>quorum</glossterm>
  <glossdef>
   <para>
    In a cluster, a cluster partition is defined to have quorum (be
    <quote>quorate</quote>) if it has the majority of nodes (or votes).
    Quorum distinguishes exactly one partition. It is part of the algorithm
    to prevent several disconnected partitions or nodes from proceeding and
    causing data and service corruption (split brain). Quorum is a
    prerequisite for fencing, which then ensures that quorum is indeed
    unique.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>RA (resource agent)</glossterm>
  <glossdef>
   <para>
    A script acting as a proxy to manage a resource (for example, to start,
    stop, or monitor a resource). &productname; supports different
    kinds of resource agents. For details, see
    <xref linkend="sec-ha-config-basics-raclasses"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&rear; (Relax and Recover)</glossterm>
  <glossdef>
   <para>
    An administrator tool set for creating disaster recovery images.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-resource"><glossterm>resource</glossterm>
  <glossdef>
   <para>
    Any type of service or application that is known to Pacemaker. Examples
    include an IP address, a file system, or a database.
   </para>
   <para>
    The term <quote>resource</quote> is also used for DRBD, where it names a
    set of block devices that are using a common connection for replication.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss-resource-con"><glossterm>resource constraint</glossterm>
  <glossdef>
   <para>
    Resource constraints let you specify which cluster nodes resources can run on, what
    order resources load in, and what other resources a specific resource is dependent on.
   </para>
   <para>
    See also <xref linkend="gloss-col-con"/>, <xref linkend="gloss-loc-con"/> and
    <xref linkend="gloss-ord-con"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>resource set</glossterm>
  <glossdef>
   <para>
    As an alternative format for defining location, colocation or order constraints, you can use
    <emphasis>resource sets</emphasis>, where primitives are grouped together in one set. When
    creating a constraint, you can specify multiple resources for the constraint to apply to.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>resource template</glossterm>
  <glossdef>
   <para>
    If you want to create lots of resources with similar configurations, defining a
    resource template is the easiest way. After being defined, it can be referenced
    in primitives or in certain types of constraints. If a template is referenced in
    a primitive, the primitive inherits all operations, instance attributes (parameters),
    meta attributes and utilization attributes defined in the template.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>RRP (redundant ring protocol)</glossterm>
  <glossdef>
   <para>
    &def-rrp;
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>SBD (&stonith; Block Device)</glossterm>
  <glossdef>
   <para>
    Provides a node fencing mechanism through the exchange of messages via shared
    block storage (SAN, iSCSI, FCoE, etc.). Can also be used in diskless mode.
    Needs a hardware or software watchdog on each node to ensure that misbehaving
    nodes are really stopped.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>SFEX (shared disk file exclusiveness)</glossterm>
  <glossdef>
   <para>
    SFEX provides storage protection over SAN.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glos-splitbrain"><glossterm>split-brain</glossterm>
  <glossdef>
   <para>
    A scenario in which the cluster nodes are divided into two or more
    groups that do not know of each other (either through a software or
    hardware failure). &stonith; prevents a split-brain situation from badly
    affecting the entire cluster. Also known as a <quote>partitioned
    cluster</quote> scenario.
   </para>
   <para>
    The term <literal>split brain</literal> is also used in DRBD but means that the two nodes
    contain different data.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>SPOF (single point of failure)</glossterm>
  <glossdef>
   <para>
    Any component of a cluster that, should it fail, triggers the failure of
    the entire cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="glo-stonith"><glossterm>&stonith;</glossterm>
  <glossdef>
   <para>
    The acronym for <quote>Shoot the other node in the head</quote>. It refers
    to the fencing mechanism that shuts down a misbehaving node to prevent it
    from causing trouble in a cluster. In a Pacemaker cluster, the implementation
    of node level fencing is &stonith;. For this, Pacemaker comes with a fencing
    subsystem, <systemitem class="daemon">pacemaker-fenced</systemitem>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>switchover</glossterm>
  <glossdef>
   <para>
    Planned, on-demand moving of services to other nodes in a cluster. See
    <xref linkend="glo-failover"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>ticket</glossterm>
  <glossdef>
   <para>
    A component used in &geo; clusters. A ticket grants the right to run
    certain resources on a specific cluster site. A ticket can only be owned
    by one site at a time. Resources can be bound to a certain ticket by
    dependencies. Only if the defined ticket is available at a site, the
    respective resources are started. Vice versa, if the ticket is removed,
    the resources depending on that ticket are automatically stopped.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>unicast</glossterm>
  <glossdef>
   <para>
    A technology for sending messages to a single network destination.
    &corosync; supports both multicast and unicast. In &corosync;,
    unicast is implemented as UDP-unicast (UDPU).
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>utilization</glossterm>
  <glossdef>
   <para>
    Tells the CRM what capacity a certain <xref linkend="gloss-resource"/> requires from a node.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&yast;</glossterm>
  <glossdef>
   <para>
    A graphical user interface for general system installation and
    administration. Use it to install &productname; on top of &sls; as
    described in the &haquick;.
   </para>
  </glossdef>
 </glossentry>
</glossary>
