<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<article version="5.0" xml:lang="en" xml:id="article-geo-clustering"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&geoquick;</title>
 <info>
  <productnumber>&productnumber;</productnumber><productname>&productname;</productname>
  <date><?dbtimestamp format="B d, Y"?></date>
  <xi:include href="common_copyright_gfdl.xml"/>
  <abstract>
   <para>
    &abstract-geoquick;
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-ha-geo-quick-concept">
  <title>Conceptual overview</title>

  &geo-clusters-concept;

  <para>
   <remark>toms 2017-07-04: for the future: maybe also add example IP addresses
    to graphic (node IPs, VIPs for cluster sites)
    </remark>
   <remark>taroth 2017-12-29: FATE#322100 - adjust graphic in case two sites
   *without* arbitrator becomes the default setup? (c#6)
   </remark>
  </para>

  <figure xml:id="fig-ha-geo-quick-example-geosetup">
   <title>Two-site cluster&mdash;2x2 nodes + arbitrator (optional)</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_geocluster.svg" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_geocluster.png" width="85%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   It is also possible to run a two-site &geo; cluster <emphasis>without</emphasis>
   an arbitrator. In this case, a &geo; cluster administrator needs to manually manage
   the tickets. If a ticket should be granted to more than one site at the same time,
   booth displays a warning.
  </para>

  <para>
   For more details on the concept, components and ticket management used for
   &geo; clusters, see the <xref linkend="book-administration"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-scenario">
  <title>Usage scenario</title>

  <para>
   In the following, we will set up a basic &geo; cluster with two cluster
   sites and one arbitrator:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     We assume the cluster sites are named <literal>&cluster1;</literal> and
     <literal>&cluster2;</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     We assume that each site consists of two nodes. The nodes
     <literal>&node1;</literal> and <literal>&node2;</literal> belong to the
     cluster <literal>&cluster1;</literal>. The nodes
     <literal>&node3;</literal> and <literal>&node4;</literal> belong to the
     cluster <literal>&cluster2;</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Site <literal>&cluster1;</literal> will get the following virtual IP
     address: <literal>&geo-vip-site1;</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Site <literal>&cluster2;</literal> will get the following virtual IP
     address: <literal>&geo-vip-site2;</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     <remark>taroth 2017-12-29: FATE#322100 - move to Geo Guide? (c#6)</remark>
     We assume that the arbitrator has the following IP address:
     <literal>&geo-ip-arbi;</literal>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Before you proceed, make sure the following requirements are fulfilled:
  </para>

  <variablelist>
   <title>Requirements</title>
   <varlistentry>
    <term>Two existing clusters</term>
    <listitem>
     <para>
      You have at least two existing clusters that you want to combine into a
      &geo; cluster. (If you need to set up two clusters first, follow the
      instructions in the <xref linkend="article-installation"/>.
      </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Meaningful cluster names</term>
    <listitem>
     <para>
      Each cluster has a meaningful cluster name defined in &corosync.conf;
      that reflects its location.
     </para>
   </listitem>
   </varlistentry>
   <varlistentry>
    <term>Arbitrator</term>
    <listitem>
     <para><remark>taroth 2017-12-29: FATE#322100 - move to Geo Guide? (see
     c#6)</remark>
      You have installed a third machine that is not part of any existing
      clusters and is to be used as arbitrator.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For detailed requirements on each item, see also
   <xref
  linkend="sec-ha-geo-quick-req"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-req">
  <title>Requirements</title>
   <para>
    All machines (cluster nodes and arbitrators) that will be part of the cluster
    need at least the following modules and extensions:
   </para>

&sys-req-sw-modules;
  <para>
   When installing the machines, select <literal>HA GEO Node</literal> as
   <systemitem>system role</systemitem>. This leads to the installation of a
   minimal system where the packages from the pattern <literal>&geo; Clustering
   for &ha; (ha_geo)</literal> are installed by default.
  </para>
  <itemizedlist>
   <title>Network requirements</title>
   &geo-clusters-req-netw-vip;
   &geo-clusters-req-netw-ports;
   &geo-clusters-req-netw-ports-drbd;
  </itemizedlist>

  &geo-clusters-req-other;

 </sect1>
 <sect1 xml:id="sec-ha-geo-scripts">
  <title>Overview of the &geo; bootstrap scripts</title>
  <itemizedlist>
   <listitem>
    <para>
     With <command>crm cluster geo_init</command>, turn a cluster into the first
     site of a &geo; cluster. The script takes some parameters like the names of
     the clusters, the arbitrator, and one or multiple tickets and creates
     &booth.conf; from them. It copies the booth configuration to all nodes on the
     current cluster site. It also configures the cluster resources needed for
     booth on the current cluster site.
    </para>
    <para>
     For details, see <xref linkend="sec-ha-geo-quick-crm-cluster-geo-init"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>crm cluster geo_join</command>, add the current cluster to an
     existing &geo; cluster. The script copies the booth configuration from an
     existing cluster site and writes it to &booth.conf; on all nodes on the
     current cluster site. It also configures the cluster resources needed for
     booth on the current cluster site.
    </para>
    <para>
     For details, see <xref linkend="sec-ha-geo-quick-crm-cluster-geo-join"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>crm cluster geo_init_arbitrator</command>, turn the current
     machine into an arbitrator for the &geo; cluster. The script copies the
     booth configuration from an existing cluster site and writes it to
     &booth.conf;.
    </para>
    <para>
     For details, see
     <xref linkend="sec-ha-geo-quick-crm-cluster-geo-init-arbitrator"/>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   All bootstrap scripts log to
   <filename>/var/log/crmsh/crmsh.log</filename>. Check the log file
   for any details of the bootstrap process. Any options set during the
   bootstrap process can be modified later (by modifying booth settings,
   modifying resources etc.). For details, see the <xref linkend="book-administration"/>.
  </para>
 </sect1>

 <sect1 xml:id="sec-ha-geo-quick-inst">
  <title>Installing the &ha; and &geo; clustering packages</title>
  <para>
   The packages for configuring and managing a &geo; cluster are included in the
   <literal>&ha;</literal> and <literal>&geo; Clustering for &ha;</literal> installation
   patterns. These patterns are only available after the &productname; is installed.
  </para>
  <para>
   You can register to the &scc; and install the &hasi; while installing &sles;,
   or after installation. For more information, see the
   <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-register-sle.html">
   &deploy;</link> for &sles;.
  </para>
  <procedure xml:id="pro-ha-geo-quick-inst-pattern">
   <title>Installing the &ha; and &geo; clustering patterns</title>
   <step>
    <para>
     Install the &ha; and &geo; clustering patterns from the command line:</para>
<screen>&prompt.root;<command>zypper install -t pattern ha_sles ha_geo</command></screen>
   </step>
   <step>
    <para>
       Install the &ha; and &geo; clustering patterns on <emphasis>all</emphasis> machines that
       will be part of your cluster.
    </para>
    <note>
     <title>Installing software packages on all nodes</title>
     <para>
      For an automated installation of &sls; &productnumber; and the &hasi;,
      use &ay; to clone existing nodes. For more
      information, see <xref linkend="sec-ha-installation-autoyast"/>.
     </para>
    </note>
   </step>
 </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-crm-cluster-geo-init">
  <title>Setting up the first site of a &geo; cluster</title>

  <para>
   Use the <command>crm cluster geo_init</command> command to turn an existing
   cluster into the first site of a &geo; cluster.
  </para>

  <procedure xml:id="pro-ha-geo-quick-crm-cluster-geo-init">
   <title>Setting up the first site (<systemitem>&cluster1;</systemitem>) with <command>crm cluster geo_init</command></title>
   <step>
    <para>
     Define a virtual IP per cluster site that can be used to access the site.
     We assume using <literal>&geo-vip-site1;</literal> and
     <literal>&geo-vip-site2;</literal> for this purpose. You do not need to
     configure the virtual IPs as cluster resources yet. This will be done by
     the bootstrap scripts.
    </para>
   </step>
   <step>
    <para>
     Define the name of at least one ticket that will grant the right to run
     certain resources on a cluster site. Use a meaningful name that reflects
     the resources that will depend on the ticket (for example,
     <literal>&ticket3;</literal>). The bootstrap scripts only need the ticket
     name&mdash;you can define the remaining details (ticket dependencies of
     the resources) later on, as described in
     <xref
      linkend="sec-ha-geo-quick-next"/>.
    </para>
   </step>
   <step>
    <para>
     Log in to a node of an existing cluster (for example, on node
     <literal>&node1;</literal> of the cluster <literal>&cluster1;</literal>).
    </para>
   </step>
   <step xml:id="st-crm-cluster-geo-init">
    <para>
     Run <command>crm cluster geo_init</command>. For example, use the following
     options:
     <remark>taroth 2017-12-29: FATE#322100 - remove the --arbitrator option?
     (see c#6)</remark>
    </para>
<screen>&prompt.root;<command>crm cluster geo_init</command> \
  --clusters<co xml:id="co-geo-init-clusters"/> "&cluster1;=&geo-vip-site1; &cluster2;=&geo-vip-site2;" \
  --tickets<co xml:id="co-geo-init-ticket"/> &ticket3; \
  --arbitrator<co xml:id="co-geo-init-arbitrator"/> &geo-ip-arbi;</screen>
    <calloutlist>
     <callout arearefs="co-geo-init-clusters">
      &geo-join-clusters;
     </callout>
     <callout arearefs="co-geo-init-ticket">
      <para>
       The name of one or multiple tickets.
      </para>
     </callout>
     <callout arearefs="co-geo-init-arbitrator">
      <para>
       The host name or IP address of a machine outside of the clusters.
      </para>
     </callout>
    </calloutlist>
   </step>
  </procedure>

  <para>
   The bootstrap script creates the booth configuration file and synchronizes
   it across the cluster sites. It also creates the basic cluster resources
   needed for booth. <xref linkend="st-crm-cluster-geo-init" xrefstyle="seletc:label"/> of
   <xref linkend="pro-ha-geo-quick-crm-cluster-geo-init" xrefstyle="select:label"/>
   would result in the following booth configuration and cluster resources:
   <remark>taroth 2017-12-29: FATE#322100 - remove the arbitrator entry? (c#6)</remark>
  </para>

  <example xml:id="ex-geo-init-booth-conf">
   <title>Booth configuration created by <command>crm cluster geo_init</command></title>
<screen># The booth configuration file is "/etc/booth/booth.conf". You need to
# prepare the same booth configuration file on each arbitrator and
# each node in the cluster sites where the booth daemon can be launched.

# "transport" means which transport layer booth daemon will use.
# Currently only "UDP" is supported.
transport="UDP"
port="9929"

arbitrator="&geo-ip-arbi;"
site="&geo-vip-site1;"
site="&geo-vip-site2;"
authfile="/etc/booth/authkey"
ticket="&ticket3;"
expire="600"</screen>
  </example>

  <example xml:id="ex-geo-init-rsc-conf">
   <title>Cluster resources created by <command>crm cluster geo_init</command></title>
<screen>primitive<co xml:id="co-geo-quick-rsc-booth-ip"/> booth-ip IPaddr2 \
  params rule #cluster-name eq &cluster1; ip=&geo-vip-site1; \
  params rule #cluster-name eq &cluster2; ip=&geo-vip-site2; \
primitive<co xml:id="co-geo-quick-rsc-booth-site"/> booth-site ocf:pacemaker:booth-site \
  meta resource-stickiness=INFINITY \
  params config=booth \
  op monitor interval=10s
group<co xml:id="co-geo-quick-rsc-g-booth"/> g-booth booth-ip booth-site \
meta target-role=Stopped<co xml:id="co-geo-quick-rsc-stopped"/></screen>
   <calloutlist>
    <callout arearefs="co-geo-quick-rsc-booth-ip">
     <para>
      A virtual IP address for each cluster site. It is required by the booth
      daemons who need a persistent IP address on each cluster site.
     </para>
    </callout>
    <callout arearefs="co-geo-quick-rsc-booth-site">
     <para>
      A primitive resource for the booth daemon. It communicates with the booth
      daemons on the other cluster sites. The daemon can be started on any
      node of the site. To make the resource stay on the same node, if possible,
      resource-stickiness is set to <literal>INFINITY</literal>.
     </para>
    </callout>
    <callout arearefs="co-geo-quick-rsc-g-booth">
     <para>
      A cluster resource group for both primitives. With this configuration, each
      booth daemon will be available at its individual IP address, independent of
      the node the daemon is running on.
     </para>
    </callout>
    <callout arearefs="co-geo-quick-rsc-stopped">
     <para>
      The cluster resource group is not started by default. After verifying the
      configuration of your cluster resources (and adding the resources you
      need to complete your setup), you need to start the resource group. See
      <xref linkend="vl-ha-geo-quick-next-req"/> for details.
     </para>
    </callout>
   </calloutlist>
  </example>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-crm-cluster-geo-join">
  <title>Adding another site to a &geo; cluster</title>

  <para>
   After you have initialized the first site of your &geo; cluster, add the
   second cluster with <literal>crm cluster geo_join</literal>, as described in
   <xref linkend="pro-ha-geo-quick-crm-cluster-geo-join"
   xrefstyle="select:label"/>.
   The script needs SSH access to an already configured cluster site and will
   add the current cluster to the &geo; cluster.
  </para>

  <procedure xml:id="pro-ha-geo-quick-crm-cluster-geo-join">
   <title>Adding the second site (<literal>&cluster2;</literal>) with <command>crm cluster geo_join</command></title>
   <step>
    <para>
     Log in to a node of the cluster site you want to add (for example, on node
     <literal>&node3;</literal> of the cluster <literal>&cluster2;</literal>).
    </para>
   </step>
   <step xml:id="st-crm-cluster-geo-join">
    <para>
     Run the <command>crm cluster geo_join</command> command. For example:
    </para>
<screen>&prompt.root;<command>crm cluster geo_join</command> \
  --cluster-node<co xml:id="co-geo-join-cluster-node"/> &geo-vip-site1;\
  --clusters<co xml:id="co-geo-join-clusters"/> "&cluster1;=&geo-vip-site1; &cluster2;=&geo-vip-site2;"
     </screen>
    <calloutlist>
     <callout arearefs="co-geo-join-cluster-node">
      <para>
       Specifies where to copy the booth configuration from. Use the IP address
       or host name of a node in an already configured &geo; cluster site.
       You can also use the virtual IP address of an already existing cluster
       site (like in this example). Alternatively, use the IP address or host
       name of an already configured arbitrator for your &geo; cluster.
      </para>
     </callout>
     <callout arearefs="co-geo-join-clusters">
      &geo-join-clusters;
     </callout>
    </calloutlist>
   </step>
  </procedure>

  <para>
   The <command>crm cluster geo_join</command> command copies the booth
   configuration from
   <xref linkend="co-geo-join-cluster-node" xrefstyle="select:label"/>, see
   <xref linkend="ex-geo-init-booth-conf" xrefstyle="select:label"/>. In
   addition, it creates the cluster resources needed for booth (see
   <xref linkend="ex-geo-init-rsc-conf"
   xrefstyle="select:label"/>).
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-crm-cluster-geo-init-arbitrator">
  <title>Adding the arbitrator</title>

  <para>
   <remark>taroth 2017-12-29: FATE#322100 - move this section to Geo Guide? (c#6)</remark>
   After you have set up all sites of your &geo; cluster with
   <command>crm cluster geo_init</command> and
   <command>crm cluster geo_join</command>, set up the arbitrator with
   <command>crm cluster geo_init_arbitrator</command>.
  </para>

  <procedure xml:id="pro-ha-geo-quick-crm-cluster-geo-init-arbitrator">
   <title>Setting up the arbitrator with <command>crm cluster geo_init_arbitrator</command></title>
   <step>
    <para>
     Log in to the machine you want to use as arbitrator.
    </para>
   </step>
   <step>
    <para>
     Run the following command. For example:
    </para>
<screen>&prompt.root;<command>crm cluster geo_init_arbitrator</command> --cluster-node<co
   xml:id="co-geo-init-arbitrator-cluster-node"/> &geo-vip-site1;</screen>
    <calloutlist>
     <callout arearefs="co-geo-init-arbitrator-cluster-node">
      <para>
       Specifies where to copy the booth configuration from. Use the IP address
       or host name of a node in an already configured &geo;
       cluster site. Alternatively, use the virtual IP address of an already
       existing cluster site (like in this example).
      </para>
     </callout>
    </calloutlist>
   </step>
  </procedure>

  <para>
   The <command>crm cluster geo_init_arbitrator</command> script copies the
   booth configuration from
   <xref linkend="co-geo-init-arbitrator-cluster-node"/>, see
   <xref linkend="ex-geo-init-booth-conf" xrefstyle="select:label"/>. It also
   enables and starts the booth service on the arbitrator. Thus, the arbitrator
   is ready to communicate with the booth instances on the cluster sites when
   the booth services are running there too.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-monitor">
  <title>Monitoring the cluster sites</title>

  <para>
   To view both cluster sites with the resources and the ticket that you have
   created during the bootstrapping process, use &hawk2;. The &hawk2; Web
   interface allows you to monitor and manage multiple (unrelated) clusters and
   &geo; clusters.
  </para>

  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     All clusters to be monitored from &hawk2;'s <guimenu>Dashboard</guimenu>
     must be running &productname; &productnumber;.
    </para>
   </listitem>
   <listitem>
    <para>
     If you did not replace the self-signed certificate for &hawk2; on every
     cluster node with your own certificate (or a certificate signed by an
     official Certificate Authority) yet, do the following: Log in to &hawk2; on
     <emphasis>every</emphasis> node in <emphasis>every</emphasis> cluster
     at least once. Verify the certificate (or add an exception in the
     browser to bypass the warning). Otherwise &hawk2; cannot connect to the
     cluster.
    </para>
   </listitem>
  </itemizedlist>

  <procedure xml:id="pro-ha-geo-quick-hawk2-dashboard">
   <title>Using the &hawk2; dashboard</title>
   <step>
    <para>
     Start a Web browser and enter the virtual IP of your first cluster site,
     <literal>&cluster1;</literal>:
    </para>
<screen>https://&geo-vip-site1;:7630/</screen>
    <para>
     Alternatively, use the IP address or host name of
     <literal>&node1;</literal> or <literal>&node2;</literal>. If you have set
     up both nodes with the bootstrap scripts, the <literal>hawk</literal>
     service should run on both nodes.
    </para>
   </step>
   <step>
    <para>
     Log in to the &hawk2; Web interface.
    </para>
   </step>
   <step>
    <para>
     From the left navigation bar, select <guimenu>Dashboard</guimenu>.
    </para>
    <para>
     &hawk2; shows an overview of the resources and nodes on the current
     cluster site. In addition, it shows any <guimenu>Tickets</guimenu> that
     have been configured for the &geo; cluster. If you need information about
     the icons used in this view, click <guimenu>Legend</guimenu>.
    </para>
    <figure>
     <title>&hawk2; dashboard with one cluster site (<literal>&cluster1;</literal>)</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk2-dashboard-site1.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk2-dashboard-site1.png" width="95%"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     To add a dashboard for the second cluster site, click <guimenu>Add
     Cluster</guimenu>.
    </para>
    <substeps>
     <step>
      <para>
       Enter the <guimenu>Cluster name</guimenu> with which to identify the
       cluster in the <guimenu>Dashboard</guimenu>. In this case,
       <literal>&cluster2;</literal>.
      </para>
     </step>
     <step>
      <para>
       Enter the fully qualified host name of one of the cluster nodes (in this
       case, <literal>&node3;</literal> or <literal>&node4;</literal>).
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Add</guimenu>. &hawk2; will display a second tab for the
       newly added cluster site with an overview of its nodes and resources.
      </para>
      <figure>
       <title>&hawk2; dashboard with both cluster sites</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk2-dashboard-two-sites.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk2-dashboard-two-sites.png" width="95%"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To view more details for a cluster site or to manage it, switch to the
     site's tab and click the chain icon.
    </para>
    <para>
     &hawk2; opens the <guimenu>Status</guimenu> view for this site in a new
     browser window or tab. From there, you can administer this part of the
     &geo; cluster.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-next">
  <title>Next steps</title>

  <para>
   The &geo; clustering bootstrap scripts provide a quick way to set up a basic
   &geo; cluster that can be used for testing purposes. However, to move the
   resulting &geo; cluster into a functioning &geo; cluster that can be used in
   production environments, more steps are required&mdash;see
   <xref
    linkend="vl-ha-geo-quick-next-req"/>.
  </para>

  <variablelist xml:id="vl-ha-geo-quick-next-req">
   <title>Required steps to complete the &geo; cluster setup</title>
   <varlistentry xml:id="vle-ha-geo-quick-booth-service-sites">
    <term>Starting the booth services on cluster sites</term>
    <listitem>
     <para>
      After the bootstrap process, the arbitrator booth service cannot communicate
      with the booth services on the cluster sites yet, because they are not
      started by default.
     </para>
     <para>
      The booth service for each cluster site is managed by the booth resource
      group <literal>g-booth</literal> (see
      <xref linkend="ex-geo-init-rsc-conf"/>). To start one instance of the
      booth service per site, start the respective booth resource group on each
      cluster site. This enables all booth instances to communicate with each
      other.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Configuring ticket dependencies and ordering constraints</term>
    <listitem>
     <para>
      To make resources depend on the ticket that you have created during the
      &geo; cluster bootstrap process, configure constraints. For each
      constraint, set a <literal>loss-policy</literal> that defines what should
      happen to the respective resources if the ticket is revoked from a cluster
      site.
     </para>
     <para>
      For details, see <xref linkend="cha-ha-geo-rsc"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Initially granting a ticket to a site</term>
    <listitem>
     <para>
      Before booth can manage a certain ticket within the &geo; cluster, you
      initially need to <emphasis>grant</emphasis> it to a site manually. You
      can use either the booth client command line tool or &hawk2; to grant a
      ticket.
     </para>
     <para>
      For details, see <xref linkend="cha-ha-geo-manage"/>.</para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The bootstrap scripts create the same booth resources on both cluster sites,
   and the same booth configuration files on all sites, including the
   arbitrator. If you extend the &geo; cluster setup (to move to a production
   environment), you will probably fine-tune the booth configuration and
   change the configuration of the booth-related cluster resources. Afterward,
   you need to synchronize the changes to the other sites of your &geo; cluster
   to take effect.
  </para>

  <note>
   <title>Synchronizing changes across cluster sites</title>
   <itemizedlist>
    <listitem>
     <para>
      To synchronize changes in the booth configuration to all cluster sites
      (including the arbitrator), use &csync;. Find more information at <xref
       linkend="cha-ha-geo-sync"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      The CIB (Cluster Information Database) is not automatically synchronized
      across cluster sites of a &geo; cluster. That means any changes in
      resource configuration that are required on all cluster sites need to be
      transferred to the other sites manually. Do so by tagging the
      respective resources, exporting them from the current CIB, and importing
      them to the CIB on the other cluster sites. For details, see
      <xref linkend="sec-ha-geo-rsc-sync-cib"/>.
      </para>
    </listitem>
   </itemizedlist>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-more">
  <title>For more information</title>
  <itemizedlist>
   <listitem>
    <para>
     More documentation for this product is available at
     <link
      xlink:href="https://documentation.suse.com/sle-ha/"/>.
     For further configuration and administration tasks, see the comprehensive
     <link xlink:href="https://documentation.suse.com/sle-ha/html/SLE-HA-all/book-administration.html">
     &geoguide;</link>.
    </para>
   </listitem>
   <listitem>
    <para>
     Find information about data replication across &geo; clusters via DRBD in the following
     <link
      xlink:href="https://documentation.suse.com/sbp/all/html/SBP-DRBD/index.html"><citetitle>&sbp;</citetitle>
     document</link>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <xi:include href="common_legal.xml"/>
</article>
