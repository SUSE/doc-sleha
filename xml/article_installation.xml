<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<article version="5.0" xml:lang="en" xml:id="article-installation"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&instquick;</title>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp?></date>
  <xi:include href="ha_authors.xml"/>
  <abstract>
   <para>
    &abstract-instquick;
   </para>
  </abstract>
  <dm:docmanager>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

  <sect1 xml:id="sec-ha-inst-quick-usage-scenario">
   <title>Usage scenario</title>
   <para>
    The procedures in this document will lead to a minimal setup of a two-node
    cluster with the following properties:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Two nodes: <systemitem class="server">&node1;</systemitem> (IP: <systemitem
       class="ipaddress">&subnetI;.1</systemitem>)
      and <systemitem class="server">&node2;</systemitem> (IP: <systemitem
       class="ipaddress">&subnetI;.2</systemitem>),
      connected to each other via network.
     </para>
    </listitem>
    <listitem>
     <para>
      A floating, virtual IP address (<systemitem class="ipaddress"
       >&subnetII;.1</systemitem>) which
      allows clients to connect to the service no matter which physical node it
      is running on.
     </para>
    </listitem>
    <listitem>
     <para>A shared storage device, used as SBD fencing mechanism.
      This avoids split brain scenarios.
     </para>
    </listitem>
    <listitem>
     <para>
      Failover of resources from one node to the other if the active host breaks
      down (<emphasis>active/passive</emphasis> setup).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    After setup of the cluster with the bootstrap scripts, we will monitor
    the cluster with the graphical &hawk2;. It is one of the cluster management
    tools included with &productnamereg;. As a basic test of whether failover of resources
    works, we will put one of the nodes into standby mode and check if the
    virtual IP address is migrated to the second node.
   </para>
   <para>
    You can use the two-node cluster for testing purposes or as a minimal
    cluster configuration that you can extend later on. Before using the
    cluster in a production environment, modify it according to your
    requirements.
   </para>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-req">
   <title>System requirements</title>
   <para>
    This section informs you about the key system requirements for the
    scenario described in <xref linkend="sec-ha-inst-quick-usage-scenario"
    xrefstyle="select:label"/>.
    To adjust the cluster for use in a production environment,
    refer to the full list in <xref linkend="cha-ha-requirements"/>.
   </para>

  <sect2 xml:id="vl-ha-inst-quick-req-hw">
   <title>Hardware requirements</title>
   <variablelist>
    <varlistentry>
     <term>Servers</term>
     <listitem>
      <para>
       Two servers with software as specified in <xref linkend="il-ha-inst-quick-req-sw"/>.
      </para> &sys-req-hw-nodes;
      <!--<para>
     The servers can be bare metal or virtual machines. They do not require
     identical hardware (memory, disk space, etc.), but they must have the
     same architecture. Cross-platform clusters are not supported.
     </para>-->
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Communication channels</term>
     <listitem> &sys-req-hw-comm-channels; </listitem>
    </varlistentry>
    <varlistentry>
     <term>Node fencing/&stonith;</term>
     <listitem> &sys-req-hw-stonith; </listitem>
    </varlistentry>
   </variablelist>
 </sect2>
  <sect2 xml:id="il-ha-inst-quick-req-sw">
   <title>Software requirements</title>
   <para>
   All nodes that will be part of the cluster need at least the following modules
   and extensions:
  </para>

&sys-req-sw-modules;

  </sect2>
  <sect2 xml:id="vl-ha-inst-quick-req-other">
   <title>Other requirements and recommendations</title>
   <variablelist>
    <varlistentry>
     <term>Time synchronization</term> &sys-req-other-ntp; </varlistentry>
    <varlistentry>
     <term>Host name and IP address</term>
     <listitem>
      <itemizedlist>
       <listitem>
        <para> Use static IP addresses. </para>
       </listitem>
       <listitem> &sys-req-other-etc-hosts; </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SSH</term>
     <listitem> &sys-req-other-ssh; <para> If you use the bootstrap scripts for setting up the
       cluster, the SSH keys will automatically be created and copied. </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-bootstrap">
  <title>Overview of the bootstrap scripts</title>
  <para>
   All commands from the <package>ha-cluster-bootstrap</package> package
   execute bootstrap scripts that require only a minimum of time and manual
   intervention.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     With <command>crm cluster init</command>, define the basic parameters needed
     for cluster communication. This leaves you with a running one-node cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>crm cluster join</command>, add more nodes to your cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>crm cluster remove</command>, remove nodes from your cluster.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   All bootstrap scripts log to <filename>/var/log/ha-cluster-bootstrap.log</filename>.
   Check this file for any details of the bootstrap process. Any options set
   during the bootstrap process can be modified later with the
   &yast; cluster module. See <xref linkend="cha-ha-ycluster"/>
   for details.
  </para>
  <para>Each script comes with a man page covering the range of functions, the
   script's options, and an overview of the files the script can create and modify.
  </para>
  <para>
   The bootstrap script <command>crm cluster init</command> checks and
   configures the following components:
  </para>

  <variablelist>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      If NTP has not been configured to start at boot time, a message appears.
      Since &productname; 15, chrony is the default implementation of NTP.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>It creates SSH keys for passwordless login between cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&csync;</term>
    <listitem>
     <para>
      It configures &csync; to replicate configuration files across all nodes
      in a cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&corosync;</term>
    <listitem>
     <para>It configures the cluster communication system.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD/watchdog</term>
    <listitem>
     <para>It checks if a watchdog exists and asks you whether to configure SBD
      as node fencing mechanism.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual floating IP</term>
    <listitem>
     <para>It asks you whether to configure a virtual IP address for cluster
      administration with &hawk2;.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Firewall</term>
    <listitem>
     <para>It opens the ports in the firewall that are needed for cluster communication.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster name</term>
    <listitem>
     <para>It defines a name for the cluster, by default
        <systemitem>hacluster</systemitem>. This
      is optional and mostly useful for &geo; clusters. Usually, the cluster
      name reflects the location and makes it easier to distinguish a site
      inside a &geo; cluster.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&qdevice;/&qnet;</term>
    <listitem>
     <para>
      This setup is not covered here. If you want to use a &qnet; server,
      you can set it up with the bootstrap script as described in
      <xref linkend="cha-ha-qdevice"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-installation">
    <title>Installing &productname;</title>
    <para>
      The packages for configuring and managing a cluster with the
      &hasi; are included in the <literal>&ha;</literal> installation
      pattern (named <literal>sles_ha</literal> on the command line).
      This pattern is only available after &productname; has been
      installed as an extension to &slsreg;.
    </para>
    <para>
      For information on how to install
      extensions, see the <link
       xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-add-ons.html">
       <citetitle>&deploy;</citetitle> for &sls; &productnumber;</link>.
    </para>

    <procedure xml:id="pro-ha-inst-quick-pattern">
      <title>Installing the <literal>&ha;</literal> pattern</title>
       <para>
        If the pattern is not installed yet, proceed as follows:
       </para>
      <step>
       <para>
        Install it via command line using Zypper:</para>
<screen>&prompt.root;<command>zypper</command> install -t pattern ha_sles</screen>
      </step>
      <step>
       <para>
          Install the &ha; pattern on <emphasis>all</emphasis> machines that
          will be part of your cluster.
       </para>
       <note>
        <title>Installing software packages on all parties</title>
        <para>
         For an automated installation of &sls; &productnumber; and &productname;
         &productnumber;, use &ay; to clone existing nodes. For more
         information, see <xref linkend="sec-ha-installation-autoyast"/>.
        </para>
       </note>
      </step>
      <step>
       <para>
         Register the machines at &scc;. Find more information in the <link
          xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-upgrade-offline.html#sec-update-registersystem">
         <citetitle>&upguide;</citetitle> for &sls; &productnumber;</link>.
       </para>
      </step>
    </procedure>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-sbd">
  <title>Using SBD as fencing mechanism</title>
  <!-- For more information, see:
   * https://trello.com/c/rclDZHPR
   * http://www.linux-ha.org/wiki/SBD_Fencing
  -->
   <!--taroth 2016-07-21: the following is copied from
    /usr/sbin/ha-cluster-init: If you have shared storage, for example a SAN or
    iSCSI target, you can use it to avoid split brain scenarios-->
   <para>
    If you have shared storage, for example, a SAN (Storage Area Network),
    you can use it to avoid split brain scenarios. To do so, configure SBD
    as node fencing mechanism. SBD uses watchdog support
    and the <literal>external/sbd</literal> &stonith; resource agent.
   </para>

  <sect2 xml:id="sec-ha-inst-quick-sbd-req">
   <title>Requirements for SBD</title>
   <para>
    During setup of the first node with <command>crm cluster init</command>, you
    can decide whether to use SBD. If yes, you need to enter the path to the shared
    storage device. By default, <command>crm cluster init</command> will automatically
    create a small partition on the device to be used for SBD.
   </para>
   <para>To use SBD, the following requirements must be met:</para>

   <itemizedlist>
    <listitem>
     <para>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
     </para>
    </listitem>
    <listitem>
     <para> The SBD device <emphasis>must not</emphasis>
      use host-based RAID, LVM2, nor reside on a DRBD* instance.
     </para>
    </listitem>
   </itemizedlist>

  <para>
   For details of how to set up shared storage, refer to the
   <link
    xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html">
    <citetitle>&storage;</citetitle> for &sls; &productnumber;</link>.
  </para>
  <!--<remark>toms, 2016-07-30: (kai) miss a link to set up FC storage</remark>
  <remark>toms 2016-08-01: we don't have yet doc for FC storage.</remark>
  <variablelist>
   <varlistentry>
    <term>iSCSI</term>
    <listitem>
     <para><link xlink:href="https://www.suse.com/doc/sles-12/stor_admin/data/cha_iscsi.html"/></para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>FCoE</term>
   <listitem>
    <para><link xlink:href="https://www.suse.com/doc/sles-12/stor_admin/data/cha_fcoe.html"/></para>
   </listitem>
   </varlistentry>
   <varlistentry>
    <term>FC</term>
    <listitem>
     <para><remark>taroth 2016-08-04: the following was based a proposal by kdupke,
     replace with something more appropriate in the future (as soon as we have
     covered this in the storage guide)</remark>
      No special configuration needed, configure it as other FC storage.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>-->
  </sect2>

  <sect2 xml:id="sec-ha-inst-quick-sbd-setup">
   <title>Enabling the softdog watchdog for SBD</title>
   <!--Taken from ha_storage_protection.xml (pro.ha.storage.protect.watchdog)?-->
   <para>
    In &sls;, watchdog support in the kernel is enabled by default: It ships
    with several kernel modules that provide hardware-specific
    watchdog drivers. The &hasi; uses the SBD daemon as the software component
    that <quote>feeds</quote> the watchdog.
   </para>
   <para>
    The following procedure uses the <systemitem>softdog</systemitem>
    watchdog.
   </para>

   <!-- Softdog Limitations -->
   &important-softdog-limit;

   <procedure xml:id="pro-ha-inst-quick-sbd-setup">
    <step>
     <para>
      Create a persistent, shared storage as described in <xref
       linkend="sec-ha-inst-quick-sbd-req"/>.
     </para>
    </step>
    <step>
     <para>
      Enable the softdog watchdog:
     </para>
     <!-- See /usr/lib/ha-cluster-functions -->
     <screen>&prompt.root;<command>echo</command> softdog > /etc/modules-load.d/watchdog.conf
&prompt.root;<command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
      <!--for the records: <remark>toms 2016-08-09: krig: "The script uses wdctl to check for the
      watchdog, so it should catch the case where the node is created but no
      module is loaded. However, I am not sure it'll be able to spot the case
      where the wrong module is loaded." (ha-devel, 2016-08-08)</remark>-->
     <para>Test if the softdog module is loaded correctly:
     </para>
     <screen>&prompt.root;<command>lsmod</command> | grep dog
softdog                16384  1</screen>
    </step>
  </procedure>

   <remark>toms 2018-04-05: we need to add a bit more info here how you do
    the tests and what to do when it fails.
    However, this needs some further info from our developers. Some info can
    be found in ha_storage_protection.xml.
    Usually it boils down to "sbd -d DEV list" and "sbd -d DEV message bob test"
   </remark>
   <para>
    We highly recommend to test the SBD fencing mechanism for proper function
    to prevent a split scenario. Such a test can be done by blocking the &corosync;
    cluster communication.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-1st-node">
  <title>Setting up the first node</title>
   <para>
   Set up the first node with the <command>crm cluster init</command> script.
   This requires only a minimum of time and manual intervention.
  </para>

  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-init">
   <title>Setting up the first node (<systemitem class="server">&node1;</systemitem>) with
    <command>crm cluster init</command></title>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine to
     use as cluster node.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
    <screen>&prompt.root;<command>crm cluster init</command> --name <replaceable>CLUSTERNAME</replaceable></screen>
    <para>Replace the <replaceable>CLUSTERNAME</replaceable>
     placeholder with a meaningful name, like the geographical location of your
     cluster (for example, <literal>&cluster1;</literal>).
     This is especially helpful to create a &geo; cluster later on,
     as it simplifies the identification of a site.
    </para>
    <remark>toms 2016-08-11: the following para relates to
     (a) FATE#320604 allow unicast,
     (b) FATE#320605 identify AWS and use unicast</remark>
    <para>
     If you need unicast instead of multicast (the default) for your cluster
     communication, use the option <option>-u</option>. After installation,
     find the value <literal>udpu</literal> in the file
     <filename>/etc/corosync/corosync.conf</filename>.
     If <command>crm cluster init</command> detects a node running on
     Amazon Web Services (AWS), the script will use unicast automatically as
     default for cluster communication.
    </para>
    <para>
     The scripts checks for NTP configuration and a hardware watchdog service.
     It generates the public and private SSH keys used for SSH access and
     &csync; synchronization and starts the respective services.
    </para>
    <!--<itemizedlist>
     <listitem>
      <para>Checks for a hardware watchdog device.</para>
     </listitem>
     <listitem>
      <para>
       Generate the public and private SSH keys, used for SSH access and
       &csync; syncronization.
      </para>
     </listitem>
     <listitem>
      <para>Start the SSH and &csync; services.</para>
     </listitem>
    </itemizedlist>-->
   </step>
   <step>
    <para>
     Configure the cluster communication layer (&corosync;):
    </para>
    <substeps>
     <step>
      <para>
       Enter a network address to bind to. By default, the script will
       propose the network address of <systemitem>eth0</systemitem>.
       Alternatively, enter a different network address, for example the
       address of <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast address. The script proposes a random address that
       you can use as default. Of course, your particular network needs to
       support this multicast address.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast port. The script proposes <literal>5405</literal>
       as default.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
    Set up SBD as node fencing mechanism:</para>
    <substeps>
     <step>
      <para>Confirm with <literal>y</literal> that you want to use SBD.</para>
     </step>
     <step>
      <para>Enter a persistent path to the partition of your block device that
       you want to use for SBD, see <xref linkend="sec-ha-inst-quick-sbd"/>.
       The path must be consistent across all nodes in the cluster.</para>
     </step>
    </substeps>
   </step>
   <step xml:id="st-crm-cluster-init-ip">
    <!-- taroth 2015-09-22: fate#318549: cluster-init: configure virtual IP for HAWK  -->
    <para>Configure a virtual IP address for cluster administration with
    &hawk2;. (We will use this virtual IP resource for testing successful
    failover later on).</para>
    <substeps>
     <step>
      <para>Confirm with <literal>y</literal> that you want to configure a
      virtual IP address.</para></step>
     <step>
      <para>Enter an unused IP address that you want to use as administration IP
       for &hawk2;: <literal>&subnetII;.1</literal>
      </para>
      <para>Instead of logging in to an individual cluster node with &hawk2;,
       you can connect to the virtual IP address.</para>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   Finally, the script will start the &pace; service to bring the
   cluster online and enable &hawk2;. The URL to use for &hawk2; is
   displayed on the screen.
  </para>

  <para>
   You now have a running one-node cluster. To view its status, proceed as follows:
  </para>
  <procedure xml:id="pro-ha-inst-quick-hawk2-login">
   <title>Logging in to the &hawk2; Web interface</title>
   <step>
    <para> On any machine, start a Web browser and make sure that JavaScript and
     cookies are enabled. </para>
   </step>
   <step>
    <para> As URL, enter the IP address or host name of any cluster node running
     the &hawk; Web service. Alternatively, enter the address of the virtual
     IP address that you configured in <xref linkend="st-crm-cluster-init-ip"/>
     of <xref linkend="pro-ha-inst-quick-setup-crm-cluster-init"/>: </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
    <note>
     <title>Certificate warning</title>
     <para> If a certificate warning appears when you try to access the URL for
      the first time, a self-signed certificate is in use. Self-signed
      certificates are not considered trustworthy by default. </para>
     <para> Ask your cluster operator for the certificate details to verify the
      certificate. </para>
     <para> To proceed anyway, you can add an exception in the browser to bypass
      the warning. </para>
     <!--FIXME: HAWK1 - see https://bugzilla.suse.com/show_bug.cgi?id=979095
      <para> For information on how to replace the self-signed certificate with a
      certificate signed by an official Certificate Authority, refer to
      <xref linkend="vle-ha-hawk-certificate"/>.</para>-->
    </note>
   </step>
   <step>
    <para> On the &hawk2; login screen, enter the
      <guimenu>Username</guimenu> and <guimenu>Password</guimenu> of the
       user that has been created during the bootstrap procedure (user <systemitem
       class="username">hacluster</systemitem>, password
      <literal>linux</literal>).</para>
    <important>
     <title>Secure password</title>
     <para>Replace the default password with a secure one as soon as possible:
     </para>
     <screen>&prompt.root;<command>passwd</command> hacluster</screen>
    </important>
   </step>
   <step>
    <para>
     Click <guimenu>Log In</guimenu>. After login, the &hawk2; Web interface
     shows the Status screen by default, displaying the current cluster
     status at a glance:
    </para>
    <figure xml:id="fig-ha-inst-quick-one-node-status">
     <title>Status of the one-node cluster in &hawk2;</title>
     <mediaobject>
      <imageobject>
       <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-2nd-node">
  <title>Adding the second node</title>
  <para>
    If you have a one-node cluster up and running, add the second cluster
    node with the <command>crm cluster join</command> bootstrap
    script, as described in <xref linkend="pro-ha-inst-quick-setup-ha-cluster-join"
    xrefstyle="select:label nopage"/>.
    The script only needs access to an existing cluster node and
    will complete the basic setup on the current machine automatically.
    For details, refer to the <command>crm cluster join</command> man page.
  </para>
  <para>
   The bootstrap scripts take care of changing the configuration specific to
   a two-node cluster, for example, SBD, &corosync;.
  </para>
  <procedure xml:id="pro-ha-inst-quick-setup-ha-cluster-join">
   <title>Adding the second node (<systemitem class="server">&node2;</systemitem>) with
    <command>crm cluster join</command></title>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine supposed to
     join the cluster.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen>&prompt.root;<command>crm cluster join</command></screen>
    <para>
     If NTP has not been configured to start at boot time, a message
     appears. The script also checks for a hardware watchdog device (which
     is important in case you want to configure SBD). You are warned if none
     is present.
    </para>
   </step>
   <step>
    <para>
     If you decide to continue anyway, you will be prompted for the IP
     address of an existing node. Enter the IP address of the first node
     (<systemitem class="server">&node1;</systemitem>, <systemitem
      class="ipaddress">&subnetI;.1</systemitem>).
    </para>
   </step>
   <step>
    <para>
     If you have not already configured a passwordless SSH access between
     both machines, you will be prompted for the &rootuser; password
     of the existing node.
    </para>
    <para>
     After logging in to the specified node, the script will copy the
     &corosync; configuration, configure SSH, &csync;, and will
     bring the current machine online as new cluster node. Apart from that,
     it will start the service needed for &hawk2;. <!--
     If you have configured shared storage with OCFS2, it will also
     automatically create the mount point directory for the OCFS2 file system.
     -->
    </para>
   </step>
  </procedure>
  <para>
   Check the cluster status in &hawk2;. Under <menuchoice>
    <guimenu>Status</guimenu>
    <guimenu>Nodes</guimenu>
   </menuchoice> you should see two nodes with a green status (see
   <xref linkend="fig-ha-inst-quick-two-node-cluster"/>).
  </para>

  <figure xml:id="fig-ha-inst-quick-two-node-cluster">
   <title>Status of the two-node cluster</title>
   <mediaobject>
    <imageobject>
     <imagedata width="80%" fileref="installquick-two-nodecluster-status.png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-test">
   <title>Testing the cluster</title>
   <para>
    <xref linkend="sec-ha-inst-quick-test-resource-failover"/> is a simple test to check if the
    cluster moves the virtual IP address to the other node in case the node
    that currently runs the resource is set to <literal>standby</literal>.
   </para>
   <para>However, a realistic test involves specific use cases and scenarios,
    including testing of your fencing mechanism to avoid a split brain
    situation. If you have not set up your fencing mechanism correctly, the cluster
    will not work  properly.</para>
   <para>Before using the cluster in a production environment, test it thoroughly
    according to your use cases or with the help of the &cmd.test.script;
    script.
    <!-- toms 2016-08-01: For SP3, refer to the "Testing Cluster Guide" or
     to Lars Pinnes iptable rules
    -->
   </para>
   <sect2 xml:id="sec-ha-inst-quick-test-resource-failover">
    <title>Testing resource failover</title>
    <para>
     As a quick test, the following procedure checks on resource failovers:
    </para>
   <remark>toms 2016-07-27: Fate#321073
    Tool for Standardize Testing of Basic Cluster Functionality</remark>
   <procedure xml:id="pro-ha-inst-quick-test">
    <title>Testing resource failover</title>
    <step>
     <para>
      Open a terminal and ping <systemitem>&subnetII;.1</systemitem>,
      your virtual IP address:
     </para>
     <screen>&prompt.root;<command>ping</command> &subnetII;.1</screen>
    </step>
    <step>
     <para>
      Log in to your cluster as described in <xref
       linkend="pro-ha-inst-quick-hawk2-login"/>.
     </para>
    </step>
    <step>
     <para>
      In &hawk2; <menuchoice>
       <guimenu>Status</guimenu>
       <guimenu>Resources</guimenu>
      </menuchoice>,
      check which node the virtual IP address (resource
      <systemitem>admin_addr</systemitem>) is running on. <!--As we did not
      configure any preferences, it will be an arbitrary node.-->
      We assume the resource is running on <systemitem class="server">&node1;</systemitem>.
      </para>
    </step>
    <step>
     <para>
      Put <systemitem class="server">&node1;</systemitem> into
      <guimenu>Standby</guimenu> mode (see <xref linkend="fig-ha-inst-quick-standby"/>).
     </para>
     <figure xml:id="fig-ha-inst-quick-standby">
      <title>Node <systemitem class="server">&node1;</systemitem> in standby mode</title>
      <mediaobject>
       <imageobject>
        <imagedata width="60%" fileref="installquick-standby-node.png"/>
       </imageobject>
      </mediaobject>
     </figure>
     <!--
     <remark>toms 2016-07-18: Should we add an alternative method? I've tested
      it with "crm node standby NODE" to put the node "offline" temporarily.
      Sometimes an administrator doesn't want to go to a server room, so it
      could be more convenient.
     </remark>
     <remark>taroth 2016-07-20: good idea, need to discuss it with krig</remark>-->
    </step>
    <step>
     <para>
      Click <menuchoice>
       <guimenu>Status</guimenu>
       <guimenu>Resources</guimenu>
      </menuchoice>. The resource <systemitem>admin_addr</systemitem>
      has been migrated to <systemitem class="server">&node2;</systemitem>.
     </para>
    </step>
   </procedure>
   <para>
    During the migration, you should see an uninterrupted flow of pings to
    the virtual IP address. This shows that the cluster setup and the floating
    IP work correctly. Cancel the <command>ping</command> command with
    <keycombo>
       <keycap function="control"/><keycap>C</keycap>
      </keycombo>.
   </para>
   </sect2>

   <sect2 xml:id="sec-ha-inst-quick-test-with-cluster-script">
    <title>Testing with the &test.script; command</title>
    <para>
     The command &cmd.test.script; runs standardized tests
     for a cluster.
     It triggers cluster failures and verifies configuration to find
     problems. Before you use your cluster in production, it is
     recommended to use this command to make sure everything works as expected.
    </para>
    <para>
     The command supports the following checks:
    </para>
    <itemizedlist>
     <listitem>
      <formalpara>
       <title>Environment check <option>-e</option>/<option>--env-check</option></title>
       <para>
        This test checks:
       </para>
      </formalpara>
      <itemizedlist>
       <listitem>
        <para> Are host names resolvable? </para>
       </listitem>
       <listitem>
        <para>
         Is the time service enabled and started?
        </para>
       </listitem>
       <listitem>
        <para>
         Does the current node have a watchdog configured?
        </para>
       </listitem>
       <listitem>
        <para>
         Is the <command>firewalld</command> service started and are cluster-related ports
         open?
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <formalpara>
       <title>Cluster state check <option>-c</option>/<option>--cluster-check</option></title>
       <para> Checks different states and services of the cluster. This test checks:</para>
      </formalpara>
      <itemizedlist>
       <listitem>
        <para>
         Are cluster services (&pace;/&corosync;) enabled and running?
        </para>
       </listitem>
       <listitem>
        <para>
         Is &stonith; enabled? It also checks, whether &stonith;-related
         resources are configured and started.
         If you configured SBD, is the SBD service started?
        </para>
       </listitem>
       <listitem>
        <para>
         Does the cluster have a quorum?
         Shows current DC nodes and nodes which are online,
         offline, and unclean.
        </para>
       </listitem>
       <listitem>
        <para>
         Do we have started, stopped, or failed resources?
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <formalpara>
       <title>Split brain check <option>--split-brain-iptables</option></title>
       <para>
        Simulates a split brain scenario by blocking the &corosync; port.
        Checks whether one node can be fenced as expected.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Kills the daemons for SBD, &corosync;, and &pace; <option>-kill-sbd</option>/<option>-kill-corosync</option>/<option>-kill-pacemakerd</option></title>
       <para>
        After running such a test, you can find a report in
        <filename>/var/lib/ha-cluster-preflight-check</filename>.
        The report includes test case description, action logging,
        and explains possible results.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Fence node check <option>--fence-node</option></title>
       <para>Fences the specific node passed from the command line.</para>
      </formalpara>
     </listitem>
    </itemizedlist>
    <para>
     For example, to test the environment, run:
    </para>
    <screen>&prompt.root;<command>crm_mon -1</command>
Stack: corosync
Current DC: &node1; (version ...) - partition with quorum
Last updated: Fri Mar 03 14:40:21 2020
Last change: Fri Mar 03 14:35:07 2020 by root via cibadmin on &node1;

2 nodes configured
1 resource configured

Online: [ &node1; &node2; ]
Active resources:

 stonith-sbd    (stonith:external/sbd): Started &node1;

&prompt.root;&cmd.test.script; -e
[2020/03/20 14:40:45]INFO: Checking hostname resolvable [Pass]
[2020/03/20 14:40:45]INFO: Checking time service [Fail]
 INFO: chronyd.service is available
 WARNING: chronyd.service is disabled
 WARNING: chronyd.service is not active
[2020/03/20 14:40:45]INFO: Checking watchdog [Pass]
[2020/03/20 14:40:45]INFO: Checking firewall [Fail]
 INFO: firewalld.service is available
 WARNING: firewalld.service is not active</screen>
    <para>
     You can inspect the result in <filename>/var/log/ha-cluster-preflight-check.log</filename>.
    </para>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-moreinfo">
   <title>For more information</title>
    <para>
     More documentation for this product is available at
     <link
      xlink:href="https://documentation.suse.com/sle-ha/"/>.
     For further configuration and administration tasks, see the comprehensive
     <link
      xlink:href="https://documentation.suse.com/sle-ha/html/SLE-HA-all/book-sleha-guide.html">
      <citetitle>&admin;</citetitle></link>.
    </para>
   </sect1>
 <xi:include href="common_copyright_quick.xml"/>
 <xi:include href="common_legal.xml"/>
</article>
