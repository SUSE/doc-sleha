<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<article version="5.0" xml:lang="en" xml:id="article-installation"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:its="http://www.w3.org/2005/11/its"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&haquick;</title>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <date><?dbtimestamp format="B d, Y"?></date>
  <xi:include href="common_copyright_gfdl.xml"/>
  <abstract>
   <para>
    &abstract-haquick;
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <meta name="title" its:translate="yes">&haquick;</meta>
  <meta name="series" its:translate="no">Products &amp; Solutions</meta>
  <meta name="description" its:translate="yes">How to set up a basic two-node cluster, using the bootstrap scripts provided by the &crmshell;</meta>
  <meta name="social-descr" its:translate="yes">Set up a basic two-node cluster</meta>
  <meta name="task" its:translate="no">
    <phrase>Installation</phrase>
    <phrase>Administration</phrase>
    <phrase>Clustering</phrase>
  </meta>
  <revhistory xml:id="rh-article-installation">
    <revision>
     <date>2023-06-20</date>
      <revdescription>
        <para>
          Updated for the initial release of &productname; &productnumber;.
        </para>
      </revdescription>
    </revision>
   </revhistory>
  </info>

  <sect1 xml:id="sec-ha-inst-quick-usage-scenario">
   <title>Usage scenario</title>
   <para>
    The procedures in this document will lead to a minimal setup of a two-node
    cluster with the following properties:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Two nodes: <systemitem class="server">&node1;</systemitem> (IP: <systemitem
       class="ipaddress">&subnetI;.1</systemitem>)
      and <systemitem class="server">&node2;</systemitem> (IP: <systemitem
       class="ipaddress">&subnetI;.2</systemitem>),
      connected to each other via network.
     </para>
    </listitem>
    <listitem>
     <para>
      A floating, virtual IP address (<systemitem class="ipaddress">&subnetI;.10</systemitem>)
      that allows clients to connect to the service no matter which node it is running on.
      This IP address is used to connect to the graphical management tool &hawk2;.
     </para>
    </listitem>
    <listitem>
     <para>A shared storage device, used as SBD fencing mechanism.
      This avoids split-brain scenarios.
     </para>
    </listitem>
    <listitem>
     <para>
      Failover of resources from one node to the other if the active host breaks
      down (<emphasis>active/passive</emphasis> setup).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    You can use the two-node cluster for testing purposes or as a minimal
    cluster configuration that you can extend later on. Before using the
    cluster in a production environment, see <xref linkend="book-administration"/>
    to modify the cluster according to your requirements.
   </para>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-req">
   <title>System requirements</title>
   <para>
    This section informs you about the key system requirements for the
    scenario described in <xref linkend="sec-ha-inst-quick-usage-scenario"
    xrefstyle="select:label"/>.
    To adjust the cluster for use in a production environment,
    refer to the full list in <xref linkend="cha-ha-requirements"/>.
   </para>

  <sect2 xml:id="vl-ha-inst-quick-req-hw">
   <title>Hardware requirements</title>
   <variablelist>
    <varlistentry>
     <term>Servers</term>
     <listitem>
      <para>
       Two servers with software as specified in <xref linkend="il-ha-inst-quick-req-sw"/>.
      </para>
      &sys-req-hw-nodes;
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Communication channels</term>
     <listitem> &sys-req-hw-comm-channels; </listitem>
    </varlistentry>
    <varlistentry>
     <term>Node fencing/&stonith;</term>
     <listitem>
      <para>
       A node fencing (&stonith;) device to avoid split-brain scenarios. This can
       be either a physical device (a power switch) or a mechanism like SBD
       (&stonith; by disk) in combination with a watchdog. SBD can be used either
       with shared storage or in diskless mode. This document describes using SBD
       with shared storage. The following requirements must be met:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         A shared storage device. For information on setting up shared storage, see
         <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html">
         &storage_guide; for &sls;</link>. If you only need basic shared storage for testing
         purposes, see <xref linkend="ha-iscsi-for-sbd"/>.
        </para>
       </listitem>
       <listitem>
        <para>The path to the shared storage device must be persistent and
         consistent across all nodes in the cluster. Use stable device names
         such as <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
        </para>
       </listitem>
       <listitem>
        <para> The SBD device <emphasis>must not</emphasis>
         use host-based RAID, LVM, or DRBD*.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       For more information on &stonith;, see <xref linkend="cha-ha-fencing"/>.
       For more information on SBD, see <xref linkend="cha-ha-storage-protect"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
 </sect2>
  <sect2 xml:id="il-ha-inst-quick-req-sw">
   <title>Software requirements</title>
   <para>
   All nodes that will be part of the cluster need at least the following modules
   and extensions:
  </para>

&sys-req-sw-modules;

  </sect2>
  <sect2 xml:id="vl-ha-inst-quick-req-other">
   <title>Other requirements and recommendations</title>
   <variablelist>
    <varlistentry>
     <term>Time synchronization</term> &sys-req-other-ntp; </varlistentry>
    <varlistentry>
     <term>Host name and IP address</term>
     <listitem>
      <itemizedlist>
       <listitem>
        <para> Use static IP addresses. </para>
       </listitem>
       <listitem>
        <para>
         Only the primary IP address is supported.
        </para>
       </listitem>
       <listitem> &sys-req-other-etc-hosts; </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SSH</term>
     <listitem> &sys-req-other-ssh; <para> If you use the bootstrap scripts for setting up the
       cluster, the SSH keys will automatically be created and copied. </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-bootstrap">
  <title>Overview of the bootstrap scripts</title>
  <para>
   The following commands execute bootstrap scripts that require only a minimum of time and manual
   intervention.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     With <command>crm cluster init</command>, define the basic parameters needed
     for cluster communication. This leaves you with a running one-node cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>crm cluster join</command>, add more nodes to your cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>crm cluster remove</command>, remove nodes from your cluster.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The options set by the bootstrap scripts might not be the same as the &pace;
   default settings. You can check which settings the bootstrap scripts changed
   in <filename>/var/log/crmsh/crmsh.log</filename>. Any options set
   during the bootstrap process can be modified later with the
   &yast; cluster module. See <xref linkend="cha-ha-ycluster"/>
   for details.
  </para>
  <para>
   The bootstrap script <command>crm cluster init</command> checks and
   configures the following components:
  </para>

  <variablelist>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      Checks if NTP is configured to start at boot time. If not, a message appears.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>Creates SSH keys for passwordless login between cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&csync;</term>
    <listitem>
     <para>
      Configures &csync; to replicate configuration files across all nodes
      in a cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&corosync;</term>
    <listitem>
     <para>Configures the cluster communication system.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD/watchdog</term>
    <listitem>
     <para>Checks if a watchdog exists and asks you whether to configure SBD
      as node fencing mechanism.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual floating IP</term>
    <listitem>
     <para>Asks you whether to configure a virtual IP address for cluster
      administration with &hawk2;.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Firewall</term>
    <listitem>
     <para>Opens the ports in the firewall that are needed for cluster communication.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster name</term>
    <listitem>
     <para>Defines a name for the cluster, by default
        <systemitem>hacluster</systemitem>. This
      is optional and mostly useful for &geo; clusters. Usually, the cluster
      name reflects the geographical location and makes it easier to distinguish a site
      inside a &geo; cluster.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&qdevice;/&qnet;</term>
    <listitem>
     <para>
      Asks you whether to configure &qdevice;/&qnet; to participate in
      quorum decisions. We recommend using &qdevice; and &qnet; for clusters
      with an even number of nodes, and especially for two-node clusters.
     </para>
     <para>
      This configuration is not covered here, but you can set it up later as
      described in <xref linkend="cha-ha-qdevice"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <note>
    <title>Cluster configuration for different platforms</title>
    <para>
      The <command>crm cluster init</command> script detects the system environment (for example,
      &ms; Azure) and adjusts certain cluster settings based on the profile for that environment.
      For more information, see the file <filename>/etc/crm/profiles.yml</filename>.
    </para>
  </note>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-installation">
    <title>Installing the &ha; packages</title>
    <para>
      The packages for configuring and managing a cluster
      are included in the <literal>&ha;</literal> installation pattern.
      This pattern is only available after the &productname; is installed.
    </para>
    <para>
      You can register to the &scc; and install &productname; while installing &sles;,
      or after installation. For more information, see the
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-register-sle.html">
     &deploy;</link> for &sles;.
    </para>
    <procedure xml:id="pro-ha-inst-quick-pattern">
      <title>Installing the &ha; pattern</title>
      <step>
       <para>
        Install the &ha; pattern from the command line:</para>
<screen>&prompt.root;<command>zypper install -t pattern ha_sles</command></screen>
      </step>
      <step>
       <para>
          Install the &ha; pattern on <emphasis>all</emphasis> machines that
          will be part of your cluster.
       </para>
       <note>
        <title>Installing software packages on all nodes</title>
        <para>
          For an automated installation of &sls; &productnumber; and &productname; &productnumber;,
          use &ay; to clone existing nodes. For more
         information, see <xref linkend="sec-ha-installation-autoyast"/>.
        </para>
       </note>
      </step>
    </procedure>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-sbd">
  <title>Using SBD for node fencing</title>
   <para>
    Before you can configure SBD with the bootstrap script, you must enable a watchdog
    on each node. &sls; ships with several kernel modules that provide hardware-specific
    watchdog drivers. &productname; uses the SBD daemon as the software component
    that <quote>feeds</quote> the watchdog.
   </para>
   <para>
    The following procedure uses the <systemitem>softdog</systemitem>
    watchdog.
   </para>

   <!-- Softdog Limitations -->
   &important-softdog-limit;

   <procedure xml:id="pro-ha-inst-quick-sbd-setup">
    <title>Enabling the softdog watchdog for SBD</title>
    <step>
     <para>
      On each node, enable the softdog watchdog:
     </para>
     <!-- See /usr/lib/ha-cluster-functions -->
     <screen>&prompt.root;<command>echo softdog > /etc/modules-load.d/watchdog.conf</command>
&prompt.root;<command>systemctl restart systemd-modules-load</command></screen>
    </step>
    <step>
     <para>Test if the softdog module is loaded correctly:
     </para>
     <screen>&prompt.root;<command>lsmod | grep dog</command>
softdog           16384  1</screen>
    </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-1st-node">
  <title>Setting up the first node</title>
   <para>
   Set up the first node with the <command>crm cluster init</command> script.
   This requires only a minimum of time and manual intervention.
  </para>

  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-init">
   <title>Setting up the first node (<systemitem class="server">&node1;</systemitem>) with
    <command>crm cluster init</command></title>
   <step>
    <para>
     Log in to the first cluster node as &rootuser;, or as a user with
     <command>sudo</command> privileges.
    </para>
    <important>
     <title><command>sudo</command> user SSH key access</title>
     <para>
      The cluster uses passwordless SSH access for communication between the nodes.
      The <command>crm cluster init</command> script checks for SSH keys and generates
      them if they do not already exist.
     </para>
     <para>
      If you intend to set up the first node as a user with <command>sudo</command> privileges,
      you must ensure the user's SSH keys exist (or will be generated) locally on the node,
      not on a remote system.
     </para>
    </important>
   </step>
   <step>
    <para>
     Start the bootstrap script:
    </para>
    <screen>&prompt.root;<command>crm cluster init --name <replaceable>CLUSTERNAME</replaceable></command></screen>
    <para>Replace the <replaceable>CLUSTERNAME</replaceable>
     placeholder with a meaningful name, like the geographical location of your
     cluster (for example, <literal>&cluster1;</literal>).
     This is especially helpful to create a &geo; cluster later on,
     as it simplifies the identification of a site.
    </para>
    <para>
     If you need to use multicast instead of unicast (the default) for your cluster
     communication, use the option <option>--multicast</option> (or <option>-U</option>).
    </para>
    <para>
     The script checks for NTP configuration and a hardware watchdog service.
     If required, it generates the public and private SSH keys used for SSH access and
     &csync; synchronization and starts the respective services.
    </para>
   </step>
   <step>
    <para>
     Configure the cluster communication layer (&corosync;):
    </para>
    <substeps>
     <step>
      <para>
       Enter a network address to bind to. By default, the script
       proposes the network address of <systemitem>eth0</systemitem>.
       Alternatively, enter a different network address, for example the
       address of <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Accept the proposed port (<literal>5405</literal>) or enter a different one.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
    Set up SBD as the node fencing mechanism:</para>
    <substeps>
     <step>
      <para>Confirm with <literal>y</literal> that you want to use SBD.</para>
     </step>
     <step>
      <para>Enter a persistent path to the partition of your block device that
       you want to use for SBD.
       The path must be consistent across all nodes in the cluster.</para>
     </step>
    </substeps>
   </step>
   <step xml:id="st-crm-cluster-init-ip">
    <para>Configure a virtual IP address for cluster administration with &hawk2;:</para>
    <substeps>
     <step>
      <para>Confirm with <literal>y</literal> that you want to configure a
      virtual IP address.</para></step>
     <step>
      <para>Enter an unused IP address that you want to use as administration IP
       for &hawk2;: <literal>&subnetI;.10</literal>
      </para>
      <para>Instead of logging in to an individual cluster node with &hawk2;,
       you can connect to the virtual IP address.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Choose whether to configure &qdevice; and &qnet;. For the minimal setup
     described in this document, decline with <literal>n</literal> for now.
     You can set up &qdevice; and &qnet; later, as described in
     <xref linkend="cha-ha-qdevice"/>.
    </para>
   </step>
  </procedure>
  <para>
   Finally, the script will start the cluster services to bring the
   cluster online and enable &hawk2;. The URL to use for &hawk2; is
   displayed on the screen.
  </para>

  <para>
   You now have a running one-node cluster. To view its status, proceed as follows:
  </para>
  <procedure xml:id="pro-ha-inst-quick-hawk2-login">
   <title>Logging in to the &hawk2; Web interface</title>
   <step>
    <para> On any machine, start a Web browser and make sure that JavaScript and
     cookies are enabled. </para>
   </step>
   <step>
    <para>As URL, enter the virtual IP address that you configured with the bootstrap script:</para>
    <screen>https://&subnetI;.10:7630/</screen>
    <note>
     <title>Certificate warning</title>
     <para> If a certificate warning appears when you try to access the URL for
      the first time, a self-signed certificate is in use. Self-signed
      certificates are not considered trustworthy by default. </para>
     <para> Ask your cluster operator for the certificate details to verify the
      certificate. </para>
     <para> To proceed anyway, you can add an exception in the browser to bypass
      the warning. </para>
    </note>
   </step>
   <step>
    <para> On the &hawk2; login screen, enter the
      <guimenu>Username</guimenu> and <guimenu>Password</guimenu> of the
       user that was created by the bootstrap script (user <systemitem
       class="username">hacluster</systemitem>, password
      <literal>linux</literal>).</para>
    <important>
     <title>Secure password</title>
     <para>Replace the default password with a secure one as soon as possible:
     </para>
     <screen>&prompt.root;<command>passwd hacluster</command></screen>
    </important>
   </step>
   <step>
    <para>
     Click <guimenu>Log In</guimenu>. The &hawk2; Web interface
     shows the Status screen by default:
    </para>
    <figure xml:id="fig-ha-inst-quick-one-node-status">
     <title>Status of the one-node cluster in &hawk2;</title>
     <mediaobject>
      <imageobject>
       <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-2nd-node">
  <title>Adding the second node</title>
  <para>
    Add a second node to the cluster with the <command>crm cluster join</command>
    bootstrap script.
    The script only needs access to an existing cluster node, and
    completes the basic setup on the current machine automatically.
  </para>
  <para>
   For more information, see the <command>crm cluster join --help</command> command.
  </para>
  <procedure xml:id="pro-ha-inst-quick-setup-crm-cluster-join">
   <title>Adding the second node (<systemitem class="server">&node2;</systemitem>) with
    <command>crm cluster join</command></title>
   <step>
    <para>
     Log in to the second node as &rootuser;, or as a user with
     <command>sudo</command> privileges.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script:
    </para>
    <para>
     If you set up the first node as &rootuser;, you can run this command with
     no additional parameters:
    </para>
<screen>&prompt.root;<command>crm cluster join</command></screen>
    <para>
     If you set up the first node as a <command>sudo</command> user, you must
     specify that user with the <option>-c</option> option:
    </para>
<screen>&prompt.user;<command>sudo crm cluster join -c <replaceable>USER</replaceable>@&node1;</command></screen>
    <para>
     If NTP is not configured to start at boot time, a message
     appears. The script also checks for a hardware watchdog device.
     You are warned if none is present.
    </para>
   </step>
   <step>
    <para>
     If you did not already specify <systemitem class="server">&node1;</systemitem>
     with <option>-c</option>, you will be prompted for the IP address of the first node.
    </para>
   </step>
   <step>
    <para>
     If you did not already configure passwordless SSH access between
     both machines, you will be prompted for the password of the first node.
    </para>
    <para>
     After logging in to the specified node, the script copies the
     &corosync; configuration, configures SSH and &csync;,
     brings the current machine online as a new cluster node, and
     starts the service needed for &hawk2;. <!--
     If you have configured shared storage with OCFS2, it will also
     automatically create the mount point directory for the OCFS2 file system.
     -->
    </para>
   </step>
  </procedure>
  <para>
   Check the cluster status in &hawk2;. Under <menuchoice>
    <guimenu>Status</guimenu>
    <guimenu>Nodes</guimenu>
   </menuchoice> you should see two nodes with a green status:
  </para>

  <figure xml:id="fig-ha-inst-quick-two-node-cluster">
   <title>Status of the two-node cluster</title>
   <mediaobject>
    <imageobject>
     <imagedata width="80%" fileref="installquick-two-nodecluster-status.png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-test">
   <title>Testing the cluster</title>
   <para>
    The following tests can help you identify issues with the cluster setup.
    However, a realistic test involves specific use cases and scenarios.
    Before using the cluster in a production environment, test it thoroughly
    according to your use cases.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The command <command>sbd -d <replaceable>DEVICE_NAME</replaceable> list</command>
      lists all the nodes that are visible to SBD. For the setup described in
      this document, the output should show both <systemitem class="server">&node1;</systemitem>
      and <systemitem class="server">&node2;</systemitem>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec-ha-inst-quick-test-resource-failover"/> is a simple test
      to check if the cluster moves the virtual IP address to the other node if
      the node that currently runs the resource is set to <literal>standby</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec-ha-inst-quick-test-with-cluster-script"/> simulates cluster
      failures and reports the results.
     </para>
    </listitem>
   </itemizedlist>
   <sect2 xml:id="sec-ha-inst-quick-test-resource-failover">
    <title>Testing resource failover</title>
    <para>
     As a quick test, the following procedure checks on resource failovers:
    </para>
   <procedure xml:id="pro-ha-inst-quick-test">
    <title>Testing resource failover</title>
    <step>
     <para>
      Open a terminal and ping <systemitem>&subnetI;.10</systemitem>,
      your virtual IP address:
     </para>
     <screen>&prompt.root;<command>ping &subnetI;.10</command></screen>
    </step>
    <step>
     <para>
      Log in to &hawk2;.
     </para>
    </step>
    <step>
     <para>
      Under <menuchoice><guimenu>Status</guimenu><guimenu>Resources</guimenu></menuchoice>,
      check which node the virtual IP address (resource
      <systemitem>admin-ip</systemitem>) is running on.
      This procedure assumes the resource is running on <systemitem class="server">&node1;</systemitem>.
      </para>
    </step>
    <step>
     <para>
      Put <systemitem class="server">&node1;</systemitem> into
      <guimenu>Standby</guimenu> mode:
     </para>
     <figure xml:id="fig-ha-inst-quick-standby">
      <title>Node <systemitem class="server">&node1;</systemitem> in standby mode</title>
      <mediaobject>
       <imageobject>
        <imagedata width="60%" fileref="installquick-standby-node.png"/>
       </imageobject>
      </mediaobject>
     </figure>
     <!--
     <remark>toms 2016-07-18: Should we add an alternative method? I've tested
      it with "crm node standby NODE" to put the node "offline" temporarily.
      Sometimes an administrator doesn't want to go to a server room, so it
      could be more convenient.
     </remark>
     <remark>taroth 2016-07-20: good idea, need to discuss it with krig</remark>-->
    </step>
    <step>
     <para>
      Click <menuchoice>
       <guimenu>Status</guimenu>
       <guimenu>Resources</guimenu>
      </menuchoice>. The resource <systemitem>admin-ip</systemitem>
      has been migrated to <systemitem class="server">&node2;</systemitem>.
     </para>
    </step>
   </procedure>
   <para>
    During the migration, you should see an uninterrupted flow of pings to
    the virtual IP address. This shows that the cluster setup and the floating
    IP work correctly. Cancel the <command>ping</command> command with
    <keycombo>
       <keycap function="control"/><keycap>C</keycap>
      </keycombo>.
   </para>
   </sect2>

   <sect2 xml:id="sec-ha-inst-quick-test-with-cluster-script">
    <title>Testing with the &cmd.test.script; command</title>
    <para>
     The command &cmd.test.script; triggers cluster failures to find
     problems. Before you use your cluster in production, it is
     recommended to use this command to make sure everything works as expected.
    </para>
    <para>
     The command supports the following checks:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>--split-brain-iptables</option></term>
      <listitem>
       <para>
        Simulates a split-brain scenario by blocking the &corosync; port.
        Checks whether one node can be fenced as expected.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--kill-sbd</option>/<option>--kill-corosync</option>/
       <option>--kill-pacemakerd</option></term>
      <listitem>
       <para>
        Kills the daemons for SBD, &corosync;, and &pace;.
        After running one of these tests, you can find a report in the
        directory <filename>/var/lib/crmsh/crash_test/</filename>.
        The report includes a test case description, action logging,
        and an explanation of possible results.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--fence-node <replaceable>NODE</replaceable></option></term>
      <listitem>
       <para>
        Fences a specific node passed from the command line.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For more information, see <command>crm cluster crash_test --help</command>.
    </para>
    <example xml:id="ex-test-with-cluster-script">
     <title>Testing the cluster: node fencing</title>
<screen>&prompt.root;<command>crm_mon -1</command>
Stack: corosync
Current DC: &node1; (version ...) - partition with quorum
Last updated: Fri Mar 03 14:40:21 2020
Last change: Fri Mar 03 14:35:07 2020 by root via cibadmin on &node1;

2 nodes configured
1 resource configured

Online: [ &node1; &node2; ]
Active resources:

 stonith-sbd    (stonith:external/sbd): Started &node1;

&prompt.root;&cmd.test.script;<command> --fence-node &node2;</command>

==============================================
Testcase:          Fence node &node2;
Fence action:      reboot
Fence timeout:     60

!!! WARNING WARNING WARNING !!!
THIS CASE MAY LEAD TO NODE BE FENCED.
TYPE Yes TO CONTINUE, OTHER INPUTS WILL CANCEL THIS CASE [Yes/No](No): <command>Yes</command>
INFO: Trying to fence node "&node2;"
INFO: Waiting 60s for node "&node2;" reboot...
INFO: Node "&node2;" will be fenced by "&node1;"!
INFO: Node "&node2;" was successfully fenced by "&node1;"</screen>
    </example>
    <para>
      To watch <systemitem class="server">&node2;</systemitem> change status during
      the test, log in to &hawk2; and navigate to <menuchoice><guimenu>Status</guimenu>
      <guimenu>Nodes</guimenu></menuchoice>.
     </para>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-next">
   <title>Next steps</title>
   <para>
    The bootstrap scripts provide a quick way to set up a basic &ha; cluster that
    can be used for testing purposes. However, to expand this cluster into a functioning
    &ha; cluster that can be used in production environments, more steps are recommended.
   </para>
   <variablelist xml:id="vl-ha-inst-quick-next-rec">
    <title>Recommended steps to complete the &ha; cluster setup</title>
    <varlistentry>
     <term>Adding more nodes</term>
     <listitem>
      <para>
       Add more nodes to the cluster using one of the following methods:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         For individual nodes, use the <command>crm cluster join</command> script
         as described in <xref linkend="sec-ha-inst-quick-setup-2nd-node"/>.
        </para>
       </listitem>
       <listitem>
        <para>
         For mass installation of multiple nodes, use &ay; as described in
         <xref linkend="sec-ha-installation-autoyast"/>.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       A regular cluster can contain up to 32 nodes. With the &pmremote; service,
       &ha; clusters can be extended to include additional nodes beyond this limit.
       See <xref linkend="article-pacemaker-remote"/> for more details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Enabling a hardware watchdog</term>
     <listitem>
      <para>
       Before using the cluster in a production environment, replace the
       <literal>softdog</literal> module with the hardware module that best fits your
       hardware. For details, see <xref linkend="sec-ha-storage-protect-watchdog"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
      <term>Adding more &stonith; devices</term>
      <listitem>
        <para>
          For critical workloads, we highly recommend using two or three &stonith; devices:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              To continue using SBD, see <xref linkend="cha-ha-storage-protect"/>.
            </para>
          </listitem>
          <listitem>
            <para>
              To use physical &stonith; devices instead, see <xref linkend="cha-ha-fencing"/>.
            </para>
          </listitem>
        </itemizedlist>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Configuring &qdevice;</term>
      <listitem>
       <para>
        If the cluster has an even number of nodes, configure &qdevice; and &qnet; to
        participate in quorum decisions. &qdevice; provides a configurable number of votes,
        allowing a cluster to sustain more node failures than the standard quorum rules allow.
        For details, see <xref linkend="cha-ha-qdevice"/>.
       </para>
      </listitem>
     </varlistentry>
   </variablelist>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-moreinfo">
   <title>For more information</title>
    <para>
     More documentation for this product is available at
     <link
      xlink:href="https://documentation.suse.com/sle-ha/15-SP5/"/>.
     For further configuration and administration tasks, see the comprehensive
     <link
      xlink:href="https://documentation.suse.com/sle-ha/15-SP5/html/SLE-HA-all/book-administration.html">
      &admin;</link>.
    </para>
   </sect1>
 <xi:include href="ha_iscsi_for_sbd.xml"/>
 <xi:include href="common_legal.xml"/>
</article>
