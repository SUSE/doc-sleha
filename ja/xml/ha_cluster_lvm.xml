<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="ha_cluster_lvm.xml" version="5.0" xml:id="cha.ha.clvm">
 <title>Cluster Logical Volume Manager (Cluster LVM)</title>
 <info>
      <abstract>
        <para>
    クラスタ上の共有ストレージを管理する場合、ストレージサブシステムに行った変更を各ノードに伝える必要があります。Logical Volume Manager 2 (LVM2)はローカルストレージの管理に多用されており、クラスタ全体のボリュームグループのトランスペアレントな管理をサポートするために拡張されています。複数のホスト間で共有されるボリュームグループは、ローカルストレージと同じコマンドを使用して管理できます。
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>編集</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec.ha.clvm.overview">
  <title>概念の概要</title>



  <para>
   Cluster LVMは、さまざまなツールと連携します。
  </para>

  <variablelist>
   <varlistentry>
    <term>分散ロックマネージャ(DLM:Distributed Lock Manager)</term>
    <listitem>
     <para> クラスタ全体のロックにより、複数のホスト間の共有リソースへのアクセスを調整します。</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Logical Volume Manager (LVM2: Logical Volume Manager2)</term>
    <listitem>
     <para>
      LVM2はディスクスペースの仮想プールを提供し、1つの論理ボリュームを複数のディスクにわたって柔軟に分散できます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster Logical Volume Manager (Cluster LVM)</term>
    <listitem>
     <para>
      <literal>Cluster LVM</literal>という用語は、LVM2がクラスタ環境で使用されていることを示しています。このため、共有ストレージ上のLVM2メタデータを保護するには、ある程度の設定調整が必要になります。SUSE Linux Enterprise 15以降、クラスタ拡張では、よく知られているclvmdの代わりに、lvmlockdを使用します。lvmlockdの詳細については、<command>lvmlockd</command>コマンドのマニュアルページ(<command>man 8 lvmlockd</command>)を参照してください。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ボリュームグループと論理ボリューム</term>
    <listitem>
     <para>
      ボリュームグループ(VG)と論理ボリューム(LV)は、LVM2の基本的な概念です。ボリュームグループは、複数の物理ディスクのストレージプールです。論理ボリュームはボリュームグループに属し、ファイルシステムを作成できるエラスティックボリュームと見なすことができます。クラスタ環境には、共有VGという概念があります。共有VGは共有ストレージで構成され、複数のホストで同時に使用することができます。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.ha.clvm.config">
  <title>クラスタLVMの設定</title>

  <para>
   以下の要件が満たされていることを確認します。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     共有ストレージデバイス(Fibre Channel、FCoE、SCSI、iSCSI SAN、DRBD*などで提供されているデバイス)が使用できること
    </para>
   </listitem>
   <listitem>
    <para>
     パッケージ<systemitem class="resource">lvm2</systemitem>と<systemitem class="resource">lvm2-lockd</systemitem>がインストールされていることを確認します。
    </para>
   </listitem>
   <listitem>
    <para>
     SUSE Linux Enterprise 15以降では、LVM2クラスタ拡張としてclvmdではなくlvmlockdを使用しています。clvmdデーモンが実行されていないことを確認します。実行されていると、lvmlockdの起動に失敗します。
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec.ha.clvm.config.resources">
   <title>クラスタリソースの作成</title>
   <para>
    クラスタ内で共有VGを設定するには、1つのノードで次の基本手順を実行します。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="pro.ha.clvm.rsc.dlm" xrefstyle="select:title"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="pro.ha.clvm.rsc.lvmlockd" xrefstyle="select:title"/></para>
    </listitem>
    <listitem>
    <para>
      <xref linkend="pro.ha.clvm.rsc.vg_lv" xrefstyle="select:title"/></para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="pro.ha.clvm.rsc.lvm_activate" xrefstyle="select:title"/>
     </para>
    </listitem>
   </itemizedlist>

   <procedure xml:id="pro.ha.clvm.rsc.dlm">
    <title>DLMリソースを作成する</title>
    <step>
     <para>
      シェルを起動して、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
      クラスタリソースの現在の設定を確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure show</screen>
    </step>
    <step>
     <para>
      すでにDLMリソース(および対応するベースグループおよびベースクローン)を設定済みである場合、<xref linkend="pro.ha.clvm.rsc.lvmlockd"/>で継続します。
     </para>
     <para>
      そうでない場合は、<xref linkend="pro.dlm.resources"/>で説明されているように、DLMリソース、および対応するベースグループとベースクローンを設定します。
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro.ha.clvm.rsc.lvmlockd">
    <title>lvmlockdリソースの作成</title>
    <step>
     <para>
      シェルを起動して、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
      次のコマンドを実行して、このリソースの使用状況を確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure ra info lvmlockd</screen>
    </step>
    <step>
     <para>
      <systemitem>lvmlockd</systemitem>リソースを次のように設定します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure primitive lvmlockd ocf:heartbeat:lvmlockd \
  op start timeout="90" \
  op stop timeout="100" \
  op monitor interval="30" timeout="90"</screen>
    </step>
    <step>
     <para>
      <systemitem>lvmlockd</systemitem>リソースがすべてのノードで起動されるようにするには、<xref linkend="pro.ha.clvm.rsc.dlm"/>で作成したストレージ用の基本グループにプリミティブリソースを追加します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure modgroup g-storage add lvmlockd</screen>
    </step>
    <step>
     <para>
      変更内容をレビューします。</para>
     <screen><prompt role="root">root # </prompt>crm configure show</screen>
    </step>
    <step>
     <para>リソースが正常に実行されているかどうかを確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm status full</screen>
    </step>
   </procedure>
   <procedure xml:id="pro.ha.clvm.rsc.vg_lv">
    <title>共有VGおよびLVの作成</title>
    <step>
     <para>
      シェルを起動して、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
     すでに2つの共有ディスクがあると仮定し、それらに対して共有VGを作成します。
     </para>
     <screen><prompt role="root">root # </prompt>vgcreate --shared vg1 /dev/sda /dev/sdb</screen>
    </step>
    <step>
     <para>
      LVを作成します。最初は有効にしないでください。
     </para>
     <screen><prompt role="root">root # </prompt>lvcreate -an -L10G -n lv1 vg1</screen>
    </step>
   </procedure>
   <procedure xml:id="pro.ha.clvm.rsc.lvm_activate">
    <title>LVM-activateリソースの作成</title>
    <step>
     <para>
      シェルを起動して、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
      次のコマンドを実行して、このリソースの使用状況を確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure ra info LVM-activate</screen>
     <para>
      このリソースは、VGのアクティブ化を管理します。共有VGおけるLVアクティブ化には、排他モードと共有モードという2つの異なるモードがあります。デフォルト設定は排他モードです。このモードは通常、<systemitem>ext4</systemitem>などのローカルファイルシステムでLVを使用するときに使用します。共有モードは、OCFS2などのクラスタファイルシステムでのみ使用します。
     </para>
    </step>
    <step>
     <para>
      VGのアクティブ化を管理するようにリソースを設定します。使用しているシナリオに応じて、次のいずれかのオプションを選択します。
     </para>
     <itemizedlist>
      <listitem>
       <para>ローカルファイルシステムの使用に対して排他アクティブ化モードを使用します。</para>
<screen><prompt role="root">root # </prompt>crm configure primitive vg1 ocf:heartbeat:LVM-activate \
  params vgname=vg1 vg_access_mode=lvmlockd \
  op start timeout=90s interval=0 \
  op stop timeout=90s interval=0 \
  op monitor interval=30s timeout=90s</screen>
      </listitem>
      <listitem>
       <para>
        OCFS2に対して共有アクティブ化モードを使用し、クローニングされた<literal>g-storage</literal>グループに追加します。
       </para>
<screen><prompt role="root">root # </prompt>crm configure primitive vg1 ocf:heartbeat:LVM-activate \
  params vgname=vg1 vg_access_mode=lvmlockd activation_mode=shared \
  op start timeout=90s interval=0 \
  op stop timeout=90s interval=0 \
  op monitor interval=30s timeout=90s
<prompt role="root">root # </prompt>crm configure modgroup g-storage add vg1</screen>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      リソースが正常に実行されているかどうかを確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm status full</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.ha.clvm.scenario.iscsi">
   <title>シナリオ: SAN上でiSCSIを使用するCluster LVM</title>
   <para>
    次のシナリオでは、iSCSIターゲットをいくつかのクライアントにエクスポートする2つのSANボックスを使用します。一般的なアイデアが、<xref linkend="fig.ha.clvm.scenario.iscsi"/>で説明されています。
   </para>
   <figure xml:id="fig.ha.clvm.scenario.iscsi">
    <title>Cluster LVMを使用した共有ディスクのセットアップ</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ha_clvm.svg" width="80%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ha_clvm.png" width="45%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <warning>
    <title>データ損失</title>
    <para>
     以降の手順を実行すると、ディスク上のデータはすべて破壊されます。
    </para>
   </warning>
   <para>
    まず、1つのSANボックスだけ設定します。各SANボックスは、そのiSCSIターゲットをエクスポートする必要があります。次の手順に従います。
   </para>

   <procedure xml:id="pro.ha.clvm.scenario.iscsi.targets">
    <title>iSCSIターゲット(SAN上)を設定する</title>
    <step>
     <para>
      YaSTを実行し、<menuchoice> <guimenu>ネットワークサービス</guimenu> <guimenu>iSCSI LIO Target (iSCSI LIOターゲット)</guimenu> </menuchoice>の順にクリックしてiSCSIサーバモジュールを起動します。
     </para>
    </step>
    <step>
     <para>
      コンピュータがブートするたびにiSCSIターゲットを起動したい場合は、<guimenu>ブート時</guimenu>を選択し、そうでない場合は、<guimenu>手動</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      ファイアウォールが実行中の場合は、<guimenu>ファイアウォールでポートを開く</guimenu>を有効にします。
     </para>
    </step>
    <step>
     <para>
      <guimenu>グローバル</guimenu>タブに切り替えます。認証が必要な場合は、受信または送信(あるいはその両方の)認証を有効にします。この例では、<guimenu>認証なし</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      新しいiSCSIターゲットを追加します。
     </para>
     <substeps performance="required">
      <step>
       <para>
        <guimenu>ターゲット</guimenu>タブに切り替えます。
       </para>
      </step>
      <step>
       <para>
        <guimenu>追加</guimenu>をクリックします。
       </para>
      </step>
      <step xml:id="st.ha.clvm.iscsi.iqn">
       <para>
        ターゲットの名前を入力します。名前は、次のようにフォーマットされます。
       </para>
<screen>iqn.<replaceable>DATE</replaceable>.<replaceable>DOMAIN</replaceable></screen>
       <para>
        フォーマットに関する詳細は、セクション3.2.6.3.1のタイプ「iqn」<citetitle/>(iSCSI修飾名)(<link xlink:href="http://www.ietf.org/rfc/rfc3720.txt"/>)を参照してください。
       </para>
      </step>
      <step>
       <para>
        より説明的な名前にしたい場合は、さまざまなターゲットで一意であれば、識別子を変更できます。
       </para>
      </step>
      <step>
       <para>
        <guimenu>追加</guimenu>をクリックします。
       </para>
      </step>
      <step>
       <para>
        <guimenu>パス</guimenu>にデバイス名を入力し、<guimenu>Scsiid</guimenu>を使用します。
       </para>
      </step>
      <step>
       <para>
        <guimenu>次へ</guimenu>を2回クリックします。
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      警告ボックスで<guimenu>はい</guimenu>を選択して確認します。
     </para>
    </step>
    <step>
     <para>
      環境設定ファイル<filename>/etc/iscsi/iscsid.conf</filename>を開き、パラメータ<literal>node.startup</literal>を<literal>automatic</literal>に変更します。
     </para>
    </step>
   </procedure>
   <para>
    次の手順に従って、iSCSIイニシエータを設定します。
   </para>
   <procedure xml:id="pro.ha.clvm.scenarios.iscsi.initiator">
    <title>iSCSIイニシエータを設定する</title>
    <step>
     <para>
      YaSTを実行し、<menuchoice> <guimenu>ネットワークサービス</guimenu> <guimenu>iSCSIイニシエータ</guimenu> </menuchoice>の順にクリックします。
     </para>
    </step>
    <step>
     <para>
      コンピュータがブートするたびに、iSCSIイニシエータを起動したい場合は、<guimenu>ブート時</guimenu>を選択し、そうでない場合は、<guimenu>手動</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      <guimenu>検出</guimenu>タブに切り替え、<guimenu>検出</guimenu>ボタンをクリックします。
     </para>
    </step>
    <step>
     <para>
      IPアドレスとiSCSIターゲットのポートを追加します(<xref linkend="pro.ha.clvm.scenario.iscsi.targets"/>参照)。通常は、ポートを既定のままにし、デフォルト値を使用できます。
     </para>
    </step>
    <step>
     <para>
      認証を使用する場合は、受信および送信用のユーザ名およびパスワードを挿入します。そうでない場合は、<guimenu>認証なし</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      <guimenu>次へ</guimenu>を選択します。検出された接続が一覧されます。
     </para>
    </step>
    <step>
     <para>
      <guimenu>完了</guimenu>をクリックして続行します。
     </para>
    </step>
    <step>
     <para>
      シェルを開いて、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
      iSCSIイニシエータが正常に起動しているかどうかテストします。
     </para>
<screen><prompt role="root">root # </prompt><command>iscsiadm</command> -m discovery -t st -p 192.168.3.100
192.168.3.100:3260,1 iqn.2010-03.de.jupiter:san1</screen>
    </step>
    <step>
     <para>
      セッションを確立します。
     </para>
<screen><prompt role="root">root # </prompt><command>iscsiadm</command> -m node -l -p 192.168.3.100 -T iqn.2010-03.de.jupiter:san1
Logging in to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]
Login to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]: successful</screen>
     <para>
      <command>lsscsi</command>でデバイス名を表示します。
     </para>
<screen>...
[4:0:0:2]    disk    IET      ...     0     /dev/sdd
[5:0:0:1]    disk    IET      ...     0     /dev/sde</screen>
     <para>
      3番目の列に<literal>IET</literal>を含むエントリを捜します。この場合、該当するデバイスは、<filename>/dev/sdd</filename>と<filename>/dev/sde</filename>です。
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro.ha.clvm.scenarios.iscsi.lvm">
    <title>共有ボリュームグループを作成する</title>
    <step>
     <para>
      <xref linkend="pro.ha.clvm.scenarios.iscsi.initiator"/>のiSCSIイニシエータを実行したノードの1つで、<systemitem class="username">root</systemitem>シェルを開きます。
     </para>
    </step>
    <step>
     <para>
     ディスク<filename>/dev/sdd</filename>および<filename>/dev/sde</filename>に共有ボリュームグループを作成します。
     </para>
<screen><prompt role="root">root # </prompt>vgcreate --shared testvg /dev/sdd /dev/sde</screen>
    </step>
    <step>
     <para>
      必要に応じて、論理ボリュームを作成します。
     </para>
<screen><prompt role="root">root # </prompt><command>lvcreate</command> --name lv1 --size 500M testvg</screen>
    </step>
    <step>
     <para>
      ボリュームグループを<command>vgdisplay</command>で確認します。
     </para>
<screen>  --- Volume group ---
      VG Name               testvg
      System ID
      Format                lvm2
      Metadata Areas        2
      Metadata Sequence No  1
      VG Access             read/write
      VG Status             resizable
      Clustered             yes
      Shared                no
      MAX LV                0
      Cur LV                0
      Open LV               0
      Max PV                0
      Cur PV                2
      Act PV                2
      VG Size               1016,00 MB
      PE Size               4,00 MB
      Total PE              254
      Alloc PE / Size       0 / 0
      Free  PE / Size       254 / 1016,00 MB
      VG UUID               UCyWw8-2jqV-enuT-KH4d-NXQI-JhH3-J24anD</screen>
    </step>
   </procedure>
   <para>
    ボリュームを作成してリソースを起動したら、<filename>/dev/testvg</filename>の下に、新しいデバイス名(<filename>/dev/testvg/lv1</filename>など)を作成する必要があります。これは、LVがアクティブ化され使用できることを示します。
   </para>
  </sect2>

  <sect2 xml:id="sec.ha.clvm.scenario.drbd">
   <title>シナリオ: DRBDを使用したCluster LVM</title>
   <para>
    市、国、または大陸の各所にデータセンターが分散している場合は、次のシナリオを使用できます。
   </para>
   <procedure xml:id="pro.ha.clvm.withdrbd">
    <title>DRBDでクラスタ対応ボリュームグループを作成する</title>
    <step>
     <para>
      プライマリ/プライマリDRBDリソースを作成する
     </para>
     <substeps performance="required">
      <step>
       <para>
        まず、<xref linkend="pro.drbd.configure"/>の説明に従って、DRBDデバイスをプライマリ/セカンダリとしてセットアップします。ディスクの状態が両方のノードで<literal>up-to-date</literal>であることを確認します。<command>drbdadm status</command>を使用してこれをチェックします。
       </para>
      </step>
      <step>
       <para>
        次のオプションを環境設定ファイル(通常は、<filename>/etc/drbd.d/r0.res</filename>)に追加します。
       </para>
<screen>resource r0 {
  net {
     allow-two-primaries;
  }
  ...
}</screen>
      </step>
      <step>
       <para>
        変更した設定ファイルをもう一方のノードにコピーします。たとえば、次のように指定します。
       </para>
<screen><prompt role="root">root # </prompt><command>scp</command> /etc/drbd.d/r0.res venus:/etc/drbd.d/</screen>
      </step>
      <step>
       <para>
        <emphasis>両方</emphasis>のノードで、次のコマンドを実行します。
       </para>
<screen><prompt role="root">root # </prompt><command>drbdadm</command> disconnect r0
<prompt role="root">root # </prompt><command>drbdadm</command> connect r0
<prompt role="root">root # </prompt><command>drbdadm</command> primary r0</screen>
      </step>
      <step>
       <para>
        ノードのステータスをチェックします。
    </para>
    <screen><prompt role="root">root # </prompt><command>drbdadm</command> status r0</screen>

      </step>
     </substeps>
    </step>
    <step>
     <para>
      lvmlockdリソースをペースメーカーの環境設定でクローンとして保存し、DLMクローンリソースに依存させます。詳細については、<xref linkend="pro.ha.clvm.rsc.dlm"/>を参照してください。次に進む前に、クラスタでこれらのリソースが正しく機動していることを確認してください。<command>crm status</command>またはWebインタフェースを使用して、実行中のサービスを確認します。
     </para>
    </step>
    <step>
     <para>
      <command>pvcreate</command>コマンドで、LVM用に物理ボリュームを準備します。たとえば、<filename>/dev/drbd_r0</filename>デバイスでは、コマンドは次のようになります。
     </para>
<screen><prompt role="root">root # </prompt><command>pvcreate</command> /dev/drbd_r0</screen>
    </step>
    <step>
     <para>
      共有ボリュームグループを作成します。
     </para>
<screen><prompt role="root">root # </prompt><command>vgcreate</command> --shared testvg /dev/drbd_r0</screen>
    </step>
    <step>
     <para>
      必要に応じて、論理ボリュームを作成します。おそらく、論理ボリュームのサイズの変更が必要になります。たとえば、次のコマンドで、4GBの論理ボリュームを作成します。
     </para>
<screen><prompt role="root">root # </prompt><command>lvcreate</command> --name lv1 -L 4G testvg</screen>
    </step>
    <step>
     <para>
      <remark role="grammar">taroth 2011-10-24: comment by bwiedemann: as file system
      mounts or raw usage - *as* raw usage passt nicht - for?</remark>VG内の論理ボリュームは、ファイルシステムのマウントまたはraw用として使用できるようになりました。論理ボリュームを使用しているサービスにコロケーションのための正しい依存性があることを確認し、VGをアクティブ化したら論理ボリュームの順序付けを行います。
     </para>
    </step>
   </procedure>
   <para>
    このような設定手順を終了すると、LVM2の環境設定は他のスタンドアロンワークステーションと同様に行えます。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ha.clvm.drbd">
  <title>有効なLVM2デバイスの明示的な設定</title>

  <para>
   複数のデバイスが同じ物理ボリュームの署名を共有していると思われる場合(マルチパスデバイスやDRBDなどのように)、LVM2がPVを走査するデバイスを明示的に設定しておくことをお勧めします。
  </para>

  <para>
   たとえば、コマンド<command>vgcreate</command>で、ミラーリングされたブロックデバイスを使用する代わりに物理デバイスを使用すると、DRBDで混乱が生じます。これにより、DRBDがスプリットブレイン状態になる可能性があります。
  </para>

  <para>
   LVM2用の単一のデバイスを非アクティブ化するには、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     ファイル<filename>/etc/lvm/lvm.conf</filename>を編集し、<literal>filter</literal>から始まる行を検索します。
    </para>
   </step>
   <step>
    <para>
     そこに記載されているパターンは正規表現として処理されます。冒頭の<quote>a</quote>は走査にデバイスパターンを受け入れることを、冒頭の<quote>r</quote>はそのデバイスパターンのデバイスを拒否することを意味します。
    </para>
   </step>
   <step>
    <para>
     <filename>/dev/sdb1</filename>という名前のデバイスを削除するには、次の表現をフィルタルールに追加します。
    </para>
<screen>"r|^/dev/sdb1$|"</screen>
    <para>
     完全なフィルタ行は次のようになります。
    </para>
<screen>filter = [ "r|^/dev/sdb1$|", "r|/dev/.*/by-path/.*|", "r|/dev/.*/by-id/.*|", "a/.*/" ]</screen>
    <para>
     DRBDとMPIOデバイスは受け入れ、その他のすべてのデバイスは拒否するフィルタ行は次のようになります。
    </para>
<screen>filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|", "r/.*/" ]</screen>
   </step>

   <step>
    <para>
     環境設定ファイルを書き込み、すべてのクラスタノードにコピーします。
    </para>
   </step>
  </procedure>
 </sect1>

<sect1 xml:id="sec.ha.clvm.migrate">
  <title>Mirror LVからCluster MDへのオンラインマイグレーション</title>
  <para>
   <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> 15では、Cluster LVMでのcmirrordの使用は非推奨になりました。クラスタ内のミラー論理ボリュームはCluster MDに移行することを強く推奨します。Cluster MDはクラスタマルチデバイスの略称で、クラスタ用のソフトウェアベースのRAIDストレージソリューションです。
  </para>

 <sect2 xml:id="sec.ha.clvm.migrate.setup.before">
  <title>マイグレーション前のサンプルセットアップ</title>
    <para>
     次のサンプルセットアップが存在するとします。
    </para>
 <itemizedlist>
  <listitem>
   <para>
    <literal>alice</literal>および<literal>bob</literal>というノードからなる2ノードクラスタ
   </para>
  </listitem>
  <listitem>
    <para>
     <literal>cluster-vg2</literal>というボリュームグループから作成された、<literal>test-lv</literal>という名前のミラー論理ボリューム
    </para>
  </listitem>
  <listitem>
   <para>
     <filename>/dev/vdb</filename>および<filename>/dev/vdc</filename>というディスクからなるボリュームグループ<literal>cluster-vg2</literal>
   </para>
  </listitem>
 </itemizedlist>
 <screen><prompt role="root">root # </prompt><command>lsblk</command>
NAME                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda                                   253:0    0   40G  0 disk
├─vda1                                253:1    0    4G  0 part [SWAP]
└─vda2                                253:2    0   36G  0 part /
vdb                                   253:16   0   20G  0 disk
├─cluster--vg2-test--lv_mlog_mimage_0 254:0    0    4M  0 lvm
│ └─cluster--vg2-test--lv_mlog        254:2    0    4M  0 lvm
│   └─cluster--vg2-test--lv           254:5    0   12G  0 lvm
└─cluster--vg2-test--lv_mimage_0      254:3    0   12G  0 lvm
  └─cluster--vg2-test--lv             254:5    0   12G  0 lvm
vdc                                   253:32   0   20G  0 disk
├─cluster--vg2-test--lv_mlog_mimage_1 254:1    0    4M  0 lvm
│ └─cluster--vg2-test--lv_mlog        254:2    0    4M  0 lvm
│   └─cluster--vg2-test--lv           254:5    0   12G  0 lvm
└─cluster--vg2-test--lv_mimage_1      254:4    0   12G  0 lvm
  └─cluster--vg2-test--lv             254:5    0   12G  0 lvm</screen>

  <important xml:id="adm.migration.fail">
  <title>マイグレーションにおける障害の発生を避ける</title>
  <para>
   マイグレーション手順を実施する前に、論理ボリュームおよび物理ボリュームで使用可能な容量と、どの程度使用されているかを確認してください。論理ボリュームの容量がすべて使い果たされている場合、マイグレーションは失敗し、ターゲットボリュームに関する<literal>insufficient free space</literal>エラーが発生します。こうしたマイグレーション障害に対応するための回避策は、ミラーログのオプションに応じて異なります。
  </para>
  <itemizedlist>
    <listitem>
     <formalpara>
      <title>ミラーログ自体が(<option>mirrored</option>オプションを使用して)ミラーリングされ、ミラーログの格納先と同じデバイスに割り当てられている場合</title>
      <para> (たとえば、<link xlink:href="https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#sec.ha.clvm.config.cmirrord"/>の説明に従って、<phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> 11または12で、cmirrordを設定した論理ボリュームを作成したケース):  </para>
     </formalpara>
     <para>
      デフォルトでは、<command>mdadm</command>が実行されることにより、デバイスとアレイデータのマイグレーションがそれぞれ開始される前に、ある程度の空き領域が確保されます。マイグレーション中には、この未使用のパディング領域をチェックして、その容量を削除することも可能です。その際には、<xref linkend="step.data-offset"/>と次の段落で説明する<option>data-offset</option>オプションを利用できます。
     </para>
     <para>
      <option>data-offset</option>の実行時には、Cluster MDがそのメタデータを書き込めるよう、デバイス上に十分な余裕を持たせる必要があります。一方で、オフセット自体の容量は抑える必要があります。デバイスの残りの領域で、移行する物理ボリュームのエクステントをすべて格納しなければならないからです。使用可能なボリューム領域は、ボリューム全体からミラーログを差し引いた残りの領域なので、オフセットのサイズはミラーログよりも小さくする必要があります。
     </para>
     <para>
      <option>data-offset</option>を128KBに設定することを推奨します。オフセット値を指定しない場合、デフォルト値の1KB (1024バイト)が使用されます。
     </para>
    </listitem>
   <listitem>
    <formalpara>
     <title>
      ミラーログが(<option>disk</option>オプションを使用して)別のデバイスに記録されている場合、あるいは(<option>core</option>オプションを使用して)メモリ上に格納されている場合: 
     </title>
     <para>
      マイグレーションを始める前に、物理ボリュームの容量を増やすか、(物理ボリュームの空き領域を確保するために)論理ボリュームのサイズを減らします。
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>
 </important>
</sect2>
<sect2 xml:id="sec.ha.clvm.migrate.lv2clustermd">
 <title>Mirror LVをCluster MDにマイグレートする</title>
  <para>
    以下の手順は、<xref linkend="sec.ha.clvm.migrate.setup.before"/>に基づいています。セットアップ環境に応じて手順を変更し、LV、VG、ディスク、Cluster MDデバイスの名前はご使用のリソースに置き換えてください。
  </para>
  <para>
  このマイグレーションではダウンタイムは発生しません。そのため、マイグレーション中でもファイルシステムをマウントすることができます。
 </para>
 <procedure>
   <step>
    <para>
     <literal>alice</literal>ノードで、次の手順を実行します。
    </para>
   <substeps>
   <step>
    <para>
     ミラー論理ボリューム<literal>test-lv</literal>をリニア論理ボリュームに変換します。
    </para>
    <screen><prompt role="root">root # </prompt>lvconvert -m0 cluster-vg2/test-lv /dev/vdc</screen>
   </step>
   <step>
    <para>
      物理ボリューム<filename>/dev/vdc</filename>をボリュームグループ<literal>cluster-vg2</literal>から削除します。
    </para>
    <screen><prompt role="root">root # </prompt>vgreduce cluster-vg2 /dev/vdc</screen>
   </step>
   <step>
    <para>
      この物理ボリュームをLVMから削除します。
    </para>
    <screen><prompt role="root">root # </prompt>pvremove /dev/vdc</screen>
    <para><command>lsblk</command>を実行すると、次のように出力されます。</para>
   <screen>NAME                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda                     253:0    0   40G  0 disk
├─vda1                  253:1    0    4G  0 part [SWAP]
└─vda2                  253:2    0   36G  0 part /
vdb                     253:16   0   20G  0 disk
└─cluster--vg2-test--lv 254:5    0   12G  0 lvm
vdc                     253:32   0   20G  0 disk</screen>
   </step>
   <step xml:id="step.data-offset">
    <para>
      ディスク<filename>/dev/vdc</filename>を持つCluster MDデバイス<filename>/dev/md0</filename>を作成します。
    </para>
    <screen><prompt role="root">root # </prompt>mdadm --create /dev/md0 --bitmap=clustered \
--metadata=1.2 --raid-devices=1 --force --level=mirror \
/dev/vdc --data-offset=128</screen>
    
    <para>
     <option>data-offset</option>オプションを使用する理由については、<xref linkend="adm.migration.fail"/>を参照してください。
    </para>
   </step>
  </substeps>
  </step>
   <step>
    <para>
     <literal>bob</literal>ノードで、次のMDデバイスをアセンブルします。
    </para>
    <screen><prompt role="root">root # </prompt>mdadm --assemble md0 /dev/vdc</screen>
    <para>
     2ノードより多いクラスタでは、クラスタ内の残りのノードについても、この手順を実行してください。
    </para>
   </step>
   <step>
    <para><literal>alice</literal>ノードに戻って、次の手順を実行します。
   </para>
   <substeps>
   <step>
    <para>
     LVM用の物理ボリュームとして、MDデバイス<filename>/dev/md0</filename>を初期化します。
    </para>
    <screen><prompt role="root">root # </prompt>pvcreate /dev/md0</screen>
   </step>
   <step>
    <para>
     MDデバイス<filename>/dev/md0</filename>をボリュームグループ<literal>cluster-vg2</literal>に追加します。
    </para>
    <screen><prompt role="root">root # </prompt>vgextend cluster-vg2 /dev/md0</screen>
   </step>
   <step>
    <para>
     ディスク<filename>/dev/vdb</filename>のデータをデバイス<filename>/dev/md0</filename>に移動します。
    </para>
    <screen><prompt role="root">root # </prompt>pvmove /dev/vdb /dev/md0</screen>
   </step>
   <step>
    <para>
     物理ボリューム<filename>/dev/vdb</filename>をボリューム<literal>group cluster-vg2</literal>から削除します。
    </para>
    <screen><prompt role="root">root # </prompt>vgreduce cluster-vg2 /dev/vdb</screen>
   </step>
   <step>
    <para>
     LVMがデバイスを物理ボリュームとして認識しなくなるように、このデバイスのラベルを削除します。
    </para>
    <screen><prompt role="root">root # </prompt>pvremove /dev/vdb</screen>
   </step>
   <step>
    <para>
     <filename>/dev/vdb</filename>をMDデバイス<filename>/dev/md0</filename>に追加します。
    </para>
    <screen><prompt role="root">root # </prompt>mdadm --grow /dev/md0 --raid-devices=2 --add /dev/vdb</screen>
   </step>
  </substeps>
  </step>
 </procedure>
</sect2>

<sect2 xml:id="ex.ha.clvm.migrate.setup.after">
 <title>マイグレーション後のサンプルセットアップ</title>
  <para>
   <command>lsblk</command>を実行すると、次のように出力されます。
  </para>
  <screen>NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
vda                       253:0    0   40G  0 disk
├─vda1                    253:1    0    4G  0 part  [SWAP]
└─vda2                    253:2    0   36G  0 part  /
vdb                       253:16   0   20G  0 disk
└─md0                       9:0    0   20G  0 raid1
  └─cluster--vg2-test--lv 254:5    0   12G  0 lvm
vdc                       253:32   0   20G  0 disk
└─md0                       9:0    0   20G  0 raid1
  └─cluster--vg2-test--lv 254:5    0   12G  0 lvm</screen>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.ha.clvm.more">
  <title>その他の情報</title>

  <para>
   lvmlockdの詳細については、<command>lvmlockd</command>コマンドのマニュアルページ(<command>man 8 lvmlockd</command>)を参照してください。
  </para>
  <para>
   詳細な情報は、<link xlink:href="http://www.clusterlabs.org/wiki/Help:Contents"/>にあるPacemakerメーリングリストから取得できます。
  </para>  
 </sect1>
</chapter>
