<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="ha_cluster_lvm.xml" version="5.0" xml:id="cha-ha-clvm">
 <title>クラスタLogical Volume Manager (クラスタLVM)</title>
 <info>
      <abstract>
        <para>
    クラスタ上の共有ストレージを管理する場合、ストレージサブシステムに行った変更を各ノードに伝える必要があります。Linux Volume Manager 2 (LVM2)はローカルストレージの管理に多用されており、クラスタ全体のボリュームグループのトランスペアレントな管理をサポートするために拡張されています。複数のホスト間で共有されるボリュームグループは、ローカルストレージと同じコマンドを使用して管理できます。
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer/>
        <dm:status>編集</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release/>
        <dm:repository/>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-clvm-overview">
  <title>概念の概要</title>



  <para>
   クラスタLVMは、さまざまなツールと連携します。
  </para>

  <variablelist>
   <varlistentry>
    <term>分散ロックマネージャ(DLM:Distributed Lock Manager)</term>
    <listitem>
     <para> クラスタ全体のロックにより、複数のホスト間の共有リソースへのアクセスを調整します。</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>論理ボリュームマネージャ(LVM2: Logical Volume Manager2)</term>
    <listitem>
     <para>
      LVM2はディスクスペースの仮想プールを提供し、1つの論理ボリュームを複数のディスクにわたって柔軟に分散できます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>クラスタ論理ボリュームマネージャ(クラスタLVM2)</term>
    <listitem>
     <para>
      <literal>クラスタLVM2</literal>という用語は、LVM2がクラスタ環境で使用されていることを示しています。このため、共有ストレージ上のLVM2メタデータを保護するには、ある程度の設定調整が必要になります。SUSE Linux Enterprise 15以降、クラスタ拡張では、よく知られているclvmdの代わりに、lvmlockdを使用します。lvmlockdの詳細については、<command>lvmlockd</command>コマンドのマニュアルページ(<command>man 8 lvmlockd</command>)を参照してください。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ボリュームグループと論理ボリューム</term>
    <listitem>
     <para>
      ボリュームグループ(VG)と論理ボリューム(LV)は、LVM2の基本的な概念です。ボリュームグループは、複数の物理ディスクのストレージプールです。論理ボリュームはボリュームグループに属し、ファイルシステムを作成できるエラスティックボリュームと見なすことができます。クラスタ環境には、共有VGという概念があります。共有VGは共有ストレージで構成され、複数のホストで同時に使用することができます。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ha-clvm-config">
  <title>クラスタLVMの設定</title>

  <para>
   以下の要件が満たされていることを確認します。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     共有ストレージデバイス(Fibre Channel、FCoE、SCSI、iSCSI SAN、DRBD*などで提供されているデバイス)が使用できること
    </para>
   </listitem>
   <listitem>
    <para>
     パッケージ<systemitem class="resource">lvm2</systemitem>と<systemitem class="resource">lvm2-lockd</systemitem>がインストールされていることを確認します。
    </para>
   </listitem>
   <listitem>
    <para>
     SUSE Linux Enterprise 15以降では、LVM2クラスタ拡張としてclvmdではなくlvmlockdを使用しています。clvmdデーモンが実行されていないことを確認します。実行されていると、lvmlockdの起動に失敗します。
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-ha-clvm-config-resources">
   <title>クラスタリソースの作成</title>
   <para>
    クラスタ内で共有VGを設定するには、1つのノードで次の基本手順を実行します。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="pro-ha-clvm-rsc-dlm" xrefstyle="select:title"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="pro-ha-clvm-rsc-lvmlockd" xrefstyle="select:title"/></para>
    </listitem>
    <listitem>
    <para>
      <xref linkend="pro-ha-clvm-rsc-vg-lv" xrefstyle="select:title"/></para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="pro-ha-clvm-rsc-lvm-activate" xrefstyle="select:title"/>
     </para>
    </listitem>
   </itemizedlist>

   <procedure xml:id="pro-ha-clvm-rsc-dlm">
    <title>DLMリソースを作成する</title>

    <note>
     <title>LVM2およびOCFS2両方のためのDLMリソース</title>
     <para>
      LVM2およびOCFS2は両方とも、クラスタ内のすべてのノード上で実行するDLMリソースを必要とするため、通常はクローンとして設定されます。OCFS2およびLVM2の両方を含むセットアップがある場合、OCFS2およびLVM2の両方に<emphasis>1つの</emphasis>DLMリソースを設定することで十分です。
     </para>
    </note>
    <step>
     <para>
      シェルを起動して、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
      クラスタリソースの現在の設定を確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure show</screen>
    </step>
    <step>
     <para>
      すでにDLMリソース(および対応するベースグループおよびベースクローン)を設定済みである場合、<xref linkend="pro-ha-clvm-rsc-lvmlockd"/>で継続します。
     </para>
     <para>
      そうでない場合は、<xref linkend="pro-ocfs2-resources"/>で説明されているように、DLMリソース、および対応するベースグループとベースクローンを設定します。
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-clvm-rsc-lvmlockd">
    <title>lvmlockdリソースの作成</title>
    <step>
     <para>
      シェルを起動して、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
      次のコマンドを実行して、このリソースの使用状況を確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure ra info lvmlockd</screen>
    </step>
    <step>
     <para>
      <systemitem>lvmlockd</systemitem>リソースを次のように設定します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure primitive lvmlockd ocf:heartbeat:lvmlockd \
  op start timeout="90" \
  op stop timeout="100" \
  op monitor interval="30" timeout="90"</screen>
    </step>
    <step>
     <para>
      <systemitem>lvmlockd</systemitem>リソースがすべてのノードで開始されるようにするには、<xref linkend="pro-ha-clvm-rsc-dlm"/>で作成した基本グループにプリミティブリソースを追加します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure modgroup base-group add lvmlockd</screen>
    </step>
    <step>
     <para>
      変更内容をレビューします。</para>
     <screen><prompt role="root">root # </prompt>crm configure show</screen>
    </step>
    <step>
     <para>リソースが正常に実行されているかどうかを確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm status full</screen>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-clvm-rsc-vg-lv">
    <title>共有VGおよびLVの作成</title>
    <step>
     <para>
      シェルを起動して、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
     すでに2つの共有ディスクがあると仮定し、それらに対して共有VGを作成します。
     </para>
     <screen><prompt role="root">root # </prompt>vgcreate --shared vg1 /dev/sda /dev/sdb</screen>
    </step>
    <step>
     <para>
      LVを作成します。最初は有効にしないでください。
     </para>
     <screen><prompt role="root">root # </prompt>lvcreate -an -L10G -n lv1 vg1</screen>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-clvm-rsc-lvm-activate">
    <title>LVM-activateリソースの作成</title>
    <step>
     <para>
      シェルを起動して、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
      次のコマンドを実行して、このリソースの使用状況を確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm configure ra info LVM-activate</screen>
     <para>
      このリソースは、VGのアクティブ化を管理します。共有VGおけるLVアクティブ化には、排他モードと共有モードという2つの異なるモードがあります。デフォルト設定は排他モードです。このモードは通常、<systemitem>ext4</systemitem>などのローカルファイルシステムでLVを使用するときに使用します。共有モードは、OCFS2などのクラスタファイルシステムでのみ使用します。
     </para>
    </step>
    <step>
     <para>
      VGのアクティブ化を管理するようにリソースを設定します。使用しているシナリオに応じて、次のいずれかのオプションを選択します。
     </para>
     <itemizedlist>
      <listitem>
       <para>ローカルファイルシステムの使用に対して排他アクティブ化モードを使用します。</para>
<screen><prompt role="root">root # </prompt>crm configure primitive vg1 ocf:heartbeat:LVM-activate \
  params vgname=vg1 vg_access_mode=lvmlockd \
  op start timeout=90s interval=0 \
  op stop timeout=90s interval=0 \
  op monitor interval=30s timeout=90s</screen>
      </listitem>
      <listitem>
       <para>
        OCFS2に対して共有アクティブ化モードを使用し、クローンベースグループに追加します。
       </para>
<screen><prompt role="root">root # </prompt>crm configure primitive vg1 ocf:heartbeat:LVM-activate \
  params vgname=vg1 vg_access_mode=lvmlockd activation_mode=shared \
  op start timeout=90s interval=0 \
  op stop timeout=90s interval=0 \
  op monitor interval=30s timeout=90s
<prompt role="root">root # </prompt>crm configure modgroup base-group add vg1</screen>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      リソースが正常に実行されているかどうかを確認します。
     </para>
     <screen><prompt role="root">root # </prompt>crm status full</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-clvm-scenario-iscsi">
   <title>シナリオ - SAN上でiSCSIを使用するLVM2</title>
   <para>
    次のシナリオでは、iSCSIターゲットをいくつかのクライアントにエクスポートする2つのSANボックスを使用します。一般的なアイデアが、<xref linkend="fig-ha-clvm-scenario-iscsi"/>で説明されています。
   </para>
   <figure xml:id="fig-ha-clvm-scenario-iscsi">
    <title>cLVMを使用した共有ディスクのセットアップ</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ha_clvm.svg" width="80%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ha_clvm.png" width="45%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <warning>
    <title>データ損失</title>
    <para>
     以降の手順を実行すると、ディスク上のデータはすべて破壊されます。
    </para>
   </warning>
   <para>
    まず、1つのSANボックスだけ設定します。各SANボックスは、そのiSCSIターゲットをエクスポートする必要があります。次の手順に従います。
   </para>

   <procedure xml:id="pro-ha-clvm-scenario-iscsi-targets">
    <title>iSCSIターゲット(SAN上)を設定する</title>
    <step>
     <para>
      YaSTを実行し、<menuchoice> <guimenu>ネットワークサービス</guimenu> <guimenu>iSCSI LIO Target (iSCSI LIOターゲット)</guimenu> </menuchoice>の順にクリックしてiSCSIサーバモジュールを起動します。
     </para>
    </step>
    <step>
     <para>
      コンピュータがブートするたびにiSCSIターゲットを起動したい場合は、<guimenu>ブート時</guimenu>を選択し、そうでない場合は、<guimenu>手動</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      ファイアウォールが実行中の場合は、<guimenu>ファイアウォールでポートを開く</guimenu>を有効にします。
     </para>
    </step>
    <step>
     <para>
      <guimenu>グローバル</guimenu>タブに切り替えます。認証が必要な場合は、受信または送信(あるいはその両方の)認証を有効にします。この例では、<guimenu>認証なし</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      新しいiSCSIターゲットを追加します。
     </para>
     <substeps performance="required">
      <step>
       <para>
        <guimenu>ターゲット</guimenu>タブに切り替えます。
       </para>
      </step>
      <step>
       <para>
        <guimenu>追加</guimenu>をクリックします。
       </para>
      </step>
      <step xml:id="st-ha-clvm-iscsi-iqn">
       <para>
        ターゲットの名前を入力します。名前は、次のようにフォーマットされます。
       </para>
<screen>iqn.<replaceable>DATE</replaceable>.<replaceable>DOMAIN</replaceable></screen>
       <para>
        フォーマットに関する詳細は、セクション3.2.6.3.1のタイプ「iqn」<citetitle/>(iSCSI修飾名)(<link xlink:href="http://www.ietf.org/rfc/rfc3720.txt"/>)を参照してください。
       </para>
      </step>
      <step>
       <para>
        より説明的な名前にしたい場合は、さまざまなターゲットで一意であれば、識別子を変更できます。
       </para>
      </step>
      <step>
       <para>
        <guimenu>追加</guimenu>をクリックします。
       </para>
      </step>
      <step>
       <para>
        <guimenu>パス</guimenu>にデバイス名を入力し、<guimenu>Scsiid</guimenu>を使用します。
       </para>
      </step>
      <step>
       <para>
        <guimenu>次へ</guimenu>を2回クリックします。
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      警告ボックスで<guimenu>はい</guimenu>を選択して確認します。
     </para>
    </step>
    <step>
     <para>
      環境設定ファイル<filename>/etc/iscsi/iscsid.conf</filename>を開き、パラメータ<literal>node.startup</literal>を<literal>automatic</literal>に変更します。
     </para>
    </step>
   </procedure>
   <para>
    次の手順に従って、iSCSIイニシエータを設定します。
   </para>
   <procedure xml:id="pro-ha-clvm-scenarios-iscsi-initiator">
    <title>iSCSIイニシエータを設定する</title>
    <step>
     <para>
      YaSTを実行し、<menuchoice> <guimenu>ネットワークサービス</guimenu> <guimenu>iSCSIイニシエータ</guimenu> </menuchoice>の順にクリックします。
     </para>
    </step>
    <step>
     <para>
      コンピュータがブートするたびに、iSCSIイニシエータを起動したい場合は、<guimenu>ブート時</guimenu>を選択し、そうでない場合は、<guimenu>手動</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      <guimenu>検出</guimenu>タブに切り替え、<guimenu>検出</guimenu>ボタンをクリックします。
     </para>
    </step>
    <step>
     <para>
      IPアドレスとiSCSIターゲットのポートを追加します(<xref linkend="pro-ha-clvm-scenario-iscsi-targets"/>参照)。通常は、ポートを既定のままにし、デフォルト値を使用できます。
     </para>
    </step>
    <step>
     <para>
      認証を使用する場合は、受信および送信用のユーザ名およびパスワードを挿入します。そうでない場合は、<guimenu>認証なし</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      <guimenu>次へ</guimenu>を選択します。検出された接続が一覧されます。
     </para>
    </step>
    <step>
     <para>
      <guimenu>完了</guimenu>をクリックして続行します。
     </para>
    </step>
    <step>
     <para>
      シェルを開いて、<systemitem class="username">root</systemitem>としてログインします。
     </para>
    </step>
    <step>
     <para>
      iSCSIイニシエータが正常に起動しているかどうかテストします。
     </para>
<screen><prompt role="root">root # </prompt><command>iscsiadm</command> -m discovery -t st -p 192.168.3.100
192.168.3.100:3260,1 iqn.2010-03.de.jupiter:san1</screen>
    </step>
    <step>
     <para>
      セッションを確立します。
     </para>
<screen><prompt role="root">root # </prompt><command>iscsiadm</command> -m node -l -p 192.168.3.100 -T iqn.2010-03.de.jupiter:san1
Logging in to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]
Login to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]: successful</screen>
     <para>
      <command>lsscsi</command>でデバイス名を表示します。
     </para>
<screen>...
[4:0:0:2]    disk    IET      ...     0     /dev/sdd
[5:0:0:1]    disk    IET      ...     0     /dev/sde</screen>
     <para>
      3番目の列に<literal>IET</literal>を含むエントリを捜します。この場合、該当するデバイスは、<filename>/dev/sdd</filename>と<filename>/dev/sde</filename>です。
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-clvm-scenarios-iscsi-lvm">
    <title>共有ボリュームグループを作成する</title>
    <step>
     <para>
      <xref linkend="pro-ha-clvm-scenarios-iscsi-initiator"/>のiSCSIイニシエータを実行したノードの1つで、<systemitem class="username">root</systemitem>シェルを開きます。
     </para>
    </step>
    <step>
     <para>
     ディスク<filename>/dev/sdd</filename>および<filename>/dev/sde</filename>に共有ボリュームグループを作成します。
     </para>
<screen><prompt role="root">root # </prompt>vgcreate --shared testvg /dev/sdd /dev/sde</screen>
    </step>
    <step>
     <para>
      必要に応じて、論理ボリュームを作成します。
     </para>
<screen><prompt role="root">root # </prompt><command>lvcreate</command> --name lv1 --size 500M testvg</screen>
    </step>
    <step>
     <para>
      ボリュームグループを<command>vgdisplay</command>で確認します。
     </para>
<screen>  --- Volume group ---
      VG Name               testvg
      System ID
      Format                lvm2
      Metadata Areas        2
      Metadata Sequence No  1
      VG Access             read/write
      VG Status             resizable
      Clustered             yes
      Shared                no
      MAX LV                0
      Cur LV                0
      Open LV               0
      Max PV                0
      Cur PV                2
      Act PV                2
      VG Size               1016,00 MB
      PE Size               4,00 MB
      Total PE              254
      Alloc PE / Size       0 / 0
      Free  PE / Size       254 / 1016,00 MB
      VG UUID               UCyWw8-2jqV-enuT-KH4d-NXQI-JhH3-J24anD</screen>
    </step>
   </procedure>
   <para>
    ボリュームを作成してリソースを起動したら、<filename>/dev/testvg</filename>の下に、新しいデバイス名(<filename>/dev/testvg/lv1</filename>など)を作成する必要があります。これは、LVがアクティブ化され使用できることを示します。
   </para>
  </sect2>


  <sect2 xml:id="sec-ha-clvm-scenario-drbd">
   <title>シナリオ: DRBDを使用したLVM2</title>
   <para>
    市、国、または大陸の各所にデータセンターが分散している場合は、次のシナリオを使用できます。
   </para>
   <procedure xml:id="pro-ha-clvm-withdrbd">
    <title>DRBDでクラスタ対応ボリュームグループを作成する</title>
    <step>
     <para>
      プライマリ/プライマリDRBDリソースを作成する
     </para>
     <substeps performance="required">
      <step>
       <para>
        まず、<xref linkend="pro-drbd-configure"/>の説明に従って、DRBDデバイスをプライマリ/セカンダリとしてセットアップします。ディスクの状態が両方のノードで<literal>up-to-date</literal>であることを確認します。<command>drbdadm status</command>を使用してこれをチェックします。
       </para>
      </step>
      <step>
       <para>
        次のオプションを環境設定ファイル(通常は、<filename>/etc/drbd.d/r0.res</filename>)に追加します。
       </para>
<screen>resource r0 {
  startup {
    become-primary-on both;
  }

  net {
     allow-two-primaries;
  }
  ...
}</screen>
      </step>
      <step>
       <para>
        変更した設定ファイルをもう一方のノードにコピーします。たとえば、次のように指定します。
       </para>
<screen><prompt role="root">root # </prompt><command>scp</command> /etc/drbd.d/r0.res venus:/etc/drbd.d/</screen>
      </step>
      <step>
       <para>
        <emphasis>両方</emphasis>のノードで、次のコマンドを実行します。
       </para>
<screen><prompt role="root">root # </prompt><command>drbdadm</command> disconnect r0
<prompt role="root">root # </prompt><command>drbdadm</command> connect r0
<prompt role="root">root # </prompt><command>drbdadm</command> primary r0</screen>
      </step>
      <step>
       <para>
        ノードのステータスをチェックします。
    </para>
    <screen><prompt role="root">root # </prompt><command>drbdadm</command> status r0</screen>

      </step>
     </substeps>
    </step>
    <step>
     <para>
      lvmlockdリソースをペースメーカーの環境設定でクローンとして保存し、DLMクローンリソースに依存させます。詳細については、<xref linkend="pro-ha-clvm-rsc-dlm"/>を参照してください。次に進む前に、クラスタでこれらのリソースが正しく機動していることを確認してください。<command>crm status</command>またはWebインタフェースを使用して、実行中のサービスを確認します。
     </para>
    </step>
    <step>
     <para>
      <command>pvcreate</command>コマンドで、LVM用に物理ボリュームを準備します。たとえば、<filename>/dev/drbd_r0</filename>デバイスでは、コマンドは次のようになります。
     </para>
<screen><prompt role="root">root # </prompt><command>pvcreate</command> /dev/drbd_r0</screen>
    </step>
    <step>
     <para>
      共有ボリュームグループを作成します。
     </para>
<screen><prompt role="root">root # </prompt><command>vgcreate</command> --shared testvg /dev/drbd_r0</screen>
    </step>
    <step>
     <para>
      必要に応じて、論理ボリュームを作成します。おそらく、論理ボリュームのサイズの変更が必要になります。たとえば、次のコマンドで、4GBの論理ボリュームを作成します。
     </para>
<screen><prompt role="root">root # </prompt><command>lvcreate</command> --name lv1 -L 4G testvg</screen>
    </step>
    <step>
     <para>
      <remark role="grammar">taroth 2011-10-24: comment by bwiedemann: as file system
      mounts or raw usage - *as* raw usage passt nicht - for?</remark>VG内の論理ボリュームは、ファイルシステムのマウントまたはraw用として使用できるようになりました。論理ボリュームを使用しているサービスにコロケーションのための正しい依存性があることを確認し、VGをアクティブ化したら論理ボリュームの順序付けを行います。
     </para>
    </step>
   </procedure>
   <para>
    このような設定手順を終了すると、LVM2の環境設定は他のスタンドアロンワークステーションと同様に行えます。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ha-clvm-drbd">
  <title>有効なLVM2デバイスの明示的な設定</title>

  <para>
   複数のデバイスが同じ物理ボリュームの署名を共有していると思われる場合(マルチパスデバイスやDRBDなどのように)、LVM2がPVを走査するデバイスを明示的に設定しておくことをお勧めします。
  </para>

  <para>
   たとえば、コマンド<command>vgcreate</command>で、ミラーリングされたブロックデバイスを使用する代わりに物理デバイスを使用すると、DRBDで混乱が生じます。これにより、DRBDがスプリットブレイン状態になる可能性があります。
  </para>

  <para>
   LVM2用の単一のデバイスを非アクティブ化するには、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     ファイル<filename>/etc/lvm/lvm.conf</filename>を編集し、<literal>filter</literal>から始まる行を検索します。
    </para>
   </step>
   <step>
    <para>
     そこに記載されているパターンは正規表現として処理されます。冒頭の<quote>a</quote>は走査にデバイスパターンを受け入れることを、冒頭の<quote>r</quote>はそのデバイスパターンのデバイスを拒否することを意味します。
    </para>
   </step>
   <step>
    <para>
     <filename>/dev/sdb1</filename>という名前のデバイスを削除するには、次の表現をフィルタルールに追加します。
    </para>
<screen>"r|^/dev/sdb1$|"</screen>
    <para>
     完全なフィルタ行は次のようになります。
    </para>
<screen>filter = [ "r|^/dev/sdb1$|", "r|/dev/.*/by-path/.*|", "r|/dev/.*/by-id/.*|", "a/.*/" ]</screen>
    <para>
     DRBDとMPIOデバイスは受け入れ、その他のすべてのデバイスは拒否するフィルタ行は次のようになります。
    </para>
<screen>filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|", "r/.*/" ]</screen>
   </step>

   <step>
    <para>
     環境設定ファイルを書き込み、すべてのクラスタノードにコピーします。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-clvm-more">
  <title>詳細</title>

  <para>
   lvmlockdの詳細については、<command>lvmlockd</command>コマンドのマニュアルページ(<command>man 8 lvmlockd</command>)を参照してください。
  </para>
  <para>
   詳細な情報は、<link xlink:href="http://www.clusterlabs.org/wiki/Help:Contents"/>にあるPacemakerメーリングリストから取得できます。
  </para>  
 </sect1>
</chapter>
